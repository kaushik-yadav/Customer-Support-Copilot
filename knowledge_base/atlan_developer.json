[
  {
    "url": "https://developer.atlan.com/conventions/",
    "content": "Documentation conventions Â¶ We use icons to quickly convey contextual details about features illustrated in code. Following is the complete list of such icons and their meaning: x.y.z Atlan CLI version Â¶ The terminal symbol in conjunction with a version number denotes when a specific feature or behavior was added to the Atlan CLI. Make sure you're at least on this version if you want to use it the way it is illustrated in any code blocks. x.y.z Java SDK version Â¶ The Java symbol in conjunction with a version number denotes when a specific feature or behavior was added to the Java SDK. Make sure you're at least on this version if you want to use it the way it is illustrated in any code blocks. Also applies to Kotlin Since the Java SDK is also used for Kotlin, the same minimum version requirement applies to any Kotlin functionality and code blocks as well. x.y.z Python SDK version Â¶ The Python symbol in conjunction with a version number denotes when a specific feature or behavior was added to the Python SDK. Make sure you're at least on this version if you want to use it the way it is illustrated in any code blocks. x.y.z Go SDK version Â¶ The Go symbol in conjunction with a version number denotes when a specific feature or behavior was added to the Go SDK. Make sure you're at least on this version if you want to use it the way it is illustrated in any code blocks. Experimental features Â¶ Some newer features are still considered experimental. These will be clearly marked as such here in developer.atlan.com , as well as in the release notes for the SDK version where they are introduced. Subject to change Please note that experimental features are subject to change prior to their final form. Such changes will not cause a change to the major version of the SDK. Conversely, for any feature not marked experimental, we aim ensure no breaking changes are made to it without incrementing the major version of the SDK. 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/dbt/",
    "content": "dbt Â¶ Atlan University See it in action in our automated enrichment course (45 mins). You can use dbt's meta field to enrich metadata resources from dbt into Atlan. Atlan will ingest the information from this field and update the assets in Atlan accordingly. With this, you have a powerful way to keep the dbt assets documented directly as part of your dbt work. The following is an example: dbt example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 version : 2 models : - name : customers description : >- # (1) This table has basic information about a customer, as well as some derived facts based on a customer's orders. meta : # (2) atlan : # (3) attributes : # (4) certificateStatus : DRAFT ownerUsers : [ \"bryan\" , \"ashwin\" ] classifications : # (5) - typeName : \"ipubxAPPb0zRcNU1Gkjs9b\" propagate : true removePropagationsOnEntityDelete : true restrictPropagationThroughLineage : true restrictPropagationThroughHierarchy : false columns : - name : customer_id description : This is a unique identifier for a customer tests : - unique - not_null - name : total_order_amount description : Total value (AUD) of a customer's orders. - name : customer_lifetime_value meta : # (6) atlan : attributes : description : Customer lifetime value. certificateStatus : DRAFT ownerUsers : [ \"ravi\" ] classifications : - typeName : \"ipubxAPPb0zRcNU1Gkjs9b\" propagate : true removePropagationsOnEntityDelete : true The description at the top level of an asset defined in dbt will already be mapped to the description field for that asset in Atlan. More detailed metadata, however, needs to be specified within the meta field. ... and within the meta field, further within the atlan sub-field. For attributes, such as certificates, announcements, or owners these need to be specified within the attributes sub-field. Classifications need to be specified within a classifications sub-field. Note that the meta field and its sub-structure (including all the detailed attributes) can also be applied to columns within a model. This rich metadata will then be loaded to the corresponding attributes on the asset in Atlan. For more details on specific examples, see the dbt tabs in the Common asset actions snippets. 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/events/",
    "content": "Events Â¶ Atlan produces events when certain activities occur in the system. You can tap into these in a push-based integration model to take action the moment they occur. To tap into the events, you need to first set up a webhook in Atlan. Have a look at the event handling pattern for more details on implementing event-handling from webhooks using either of the SDKs. 2"
  },
  {
    "url": "https://developer.atlan.com/concepts/review/",
    "content": "Other important concepts Â¶ Type definitions Â¶ Type definitions (or typedefs for short) describe the properties and relationships that each different type of asset can have in Atlan. Type definitions are the structure for metadata In an object-oriented programming sense, think of a type definition as the class itself. They describe the underlying data model of Atlan. For example: The model for database tables in Atlan is defined by the Table typedef. The Table typedef describes characteristics unique to database tables, such as column counts and row counts. The Table typedef inherits from an Asset typedef. (As do most other objects in Atlan.) The Asset typedef describes characteristics that apply to all of these objects, such as certificates and announcements. classDiagram\n  class Asset {\n    <<abstract>>\n    name\n    qualifiedName\n    certificateStatus\n    certificateStatusMessage\n    announcementType\n    announcementTitle\n    announcementMessage\n    ...\n    assignedTerms()\n  }\n  class Table {\n    columnCount\n    rowCount\n    atlanSchema()\n    columns()\n  }\n  class Column {\n    dataType\n    isNullable\n    table()\n  }\n  Asset <|-- Table : extends\n  Asset <|-- Column : extends Special assets Â¶ While all assets follow the above principles, there are two types of assets to be aware of that have further specific uses in Atlan. Connections Â¶ classDiagram\n  class Connection {\n    ...\n    connector[Name|Type]\n    ...\n  } Connections play several important roles: They form the basis of Atlan's access control policies. Their connectorName property (renamed connectorType in some SDKs) decides the icon Atlan will use for assets contained within the connection. Processes Â¶ Processes form the basis for Atlan's data lineage. They define how data inputs ( sources ) are translated into data outputs ( targets ). Without a process asset to link these upstream and downstream assets, you cannot have data lineage in Atlan. graph LR\n  s1[(Source 1)]\n  s2[(Source 2)]\n  s3[(Source 3)]\n  t1[(Target 1)]\n  t2[(Target 2)]\n  p([Process])\n  s1 & s2 & s3-->|upstream|p-->|downstream|t1 & t2 Tags Â¶ Tags give you a way to classify and group assets in different ways, for example: By industry standard information security or sensitivity schemes (for example: PII) By department or business domain (for example: HR, Finance, Marketing, and so on) By key characteristic for alerting (for example: data quality issue, load failure, or similar) Really any other way you want to group together your assets Propagation Â¶ What's special about tags? Atlan can propagate tags for you automatically, to related assets: From upstream assets to downstream assets (via lineage) From parent assets to child assets (for example, from a table to all of its columns) From a term to all of its linked assets This becomes particularly powerful when using tags to represent key information you want to let your users know about. For example: Tagging problematic assets Â¶ If you find a problem on an asset, you can tag that asset as having a known issue. With propagation, Atlan will automatically tag all downstream (impacted) assets as having a known issue as well. Even better, you can see from that propagated tag which upstream asset(s) are the source of that known issue. Tagging sensitive assets Â¶ You can create a glossary of terms core to your business like customer details, accounts, etc. You can assign the terms in that glossary to the data assets that hold that information. You can then tag the terms with sensitivity ratings (like PII, Confidential, Public, etc). With propagation, Atlan will automatically tag all related data assets with those same sensitivity ratings. Even better, any assets or fields derived from those assets (even if named differently) will be propagated that sensitivity rating as well. Branding Â¶ You can also \"brand\" tags to provide quick visual distinction: Choose from a predefined list of icons Apply a color to the tag Or even upload your own image to use as an icon to represent the tag Access control Â¶ Tags can also be used to control access to assets, through purposes . When combined with propagation, you gain a very powerful, automated means to protect your most sensitive data. Custom metadata Â¶ Custom metadata gives you a way to extend the built-in types with your own attributes. Structurally custom metadata is composed of: An overall name (sometimes referred to as a \"set\") Individual attributes contained within that \"set\" (Optional) Restrictions on which assets can possess values for the custom metadata Runtime resolution Note that unlike built-in types and attributes, custom metadata can only be resolved at runtime. Therefore custom metadata attributes are not strongly-typed in the SDKs the way built-in types and attributes are â€” they must be handled a little differently. Badges Â¶ What's special about custom metadata? Firstly, custom metadata gives you a way to define your own attributes for assets. In addition, on top of custom metadata attributes you can create badges to callout important information on assets. Branding Â¶ You can also \"brand\" custom metadata to provide quick visual distinction: Choose from a predefined list of emojis Or even upload your own image to use as an icon to represent the custom metadata This \"branding\" will also be used for any badges you create over the custom metadata attributes. 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/",
    "content": "Integration options Â¶ Throughout the portal you can focus on your preferred integration approach (and switch between them as you like): CLI Use the Atlan CLI to manage data contracts for assets in Atlan. Get started with CLI dbt Use dbt's meta field to enrich metadata resources straight from dbt into Atlan. Get started with dbt Java Pull our Java SDK from Maven Central, just like any other dependency. Get started with Java Python Pull our Python SDK from PyPI, just like any other dependency. Get started with Python Kotlin Pull our Java SDK from Maven Central, just like any other dependency. Get started with Kotlin Scala Pull our Java SDK from Maven Central, just like any other dependency. Get started with Scala Clojure Pull our Java SDK from Maven Central, just like any other dependency. Get started with Clojure Go Pull our Go SDK from GitHub, just like any other dependency. Get started with Go Events Tap into events Atlan produces to take immediate action, as metadata changes. Get started with events Raw REST API You can call directly into our REST API, though we would recommend the SDKs. Get started with raw REST APIs 2"
  },
  {
    "url": "https://developer.atlan.com/concepts/",
    "content": "Overall site map Â¶ Introduction Â¶ If you are new to Atlan, or to developing with Atlan, start with one of the following two options. These will set you up to develop with Atlan, step-by-step. Atlan University Self-paced, video-based walkthrough of the essentials of Atlan as a platform. Atlan Platform Essentials certification Introductory walkthrough Documentation-based walkthrough, including step-by-step examples. Start the walkthrough Jumping in Â¶ If you are confident setting yourself up, and instead want to jump straight into specific examples, you can search (upper-right) or use the top-level menu: Common tasks Common operations on assets, that are available across all assets. Discover actions Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Searching Delve deep into searching and aggregating metadata. Learn more about searching Events Delve deep into the details of the events Atlan triggers. Learn more about events 2"
  },
  {
    "url": "https://developer.atlan.com/concepts/admin/",
    "content": "Administration Â¶ In addition to providing context to metadata through assets, tags, and custom metadata there is of course a wealth of administrative operations you can also access programmatically. These include: Creating and maintaining tags Creating and maintaining custom metadata structures Managing access control Managing users and groups Running built-in packages and workflows and more (see Snippets for the full list) 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/go/",
    "content": "Go SDK Â¶ Obtain the SDK Â¶ Pre-release state The Go SDK is currently in a pre-release, experimental state. While in this state, we reserve the right to make any changes to it (including breaking changes) without worrying about backwards compatibility, semantic versioning, and so on. If you are eager to experiment with it, it is available on GitHub . You can use Go dependencies to install it directly from there. We welcome your feedback during the pre-release, but cannot commit to any specific revisions or timelines at this point in time. Configure the SDK Â¶ There are two ways to configure the SDK: Using environment variables Â¶ ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https://tenant.atlan.com ) Here's an example of setting those environment variables: Set environment variables export ATLAN_BASE_URL=https://tenant.atlan.com export ATLAN_API_KEY=\"...\" main.go 1 2 3 4 5 6 7 8 9 package main import ( \"github.com/atlanhq/atlan-go/atlan/assets\" ) func main () { ctx := assets . NewContext () } On client creation Â¶ If you prefer to not use environment variables, you can do the following: main.go 1 2 3 4 5 6 7 8 9 package main import ( \"github.com/atlanhq/atlan-go/atlan/assets\" ) func main () { ctx , _ := assets . Context ( \"https://tenant.atlan.com\" , \"...\" ) } Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ What's next? Â¶ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Searching Delve deep into searching and aggregating metadata. Learn more about searching Events Delve deep into the details of the events Atlan triggers. Learn more about events Error-handling Â¶ The SDK defines exceptions for the following categories of error: Exception Description ApiConnectionError Errors when the SDK is unable to connect to the API, for example due to a lack of network access or timeouts. AuthenticationError Errors when the API token configured for the SDK is invalid or expired. ConflictError Errors when there is some conflict with an existing asset and the operation cannot be completed as a result. InvalidRequestError Errors when the request sent to Atlan does not match its expectations. If you are using the built-in methods like toCreate() and toUpdate() this exception should be treated as a bug in the SDK. (These operations take responsibility for avoiding this error.) LogicError Errors where some assumption made in the SDK's code is proven incorrect. If ever raised, they should be reported as bugs against the SDK. NotFoundError Errors when the requested resource or asset does not exist in Atlan. PermissionError Errors when the API token used by the SDK does not have permission to access a resource or carry out an operation on a specific asset . RateLimitError Errors when the Atlan server is being overwhelmed by requests. A given API call could fail due to all of the errors above. So these all extend a generic AtlanError exception, and all API operations can potentially raise AtlanError . Example For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionError , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionError will be raised. Advanced configuration Â¶ Atlan is a distributed, cloud-native application, where network problems can arise. The SDK therefore automatically attempts to handle ephemeral problems. Logging Â¶ The SDK uses the slog library internally to provide a flexible framework for emitting log messages. Go You can enable logging for your SDK script by adding the following lines above your snippets: main.go 1 2 3 4 5 6 7 8 9 10 package main import ( \"github.com/atlanhq/atlan-go/atlan/assets\" ) func main () { ctx := assets . NewContext () ctx . SetLogger ( true , \"debug\" ) // (1)! } You can enable logging by using .SetLogger() on the context object with various logging levels: \"debug\" : used to give detailed information, typically of interest only when diagnosing problems (mostly used level in SDK). \"info\" : used to confirm that things are working as expected. \"warn\" : used as an indication that something unexpected happened, or as a warning of some problem in the near future. \"error\" : indicates that due to a more serious problem, the SDK has not been able to perform some operation. 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/clojure/",
    "content": "Clojure SDK Â¶ Obtain the SDK Â¶ For Clojure, you can reuse the existing Java SDK as-is. It is available on Maven Central, ready to be included in your project: deps.edn deps.edn 1 2 3 4 5 6 7 8 9 10 11 12 13 { :aliases { :run { :ns-default my.proj :main-opts [ \"-m\" \"my.proj\" ] :jvm-opts [ \"-Dclojure.tools.logging.factory=clojure.tools.logging.impl/slf4j-factory\" ] :deps { com.atlan/atlan-java { :mvn/version \"4.2.0\" } ;; (1) org.clojure/tools.logging { :mvn/version \"1.3.0\" } ;; (2) org.slf4j/slf4j-simple { :mvn/version \"2.0.7\" } } } }} Include the latest version of the Java SDK in your project as a dependency (replace the version number with the latest version indicated in the badges above). The Java SDK uses slf4j for logging purposes. You can include the org.clojure/tools.logging utility as a simple binding mechanism to send any logging information out to your console (standard out). Configure the SDK Â¶ There are two ways to configure the SDK: Using environment variables Â¶ ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https://tenant.atlan.com ) Here's an example of setting those environment variables: Set environment variables export ATLAN_BASE_URL=https://tenant.atlan.com export ATLAN_API_KEY=\"...\" src/my/proj.clj 1 2 3 4 5 6 7 8 9 ( ns my.proj ( :import com.atlan.AtlanClient ) ( :require [ clojure.tools.logging :as logger ])) ( defn -main [ & args ] ( with-open [ client ( AtlanClient. )] ;; Do something with the client ( logger/info \"Using the AtlanClient...\" ))) On client creation Â¶ If you prefer to not use environment variables, you can do the following: src/my/proj.clj 1 2 3 4 5 6 7 8 9 ( ns my.proj ( :import com.atlan.AtlanClient ) ( :require [ clojure.tools.logging :as logger ])) ( defn -main [ & args ] ( with-open [ client ( AtlanClient. \"https://tenant.atlan.com\" \"...\" )] ;; Do something with the client ( logger/info \"Using the AtlanClient...\" ))) What's next? Â¶ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Searching Delve deep into searching and aggregating metadata. Learn more about searching Events Delve deep into the details of the events Atlan triggers. Learn more about events Error-handling Â¶ The SDK defines checked exceptions for the following categories of error: Exception Description ApiConnectionException Errors when the SDK is unable to connect to the API, for example due to a lack of network access or timeouts. AuthenticationException Errors when the API token configured for the SDK is invalid or expired. ConflictException Errors when there is some conflict with an existing asset and the operation cannot be completed as a result. InvalidRequestException Errors when the request sent to Atlan does not match its expectations. If you are using the built-in methods like toCreate() and toUpdate() this exception should be treated as a bug in the SDK. (These operations take responsibility for avoiding this error.) LogicException Errors where some assumption made in the SDK's code is proven incorrect. If ever raised, they should be reported as bugs against the SDK. NotFoundException Errors when the requested resource or asset does not exist in Atlan. PermissionException Errors when the API token used by the SDK does not have permission to access a resource or carry out an operation on a specific asset . RateLimitException Errors when the Atlan server is being overwhelmed by requests. A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . Example For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Advanced configuration Â¶ Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. Logging Â¶ The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Log4j2 deps.edn 7 8 9 10 11 12 :deps { com.atlan/atlan-java { :mvn/version \"4.2.0\" } org.clojure/tools.logging { :mvn/version \"1.3.0\" } org.apache.logging.log4j/log4j-core { :mvn/version \"2.20.0\" } ;; (1) org.apache.logging.log4j/log4j-slf4j2-impl { :mvn/version \"2.20.0\" } } Add log4j2 bindings. src/resources/log4j2.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration status= \"WARN\" > <Appenders> <Console name= \"ConsoleAppender\" target= \"SYSTEM_OUT\" > <PatternLayout> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </PatternLayout> </Console> <File name= \"FileAppender\" fileName= \"tmp/debug.log\" append= \"false\" > <PatternLayout> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </PatternLayout> </File> </Appenders> <Loggers> <Root level= \"DEBUG\" > <AppenderRef ref= \"ConsoleAppender\" level= \"INFO\" /> <AppenderRef ref= \"FileAppender\" /> </Root> <Logger name= \"com.atlan\" level= \"DEBUG\" /> </Loggers> </Configuration> Retries Â¶ The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. More details on how they work If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: Configure the maximum number of retries ( with-open [ client ( AtlanClient. )] ( .setMaxNetworkRetries client 10 )) Timeouts Â¶ The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Configure the maximum time to wait before timing out ( with-open [ client ( AtlanClient. )] ( .setReadTimeout client ( * 120 1000 ))) ;; (1)! Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Multi-tenant connectivity Â¶ Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Create a client 1 2 ( with-open [ client ( AtlanClient. \"https://tenant.atlan.com\" )] ;; (1)! ( .setApiToken client \"...\" )) Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Use a specific client 1 2 3 4 5 6 7 8 9 10 11 12 13 14 ( with-open [ client ( AtlanClient. )] ( let [ term ( -> ( GlossaryTerm/creator \"Example Term\" ;; (1) \"836830be-5a11-4094-8346-002e0320684f\" nil ) .build )] ( .save ( .assets client ) term ) ;; (2) ( let [ request-options ( -> ( RequestOptions/from client ) ;; (3) ( .maxNetworkRetries 10 ) .build )] ( .save ( .assets client ) term request-options )) ( .save term client ))) ;; (4) Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy Â¶ To use the Java SDK with a proxy, you need to send in some additional parameters when running any java ... command. These are described in detail in the Java documentation , but are summarized here for simplicity: HTTPS SOCKS https.proxyHost should be set to the hostname for your HTTPS proxy https.proxyPort should be set to the port for your HTTPS proxy (default being 443) Run command using an HTTPS proxy 1 clj -J-Dhttps.proxyHost = hostname -J-Dhttps.proxyPort = 8080 -M:run socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Run command using a SOCKS proxy 1 clj -J-DsocksProxyHost = hostname -J-DsocksProxyPort = 8080 -M:run Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: Authenticate to proxy 1 2 3 ( with-open [ client ( AtlanClient. )] ( let [ pa ( PasswordAuthentication. \"username\" ( char-array \"password\" ))] ;; (1) ( .setProxyCredential client pa )))) ;; (2) You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. ... and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute. 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/cli/",
    "content": "Atlan CLI Â¶ Limited functionality (so far) You can use Atlan's command-line interface (CLI) to manage some metadata in Atlan. Currently data contracts and metadata for a limited set of asset types can be managed through the CLI. Obtain the CLI Â¶ 0.1.0 For now, the CLI must be downloaded as a pre-built binary: Disclaimer â€” closed preview This feature is in closed preview and therefore any download and installation from this link will be subject to the terms applicable to Product Release Stages . Contact your Atlan Customer Success Manager for your preview today. If your organization is already part of the closed preview, your installation of the feature from this link shall become subject to the terms and scope of preview as agreed with your organization. Accordingly, any use of the feature outside the agreed scope may result in revocation of the closed preview for your organization. Please contact your system administrator before downloading. Homebrew MacOS (M1) MacOS (Intel) Linux Windows Windows Recommended When installed via Homebrew, you can easily keep things up-to-date. If you do not use it already, see Homebrew's own installation documents for setting up Homebrew itself . brew tap atlanhq/atlan\nbrew install atlan curl -o atlan.tgz -L https://github.com/atlanhq/atlan-cli-releases/releases/latest/download/atlan_Darwin_arm64.tar.gz\ntar xf atlan.tgz curl -o atlan.tgz -L https://github.com/atlanhq/atlan-cli-releases/releases/latest/download/atlan_Darwin_amd64.tar.gz\ntar xf atlan.tgz curl -o atlan.tgz -L https://github.com/atlanhq/atlan-cli-releases/releases/latest/download/atlan_Linux_amd64.tar.gz\ntar -zxf atlan.tgz curl -o atlan.zip -L https://github.com/atlanhq/atlan-cli-releases/releases/latest/download/atlan_Windows_amd64.zip\nunzip atlan.zip Configure the CLI Â¶ 0.1.0 You can configure the CLI using a config file or in some cases environment variables, with the following minimum settings 1 : .atlan/config.yaml 1 2 3 4 5 atlan_api_key : eyZid92... # (1) atlan_base_url : https://tenant.atlan.com # (2) log : enabled : false # (3) level : info # (4) An API token that has access to your assets. The base URL of your tenant (including the https:// ). (Optional) Enable logging to produce more details on what the CLI is doing. When logging is enabled, specify the level of verbosity. Environment variables 1 ATLAN_API_KEY = eyZid92... # (1) An API token that has access to your assets. Define data sources Â¶ You should also define data sources in the config file: .atlan/config.yaml 6 7 8 9 10 11 12 data_source snowflake : # (1) type : snowflake # (2) connection : # (3) name : snowflake-prod # (4) qualified_name : \"default/snowflake/1234567890\" # (5) database : db # (6) schema : analytics # (7) Each data source definition must start with data_source , followed by a space and a unique reference name for the data source ( snowflake in this example). Reference name is your choice The reference name you give in the configuration file is only used here and as a reference in any data contracts you define. It need not match the name of the connection or data source in Atlan itself. You must indicate the type of connector for the data source (see connector types for options). Details of the connection must also be provided. You must provide the name of the connection, as it appears in Atlan. You must provide the unique qualified_name of the connection in Atlan. (Optional) You can also specify the database to use for this connection's assets by default, if none is specified in the data contract. (Optional) You can also specify the schema to use for this connection's assets by default, if none is specified in the data contract. These ensure the CLI can map the details you specify in your data contract to the appropriate corresponding asset in Atlan. What's next? Â¶ With the CLI, you can: Manage data contracts Upload and download files from Atlan's backing object store Sync metadata to a limited set of asset types Integrate data contracts with CI/CD processing When both are specified, environment variables will take precedence. â†© 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/java/",
    "content": "Java SDK Â¶ Atlan University Walk through step-by-step in our intro to custom integration course (30 mins). Obtain the SDK Â¶ The SDK is available on Maven Central, ready to be included in your project: Gradle Maven build.gradle repositories { mavenCentral () } dependencies { implementation \"com.atlan:atlan-java:+\" // (1) testRuntimeOnly 'ch.qos.logback:logback-classic:1.2.11' // (2) } Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include logback as a simple binding mechanism to send any logging information out to your console (standard out), at INFO -level or above. pom.xml <dependency> <groupId> com.atlan </groupId> <artifactId> atlan-java </artifactId> <version> ${atlan.version} </version> </dependency> Configure the SDK Â¶ There are two ways to configure the SDK: Using environment variables Â¶ ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https://tenant.atlan.com ) Here's an example of setting those environment variables: Set environment variables export ATLAN_BASE_URL=https://tenant.atlan.com export ATLAN_API_KEY=\"...\" AtlanLiveTest.java 1 2 3 4 5 6 7 8 9 import com.atlan.AtlanClient ; public class AtlanLiveTest { public static void main ( String [] args ) { try ( AtlanClient client = new AtlanClient ()) { // Do something with the client } } } On client creation Â¶ If you prefer to not use environment variables, you can do the following: AtlanLiveTest.java 1 2 3 4 5 6 7 8 9 10 11 12 import com.atlan.AtlanClient ; public class AtlanLiveTest { public static void main ( String [] args ) { try ( AtlanClient client = new AtlanClient ( \"https://tenant.atlan.com\" , \"...\" ) ) { // Do something with the client } } } Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. (Note that you can also explicity provide only the tenant URL, and the constructor will look for only the API key through an environment variable.) That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ What's next? Â¶ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Searching Delve deep into searching and aggregating metadata. Learn more about searching Events Delve deep into the details of the events Atlan triggers. Learn more about events Error-handling Â¶ The SDK defines checked exceptions for the following categories of error: Exception Description ApiConnectionException Errors when the SDK is unable to connect to the API, for example due to a lack of network access or timeouts. AuthenticationException Errors when the API token configured for the SDK is invalid or expired. ConflictException Errors when there is some conflict with an existing asset and the operation cannot be completed as a result. InvalidRequestException Errors when the request sent to Atlan does not match its expectations. If you are using the built-in methods like toCreate() and toUpdate() this exception should be treated as a bug in the SDK. (These operations take responsibility for avoiding this error.) LogicException Errors where some assumption made in the SDK's code is proven incorrect. If ever raised, they should be reported as bugs against the SDK. NotFoundException Errors when the requested resource or asset does not exist in Atlan. PermissionException Errors when the API token used by the SDK does not have permission to access a resource or carry out an operation on a specific asset . RateLimitException Errors when the Atlan server is being overwhelmed by requests. A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . Example For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Advanced configuration Â¶ Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. Logging Â¶ The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Log4j2 build.gradle dependencies { implementation \"com.atlan:atlan-java:+\" implementation \"org.apache.logging.log4j:log4j-core:2.22.0\" // (1) implementation \"org.apache.logging.log4j:log4j-slf4j2-impl:2.22.0\" } Replace the ch.qos.logback:logback-classic:1.2.11 logback binding with log4j2 bindings. src/main/resources/log4j2.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration status= \"WARN\" > <Appenders> <Console name= \"ConsoleAppender\" target= \"SYSTEM_OUT\" > <PatternLayout> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </PatternLayout> </Console> <File name= \"FileAppender\" fileName= \"tmp/debug.log\" append= \"false\" > <PatternLayout> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </PatternLayout> </File> </Appenders> <Loggers> <Root level= \"DEBUG\" > <AppenderRef ref= \"ConsoleAppender\" level= \"INFO\" /> <AppenderRef ref= \"FileAppender\" /> </Root> <Logger name= \"com.atlan\" level= \"DEBUG\" /> </Loggers> </Configuration> Retries Â¶ The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. More details on how they work If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: Configure the maximum number of retries try ( AtlanClient client = new AtlanClient ()) { client . setMaxNetworkRetries ( 10 ); } Timeouts Â¶ The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Configure the maximum time to wait before timing out try ( AtlanClient client = new AtlanClient ()) { client . setReadTimeout ( 120 * 1000 ); // (1)! } Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Multi-tenant connectivity Â¶ Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Create a client 1 2 3 try ( AtlanClient client = new AtlanClient ( \"https://tenant.atlan.com\" )) { // (1)! client . setApiToken ( \"...\" ); } Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Use a specific client 1 2 3 4 5 6 7 8 9 10 11 12 try ( AtlanClient client = new AtlanClient ()) { GlossaryTerm term = GlossaryTerm . creator ( // (1) \"Example Term\" , \"836830be-5a11-4094-8346-002e0320684f\" , null ) . build (); client . assets . save ( term ); // (2) client . assets . save ( term , RequestOptions . from ( client ). maxNetworkRetries ( 10 ). build ()); // (3) term . save ( client ); // (4) } Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy Â¶ To use the Java SDK with a proxy, you need to send in some additional parameters when running any java ... command. These are described in detail in the Java documentation , but are summarized here for simplicity: HTTPS SOCKS https.proxyHost should be set to the hostname for your HTTPS proxy https.proxyPort should be set to the port for your HTTPS proxy (default being 443) Run command using an HTTPS proxy 1 java -Dhttps.proxyHost = hostname -Dhttps.proxyPort = 8080 com.atlan.samples.SomeClassToRun socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Run command using a SOCKS proxy 1 java -DsocksProxyHost = hostname -DsocksProxyPort = 8080 com.atlan.samples.SomeClassToRun Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: Authenticate to proxy 1 2 3 4 5 6 PasswordAuthentication pa = new PasswordAuthentication ( // (1) \"username\" , // (2) \"password\" . toCharArray ()); // (3) try ( AtlanClient client = new AtlanClient ()) { client . setProxyCredential ( pa ); // (4) } You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. ...and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute. 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/kotlin/",
    "content": "Kotlin SDK Â¶ Obtain the SDK Â¶ For Kotlin, you can reuse the existing Java SDK as-is. It is available on Maven Central, ready to be included in your project: Gradle build.gradle.kts repositories { mavenCentral () } dependencies { implementation ( \"com.atlan:atlan-java:+\" ) // (1) implementation ( \"io.github.microutils:kotlin-logging-jvm:3.0.5\" ) // (2) implementation ( \"org.slf4j:slf4j-simple:2.0.7\" ) } Include the latest version of the Java SDK in your project as a dependency. You can also give a specific version instead of the + , if you'd like. The Java SDK uses slf4j for logging purposes. You can include slf4j-simple as a simple binding mechanism to send any logging information out to your console (standard out), along with the kotlin-logging-jvm microutil. Configure the SDK Â¶ There are two ways to configure the SDK: Using environment variables Â¶ ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https://tenant.atlan.com ) Here's an example of setting those environment variables: Set environment variables export ATLAN_BASE_URL=https://tenant.atlan.com export ATLAN_API_KEY=\"...\" AtlanLiveTest.kt 1 2 3 4 5 6 7 import com.atlan.AtlanClient fun main () { AtlanClient (). use { client -> // Do something with the client } } On client creation Â¶ If you prefer to not use environment variables, you can do the following: AtlanLiveTest.kt 1 2 3 4 5 6 7 8 9 10 import com.atlan.AtlanClient fun main () { AtlanClient ( \"https://tenant.atlan.com\" , \"...\" ) ) { // Do something with the client } } What's next? Â¶ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Searching Delve deep into searching and aggregating metadata. Learn more about searching Events Delve deep into the details of the events Atlan triggers. Learn more about events Error-handling Â¶ The SDK defines checked exceptions for the following categories of error: Exception Description ApiConnectionException Errors when the SDK is unable to connect to the API, for example due to a lack of network access or timeouts. AuthenticationException Errors when the API token configured for the SDK is invalid or expired. ConflictException Errors when there is some conflict with an existing asset and the operation cannot be completed as a result. InvalidRequestException Errors when the request sent to Atlan does not match its expectations. If you are using the built-in methods like toCreate() and toUpdate() this exception should be treated as a bug in the SDK. (These operations take responsibility for avoiding this error.) LogicException Errors where some assumption made in the SDK's code is proven incorrect. If ever raised, they should be reported as bugs against the SDK. NotFoundException Errors when the requested resource or asset does not exist in Atlan. PermissionException Errors when the API token used by the SDK does not have permission to access a resource or carry out an operation on a specific asset . RateLimitException Errors when the Atlan server is being overwhelmed by requests. A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . Example For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Advanced configuration Â¶ Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. Logging Â¶ The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Log4j2 build.gradle.kts dependencies { implementation ( \"com.atlan:atlan-java:+\" ) implementation ( \"io.github.microutils:kotlin-logging-jvm:3.0.5\" ) implementation ( \"org.apache.logging.log4j:log4j-core:2.22.0\" ) // (1) implementation ( \"org.apache.logging.log4j:log4j-slf4j2-impl:2.22.0\" ) } Replace the org.slf4j:slf4j-simple:2.0.7 binding with log4j2 bindings. src/main/resources/log4j2.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration status= \"WARN\" > <Appenders> <Console name= \"ConsoleAppender\" target= \"SYSTEM_OUT\" > <PatternLayout> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </PatternLayout> </Console> <File name= \"FileAppender\" fileName= \"tmp/debug.log\" append= \"false\" > <PatternLayout> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </PatternLayout> </File> </Appenders> <Loggers> <Root level= \"DEBUG\" > <AppenderRef ref= \"ConsoleAppender\" level= \"INFO\" /> <AppenderRef ref= \"FileAppender\" /> </Root> <Logger name= \"com.atlan\" level= \"DEBUG\" /> </Loggers> </Configuration> Retries Â¶ The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. More details on how they work If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: Configure the maximum number of retries AtlanClient (). use { client -> client . setMaxNetworkRetries ( 10 ) } Timeouts Â¶ The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Configure the maximum time to wait before timing out AtlanClient (). use { client -> client . setReadTimeout ( 120 * 1000 ) // (1)! } Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Multi-tenant connectivity Â¶ Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Create a client 1 2 3 AtlanClient ( \"https://tenant.atlan.com\" ). use { client -> // (1)! client . setApiToken ( \"...\" ) } Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Use a specific client 1 2 3 4 5 6 7 8 9 10 11 12 AtlanClient (). use { client -> val term = GlossaryTerm . creator ( // (1) \"Example Term\" , \"836830be-5a11-4094-8346-002e0320684f\" , null ) . build () client . assets . save ( term ) // (2) client . assets . save ( term , RequestOptions . from ( client ). maxNetworkRetries ( 10 ). build ()) // (3) term . save ( client ) // (4) } Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy Â¶ To use the Java SDK with a proxy, you need to send in some additional parameters when running any java ... command. These are described in detail in the Java documentation , but are summarized here for simplicity: HTTPS SOCKS https.proxyHost should be set to the hostname for your HTTPS proxy https.proxyPort should be set to the port for your HTTPS proxy (default being 443) Run command using an HTTPS proxy 1 java -Dhttps.proxyHost = hostname -Dhttps.proxyPort = 8080 com.atlan.samples.SomeClassToRun socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Run command using a SOCKS proxy 1 java -DsocksProxyHost = hostname -DsocksProxyPort = 8080 com.atlan.samples.SomeClassToRun Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: Authenticate to proxy 1 2 3 4 5 6 val pa = PasswordAuthentication ( // (1) \"username\" , // (2) \"password\" . toCharArray ()) // (3) AtlanClient (). use { client -> client . setProxyCredential ( pa ) // (4) } You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. ...and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute. 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/python/",
    "content": "Python SDK Â¶ Atlan University Walk through step-by-step in our intro to custom integration course (30 mins). Obtain the SDK Â¶ The SDK is currently available on pypi . You can use pip to install it as follows: Install the SDK pip install pyatlan Configure the SDK Â¶ There are two ways to configure the SDK: Using environment variables Â¶ ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https://tenant.atlan.com ) Here's an example of setting those environment variables: Set environment variables export ATLAN_BASE_URL=https://tenant.atlan.com export ATLAN_API_KEY=\"...\" atlan_live_test.py 1 2 3 from pyatlan.client.atlan import AtlanClient client = AtlanClient () On client creation Â¶ If you prefer to not use environment variables, you can do the following: atlan_live_test.py 1 2 3 4 5 6 from pyatlan.client.atlan import AtlanClient client = AtlanClient ( base_url = \"https://tenant.atlan.com\" , api_key = \"...\" ) Careful not to expose your API token! We generally discourage including your API token directly in your code, in case you accidentally commit it into a (public) version control system. But it's your choice exactly how you manage the API token and including it for use within the client. (optional) Want to create a client using an API token GUID? In some scenarios, you may not want to expose the entire API token or manage environment variables. Instead, you can provide the GUID of the API token, and the SDK will internally fetch the actual access token. When to use this approach: Building apps that use the SDK where token security is a concern When you want to avoid exposing full API tokens in your configuration For containerized applications that need secure token management Prerequisites: Before using this approach, ensure your Argo template is configured with CLIENT_ID and CLIENT_SECRET : Argo template configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 container : image : ghcr.io/atlanhq/designation-based-group-provisioning:1.0.2 imagePullPolicy : IfNotPresent env : - name : CLIENT_ID valueFrom : secretKeyRef : name : \"argo-client-creds\" key : \"login\" - name : CLIENT_SECRET valueFrom : secretKeyRef : name : \"argo-client-creds\" key : \"password\" Python 7.1.4 Creating a client with API token GUID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from pyatlan.client.atlan import AtlanClient from pyatlan.model.fluent_search import FluentSearch from pyatlan.model.query import CompoundQuery from pyatlan.model.assets import AtlasGlossary # Initialize client using API token GUID token_client = AtlanClient . from_token_guid ( guid = \"c5e249d7-abcc-4ad5-87a1-831d7b810df4\" # (1) ) # Perform operations with the client (requires appropriate permissions) results = ( FluentSearch () . where ( CompoundQuery . active_assets ()) . where ( CompoundQuery . asset_type ( AtlasGlossary )) . page_size ( 100 ) . execute ( client = token_client ) ) assert results and results . count >= 1 print ( f \"Found { results . count } glossary assets\" ) Create client from token GUID : Use AtlanClient.from_token_guid() to create a client using the GUID of an API token. The SDK will automatically fetch the actual access token using the configured CLIENT_ID and CLIENT_SECRET . That's it â€” once these are set you can start using your SDK to make live calls against your Atlan instance! ðŸŽ‰ What's next? Â¶ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Searching Delve deep into searching and aggregating metadata. Learn more about searching Events Delve deep into the details of the events Atlan triggers. Learn more about events Error-handling Â¶ The SDK defines exceptions for the following categories of error: Exception Description ApiConnectionError Errors when the SDK is unable to connect to the API, for example due to a lack of network access or timeouts. AuthenticationError Errors when the API token configured for the SDK is invalid or expired. ConflictError Errors when there is some conflict with an existing asset and the operation cannot be completed as a result. InvalidRequestError Errors when the request sent to Atlan does not match its expectations. If you are using the built-in methods like toCreate() and toUpdate() this exception should be treated as a bug in the SDK. (These operations take responsibility for avoiding this error.) LogicError Errors where some assumption made in the SDK's code is proven incorrect. If ever raised, they should be reported as bugs against the SDK. NotFoundError Errors when the requested resource or asset does not exist in Atlan. PermissionError Errors when the API token used by the SDK does not have permission to access a resource or carry out an operation on a specific asset . RateLimitError Errors when the Atlan server is being overwhelmed by requests. A given API call could fail due to all of the errors above. So these all extend a generic AtlanError exception, and all API operations can potentially raise AtlanError . Example For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionError , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionError will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Advanced configuration Â¶ Atlan is a distributed, cloud-native application, where network problems can arise. The SDK therefore automatically attempts to handle ephemeral problems. Logging Â¶ The SDK uses logging module of the standard library that can provide a flexible framework for emitting log messages. Python You can enable logging for your SDK script by\nadding the following lines above your snippets: atlan_python_sdk_test.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import logging from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary logging . basicConfig ( level = logging . DEBUG ) # (1) # logging.config.fileConfig('pyatlan/logging.conf') # (2) # SDK code snippets client = AtlanClient () glossary = client . asset . get_by_guid ( asset_type = AtlasGlossary , guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" ) You can enable logging by using basicConfig with various logging levels: logging.DEBUG : used to give detailed information,\ntypically of interest only when diagnosing problems (mostly used level in SDK). logging.INFO : used to confirm that things are working as expected. logging.WARN : used as an indication that something unexpected happened,\nor as a warning of some problem in the near future. logging.ERROR : indicates that due to a more serious problem, the SDK\nhas not been able to perform some operation. logging.CRITICAL : indicates a serious error, suggesting that the\nprogram itself may be unable to continue running (not used in SDK as of now). By default, logs will appear in your console.\nIf you want to use file logging, you can add the following line: logging.config.fileConfig('pyatlan/logging.conf') : this will\ngenerate logs according to the configuration defined in pyatlan/logging.conf and will generate two log files: /tmp/pyatlan.log : default log file. /tmp/pyatlan.json : log file in JSON format. Retries Â¶ The SDK handles automatically retrying your requests when it detects certain problems: When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 429 response indicating that the request rate limit has been exceeded, and you need to retry after some time. When there is a 50x response indicating that something went wrong on the server side. More details on how they work If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using an exponential backoff. (Currently the values for the exponential backoff are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 5 by default.) Timeouts Â¶ By default, the SDK AtlanClient() has the following timeout settings: read_timeout : 900.0 seconds ( 15 minutes) connect_timeout : 30.0 seconds If you need to override these defaults, you can do so as shown in the example below: Override SDK client default timeout 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () # Timeout values in seconds client . read_timeout = 1800.0 # 30 minutes client . connect_timeout = 60.0 # 1 minute Multi-tenant connectivity Â¶ Since version 1.0.0, the Python SDK supports connecting to multiple tenants.[^1] When you use the AtlanClient() method you are actually setting a default client. This default client will be used behind-the-scenes for any operations that need information specific to an Atlan tenant. When you want to override that default client you can create a new one and use the set_default_client() method to change it: Create a client 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client2 = AtlanClient ( # (1) base_url = \"https://tenant.atlan.com\" , api_key = \"...\" ) Atlan . set_default_client ( client2 ) # (2) The AtlanClient() method will return a client for the given base URL, creating a new client and setting this new client as the default client. If you want to switch between clients that you have already created, you can use Atlan.set_default_client() to change between them. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. (And since in the majority of use cases you only need access to a single tenant, this means you can most likely just rely on the default client and the fallback behavior.) Proxies Â¶ Pyatlan uses the Requests library which supports proxy configuration via environment variables. Requests relies on the proxy configuration defined by standard environment variables http_proxy, https_proxy, no_proxy, and all_proxy. Uppercase variants of these variables are also supported. You can therefore set them to configure Pyatlan (only set the ones relevant to your needs): Configure a proxy export HTTP_PROXY = \"http://10.10.1.10:3128\" export HTTPS_PROXY = \"http://10.10.1.10:1080\" export ALL_PROXY = \"socks5://10.10.1.10:3434\" To use HTTP Basic Auth with your proxy, use the http://user:password@host/ syntax in any of the above configuration entries: Configure a proxy with authentication export HTTPS_PROXY = \"http://user:pass@10.10.1.10:1080\" Currently, the way this is implemented limits you to either avoiding multiple threads in your Python code (if you need to use multiple clients), or if you want to use multiple threads you should only use a single client. Asynchronous SDK operations Â¶ 8.0.0 Starting from version v8.0 , it's possible to run SDK code asynchronously. The async API is designed to mirror the synchronous SDK clients, maintaining familiar client patterns and caching behavior to ensure a smooth developer experience ( see release notes for complete changes ). To get started, you need to initialize an AsyncAtlanClient : Create an async client 1 2 3 4 5 6 from pyatlan.client.aio import AsyncAtlanClient client = AsyncAtlanClient ( # (1) base_url = \"https://tenant.atlan.com\" , api_key = \"...\" ) Create an async client using the same configuration pattern as the synchronous client. Basic search example Â¶ This example demonstrates how to perform an asynchronous search for tables. The API is nearly identical to the synchronous version, with the addition of async/await keywords: Run an async search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import asyncio from pyatlan.client.aio import AsyncAtlanClient from pyatlan.model.fluent_search import FluentSearch from pyatlan.model.search import Term from pyatlan.model.assets import Asset client = AsyncAtlanClient ( base_url = \"https://tenant.atlan.com\" , api_key = \"...\" ) async def search_tables (): \"\"\"Search for all active tables\"\"\" results = await client . asset . search ( # (1) criteria = FluentSearch () . where ( Term . with_state ( \"ACTIVE\" )) . where ( Asset . TYPE_NAME . eq ( \"Table\" )) . to_request (), ) # Process results asynchronously async for table in results : # (2) print ( f \"Found table: { table . name } \" ) return results . count # Run the async function total_count = asyncio . run ( search_tables ()) print ( f \"Total tables found: { total_count } \" ) Async search : Build search requests using the same FluentSearch pattern as the synchronous client. Use await since the async client returns a coroutine object. Async iteration : Use async for to iterate through results, as the async client returns AsyncIndexSearchResults which implements __aiter__ . Concurrent operations for improved performance Â¶ The real power of async comes from running multiple operations concurrently . Instead of waiting for each operation to complete sequentially, you can execute them in parallel and reduce total execution time: Performance comparison: Synchronous : Total time = operationâ‚ + operationâ‚‚ + ... + operationâ‚™ Asynchronous : Total time = max(operationâ‚, operationâ‚‚, ..., operationâ‚™) Run concurrent async searches 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import asyncio from pyatlan.client.aio import AsyncAtlanClient from pyatlan.model.fluent_search import FluentSearch from pyatlan.model.search import Term from pyatlan.model.assets import Asset client = AsyncAtlanClient ( base_url = \"https://tenant.atlan.com\" , api_key = \"...\" ) async def search_tables (): # (1) \"\"\"Search for all active tables\"\"\" results = await client . asset . search ( criteria = FluentSearch () . where ( Term . with_state ( \"ACTIVE\" )) . where ( Asset . TYPE_NAME . eq ( \"Table\" )) . to_request (), ) return results . count async def search_columns (): # (2) \"\"\"Search for all active columns\"\"\" results = await client . asset . search ( criteria = FluentSearch () . where ( Term . with_state ( \"ACTIVE\" )) . where ( Asset . TYPE_NAME . eq ( \"Column\" )) . to_request (), ) return results . count async def concurrent_search (): \"\"\"Run table and column searches concurrently\"\"\" # Execute both searches at the same time table_count , column_count = await asyncio . gather ( # (3) search_tables (), search_columns () ) return { \"tables\" : table_count , \"columns\" : column_count , \"total_assets\" : table_count + column_count } async def main (): \"\"\"Main function to execute concurrent asset searches\"\"\" return await concurrent_search () if __name__ == \"__main__\" : result = asyncio . run ( main ()) print ( f \"Search completed: { result [ 'total_assets' ] } assets found\" ) print ( f \"Tables: { result [ 'tables' ] } , Columns: { result [ 'columns' ] } \" ) Tables search function : Define an async function to search for tables and return the count. Columns search function : Define an async function to search for columns and return the count. Concurrent execution : Use asyncio.gather() to run both searches simultaneously. You can also use asyncio.as_completed() if you want to process results as they become available: tasks = [ search_tables (), search_columns ()] for coro in asyncio . as_completed ( tasks ): result = await coro print ( f \"Operation completed with { result } assets\" ) When to use async Async is most beneficial when you have: Multiple independent operations that can run concurrently I/O-heavy workloads like API calls, database queries, or file operations Long-running operations where parallelization provides significant time savings For simple, single operations, the synchronous client may be more straightforward to use. 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/raw/",
    "content": "Raw REST API Â¶ Atlan University Walk through step-by-step in our intro to custom integration course (30 mins). Not for the faint of heart There are a number of details and nuances to understand about the underlying REST APIs to use them most effectively: the underlying REST API communications (HTTP protocols, methods, headers, endpoint URLs, response codes, etc) translating the complex JSON structures for each request and response payload knowing exactly which values are required (and which aren't) depending on the object you're interacting with, what operation you're carrying out, etc (including which exact string (and capitalization) to use for values that are really enumerations behind-the-scenes) Ultimately, all pull-based integration mechanisms (including the SDKs) use the REST API; however, the SDKs also encode best practices to avoid the need to understand all these details and low-level nuances. If you want to adopt these best practices from the start, we would strongly recommend directly using any of our SDKs rather than the raw REST APIs directly: Java Python Kotlin That being said, we have documented the raw REST API interactions in most cases. So if you really want to interact with the APIs directly, there should still be some guidance â€” anywhere you see Raw REST API gives details on endpoint URLs, methods, and payloads. Postman Â¶ If all you really want to do is some initial experimentation directly against the API, you can use Postman . We do not maintain a Postman collection of requests, but you can use the Raw REST API tab to find the endpoint and an example payload to use for most operations here on developer.atlan.com. OpenAPI spec Â¶ Why not just publish an OpenAPI spec? We did try this in the past, as we liked the idea of generating client libraries using tools like the OpenAPI Generator . Unfortunately, in our own testing of these tools, we found that: We could generate code that is free from syntax errors, but the generated code was not fully functional. Problems we found: The generated code drops significant portions of data from payloads. (Our APIs are payload-rich and endpoint-light, while the generators seem to favor endpoint-rich and payload-light APIs.) The various objects the generator creates often make developer consumption cumbersome â€” long, difficult-to-read object names; many variations of similar objects; and so on. The generated code naturally does not include any validation that can't be encoded in the OpenAPI spec. To add this we'd need to wrap the generated code with another layer anyway. After several attempts at mangling the OpenAPI spec to meet the constraints of the code generator, we eventually decided to go the way we've seen other API-first companies adopt. We found very few API-first companies appear to rely on these code generators, but rather directly maintain their own SDKs for which they may have their own code generators. (Which is in fact exactly what we're doing as well.) Request for feedback Â¶ If you use the raw REST APIs rather than one of the provided SDKs, we would love to understand more. If you can spare a few minutes to fill out our survey , we would be very grateful! 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/access/",
    "content": "Access control and personalization Â¶ Atlan provides a number of ways to manage access control , at both coarse-grained and granular levels. The primary vehicles for granular access control are personas , purposes , and the access policies defined within these. Both personas and purposes can also be used to personalize the experience, by controlling: which assets are visible to a particular audience which details of each of those assets are visible 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/",
    "content": "Common tasks Â¶ As developers ourselves, we know that learning is often easiest through example. In this Common tasks section you will find examples and explanations of the most common tasks in Atlan â€” those that are available on all assets. However, if you want to find out how to do something unique to a particular kind of asset, or to do something orthogonal to assets, you may also want to explore: Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures 2"
  },
  {
    "url": "https://developer.atlan.com/sdks/scala/",
    "content": "Scala SDK Â¶ Obtain the SDK Â¶ For Scala, you can reuse the existing Java SDK as-is. It is available on Maven Central, ready to be included in your project: sbt build.sbt 1 2 3 4 5 6 7 8 9 10 11 12 13 name := \"Main\" version := \"1.0\" scalaVersion := \"2.13.14\" lazy val main = project . in ( file ( \".\" )) . settings ( name := \"Main\" , libraryDependencies ++= Seq ( \"com.atlan\" % \"atlan-java\" % \"1.13.0\" , // (1) \"com.typesafe.scala-logging\" %% \"scala-logging\" % \"3.9.5\" // (2) ) ) Include the latest version of the Java SDK in your project as a dependency (replace the version number with the latest version indicated in the badges above). The Java SDK uses slf4j for logging purposes. You can include the scala-logging utility as a simple binding mechanism to send any logging information out to your console (standard out). Configure the SDK Â¶ There are two ways to configure the SDK: Using environment variables Â¶ ATLAN_API_KEY should be given your Atlan API token , for authentication ( don't forget to assign one or more personas to the API token to give access to existing assets! ) ATLAN_BASE_URL should be given your Atlan URL (for example, https://tenant.atlan.com ) Here's an example of setting those environment variables: Set environment variables export ATLAN_BASE_URL=https://tenant.atlan.com export ATLAN_API_KEY=\"...\" AtlanLiveTest.scala 1 2 3 4 5 6 7 8 import com . atlan . AtlanClient import scala . util . Using object Main extends App { Using ( new AtlanClient ()) { client => // Do something with the client } } On client creation Â¶ If you prefer to not use environment variables, you can do the following: AtlanLiveTest.scala 1 2 3 4 5 6 7 8 9 10 11 import com . atlan . AtlanClient import scala . util . Using object Main extends App { Using ( new AtlanClient ( \"https://tenant.atlan.com\" , \"...\" ) ) { client => // Do something with the client } } What's next? Â¶ Delve into more detailed examples: Common tasks Common operations on assets, that are available across all assets. Discover actions Asset-specific Operations that are specific to certain assets. Focus on a specific kind of asset Governance structures Operations dealing with governance structures, rather than assets. Manage governance structures Samples Real code samples our customers use to solve particular use cases. Review live samples Searching Delve deep into searching and aggregating metadata. Learn more about searching Events Delve deep into the details of the events Atlan triggers. Learn more about events Error-handling Â¶ The SDK defines checked exceptions for the following categories of error: Exception Description ApiConnectionException Errors when the SDK is unable to connect to the API, for example due to a lack of network access or timeouts. AuthenticationException Errors when the API token configured for the SDK is invalid or expired. ConflictException Errors when there is some conflict with an existing asset and the operation cannot be completed as a result. InvalidRequestException Errors when the request sent to Atlan does not match its expectations. If you are using the built-in methods like toCreate() and toUpdate() this exception should be treated as a bug in the SDK. (These operations take responsibility for avoiding this error.) LogicException Errors where some assumption made in the SDK's code is proven incorrect. If ever raised, they should be reported as bugs against the SDK. NotFoundException Errors when the requested resource or asset does not exist in Atlan. PermissionException Errors when the API token used by the SDK does not have permission to access a resource or carry out an operation on a specific asset . RateLimitException Errors when the Atlan server is being overwhelmed by requests. A given API call could fail due to all of the errors above. So these all extend a generic AtlanException checked exception, and all API operations throw AtlanException . Example For example, when creating a connection there is an asynchronous process that grants permissions to the admins of that connection. So there can be a slight delay between creating the connection and being permitted to do any operations with the connection. During that delay, any attempt to interact with the connection will result in a PermissionException , even if your API token was used to create connection in the first place. Another example you may occasionally hit is some network issue that causes your connection to Atlan to be interrupted. In these cases, an ApiConnectionException will be raised. Don't worry, the SDK retries automatically While these are useful to know for detecting issues, the SDK automatically retries on such problems. Advanced configuration Â¶ Atlan is a distributed, cloud-native application, where network problems can arise. These advanced configuration options allow you to optimize how the SDK handles such ephemeral problems. Logging Â¶ The SDK uses slf4j to be logging framework-agnostic. You can therefore configure your own preferred logging framework: Log4j2 build.sbt 9 10 11 12 13 14 15 libraryDependencies ++= Seq ( \"com.atlan\" % \"atlan-java\" % \"1.12.10\" , \"com.typesafe.scala-logging\" %% \"scala-logging\" % \"3.9.5\" , \"org.apache.logging.log4j\" % \"log4j-core\" % \"2.22.0\" % Runtime , // (1) \"org.apache.logging.log4j\" % \"log4j-slf4j2-impl\" % \"2.22.0\" % Runtime , \"org.apache.logging.log4j\" %% \"log4j-api-scala\" % \"13.1.0\" ) Add log4j2 bindings. src/main/resources/log4j2.xml <?xml version=\"1.0\" encoding=\"UTF-8\"?> <Configuration status= \"WARN\" > <Appenders> <Console name= \"ConsoleAppender\" target= \"SYSTEM_OUT\" > <PatternLayout> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </PatternLayout> </Console> <File name= \"FileAppender\" fileName= \"tmp/debug.log\" append= \"false\" > <PatternLayout> <pattern> %d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n </pattern> </PatternLayout> </File> </Appenders> <Loggers> <Root level= \"DEBUG\" > <AppenderRef ref= \"ConsoleAppender\" level= \"INFO\" /> <AppenderRef ref= \"FileAppender\" /> </Root> <Logger name= \"com.atlan\" level= \"DEBUG\" /> </Loggers> </Configuration> Retries Â¶ The SDK handles automatically retrying your requests when it detects certain problems: When an ApiConnectionException occurs that is caused by an underlying ConnectException or SocketTimeoutException . When there is a 403 response indicating that permission for an operation is not (yet) available. When there is a 500 response indicating that something went wrong on the server side. More details on how they work If any request encounters one of these problems, it will be retried. Before each retry, the SDK will apply a delay using: An exponential backoff (starting from 500ms) A jitter (in the range of 75-100% of the backoff delay) Each retry will be at least 500ms, and at most 5s. (Currently these values are not configurable.) For each request that encounters any of these problems, only up to a maximum number of retries will be attempted. (This is set to 3 by default.) You can configure the maximum number of retries globally using setMaxNetworkRetries() on a client. Set this to an integer: Configure the maximum number of retries Using ( new AtlanClient ()) { client => client . setMaxNetworkRetries ( 10 ) } Timeouts Â¶ The SDK will only wait so long for a response before assuming a network problem has occurred and the request should be timed out. By default, this is set to 80 seconds. You can configure the maximum time the SDK will wait before timing out a request using setReadTimeout() on a client. Set this to an integer giving the number of milliseconds before timing out: Configure the maximum time to wait before timing out Using ( new AtlanClient ()) { client => client . setReadTimeout ( 120 * 1000 ) // (1)! } Remember this must be given in milliseconds. This example sets the timeout to 2 minutes (120 seconds * 1000 milliseconds). Multi-tenant connectivity Â¶ Since version 0.9.0, the Java SDK supports connecting to multiple tenants. From version 4.0.0 onwards you can create any number of clients against any number of different tenants, since every operation that interacts with a tenant now explicitly requires a client to be provided to it: Create a client 1 2 3 Using ( new AtlanClient ( \"https://tenant.atlan.com\" )) { client => // (1)! client . setApiToken ( \"...\" ) } Constructing a new client with a different tenant's URL is sufficient to create connectivity to that other tenant. You can also (optionally) provide a second argument to directly give the API token for the tenant. Use a specific client 1 2 3 4 5 6 7 8 9 10 11 12 Using ( new AtlanClient ()) { client => // (1)! val term = GlossaryTerm . creator ( // (1) \"Example Term\" , \"836830be-5a11-4094-8346-002e0320684f\" , null ) . build () client . assets . save ( term ) // (2) client . assets . save ( term , RequestOptions . from ( client ). maxNetworkRetries ( 10 ). build ()) // (3) term . save ( client ) // (4) } Create an object as usual. You can access the operations for assets directly on the client, under client.assets . These will generally give you the most flexibility â€” they can handle multiple objects at a time and allow overrides. Every operation on the client itself has a variant with an (optional) final argument through which you can override settings like retry limits or timeouts for this single request. You can use the from(client) factory method to initialize the request options with all the settings of your client, and then you only need to chain on those you want to override for this particular request. Alternatively, you can pass the client to the operation on the object itself. Limit the number of clients to those you must have Each client you create maintains its own independent copy of various caches. So the more clients you have, the more resources your code will consume. For this reason, we recommended limiting the number of clients you create to the bare minimum you require â€” ideally just a single client per tenant. Using a proxy Â¶ To use the Java SDK with a proxy, you need to send in some additional parameters when running any java ... command. These are described in detail in the Java documentation , but are summarized here for simplicity: HTTPS SOCKS https.proxyHost should be set to the hostname for your HTTPS proxy https.proxyPort should be set to the port for your HTTPS proxy (default being 443) Run command using an HTTPS proxy 1 java -Dhttps.proxyHost = hostname -Dhttps.proxyPort = 8080 com.atlan.samples.SomeClassToRun socksProxyHost should be set to the hostname for your SOCKS proxy socksProxyPort should be set to the port for your SOCKS proxy (default being 1080) Run command using a SOCKS proxy 1 java -DsocksProxyHost = hostname -DsocksProxyPort = 8080 com.atlan.samples.SomeClassToRun Providing credentials to the proxy In either case, if you need to authenticate to your proxy, you will need to wrap whatever code you want to run to set up these credentials using something like the following: Authenticate to proxy 1 2 3 4 5 6 val pa = PasswordAuthentication ( // (1) \"username\" , // (2) \"password\" . toCharArray ()) // (3) Using ( new AtlanClient ()) { client => client . setProxyCredential ( pa ) // (4) } You need to create a built-in Java PasswordAuthentication object. Provide your username as the first argument. ...and your password as the second argument, as a char[] . (Of course, you should not hard-code your password in your code itself, but rather pull it from elsewhere.) Then use setProxyCredential() to pass this PasswordAuthentication object to the Atlan client, before any of the rest of the code will execute. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/access/personas/",
    "content": "/api/meta/entity/bulk (DELETE) /api/meta/entity/bulk (POST) /api/meta/search/indexsearch (POST) Personas Â¶ Personas are a way of curating assets for a group of users. List personas Â¶ 0.0.12 1.4.0 4.0.0 To retrieve a listing of personas, run a search and page the results: Java Python Kotlin Go Raw REST API List personas 1 2 3 4 5 6 Persona . select ( client ) // (1) . stream () // (2) . filter ( a -> a instanceof Persona ) // (3) . forEach ( p -> { // (4) log . info ( \"Persona: {}\" , p ); }); To start building up a query specifically for personas, you can use the select() convenience method on Persona itself. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. (Optional) You can do any other operations you might do on a stream, such as filtering the results to ensure they are of a certain type. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. List personas 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona from pyatlan.model.fluent_search import CompoundQuery , FluentSearch client = AtlanClient () search_request = ( FluentSearch () # (1) . where ( CompoundQuery . active_assets ()) . where ( CompoundQuery . asset_type ( Persona )) # (2) ) . to_request () # (3) results = client . asset . search ( search_request ) # (4) for asset in results : # (5) if isinstance ( asset , Persona ): # Do something with the Persona Begin building up a query combining multiple conditions. Ensure that we include only objects of type Persona . Build this query into a new search request. Run the search. Page through the results (each asset in the results will be a persona). List personas 1 2 3 4 5 6 Persona . select ( client ) // (1) . stream () // (2) . filter { it is Persona } // (3) . forEach { // (4) log . info { \"Persona: $ it \" } } To start building up a query specifically for personas, you can use the select() convenience method on Persona itself. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. (Optional) You can do any other operations you might do on a stream, such as filtering the results to ensure they are of a certain type. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. List personas 1 2 3 4 5 6 7 8 9 10 11 12 13 response , atlanErr := assets . NewFluentSearch (). // (1) PageSizes ( 20 ). ActiveAssets (). AssetType ( \"Persona\" ). // (2) Execute () // (3) if atlanErr != nil { fmt . Println ( \"Error:\" , atlanErr ) } for _ , entity := range response [ 0 ]. Entities { // (4) if entity . TypeName != nil && * entity . TypeName == \"Persona\" { // Do something with the Persona } } Begin building up a query combining multiple conditions. Ensure that we include only objects of type Persona . Run the search. Page through the results (each asset in the results will be a persona). POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"dsl\" : { \"query\" : { // (1) \"bool\" : { \"filter\" : [ { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"Persona\" // (2) } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Begin building up a query combining multiple conditions. Ensure that we include only objects of type Persona . Create a persona Â¶ 0.0.12 2.0.0 4.0.0 To create a new persona: Java Python Kotlin Go Raw REST API Create a persona 1 2 3 Persona toCreate = Persona . creator ( \"Data Assets\" ). build (); // (1) AssetMutationResponse response = toCreate . save ( client ); // (2) Persona persona = ( Persona ) response . getCreatedAssets (). get ( 0 ); // (3) Like other builder patterns in the SDK, the creator() method ensures all required information is provided for the persona. To create the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then retrieve the resulting details of the created persona from the response (you may of course want to do some type checking first). Create a persona 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona client = AtlanClient () to_create = Persona . creator ( name = \"Data Assets\" ) # (1) response = client . asset . save ( to_create ) # (2) p = response . assets_created ( asset_type = Persona )[ 0 ] # (3) Like other builder patterns in the SDK, the create() method ensures all required information is provided for the persona. To create the persona in Atlan, call the save() method against the object you've built. You can then retrieve the resulting details of the created persona from the response (you may of course want to do some type checking first). Create a persona 1 2 3 val toCreate = Persona . creator ( \"Data Assets\" ). build () // (1) val response = toCreate . save ( client ) // (2) val persona = response . createdAssets [ 0 ] as Persona // (3) Like other builder patterns in the SDK, the creator() method ensures all required information is provided for the persona. To create the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then retrieve the resulting details of the created persona from the response (you may of course want to do some type checking first). Create a persona 1 2 3 4 5 6 7 8 9 10 11 toCreate := & assets . Persona {} toCreate . Creator ( \"Data Assets\" ) // (1) response , atlanErr := assets . Save ( toCreate ) // (2) if atlanErr != nil { fmt . Println ( \"Error:\" , atlanErr ) } else { for _ , entity := range response . MutatedEntities . CREATE { // (3) fmt . Println ( \"Persona ID:\" , entity . Guid , \"Display Text:\" , entity . DisplayText ) // Do something with the Persona } } Like other builder patterns in the SDK, the Creator() method ensures all required information is provided for the persona. To create the persona in Atlan, call the Save() method against the object you've built. You can then retrieve the resulting details of the created persona from the response (you may of course want to do some type checking first). POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"entities\" : [ // (1) { \"typeName\" : \"Persona\" , // (2) \"attributes\" : { \"displayName\" : \"Data Assets\" , // (3) \"isAccessControlEnabled\" : true , // (4) \"qualifiedName\" : \"Data Assets\" , // (5) \"name\" : \"Data Assets\" // (6) } } ] } Wrap the persona definition in an entities array. Ensure the type of each nested object is exactly Persona . Use the displayName to provide the name for the persona as you want it to appear in the UI. Ensure you explicitly set the access control to enabled when creating it. You must provide a qualifiedName for the persona, although this will be generated and overwritten by the back-end You must provide a name for the persona, although this will also be normalized by the back-end so will be slightly different once created. Retrieve a persona Â¶ 0.0.12 1.4.0 4.0.0 To retrieve a persona by its name: Java Python Kotlin Go Raw REST API Retrieve a persona 1 List < Persona > list = Persona . findByName ( client , \"Data Assets\" ); // (1) The findByName() method handles searching for the persona based on its name, which could therefore return more than one result. You can also (optionally) provide a second parameter with a list of attributes to retrieve for each persona. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve a persona 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () result = client . asset . find_personas_by_name ( \"Data Assets\" ) # (1) The asset.find_personas_by_name() method handles searching for the persona based on its name, which could therefore return more than one result. Retrieve a persona 1 val list = Persona . findByName ( client , \"Data Assets\" ) // (1) The findByName() method handles searching for the persona based on its name, which could therefore return more than one result. You can also (optionally) provide a second parameter with a list of attributes to retrieve for each persona. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve a persona 1 response , atlanErr := assets . FindPersonasByName ( \"Data Assets\" ) // (1) The assets.FindPersonasByName() method handles searching for the persona based on its name, which could therefore return more than one result. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"dsl\" : { \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"Persona\" // (1) } } }, { \"term\" : { \"name.keyword\" : { \"value\" : \"Data Assets\" // (2) } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Define the search to include results for a type exactly matching Persona , and... ... with the exact name of the persona you want to find. Update a persona Â¶ 0.0.12 2.0.0 4.0.0 To update a persona: Java Python Kotlin Go Raw REST API Update a persona 1 2 3 4 5 6 7 Persona toUpdate = Persona . updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) true ) // (4) . description ( \"Now with a description!\" ) // (5) . build (); AssetMutationResponse response = toUpdate . save ( client ); // (6) Use the updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then chain on any other updates, such as changing the description of the persona. To update the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update a persona 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona client = AtlanClient () to_update = Persona . updater ( # (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , # (2) \"Data Assets\" , # (3) True # (4) ) to_update . description = \"Now with a description!\" # (5) response = client . asset . save ( to_update ) # (7) Use the updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then add on any other updates, such as changing the description of the persona. To update the persona in Atlan, call the save() method with the object you've built. Update a persona 1 2 3 4 5 6 7 val toUpdate = Persona . updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) true ) // (4) . description ( \"Now with a description!\" ) // (5) . build () val response = toUpdate . save ( client ) // (6) Use the updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then chain on any other updates, such as changing the description of the persona. To update the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update a persona 1 2 3 4 5 6 7 8 9 toUpdate := & assets . Persona {} toUpdate . Updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) true , // (4) ) description := \"Now with a description \" toUpdate . Description = & description // (5) response , atlanErr := assets . Save ( toUpdate ) // (6) Use the updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then add on any other updates, such as changing the description of the persona. To update the persona in Atlan, call the Save() method with the object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"entities\" : [ // (1) { \"typeName\" : \"Persona\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (3) \"name\" : \"Data Assets\" // (4) \"isAccessControlEnabled\" : true , // (5) \"description\" : \"Now with a description!\" , // (6) } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name Persona . You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then add on any other updates, such as changing the description of the persona. Delete a persona Â¶ 0.0.12 1.4.0 4.0.0 To permanently delete a persona: Java Python Kotlin Go Raw REST API Delete a persona 1 Persona . purge ( client , \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" ); // (1) To permanently delete a persona in Atlan, call the purge() method with the GUID of the persona. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a persona 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . asset . purge_by_guid ( \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" ) # (1) To permanently delete a persona in Atlan, call the asset.purge_by_guid() method with the GUID of the persona. Delete a persona 1 Persona . purge ( client , \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" ) // (1) To permanently delete a persona in Atlan, call the purge() method with the GUID of the persona. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a persona 1 assets . PurgeByGuid ([] string { \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" }) // (1) To permanently delete a persona in Atlan, call the assets.PurgeByGuid() method with the GUID of the persona. DELETE /api/meta/entity/bulk?guid=67e08ab7-9688-40bc-ae4a-da2bc06b1588&deleteType=PURGE 1 // (1) All the details for deleting the persona are specified in the URL directly. Note that you must provide the GUID of the persona to delete it. Activate or deactivate a persona Â¶ 0.0.12 1.4.0 4.0.0 Alternatively, if you only want to temporarily deactivate a persona: Java Python Kotlin Go Raw REST API Deactivate a persona 1 2 3 4 5 6 Persona toUpdate = Persona . updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) false ) // (4) . build (); AssetMutationResponse response = toUpdate . save ( client ); // (5) Use the updater() method to update the persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. Setting this to false will deactivate the persona, while setting it to true will activate the persona. To then apply that activation / deactivation to the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the state in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Deactivate a persona 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona client = AtlanClient () to_update = Persona . updater ( # (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , # (2) \"Data Assets\" , # (3) False # (4) ) response = client . asset . save ( to_update ) # (5) Use the updater() method to update the persona. You must provide the qualified_name of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. Setting this to False will deactivate the persona, while setting it to True will activate the persona. To then apply that activation / deactivation to the persona in Atlan, call the save() method with the object you've built. Deactivate a persona 1 2 3 4 5 6 val toUpdate = Persona . updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) false ) // (4) . build () val response = toUpdate . save ( client ) // (5) Use the updater() method to update the persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. Setting this to false will deactivate the persona, while setting it to true will activate the persona. To then apply that activation / deactivation to the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the state in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Deactivate a persona 1 2 3 4 5 6 7 toUpdate := & assets . Persona {} toUpdate . Updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) false , // (4) ) response , atlanErr := assets . Save ( toUpdate ) // (5) Use the Updater() method to update the persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. Setting this to False will deactivate the persona, while setting it to True will activate the persona. To then apply that activation / deactivation to the persona in Atlan, call the Save() method with the object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ // (1) { \"typeName\" : \"Persona\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (3) \"name\" : \"Data Assets\" // (4) \"isAccessControlEnabled\" : false // (5) } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name Persona . You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. Setting this to false will deactivate the persona, while setting it to true will activate the persona. Add subjects to a persona Â¶ 0.0.12 1.4.0 4.0.0 Similarly, adding subjects to a persona is a matter of updating the persona: Java Python Kotlin Go Raw REST API Add subjects to a persona 1 2 3 4 5 6 7 8 9 10 Persona toUpdate = Persona . updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) false ) // (4) . personaGroup ( \"group1\" ) // (5) . personaGroup ( \"group2\" ) . personaUser ( \"jsmith\" ) // (6) . personaUser ( \"jdoe\" ) . build (); AssetMutationResponse response = toUpdate . save ( client ); // (7) Use the updater() method to update the persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then chain any number of updates to the personaGroup() property. These should be internal names of groups that you want to be controlled through the persona's policies. Similarly, you can chain any number of updates to the personaUser() property. These should be usernames of users that you want to be controlled through the persona's policies. To then apply those membership updates to the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add subjects to a persona 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona client = AtlanClient () to_update = Persona . updater ( # (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , # (2) \"Data Assets\" , # (3) True # (4) ) to_update . persona_groups = [ \"group1\" , \"group2\" ] # (5) to_update . persona_users = [ \"jsmith\" , \"jdoe\" ] # (6) response = client . asset . save ( to_update ) # (7) Use the updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then add any number of groups to the persona_groups property. These should be internal names of groups that you want to be controlled through the persona's policies. Similarly, you can add any number of users to the persona_users property. These should be usernames of users that you want to be controlled through the persona's policies. To then apply those membership updates to the persona in Atlan, call the save() method against the object you've built. Add subjects to a persona 1 2 3 4 5 6 7 8 9 10 val toUpdate = Persona . updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) false ) // (4) . personaGroup ( \"group1\" ) // (5) . personaGroup ( \"group2\" ) . personaUser ( \"jsmith\" ) // (6) . personaUser ( \"jdoe\" ) . build () val response = toUpdate . save ( client ) // (7) Use the updater() method to update the persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then chain any number of updates to the personaGroup() property. These should be internal names of groups that you want to be controlled through the persona's policies. Similarly, you can chain any number of updates to the personaUser() property. These should be usernames of users that you want to be controlled through the persona's policies. To then apply those membership updates to the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add subjects to a persona 1 2 3 4 5 6 7 8 9 toUpdate := & assets . Persona {} toUpdate . Updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) false , // (4) ) toUpdate . PersonaGroups = & [] string { \"group1\" , \"group2\" } // (5) toUpdate . PersonaUsers = & [] string { \"jsmith\" , \"jdoe\" } // (6) response , atlanErr := assets . Save ( toUpdate ) // (7) Use the Updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then add any number of groups to the PersonaGroups property. These should be internal names of groups that you want to be controlled through the persona's policies. Similarly, you can add any number of users to the PersonaUsers property. These should be usernames of users that you want to be controlled through the persona's policies. To then apply those membership updates to the persona in Atlan, call the Save() method against the object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"entities\" : [ // (1) { \"typeName\" : \"Persona\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (3) \"name\" : \"Data Assets\" // (4) \"isAccessControlEnabled\" : false , // (5) \"personaGroups\" : [ \"group1\" , \"group2\" ], // (6) \"personaUsers\" : [ \"jsmith\" , \"jdoe\" ] // (7) } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name Persona . You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then add any number of groups to the personaGroups property. These should be internal names of groups that you want to be controlled through the persona's policies. Similarly, you can add any number of users to the personaUsers property. These should be usernames of users that you want to be controlled through the persona's policies. Add policies to a persona Â¶ Do not add policies in bulk Be careful to only add policies one-by-one to a persona. While the SDKs will allow you to add them in bulk, currently this results in a persona where only the final policy in the batch is active at the end of the operation. API token must be a connection admin To manage policies for a connection, the API token must be a connection admin on that connection. When you create a connection using an API token, the API token is automatically made a connection admin; however, for any other connection you must carry out extra steps to make the API token a connection admin . Add a metadata policy Â¶ 0.0.12 7.0.0 4.0.0 To add a metadata policy to a persona: Java Python Kotlin Go Raw REST API Add metadata policy to persona 1 2 3 4 5 6 7 8 9 AuthPolicy metadata = Persona . createMetadataPolicy ( // (1) \"Simple read access\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) AuthPolicyType . ALLOW , // (4) Set . of ( PersonaMetadataAction . READ ), // (5) \"default/snowflake/1234567890\" , // (6) Set . of ( \"entity:default/snowflake/1234567890\" )) // (7) . build (); AssetMutationResponse response = metadata . save ( client ); // (8) Use the createMetadataPolicy() method to start building a metadata policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. To include all permissions If you want to include all permissions, you can simply use Arrays.asList(PersonaMetadataAction.values()) . Specify the qualifiedName of the connection whose assets this policy should control. Specify the set of qualifiedName prefixes for the assets this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualifiedName of the connection itself. To then add the policy to the persona in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add metadata policy to persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona from pyatlan.model.enums import AuthPolicyType , PersonaMetadataAction client = AtlanClient () metadata = Persona . create_metadata_policy ( # (1) client = client , # (2) name = \"Simple read access\" , # (3) persona_id = \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , # (4) policy_type = AuthPolicyType . ALLOW , # (5) actions = { PersonaMetadataAction . READ }, # (6) connection_qualified_name = \"default/snowflake/1234567890\" , # (7) resources = { \"entity:default/snowflake/1234567890\" }, # (8) ) response = client . asset . save ( metadata ) # (9) Use the create_metadata_policy() method to start building a metadata policy with the minimal required information. You must provide a client instance. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. Specify the qualified_name of the connection whose assets this policy should control. Specify the set of qualified_name prefixes for the assets this policy should control. Each qualified_name should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualified_name of the connection itself. To then add the policy to the persona in Atlan, call the save() method with the policy object you've built. Add metadata policy to persona 1 2 3 4 5 6 7 8 9 val metadata = Persona . createMetadataPolicy ( // (1) \"Simple read access\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) AuthPolicyType . ALLOW , // (4) setOf ( PersonaMetadataAction . READ ), // (5) \"default/snowflake/1234567890\" , // (6) setOf ( \"entity:default/snowflake/1234567890\" )) // (7) . build () val response = metadata . save ( client ) // (8) Use the createMetadataPolicy() method to start building a metadata policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. To include all permissions If you want to include all permissions, you can simply use PersonaMetadataAction.values().toList() . Specify the qualifiedName of the connection whose assets this policy should control. Specify the set of qualifiedName prefixes for the assets this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualifiedName of the connection itself. To then add the policy to the persona in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add metadata policy to persona 1 2 3 4 5 6 7 8 9 10 Persona := & assets . Persona {} metadata , _ := Persona . CreateMetadataPolicy ( // (1) \"Simple read access\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) atlan . AuthPolicyTypeAllow , // (4) [] atlan . PersonaMetadataAction { atlan . PersonaMetadataActionRead }, // (5) \"default/snowflake/1234567890\" , // (6) [] string { \"entity:default/snowflake/1234567890\" }, // (7) ) response , atlanErr := assets . Save ( metadata ) // (8) Use the CreateMetadataPolicy() method to start building a metadata policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. Specify the qualifiedName of the connection whose assets this policy should control. Specify the set of qualifiedName prefixes for the assets this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualifiedName of the connection itself. To then add the policy to the persona in Atlan, call the save() method with the policy object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"entities\" : [ // (1) { \"typeName\" : \"AuthPolicy\" , // (2) \"attributes\" : { \"policySubCategory\" : \"metadata\" , // (3) \"policyCategory\" : \"persona\" , // (4) \"policyType\" : \"allow\" , // (5) \"policyServiceName\" : \"atlas\" , // (6) \"connectionQualifiedName\" : \"default/snowflake/1234567890\" , // (7) \"policyResources\" : [ \"entity:default/snowflake/1234567890\" // (8) ], \"name\" : \"Simple read access\" , // (9) \"qualifiedName\" : \"Simple read access\" , // (10) \"policyActions\" : [ \"persona-asset-read\" // (11) ], \"accessControl\" : { // (12) \"typeName\" : \"Persona\" , // (13) \"guid\" : \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" // (14) }, \"policyResourceCategory\" : \"CUSTOM\" // (15) } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name AuthPolicy . You must use a policy subcategory of metadata . You must use a policy category of persona . Specify the type of policy (granting or denying the actions specified next). You must use a policy service name of atlas . Specify the qualifiedName of the connection whose assets will be controlled by this policy. Specify the set of qualifiedName prefixes for the assets this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualifiedName of the connection itself. You must give the policy a name. You must give the policy itself a qualifiedName , although this will be overwritten by a generated value by the back-end. Specify the set of permissions you want to allow (or deny) in this policy. To review available permissions To review the available permissions, see the SDKs â€” for example, the PersonaMetadataAction enum in the Java SDK. Use an embedded accessControl object to define the persona to attach this policy to. The embedded type name of the accessControl object must be exactly Persona . You must provide the GUID of the persona to attach this policy to. You must set the policy resource category to CUSTOM . Add a data policy Â¶ 0.0.12 7.0.0 4.0.0 To add a data policy to a persona: Java Python Kotlin Go Raw REST API Add data policy to persona 1 2 3 4 5 6 7 8 AuthPolicy data = Persona . createDataPolicy ( // (1) \"Allow access to data\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) AuthPolicyType . ALLOW , // (4) \"default/snowflake/1234567890\" , // (5) Set . of ( \"entity:default/snowflake/1234567890\" )) // (6) . build (); AssetMutationResponse response = data . save ( client ); // (7) Use the createDataPolicy() method to start building a data policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying access to the data of the resources specified next). Specify the qualifiedName of the connection whose assets this policy should control. Specify the set of qualifiedName prefixes for the assets this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualifiedName of the connection itself. To then add the policy to the persona in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add data policy to persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona from pyatlan.model.enums import AuthPolicyType client = AtlanClient () data = Persona . create_data_policy ( # (1) client = client , # (2) name = \"Allow access to data\" , # (3) persona_id = \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , # (4) policy_type = AuthPolicyType . ALLOW , # (5) connection_qualified_name = \"default/snowflake/1234567890\" , # (6) resources = { \"entity:default/snowflake/1234567890\" }, # (7) ) response = client . asset . save ( data ) # (8) Use the create_data_policy() method to start building a data policy with the minimal required information. You must provide a client instance. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying access to the data of the resources specified next). Specify the qualifiedName of the connection whose assets this policy should control. Specify the set of qualified_name prefixes for the assets this policy should control. Each qualified_name should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualified_name of the connection itself. To then add the policy to the persona in Atlan, call the save() method with the policy object you've built. Add data policy to persona 1 2 3 4 5 6 7 8 val data = Persona . createDataPolicy ( // (1) \"Allow access to data\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) AuthPolicyType . ALLOW , // (4) \"default/snowflake/1234567890\" , // (5) setOf ( \"entity:default/snowflake/1234567890\" )) // (6) . build () val response = data . save ( client ) // (7) Use the createDataPolicy() method to start building a data policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying access to the data of the resources specified next). Specify the qualifiedName of the connection whose assets this policy should control. Specify the set of qualifiedName prefixes for the assets this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualifiedName of the connection itself. To then add the policy to the persona in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add data policy to persona 1 2 3 4 5 6 7 8 9 Persona := & assets . Persona {} data , _ := Persona . CreateDataPolicy ( // (1) \"Allow access to data\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) atlan . AuthPolicyTypeAllow , // (4) \"default/snowflake/1234567890\" , // (5) [] string { \"entity:default/snowflake/1234567890\" }, // (6) ) response , atlanErr := assets . Save ( data ) // (7) Use the CreateDataPolicy() method to start building a data policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying access to the data of the resources specified next). Specify the qualifiedName of the connection whose assets this policy should control. Specify the set of qualifiedName prefixes for the assets this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualifiedName of the connection itself. To then add the policy to the persona in Atlan, call the Save() method with the policy object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \"entities\" : [ // (1) { \"typeName\" : \"AuthPolicy\" , // (2) \"attributes\" : { \"policySubCategory\" : \"data\" , // (3) \"policyCategory\" : \"persona\" , // (4) \"policyType\" : \"allow\" , // (5) \"policyServiceName\" : \"heka\" , // (6) \"connectionQualifiedName\" : \"default/snowflake/1234567890\" , // (7) \"policyResources\" : [ \"entity-type:*\" , // (8) \"entity:default/snowflake/1234567890\" // (9) ], \"name\" : \"Allow access to data\" , // (10) \"qualifiedName\" : \"Allow access to data\" , // (11) \"policyActions\" : [ \"select\" // (12) ], \"accessControl\" : { // (13) \"typeName\" : \"Persona\" , // (14) \"guid\" : \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" // (15) }, \"policyResourceCategory\" : \"ENTITY\" // (16) } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name AuthPolicy . You must use a policy subcategory of data . You must use a policy category of persona . Specify the type of policy (granting or denying the actions specified next). You must use a policy service name of heka . Specify the qualifiedName of the connection whose assets will be controlled by this policy. You must include a resource of entity-type:* in the list of resources. Specify the set of qualifiedName prefixes for the assets this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all assets within a connection, this can simply be the qualifiedName of the connection itself. You must give the policy a name. You must give the policy itself a qualifiedName , although this will be overwritten by a generated value by the back-end. Specify the set of permissions you want to allow (or deny) in this policy. A data policy for a persona can only allow or deny select permissions. Use an embedded accessControl object to define the persona to attach this policy to. The embedded type name of the accessControl object must be exactly Persona . You must provide the GUID of the persona to attach this policy to. You must set the policy resource category to ENTITY . Add a glossary policy Â¶ 0.0.12 1.4.0 4.0.0 To add a glossary policy to a persona: Java Python Kotlin Go Raw REST API Add glossary policy to persona 1 2 3 4 5 6 7 8 AuthPolicy glossary = Persona . createGlossaryPolicy ( // (1) \"All glossaries\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) AuthPolicyType . ALLOW , // (4) Set . of ( PersonaGlossaryAction . CREATE , PersonaGlossaryAction . UPDATE ), // (5) Set . of ( \"entity:OpU9a9kG825gAqpamXugf\" )) // (6) . build (); AssetMutationResponse response = glossary . save ( client ); // (7) Use the createGlossaryPolicy() method to start building a glossary policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. To include all permissions If you want to include all permissions, you can simply use Arrays.asList(PersonaGlossaryAction.values()) . Specify the set of qualifiedName s of glossaries this policy should control. Each qualifiedName should itself be prefixed with entity: . To then add the policy to the persona in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add glossary policy to persona 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona from pyatlan.model.enums import AuthPolicyType , PersonaGlossaryAction client = AtlanClient () glossary = Persona . create_glossary_policy ( # (1) name = \"All glossaries\" , # (2) persona_id = \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , # (3) policy_type = AuthPolicyType . ALLOW , # (4) actions = { PersonaGlossaryAction . CREATE , PersonaGlossaryAction . UPDATE }, # (5) resources = { \"entity:OpU9a9kG825gAqpamXugf\" }, # (6) ) response = client . asset . save ( glossary ) # (7) Use the create_glossary_policy() method to start building a glossary policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. Specify the set of qualified_name s of glossaries this policy should control. Each qualified_name should itself be prefixed with entity: . To then add the policy to the persona in Atlan, call the save() method with the policy object you've built. Add glossary policy to persona 1 2 3 4 5 6 7 8 val glossary = Persona . createGlossaryPolicy ( // (1) \"All glossaries\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) AuthPolicyType . ALLOW , // (4) setOf ( PersonaGlossaryAction . CREATE , PersonaGlossaryAction . UPDATE ), // (5) setOf ( \"entity:OpU9a9kG825gAqpamXugf\" )) // (6) . build () val response = glossary . save ( client ) // (7) Use the createGlossaryPolicy() method to start building a glossary policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. To include all permissions If you want to include all permissions, you can simply use PersonaGlossaryAction.values().toList() . Specify the set of qualifiedName s of glossaries this policy should control. Each qualifiedName should itself be prefixed with entity: . To then add the policy to the persona in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add glossary policy to persona 1 2 3 4 5 6 7 8 9 Persona := & assets . Persona {} glossary , _ := Persona . CreateGlossaryPolicy ( // (1) \"All glossaries\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) atlan . AuthPolicyTypeAllow , // (4) [] atlan . PersonaGlossaryAction { atlan . PersonaGlossaryActionCreate , atlan . PersonaGlossaryActionUpdate }, // (5) [] string { \"entity:OpU9a9kG825gAqpamXugf\" }, // (6) ) response , err := assets . Save ( glossary ) // (7) Use the CreateGlossaryPolicy() method to start building a glossary policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. Specify the set of qualifiedName s of glossaries this policy should control. Each qualifiedName should itself be prefixed with entity: . To then add the policy to the persona in Atlan, call the Save() method with the policy object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"entities\" : [ // (1) { \"typeName\" : \"AuthPolicy\" , // (2) \"attributes\" : { \"policySubCategory\" : \"glossary\" , // (3) \"policyCategory\" : \"persona\" , // (4) \"policyType\" : \"allow\" , // (5) \"policyServiceName\" : \"atlas\" , // (6) \"policyResources\" : [ \"entity:OpU9a9kG825gAqpamXugf\" // (7) ], \"name\" : \"All glossaries\" , // (8) \"qualifiedName\" : \"All glossaries\" , // (9) \"policyActions\" : [ \"persona-glossary-create\" , // (10) \"persona-glossary-update\" ], \"accessControl\" : { // (11) \"typeName\" : \"Persona\" , // (12) \"guid\" : \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" // (13) }, \"policyResourceCategory\" : \"CUSTOM\" // (14) } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name AuthPolicy . You must use a policy subcategory of glossary . You must use a policy category of persona . Specify the type of policy (granting or denying the actions specified next). You must use a policy service name of atlas . Specify the set of qualifiedName s of glossaries this policy should control. Each qualifiedName should itself be prefixed with entity: . You must give the policy a name. You must give the policy itself a qualifiedName , although this will be overwritten by a generated value by the back-end. Specify the set of permissions you want to allow (or deny) in this policy. To review available permissions To review the available permissions, see the SDKs â€” for example, the PersonaGlossaryAction enum in the Java SDK. Use an embedded accessControl object to define the persona to attach this policy to. The embedded type name of the accessControl object must be exactly Persona . You must provide the GUID of the persona to attach this policy to. You must set the policy resource category to CUSTOM . Add a domain policy Â¶ 0.0.12 1.7.0 4.0.0 To add a domain policy to a persona: Java Python Kotlin Go Raw REST API Add domain policy to persona 1 2 3 4 5 6 7 AuthPolicy domain = Persona . createDomainPolicy ( // (1) \"Read access to some domains\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) Set . of ( PersonaDomainAction . READ_DOMAIN , PersonaDomainAction . READ_SUBDOMAIN , PersonaDomainAction . READ_PRODUCTS ), // (4) Set . of ( \"entity:default/domain/marketing\" , \"entity:default/domain/finance\" )) // (5) . build (); AssetMutationResponse response = domain . save ( client ); // (6) Use the createDomainPolicy() method to start building a domain policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the set of permissions you want to allow in this policy. To include all permissions If you want to include all permissions, you can simply use Arrays.asList(PersonaDomainAction.values()) . Specify the set of qualifiedName s for the domains this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all domains, this can simply be a single value of entity:All domains . To then add the policy to the persona in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add domain policy to persona 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona from pyatlan.model.enums import AuthPolicyType , PersonaDomainAction client = AtlanClient () domain = Persona . create_domain_policy ( # (1) name = \"Read access to some domains\" , # (2) persona_id = \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , # (3) actions = { PersonaDomainAction . READ_DOMAIN , PersonaDomainAction . READ_SUBDOMAIN , PersonaDomainAction . READ_PRODUCTS }, # (4) resources = { \"entity:default/domain/marketing\" , \"entity:default/domain/finance\" }, # (5) ) response = client . asset . save ( domain ) # (6) Use the create_domain_policy() method to start building a domain policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the set of permissions you want to allow in this policy. Specify the set of qualified_name s for the domains this policy should control. Each qualified_name should itself be prefixed with entity: . To control all domains, this can simply be a single value of entity:All domains . To then add the policy to the persona in Atlan, call the save() method with the policy object you've built. Add domain policy to persona 1 2 3 4 5 6 7 val domain = Persona . createDomainPolicy ( // (1) \"Read access to some domains\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) setOf ( PersonaDomainAction . READ_DOMAIN , PersonaDomainAction . READ_SUBDOMAIN , PersonaDomainAction . READ_PRODUCTS ), // (4) setOf ( \"entity:default/domain/marketing\" , \"entity:default/domain/finance\" )) // (5) . build () val response = domain . save ( client ) // (6) Use the createDomainPolicy() method to start building a domain policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the set of permissions you want to allow in this policy. To include all permissions If you want to include all permissions, you can simply use PersonaDomainAction.values().toList() . Specify the set of qualifiedName s for the domains this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all domains, this can simply be a single value of entity:All domains . To then add the policy to the persona in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add domain policy to persona 1 2 3 4 5 6 7 8 Persona := & assets . Persona {} domain , _ := Persona . CreateDomainPolicy ( // (1) \"Allow access to domain\" , // (2) \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" , // (3) [] atlan . PersonaDomainAction { atlan . PersonaDomainActionRead , atlan . PersonaDomainActionReadSubdomain , atlan . PersonaDomainActionReadProducts }, // (4) [] string { \"entity:default/domain/marketing\" , \"entity:default/domain/finance\" }, // (5) ) response , err := assets . Save ( domain ) // (6) Use the CreateDomainPolicy() method to start building a domain policy with the minimal required information. You must give the policy a name. You must provide the GUID of the persona to attach this policy to. Specify the set of permissions you want to allow in this policy. Specify the set of qualifiedName s for the domains this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all domains, this can simply be a single value of entity:All domains . To then add the policy to the persona in Atlan, call the Save() method with the policy object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"entities\" : [ // (1) { \"typeName\" : \"AuthPolicy\" , // (2) \"attributes\" : { \"policySubCategory\" : \"domain\" , // (3) \"policyCategory\" : \"persona\" , // (4) \"policyType\" : \"allow\" , // (5) \"policyServiceName\" : \"atlas\" , // (6) \"policyResources\" : [ \"entity:default/domain/marketing\" , // (7) \"entity:default/domain/finance\" ], \"name\" : \"Read access to some domains\" , // (8) \"qualifiedName\" : \"Read access to some domains\" , // (9) \"policyActions\" : [ \"persona-domain-read\" , // (10) \"persona-domain-sub-domain-read\" , \"persona-domain-product-read\" ], \"accessControl\" : { // (11) \"typeName\" : \"Persona\" , // (12) \"guid\" : \"67e08ab7-9688-40bc-ae4a-da2bc06b1588\" // (13) }, \"policyResourceCategory\" : \"CUSTOM\" // (14) } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name AuthPolicy . You must use a policy subcategory of domain . You must use a policy category of persona . The type of policy should always be allow . You must use a policy service name of atlas . Specify the set of qualifiedName s for the domains this policy should control. Each qualifiedName should itself be prefixed with entity: . To control all domains, this can simply be a single value of entity:All domains . You must give the policy a name. You must give the policy itself a qualifiedName , although this will be overwritten by a generated value by the back-end. Specify the set of permissions you want to allow in this policy. To review available permissions To review the available permissions, see the SDKs â€” for example, the PersonaDomainAction enum in the Java SDK. Use an embedded accessControl object to define the persona to attach this policy to. The embedded type name of the accessControl object must be exactly Persona . You must provide the GUID of the persona to attach this policy to. You must set the policy resource category to CUSTOM . List policies in a persona Â¶ 0.0.12 1.4.0 4.0.0 To list all the policies in a persona: Java Python Kotlin Go Raw REST API List all policies in a persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Persona . select ( client ) // (1) . where ( Persona . NAME . eq ( \"Data Assets\" )) // (2) . includeOnResults ( Persona . POLICIES ) // (3) . includeOnRelations ( AuthPolicy . NAME ) // (4) . includeOnRelations ( AuthPolicy . POLICY_TYPE ) . includeOnRelations ( AuthPolicy . POLICY_RESOURCES ) . includeOnRelations ( AuthPolicy . POLICY_ACTIONS ) . stream () // (5) . filter ( a -> a instanceof Persona ) . forEach ( p -> { // (6) Set < IAuthPolicy > policies = (( Persona ) p ). getPolicies (); for ( IAuthPolicy policy : policies ) { // Do something with each policy } }); Start by selecting a persona, here using a FluentSearch-based approach. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can select the persona by whatever you like, in this example we are selecting based on its name. Include the policies for the persona as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and resources controlled by each policy. You can then directly stream the results of the search. For each result of the search (itself a Persona), you can then retrieve its policies and iterate through them. List all policies in a persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from typing import cast from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona , AuthPolicy from pyatlan.model.fluent_search import FluentSearch client = AtlanClient () request = ( FluentSearch () . where ( FluentSearch . asset_type ( Persona )) # (1) . where ( Persona . NAME . eq ( \"Data Assets\" )) # (2) . include_on_results ( Persona . POLICIES ) # (3) . include_on_relations ( AuthPolicy . NAME ) # (4) . include_on_relations ( AuthPolicy . POLICY_TYPE ) . include_on_relations ( AuthPolicy . POLICY_RESOURCES ) . include_on_relations ( AuthPolicy . POLICY_ACTIONS ) ) . to_request () # (5) response = client . asset . search ( request ) # (6) for p in response : # (7) policies = cast ( Persona , p ) . policies for policy in policies : # Do something with each policy Start by selecting a persona, here using a FluentSearch-based approach. You can select the persona by whatever you like, in this example we are selecting based on its name. Include the policies for the persona as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and resources controlled by each policy. You can then translate the FluentSearch into a search request. Run a search using the search request. For each result of the search (itself a Persona), you can then retrieve its policies and iterate through them. List all policies in a persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Persona . select ( client ) // (1) . where ( Persona . NAME . eq ( \"Data Assets\" )) // (2) . includeOnResults ( Persona . POLICIES ) // (3) . includeOnRelations ( AuthPolicy . NAME ) // (4) . includeOnRelations ( AuthPolicy . POLICY_TYPE ) . includeOnRelations ( AuthPolicy . POLICY_RESOURCES ) . includeOnRelations ( AuthPolicy . POLICY_ACTIONS ) . stream () // (5) . filter { it is Persona } . forEach { // (6) val policies = ( it as Persona ). policies for ( policy in policies ) { // Do something with each policy } } Start by selecting a persona, here using a FluentSearch-based approach. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can select the persona by whatever you like, in this example we are selecting based on its name. Include the policies for the persona as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and resources controlled by each policy. You can then directly stream the results of the search. For each result of the search (itself a Persona), you can then retrieve its policies and iterate through them. List all policies in a persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 response , atlanErr := assets . NewFluentSearch (). PageSizes ( 20 ). AssetType ( \"Persona\" ). // (1) Where ( ctx . Persona . NAME . Eq ( \"Data Assets\" )). // (2) IncludeOnResults ( \"policies\" ). // (3) IncludeOnRelations ( \"name\" ). // (4) IncludeOnRelations ( \"policyActions\" ). IncludeOnRelations ( \"policyResources\" ). IncludeOnRelations ( \"policyType\" ). Execute () // (5) if atlanErr != nil { fmt . Println ( \"Error:\" , atlanErr ) } for _ , entity := range response [ 0 ]. Entities { // (6) if entity . TypeName != nil && * entity . TypeName == \"Persona\" { fmt . Println ( \"Persona Found: Name:\" , * entity . Name , \"QualifiedName:\" , * entity . QualifiedName ) for _ , policy := range * entity . Policies { fmt . Println ( \"Policy Found: QualifiedName:\" , * policy . UniqueAttributes . QualifiedName ) // Do something with the policies } } } Start by selecting a persona, here using a FluentSearch-based approach. You can select the persona by whatever you like, in this example we are selecting based on its name. Include the policies for the persona as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and resources controlled by each policy. Run a fluent search request using the Execute() . For each result of the search (itself a Persona), you can then retrieve its policies and iterate through them. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 { \"dsl\" : { // (1) \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"Persona\" } } }, { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { \"name.keyword\" : { \"value\" : \"Data Assets\" // (2) } } } ] } }, \"sort\" : [ { \"__guid\" : { \"order\" : \"asc\" } } ], \"track_total_hits\" : true }, \"attributes\" : [ \"policies\" // (3) ], \"relationAttributes\" : [ // (4) \"name\" , \"policyType\" , \"policyResources\" , \"policyActions\" ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Start by running a search for personas. You can select the persona by whatever you like, in this example we are selecting based on its name. Include the policies for the persona as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and resources controlled by each policy. Personalize the persona Â¶ 0.0.12 2.1.4 4.0.0 To personalize which details to show for assets within a persona: Java Python Kotlin Go Raw REST API Personalize the persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Persona toUpdate = Persona . updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) true ) // (4) . denyAssetTab ( AssetSidebarTab . LINEAGE ) // (5) . denyAssetTab ( AssetSidebarTab . RELATIONS ) . denyAssetTab ( AssetSidebarTab . QUERIES ) . denyAssetType ( \"Table\" ) // (6) . denyAssetType ( \"Column\" ) . denyAssetFilter ( AssetFilterGroup . TAGS ) // (7) . denyAssetFilter ( AssetFilterGroup . OWNERS ) . denyAssetFilter ( AssetFilterGroup . CERTIFICATE ) . denyCustomMetadataGuid ( \"59220d25-5d39-4f3a-8de5-072098bee793\" ) // (8) . denyCustomMetadataGuid ( \"bb0c9836-94fd-4a54-9007-0f25fb802c2c\" ) . build (); AssetMutationResponse response = toUpdate . save ( client ); // (9) Use the updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then chain preferences on which metadata tabs should be hidden when using this persona. You can then set preferences on which asset types should be hidden when using this persona. You can then set preferences on which asset filters should be hidden when using this persona. You can then set preferences on which custom metadata should be hidden when using this persona. To update the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Personalize the persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona from pyatlan.model.enums import AssetSidebarTab , AssetFilterGroup client = AtlanClient () to_update = Persona . updater ( # (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , # (2) \"Data Assets\" , # (3) True # (4) ) to_update . deny_asset_tabs = { # (5) AssetSidebarTab . LINEAGE . value , AssetSidebarTab . RELATIONS . value , AssetSidebarTab . QUERIES . value , } to_update . deny_asset_types = { \"Table\" , \"Column\" } # (6) to_update . deny_asset_filters = { # (7) AssetFilterGroup . TAGS . value , AssetFilterGroup . OWNERS . value , AssetFilterGroup . CERTIFICATE . value , } to_update . deny_custom_metadata_guids = { # (8) \"59220d25-5d39-4f3a-8de5-072098bee793\" , \"bb0c9836-94fd-4a54-9007-0f25fb802c2c\" , } response = client . asset . save ( to_update ) # (9) Use the updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then set preferences on which metadata tabs should be hidden when using this persona. You can then set preferences on which asset types should be hidden when using this persona. You can then set preferences on which asset filters should be hidden when using this persona. You can then set preferences on which custom metadata should be hidden when using this persona. To update the persona in Atlan, call the save() method with the object you've built. Personalize the persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 val toUpdate = Persona . updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) true ) // (4) . denyAssetTab ( AssetSidebarTab . LINEAGE ) // (5) . denyAssetTab ( AssetSidebarTab . RELATIONS ) . denyAssetTab ( AssetSidebarTab . QUERIES ) . denyAssetType ( \"Table\" ) // (6) . denyAssetType ( \"Column\" ) . denyAssetFilter ( AssetFilterGroup . TAGS ) // (7) . denyAssetFilter ( AssetFilterGroup . OWNERS ) . denyAssetFilter ( AssetFilterGroup . CERTIFICATE ) . denyCustomMetadataGuid ( \"59220d25-5d39-4f3a-8de5-072098bee793\" ) // (8) . denyCustomMetadataGuid ( \"bb0c9836-94fd-4a54-9007-0f25fb802c2c\" ) . build () val response = toUpdate . save ( client ) // (9) Use the updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then chain preferences on which metadata tabs should be hidden when using this persona. You can then set preferences on which asset types should be hidden when using this persona. You can then set preferences on which asset filters should be hidden when using this persona. You can then set preferences on which custom metadata should be hidden when using this persona. To update the persona in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Personalize the persona 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 toUpdate := & assets . Persona {} toUpdate . Updater ( // (1) \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (2) \"Data Assets\" , // (3) true , // (4) ) toUpdate . DenyAssetTabs = & [] string { // (5) atlan . AssetSidebarTabLineage . Name , atlan . AssetSidebarTabRelations . Name , atlan . AssetSidebarTabQueries . Name , } toUpdate . DenyAssetTypes = & [] string { \"Table\" , \"Column\" } // (6) toUpdate . DenyAssetFilters = & [] string { // (7) atlan . AssetFilterGroupTags . Name , atlan . AssetFilterGroupOwners . Name , atlan . AssetFilterGroupCertificate . Name , } toUpdate . DenyCustomMetadataGuids = & [] string { // (8) \"59220d25-5d39-4f3a-8de5-072098bee793\" , \"bb0c9836-94fd-4a54-9007-0f25fb802c2c\" , } response , atlanErr := assets . Save ( toUpdate ) // (9) Use the Updater() method to update a persona. You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then set preferences on which metadata tabs should be hidden when using this persona. You can then set preferences on which asset types should be hidden when using this persona. You can then set preferences on which asset filters should be hidden when using this persona. You can then set preferences on which custom metadata should be hidden when using this persona. To update the persona in Atlan, call the Save() method with the object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"entities\" : [ // (1) { \"typeName\" : \"Persona\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/M5HnBQ8QWhrAVGuvBx8iSW\" , // (3) \"name\" : \"Data Assets\" // (4) \"isAccessControlEnabled\" : true , // (5) \"denyAssetTabs\" : [ // (6) \"Lineage\" , \"Relations\" , \"Queries\" ], \"denyAssetTypes\" : [ // (7) \"Table\" , \"Column\" ], \"denyAssetFilters\" : [ // (8) \"__traitNames\" , \"owners\" , \"certificateStatus\" ], \"denyCustomMetadataGuids\" : [ // (9) \"59220d25-5d39-4f3a-8de5-072098bee793\" , \"bb0c9836-94fd-4a54-9007-0f25fb802c2c\" ], } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name Persona . You must provide the qualifiedName of the persona. You must provide the name of the persona. You must provide whether the persona should be active (enabled) or deactivated after the update. You can then set preferences on which metadata tabs should be hidden when using this persona. You can then set preferences on which asset types should be hidden when using this persona. You can then set preferences on which asset filters should be hidden when using this persona. You can then set preferences on which custom metadata should be hidden when using this persona. To review available tabs/filters To review the values of tabs and filters, refer to the SDKs.\nFor example, check the AssetSidebarTab and AssetFilterGroup enums in the SDKs. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/access/queries/",
    "content": "/api/sql/query/stream (POST) Run queries on an asset 1.9.3 4.0.0 To run SQL queries on an asset: Java Python Kotlin Raw REST API Running SQL query on an asset 1 2 3 4 5 6 7 QueryRequest query = QueryRequest . creator ( // (1) \"SELECT * FROM \\\"PACKAGETYPES\\\" LIMIT 50;\" , \"default/snowflake/1705755637\" ) . defaultSchema ( \"RAW.WIDEWORLDIMPORTERS_WAREHOUSE\" ) // (2) . build (); QueryResponse response = client . queries . stream ( query ); // (3) To create a minimal query object, use the QueryRequest creator method and provide the following arguments: SQL query to run. unique name of the connection to use for the query. You must provide default schema name to use for unqualified objects in the SQL, in the form DB.SCHEMA . You can now execute the query using the stream() method. Running SQL query on an asset 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.query import QueryRequest client = AtlanClient () query = QueryRequest ( # (1) sql = 'SELECT * FROM \"PACKAGETYPES\" LIMIT 50;' , data_source_name = \"default/snowflake/1705755637\" , default_schema = \"RAW.WIDEWORLDIMPORTERS_WAREHOUSE\" , ) response = client . queries . stream ( request = query ) # (2) To build a query, you need to use the QueryRequest and provide the following parameters: sql : SQL query to run. data_source_name : unique name of the connection to use for the query. default_schema : default schema name to use for\n     unqualified objects in the SQL, in the form DB.SCHEMA . You can now execute the query using the stream() method. Running SQL query on an asset 1 2 3 4 5 6 7 val query = QueryRequest . creator ( // (1) \"SELECT * FROM \\\"PACKAGETYPES\\\" LIMIT 50;\" , \"default/snowflake/1705755637\" ) . defaultSchema ( \"RAW.WIDEWORLDIMPORTERS_WAREHOUSE\" ) // (2) . build () val response = client . queries . stream ( query ) // (3) To create a minimal query object, use the QueryRequest creator method and provide the following arguments: SQL query to run. unique name of the connection to use for the query. You must provide default schema name to use for unqualified objects in the SQL, in the form DB.SCHEMA . You can now execute the query using the stream() method. POST /api/sql/query/stream 1 2 3 4 5 { \"sql\" : \"SELECT * FROM \\\"PACKAGETYPES\\\" LIMIT 50;\" , // (1) \"dataSourceName\" : \"default/snowflake/1705755637\" , \"defaultSchema\" : \"RAW.WIDEWORLDIMPORTERS_WAREHOUSE\" } You must provide the following properties: sql : SQL query to run. dataSourceName : unique name of the connection to use for the query. defaultSchema : default schema name to use for unqualified objects\n    in the SQL, in the form DB.SCHEMA . Use API token to run queries Â¶ 7.0.0 4.0.0 You can also grant permission to run SQL queries on an asset using an API token, if you want. (This must be explicitly granted, as it is not possible by default.) You can even mask certain information through data policies on purposes linked to the API token. API token permissions Before executing queries on an asset using an API token, ensure that the token is linked to a persona with a data policy that permits queries for that specific asset . Java Python Kotlin Raw REST API Running SQL query on an asset with API token 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 AuthPolicy data = Purpose . createDataPolicy ( // (1) \"Mask the data\" , // (2) purpose . getGuid (), // (3) AuthPolicyType . DATA_MASK , // (4) null , List . of ( token . getApiTokenUsername ()), // (5) false ) . policyMaskType ( DataMaskingType . REDACT ) // (6) . build (); AssetMutationResponse response = client . assets . save ( List . of ( data ), false ); // (7) try ( AtlanClient tokenClient = new AtlanClient ( client . getBaseUrl (), token . getAttributes (). getAccessToken ())) { // (8) QueryRequest query = QueryRequest . creator ( // (9) \"SELECT * FROM \\\"PACKAGETYPES\\\" LIMIT 50;\" , \"default/snowflake/1705755637\" ) . defaultSchema ( \"RAW.WIDEWORLDIMPORTERS_WAREHOUSE\" ) // (10) . build (); QueryResponse response = tokenClient . queries . stream ( query ); // (11) } Use the createDataPolicy() method to start building a data policy with the minimal required information. You must give the policy a name. You must provide the GUID of the Purpose to attach this policy to. Specify the type of policy (granting, denying or masking the data of assets with the tags in the purpose). Set the policy user to the API token . Set the type of masking to REDACT to redact the tagged elements in the query response. To then add the policy to the purpose in Atlan, call the save() method with the policy object you've built. Create a new AtlanClient set up to use the new API token. To create a minimal query object, use the QueryRequest creator method and provide the following arguments: SQL query to run. unique name of the connection to use for the query. You must provide default schema name to use for unqualified objects in the SQL, in the form DB.SCHEMA . You can now execute the query using the stream() method. Running SQL query on an asset with API token 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.query import QueryRequest client = AtlanClient () data = Purpose . create_data_policy ( # (1) client = client , # (2) name = \"Mask the data\" , # (3) purpose_id = purpose . guid , # (4) policy_type = AuthPolicyType . DATA_MASK , # (5) policy_users = { f \"service-account- { token . client_id } \" }, # (6) all_users = False , # (7) ) data . policy_mask_type = DataMaskingType . REDACT # (8) response = client . asset . save ( data ) # (9) token_client = AtlanClient ( # (10) base_url = client . base_url , api_key = token . attributes . access_token ) query = QueryRequest ( # (11) sql = 'SELECT * FROM \"PACKAGETYPES\" LIMIT 50;' , data_source_name = \"default/snowflake/1705755637\" , default_schema = \"RAW.WIDEWORLDIMPORTERS_WAREHOUSE\" , ) response = token_client . queries . stream ( request = query ) # (12) Use the create_data_policy() method to start building\na data policy with the minimal required information. You must provide a client instance. You must give the policy a name. You must provide the GUID of the purpose to attach this policy to. Specify the type of policy (granting, denying or\nmasking the data of assets with the tags in the purpose). Set the policy_users to the API token . Set the all_users option to False as\nthis policy is intended specifically for the API token. Set the type of masking to REDACT to redact the tagged elements in the query response. To then add the policy to the purpose in Atlan,\ncall the save() method with the policy object you've built. Create a new client with the API token. To build a query, you need to use the QueryRequest and provide the following parameters: sql : SQL query to run. data_source_name : unique name of the connection to use for the query. default_schema : default schema name to use for\n     unqualified objects in the SQL, in the form DB.SCHEMA . You can now execute the query using the stream() method. Running SQL query on an asset with API token 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 val data = Purpose . createDataPolicy ( // (1) \"Mask the data\" , // (2) purpose . getGuid (), // (3) AuthPolicyType . DATA_MASK , // (4) null , List . of ( token . getApiTokenUsername ()), // (5) false ) . policyMaskType ( DataMaskingType . REDACT ) // (6) . build () val response = client . assets . save ( listOf ( data ), false ) // (7) AtlanClient ( client . getBaseUrl (), token . getAttributes (). getAccessToken ()). use { tokenClient -> // (8) val query = QueryRequest . creator ( // (9) \"SELECT * FROM \\\"PACKAGETYPES\\\" LIMIT 50;\" , \"default/snowflake/1705755637\" ) . defaultSchema ( \"RAW.WIDEWORLDIMPORTERS_WAREHOUSE\" ) // (10) . build () val response = tokenClient . queries . stream ( query ) // (11) } Use the createDataPolicy() method to start building a data policy with the minimal required information. You must give the policy a name. You must provide the GUID of the Purpose to attach this policy to. Specify the type of policy (granting, denying or masking the data of assets with the tags in the purpose). Set the policy user to the API token . Set the type of masking to REDACT to redact the tagged elements in the query response. To then add the policy to the purpose in Atlan, call the save() method with the policy object you've built. Create a new AtlanClient set up to use the new API token. To create a minimal query object, use the QueryRequest creator method and provide the following arguments: SQL query to run. unique name of the connection to use for the query. You must provide default schema name to use for unqualified objects in the SQL, in the form DB.SCHEMA . You can now execute the query using the stream() method. POST /api/sql/query/stream 1 2 3 4 5 { \"sql\" : \"SELECT * FROM \\\"PACKAGETYPES\\\" LIMIT 50;\" , // (1) \"dataSourceName\" : \"default/snowflake/1705755637\" , \"defaultSchema\" : \"RAW.WIDEWORLDIMPORTERS_WAREHOUSE\" } You must provide the following properties: sql : SQL query to run. dataSourceName : unique name of the connection to use for the query. defaultSchema : default schema name to use for\n     unqualified objects in the SQL, in the form DB.SCHEMA . Policy implementation delay Be aware that there is a delay of a few minutes after applying new policies\nto the token before they become fully effective. If you run a query immediately\nafter creating the policy, you may still observe unredacted information until\nthe policy is fully implemented. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/access/policies/",
    "content": "/api/meta/entity/bulk (DELETE) /api/meta/entity/bulk (POST) /api/meta/search/indexsearch (POST) Policies Â¶ Policies control which assets users can access, and what operations they can carry out on those assets. Retrieve policies Â¶ From a persona Â¶ 0.0.15 1.4.0 4.0.0 To retrieve a policy from a persona, you need to search for the policy by some characteristic: Java Python Kotlin Go Raw REST API Retrieve policies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 AuthPolicy . select ( client ) // (1) . where ( AuthPolicy . POLICY_CATEGORY . eq ( \"persona\" )) . where ( AuthPolicy . POLICY_RESOURCES . startsWith ( \"entity:default/snowflake/1696324735\" )) // (2) . includeOnResults ( AuthPolicy . NAME ) // (3) . includeOnResults ( AuthPolicy . ACCESS_CONTROL ) . includeOnResults ( AuthPolicy . POLICY_RESOURCES ) . includeOnResults ( AuthPolicy . CONNECTION_QUALIFIED_NAME ) . includeOnResults ( AuthPolicy . POLICY_TYPE ) . includeOnResults ( AuthPolicy . POLICY_SUB_CATEGORY ) . includeOnRelations ( IAccessControl . IS_ACCESS_CONTROL_ENABLED ) // (4) . includeOnRelations ( Asset . NAME ) . stream () // (5) . filter ( a -> a instanceof AuthPolicy ) . forEach ( p -> { // (6) AuthPolicy policy = ( AuthPolicy ) p ; }); Start by selecting policies, here using a FluentSearch-based approach. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls (specifically in this example any assets in a particular snowflake connection). Include details about the policy itself in each search result, such as the access control mechanism the policy is defined within (the persona). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that persona is enabled or not. You can then directly stream the results of the search. For each result of the search (itself an AuthPolicy), you can then decide what to do with it. Retrieve policies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from typing import cast from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AuthPolicy , AccessControl from pyatlan.model.fluent_search import FluentSearch client = AtlanClient () request = ( FluentSearch () . where ( FluentSearch . asset_type ( AuthPolicy )) # (1) . where ( AuthPolicy . POLICY_CATEGORY . eq ( \"persona\" )) . where ( AuthPolicy . POLICY_RESOURCES . startswith ( \"entity:default/snowflake/1696324735\" )) # (2) . include_on_results ( AuthPolicy . NAME ) # (3) . include_on_results ( AuthPolicy . ACCESS_CONTROL ) . include_on_results ( AuthPolicy . POLICY_RESOURCES ) . include_on_results ( AuthPolicy . CONNECTION_QUALIFIED_NAME ) . include_on_results ( AuthPolicy . POLICY_TYPE ) . include_on_results ( AuthPolicy . POLICY_SUB_CATEGORY ) . include_on_relations ( AccessControl . IS_ACCESS_CONTROL_ENABLED ) # (4) . include_on_relations ( AccessControl . NAME ) ) . to_request () # (5) response = client . asset . search ( request ) # (6) for p in response : # (7) policy = cast ( AuthPolicy , p ) Start by selecting policies, here using a FluentSearch-based approach. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls (specifically in this example any assets in a particular snowflake connection). Include details about the policy itself in each search result, such as the access control mechanism the policy is defined within (the persona). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that persona is enabled or not. You can then translate the FluentSearch into a search request. Run a search using the search request. For each result of the search (itself an AuthPolicy), you can then decide what to do with it. Retrieve policies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 AuthPolicy . select ( client ) // (1) . where ( AuthPolicy . POLICY_CATEGORY . eq ( \"persona\" )) . where ( AuthPolicy . POLICY_RESOURCES . startsWith ( \"entity:default/snowflake/1696324735\" )) // (2) . includeOnResults ( AuthPolicy . NAME ) // (3) . includeOnResults ( AuthPolicy . ACCESS_CONTROL ) . includeOnResults ( AuthPolicy . POLICY_RESOURCES ) . includeOnResults ( AuthPolicy . CONNECTION_QUALIFIED_NAME ) . includeOnResults ( AuthPolicy . POLICY_TYPE ) . includeOnResults ( AuthPolicy . POLICY_SUB_CATEGORY ) . includeOnRelations ( IAccessControl . IS_ACCESS_CONTROL_ENABLED ) // (4) . includeOnRelations ( Asset . NAME ) . stream () // (5) . filter { it is AuthPolicy } . forEach { // (6) val policy = it as AuthPolicy } Start by selecting policies, here using a FluentSearch-based approach. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls (specifically in this example any assets in a particular snowflake connection). Include details about the policy itself in each search result, such as the access control mechanism the policy is defined within (the persona). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that persona is enabled or not. You can then directly stream the results of the search. For each result of the search (itself an AuthPolicy), you can then decide what to do with it. Retrieve policies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 response , atlanErr := assets . NewFluentSearch (). AssetType ( \"AuthPolicy\" ). // (1) Where ( ctx . AuthPolicy . POLICY_CATEGORY . Eq ( \"persona\" )). Where ( ctx . AuthPolicy . POLICY_RESOURCES . StartsWith ( \"entity:default/snowflake/1696324735\" , nil )). // (2) IncludeOnResults ( ctx . AuthPolicy . NAME . GetAtlanFieldName ()). // (3) IncludeOnResults ( ctx . AuthPolicy . ACCESS_CONTROL . GetAtlanFieldName ()). IncludeOnResults ( ctx . AuthPolicy . POLICY_RESOURCES . GetAtlanFieldName ()). IncludeOnResults ( ctx . AuthPolicy . CONNECTION_QUALIFIED_NAME . GetAtlanFieldName ()). IncludeOnResults ( ctx . AuthPolicy . POLICY_TYPE . GetAtlanFieldName ()). IncludeOnResults ( ctx . AuthPolicy . POLICY_SUB_CATEGORY . GetAtlanFieldName ()). IncludeOnRelations ( ctx . AccessControl . IS_ACCESS_CONTROL_ENABLED . GetAtlanFieldName ()). // (4) IncludeOnRelations ( ctx . AccessControl . NAME . GetAtlanFieldName ()). Execute () // (5) if atlanErr != nil { logger . Log . Errorf ( \"Error: %v\" , atlanErr ) } for _ , entity := range response [ 0 ]. Entities { // (6) if entity . TypeName != nil && * entity . TypeName == \"AuthPolicy\" { // Do something with the policy } } Start by selecting policies, here using a FluentSearch-based approach. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls (specifically in this example any assets in a particular snowflake connection). Include details about the policy itself in each search result, such as the access control mechanism the policy is defined within (the persona). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that persona is enabled or not. Run a search using the search request. For each result of the search (itself an AuthPolicy), you can then decide what to do with it. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 { \"dsl\" : { // (1) \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"AuthPolicy\" } } }, { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { \"policyCategory\" : { \"value\" : \"persona\" } } }, { \"prefix\" : { \"policyResources\" : { // (2) \"value\" : \"entity:default/snowflake/1696324735\" } } } ] } }, \"sort\" : [ { \"__guid\" : { \"order\" : \"asc\" } } ], \"track_total_hits\" : true }, \"attributes\" : [ \"name\" , \"accessControl\" , // (3) \"policyResources\" , \"connectionQualifiedName\" , \"policyType\" , \"policySubCategory\" ], \"relationAttributes\" : [ // (4) \"isAccessControlEnabled\" , \"name\" ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Start by running a search for policies. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls. Include details about the policy itself in each search result, such as the accessControl mechanism the policy is defined within (the persona). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that persona is enabled or not. From a purpose Â¶ 0.0.15 6.0.0 4.0.0 Similarly, to retrieve a policy from a purpose you need to search for the policy by some characteristic: Java Python Kotlin Go Raw REST API Retrieve policies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 String tagId = client . getAtlanTagCache (). getIdForName ( \"Issue\" ); // (1) AuthPolicy . select ( client ) // (2) . where ( AuthPolicy . POLICY_CATEGORY . eq ( \"purpose\" )) . where ( AuthPolicy . POLICY_RESOURCES . startsWith ( \"tag:\" + tagId )) // (3) . includeOnResults ( AuthPolicy . NAME ) // (4) . includeOnResults ( AuthPolicy . ACCESS_CONTROL ) . includeOnResults ( AuthPolicy . POLICY_RESOURCES ) . includeOnRelations ( IAccessControl . IS_ACCESS_CONTROL_ENABLED ) // (5) . includeOnRelations ( Asset . NAME ) . stream () // (6) . filter ( a -> a instanceof AuthPolicy ) . forEach ( p -> { // (7) AuthPolicy policy = ( AuthPolicy ) p ; }); Since purposes work around Atlan tags, you may first want to retrieve the tag of interest (you need its internal ID rather than human-readable name). Start by selecting policies, here using a FluentSearch-based approach. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls (specifically in this example the tag we retrieved earlier). Include details about the policy itself in each search result, such as the access control mechanism the policy is defined within (the purpose). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that purpose is enabled or not. You can then directly stream the results of the search. For each result of the search (itself an AuthPolicy), you can then decide what to do with it. Retrieve policies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from typing import cast from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AuthPolicy , AccessControl from pyatlan.model.fluent_search import FluentSearch client = AtlanClient () tag_id = client . atlan_tag_cache . get_id_for_name ( \"Issue\" ) # (1) request = ( FluentSearch () . where ( FluentSearch . asset_type ( AuthPolicy )) # (2) . where ( AuthPolicy . POLICY_CATEGORY . eq ( \"purpose\" )) . where ( AuthPolicy . POLICY_RESOURCES . startswith ( f \"tag: { tag_id } \" )) # (3) . include_on_results ( AuthPolicy . NAME ) # (4) . include_on_results ( AuthPolicy . ACCESS_CONTROL ) . include_on_results ( AuthPolicy . POLICY_RESOURCES ) . include_on_relations ( AccessControl . IS_ACCESS_CONTROL_ENABLED ) # (5) . include_on_relations ( AccessControl . NAME ) ) . to_request () # (6) response = client . asset . search ( request ) # (7) for p in response : # (8) policy = cast ( AuthPolicy , p ) Since purposes work around Atlan tags, you may first want to retrieve the tag of interest (you need its internal ID rather than human-readable name). Start by selecting policies, here using a FluentSearch-based approach. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls (specifically in this example the tag we retrieved earlier). Include details about the policy itself in each search result, such as the access control mechanism the policy is defined within (the purpose). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that purpose is enabled or not. You can then translate the FluentSearch into a search request. Run a search using the search request. For each result of the search (itself an AuthPolicy), you can then decide what to do with it. Retrieve policies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 val tagId = client . atlanTagCache . getIdForName ( \"Issue\" ) // (1) AuthPolicy . select ( client ) // (2) . where ( AuthPolicy . POLICY_CATEGORY . eq ( \"purpose\" )) . where ( AuthPolicy . POLICY_RESOURCES . startsWith ( \"tag: $ tagId \" )) // (3) . includeOnResults ( AuthPolicy . NAME ) // (4) . includeOnResults ( AuthPolicy . ACCESS_CONTROL ) . includeOnResults ( AuthPolicy . POLICY_RESOURCES ) . includeOnRelations ( IAccessControl . IS_ACCESS_CONTROL_ENABLED ) // (5) . includeOnRelations ( Asset . NAME ) . stream () // (6) . filter { it is AuthPolicy } . forEach { // (7) val policy = it as AuthPolicy } Since purposes work around Atlan tags, you may first want to retrieve the tag of interest (you need its internal ID rather than human-readable name). Start by selecting policies, here using a FluentSearch-based approach. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls (specifically in this example the tag we retrieved earlier). Include details about the policy itself in each search result, such as the access control mechanism the policy is defined within (the purpose). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that purpose is enabled or not. You can then directly stream the results of the search. For each result of the search (itself an AuthPolicy), you can then decide what to do with it. Retrieve policies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 tagID , _ := assets . GetAtlanTagIDForName ( \"Issue\" ) // (1) response , atlanErr := assets . NewFluentSearch (). AssetType ( \"AuthPolicy\" ). // (2) Where ( ctx . AuthPolicy . POLICY_CATEGORY . Eq ( \"purpose\" )). Where ( ctx . AuthPolicy . POLICY_RESOURCES . StartsWith ( \"tag:\" + tagID , nil )). // (3) IncludeOnResults ( ctx . AuthPolicy . NAME . GetAtlanFieldName ()). // (4) IncludeOnResults ( ctx . AuthPolicy . ACCESS_CONTROL . GetAtlanFieldName ()). IncludeOnResults ( ctx . AuthPolicy . POLICY_RESOURCES . GetAtlanFieldName ()). IncludeOnRelations ( ctx . AccessControl . IS_ACCESS_CONTROL_ENABLED . GetAtlanFieldName ()). // (5) IncludeOnRelations ( ctx . AccessControl . NAME . GetAtlanFieldName ()). Execute () // (6) if atlanErr != nil { logger . Log . Errorf ( \"Error: %v\" , atlanErr ) } for _ , entity := range response [ 0 ]. Entities { // (7) if entity . TypeName != nil && * entity . TypeName == \"AuthPolicy\" { // Do something with the Policy } } Since purposes work around Atlan tags, you may first want to retrieve the tag of interest (you need its internal ID rather than human-readable name). Start by selecting policies, here using a FluentSearch-based approach. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls (specifically in this example the tag we retrieved earlier). Include details about the policy itself in each search result, such as the access control mechanism the policy is defined within (the purpose). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that purpose is enabled or not. Run a search using the search request. For each result of the search (itself an AuthPolicy), you can then decide what to do with it. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 { \"dsl\" : { // (1) \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"AuthPolicy\" } } }, { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { \"policyCategory\" : { \"value\" : \"purpose\" } } }, { \"prefix\" : { \"policyResources\" : { // (2) \"value\" : \"tag:RRbkpEJKNC4qsbKB7fKFNN\" } } } ] } }, \"sort\" : [ { \"__guid\" : { \"order\" : \"asc\" } } ], \"track_total_hits\" : true }, \"attributes\" : [ // (4) \"name\" , \"accessControl\" , \"policyResources\" , \"policyCategory\" , \"policySubCategory\" ], \"relationAttributes\" : [ \"isAccessControlEnabled\" , \"name\" ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Start by running a search for policies. You can select the policy by whatever you like, in this example we are selecting based on the resources it controls (specifically via the tag defined as part of the purpose). Note that the tag needs to be given as its internal ID, not the human-readable name. Include details about the policy itself in each search result, such as the accessControl mechanism the policy is defined within (the purpose). Include all the attributes you want about the access control mechanism on the relations of the search results. Here we are including the name of and whether that purpose is enabled or not. Update policies Â¶ Different update approach from most assets Unlike most assets, to update policies you should first retrieve the existing policy and then update it in its entirety. You can do this by either retrieving the entire policy asset by its GUID (if you know it), or by retrieving the policy using the instructions above under Retrieve policies . You must request at least the attributes defined in that section on each policy to be able to update the policy. 0.0.15 1.4.0 4.0.0 To update an existing policies, once you have retrieved it: Java Python Kotlin Go Raw REST API Update an existing policy 14 15 16 17 AuthPolicy policy = policy . toBuilder () // (1) . description ( \"Revised explanation about what this policy does.\" ) // (2) . build (); // (3) AssetMutationResponse response = policy . save ( client ); // (4) Assuming you have already retrieved the policy you want to update ( policy in this example), you can turn it into a mutable object using toBuilder() . You can then apply any updates you want to the policy. These will either overwrite (where only a single value is allowed, such as description ) or append to the existing values defined in the policy. Build up your changes. You can then save the revised policy back to Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update an existing policy 22 23 24 policy . policy_type = AuthPolicyType . ALLOW # (1) policy . policy_actions = [ PersonaMetadataAction . READ , PersonaMetadataAction . UPDATE , PersonaMetadataAction . CREATE , PersonaMetadataAction . DELETE ] # (2) client . asset . save ( policy ) # (3) Assuming you have already retrieved the policy you want to update ( policy in this example), you can directly modify its attributes, such as policy_type . You can then apply updates to the policy by replacing the existing values with the new ones. This overwrites the previous values for attributes like policy_actions . You can then save the revised policy back to Atlan. Update an existing policy 14 15 16 17 val toUpdate = policy . toBuilder () // (1) . description ( \"Revised explanation about what this policy does.\" ) // (2) . build () // (3) val response = toUpdate . save ( client ) // (4) Assuming you have already retrieved the policy you want to update ( policy in this example), you can turn it into a mutable object using toBuilder() . You can then apply any updates you want to the policy. These will either overwrite (where only a single value is allowed, such as description ) or append to the existing values defined in the policy. Build up your changes. You can then save the revised policy back to Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update an existing policy 17 18 19 entity . PolicyType = & atlan . AuthPolicyTypeAllow // (1) entity . PolicyActions = & [] string { atlan . PersonaMetadataActionRead . Name , atlan . PersonaMetadataActionUpdate . Name , atlan . PersonaMetadataActionDelete . Name , atlan . PersonaMetadataActionCreate . Name } // (2) assets . Save ( & entity ) // (3) Assuming you have already retrieved the policy you want to update ( entity in this example), you can directly modify its attributes, such as PolicyType . You can then apply updates to the policy by replacing the existing values with the new ones. This overwrites the previous values for attributes like PolicyActions . You can then Save the revised policy back to Atlan. Multiple API calls required You will need to first retrieve the policy you want to update. You can then replace any values in the response payload for that policy and POST the revised payload to /api/meta/entity/bulk . Remove policies Â¶ To remove a policy, you need only delete it as you would any other asset . From a persona Â¶ 0.0.15 1.4.0 4.0.0 To find the GUID of a specific policy in a persona: Java Python Kotlin Go Raw REST API Find a persona policy's GUID 1 2 3 4 5 6 7 List < Persona > list = Persona . findByName ( client , \"Data Assets\" ); // (1) Persona persona = Persona . get ( client , list . get ( 0 ). getGuid (), true ); // (2) for ( AuthPolicy policy : persona . getPolicies ()) { // (3) log . info ( \"Policy {} has guid = {}\" , policy . getDisplayText (), // (4) policy . getGuid ()); // (5) } If you already have the persona or its GUID or qualifiedName, you can simply use it directly. This example reuses the search by name to obtain it. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Once you have the minimal information about the persona, you may still need to retrieve the full persona itself (to ensure you have all of its policies and their inner details). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through these policies... ...and check each policy's displayText for the name that's been given to the policy. ...and retrieve each policy's guid to be able to individually delete the appropriate policy. Find a persona policy's GUID 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Persona client = AtlanClient () result = client . asset . find_personas_by_name ( \"Data Assets\" ) # (1) persona = client . asset . get_by_guid ( result [ 0 ] . guid , asset_type = Persona ) # (2) for policy in persona . policies : # (3) print ( f \"Policy { policy . display_text } has guid = { policy . guid } \" # (4) ) If you already have the persona or its GUID or qualified_name, you can simply use it directly. This example reuses the search by name to obtain it. Once you have the minimal information about the persona, you may still need to retrieve the full persona itself (to ensure you have all of its policies and their inner details). You can then iterate through these policies... ...and check each policy's display_text for the name that's been given to the policy, and retrieve each policy's guid to be able to individually delete the appropriate policy. Find a persona policy's GUID 1 2 3 4 5 6 7 val list = Persona . findByName ( client , \"Data Assets\" ) // (1) val persona = Persona . get ( client , list [ 0 ] . getGuid (), true ) // (2) for ( policy in persona . policies ) { // (3) log . info { \"Policy ${ policy . displayText } has guid = ${ policy . guid } \" } // (4) } If you already have the persona or its GUID or qualifiedName, you can simply use it directly. This example reuses the search by name to obtain it. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Once you have the minimal information about the persona, you may still need to retrieve the full persona itself (to ensure you have all of its policies and their inner details). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through these policies... ...and check each policy's displayText for the name that's been given to the policy and each policy's guid to be able to individually delete the appropriate policy. Find a persona policy's GUID 1 2 3 4 5 6 7 8 9 PersonaName := \"Data Assets\" result , atlanErr := assets . FindPersonasByName ( PersonaName ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error: %v\" , atlanErr ) } persona , _ := assets . GetByGuid [ * assets . Persona ]( * result . Entities [ 0 ]. Guid ) // (2) for _ , policy := range * persona . Policies { // (3) fmt . Printf ( \"Policy %v has guid = %v\" , * policy . DisplayName , * policy . Guid ) // (4) } If you already have the persona or its GUID or qualified_name, you can simply use it directly. This example reuses the search by name to obtain it. Once you have the minimal information about the persona, you may still need to retrieve the full persona itself (to ensure you have all of its policies and their inner details). You can then iterate through these policies... ...and check each policy's DisplayName for the name that's been given to the policy, and retrieve each policy's Guid to be able to individually delete the appropriate policy. Multiple API calls required You will need to first run a search for all personas with a given name . You can then retrieve the full persona by its GUID , to see all of its policies and their details. You can then iterate through those details to see the displayText for the name that's been given to each policy, and retrieve each policy's guid to be able to individually delete the appropriate policy. From a purpose Â¶ 0.0.15 1.4.0 4.0.0 To find the GUID of a specific policy in a purpose: Java Python Kotlin Go Raw REST API Find a purpose policy's GUID 1 2 3 4 5 6 7 List < Purpose > list = Purpose . findByName ( client , \"Known Issues\" ); // (1) Purpose purpose = Purpose . get ( client , list . get ( 0 ). getGuid (), true ); // (2) for ( AuthPolicy policy : purpose . getPolicies ()) { // (3) log . info ( \"Policy {} has guid = {}\" , policy . getDisplayText (), // (4) policy . getGuid ()); // (5) } If you already have the purpose or its GUID or qualifiedName, you can simply use it directly. This example reuses the search by name to obtain it. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Once you have the minimal information about the purpose, you may still need to retrieve the full purpose itself (to ensure you have all of its policies and their inner details). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through these policies... ...and check each policy's displayText for the name that's been given to the policy. ...and retrieve each policy's guid to be able to individually delete the appropriate policy. Find a purpose policy's GUID 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Purpose client = AtlanClient () result = client . asset . find_purposes_by_name ( \"Data Assets\" ) # (1) purpose = client . asset . get_by_guid ( result [ 0 ] . guid , asset_type = Purpose ) # (2) for policy in purpose . policies : # (3) print ( f \"Policy { policy . display_text } has guid = { policy . guid } \" # (4) ) If you already have the purpose or its GUID or qualified_name, you can simply use it directly. This example reuses the search by name to obtain it. Once you have the minimal information about the purpose, you may still need to retrieve the full purpose itself (to ensure you have all of its policies and their inner details). You can then iterate through these policies... ...and check each policy's display_text for the name that's been given to the policy, and retrieve each policy's guid to be able to individually delete the appropriate policy. Find a purpose policy's GUID 1 2 3 4 5 6 7 val list = Purpose . findByName ( client , \"Known Issues\" ) // (1) val purpose = Purpose . get ( client , list [ 0 ] . guid , true ) // (2) for ( policy in purpose . policies ) { // (3) log . info { \"Policy ${ policy . displayText } has guid = ${ policy . guid } \" } // (4) } If you already have the purpose or its GUID or qualifiedName, you can simply use it directly. This example reuses the search by name to obtain it. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Once you have the minimal information about the purpose, you may still need to retrieve the full purpose itself (to ensure you have all of its policies and their inner details). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through these policies... ...and check each policy's displayText for the name that's been given to the policy, and retrieve each policy's guid to be able to individually delete the appropriate policy. Find a purpose policy's GUID 1 2 3 4 5 6 7 8 9 PurposeName := \"Data Assets\" result , atlanErr := assets . FindPurposesByName ( PurposeName ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error: %v\" , atlanErr ) } purpose , _ := assets . GetByGuid [ * assets . Purpose ]( * result . Entities [ 0 ]. Guid ) // (2) for _ , policy := range * purpose . Policies { // (3) fmt . Printf ( \"Policy %v has guid = %v\\n\" , * policy . DisplayName , * policy . Guid ) // (4) } If you already have the purpose or its GUID or qualifiedName, you can simply use it directly. This example reuses the search by name to obtain it. Once you have the minimal information about the purpose, you may still need to retrieve the full purpose itself (to ensure you have all of its policies and their inner details). You can then iterate through these policies... ...and check each policy's DisplayName for the name that's been given to the policy, and retrieve each policy's Guid to be able to individually delete the appropriate policy. Multiple API calls required You will need to first run a search for all purposes with a given name . You can then retrieve the full purpose by its GUID , to see all of its policies and their details. You can then iterate through those details to see the displayText for the name that's been given to each policy, and retrieve each policy's guid to be able to individually delete the appropriate policy. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/access/events/",
    "content": "/api/service/events/login (GET) /api/service/events/main (GET) Retrieve access events Â¶ All events Â¶ 1.3.3 4.0.0 You can retrieve and filter all access-related events using: Java Python Kotlin Raw REST API Filter all events 1 2 3 4 5 6 7 8 9 10 11 12 13 client // (1) . logs // (2) . getEvents ( KeycloakEventRequest . builder () // (3) . dateFrom ( \"2023-01-01\" ) // (4) . dateTo ( \"2023-01-31\" ) // (5) . type ( KeycloakEventType . LOGIN ) // (6) . type ( KeycloakEventType . LOGOUT ) . build ()) . stream () // (7) . limit ( 1000 ) // (8) . forEach ( event -> { // (9) // Do something with each event }); From a client... ... access the logs endpoints. The getEvents() method allows you to filter across all access events that are logged. (Optional) You can filter by events only back to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by events only up to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by one or more types of events, for example focusing only on logins and logouts. Like other paginated resources, you can iterate or stream the results. The access events will be lazily-fetched from Atlan. When streaming, you can apply any additional constraints such as limiting or further filtering. And of course, you can then actually do something with each event. Filter all events 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyatlan.model.enums import KeycloakEventType from pyatlan.model.keycloak_events import KeycloakEventRequest from pyatlan.client.atlan import AtlanClient request = KeycloakEventRequest ( # (1) date_from = \"2023-01-01\" , # (2) date_to = \"2023-01-31\" , # (3) types = [ KeycloakEventType . LOGIN , KeycloakEventType . LOGOUT ] # (4) ) client = AtlanClient () events = client . admin . get_keycloak_events ( request ) # (5) for event in events : # (6) # Do something with each event Begin by defining your filter criteria in a KeycloakEventRequest . (Optional) You can filter by events only back to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by events only up to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by one or more types of events, for example focusing only on logins and logouts. From a client, call the admin.get_keycloak_events() method with your requested filters. Like other paginated resources, you can iterate directly through the results. The access events will be lazily-fetched from Atlan. Filter all events 1 2 3 4 5 6 7 8 9 10 11 12 13 client // (1) . logs // (2) . getEvents ( KeycloakEventRequest . builder () // (3) . dateFrom ( \"2023-01-01\" ) // (4) . dateTo ( \"2023-01-31\" ) // (5) . type ( KeycloakEventType . LOGIN ) // (6) . type ( KeycloakEventType . LOGOUT ) . build ()) . stream () // (7) . limit ( 1000 ) // (8) . forEach { // (9) // Do something with each event } From a client... ... access the logs endpoints. The getEvents() method allows you to filter across all access events that are logged. (Optional) You can filter by events only back to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by events only up to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by one or more types of events, for example focusing only on logins and logouts. Like other paginated resources, you can iterate or stream the results. The access events will be lazily-fetched from Atlan. When streaming, you can apply any additional constraints such as limiting or further filtering. And of course, you can then actually do something with each event. GET /api/service/events/login?dateFrom=2023-01-01&dateTo=2023-01-31&type=LOGIN&type=LOGOUT 1 // (1)! All parameters for filtering the logs are query parameters sent in the URL itself. Common event types Some of the common event types you might want to filter on include: Type Meaning LOGIN User has logged in. LOGOUT User has logged out. REGISTER User has registered. UPDATE_EMAIL Email address for an account has changed. UPDATE_PASSWORD Password for an account has changed. SEND_VERIFY_EMAIL Verification email has been sent. VERIFY_EMAIL Email address for an account has been verified. SEND_RESET_PASSWORD Password reset email has been sent. RESET_PASSWORD Password for the account has been reset. CODE_TO_TOKEN Application / client has exchanged a code for a token. REFRESH_TOKEN Application / client has refreshed a token. Admin events Â¶ 1.3.3 4.0.0 You can retrieve and filter administrative events using: Java Python Kotlin Raw REST API Filter admin events 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 client // (1) . logs // (2) . getAdminEvents ( AdminEventRequest . builder () // (3) . dateFrom ( \"2023-01-01\" ) // (4) . dateTo ( \"2023-01-31\" ) // (5) . operationType ( AdminOperationType . CREATE ) // (6) . operationType ( AdminOperationType . UPDATE ) . resourceType ( AdminResourceType . REALM_ROLE ) // (7) . resourceType ( AdminResourceType . REALM_ROLE_MAPPING ) . resourcePath ( \"roles/connection_admins_e71551e0-7f59-44bb-989c-e434f2e5bcae\" ) // (8) . build ()) . stream () // (9) . limit ( 1000 ) // (10) . forEach ( event -> { // (11) // Do something with each event }); From a client... ... access the logs endpoints. The getAdminEvents() method allows you to filter across all admin events that are logged. (Optional) You can filter by events only back to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by events only up to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by one or more operations, for example focusing only on creation and updates. (Optional) You can filter by one or more resource types, for example only new roles or mappings to roles. (Optional) You can filter by a specific resource, such as the role associated with all admins for a specific connection in Atlan. Like other paginated resources, you can iterate or stream the results. The access events will be lazily-fetched from Atlan. When streaming, you can apply any additional constraints such as limiting or further filtering. And of course, you can then actually do something with each event. Filter admin events 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pyatlan.model.enums import AdminOperationType , AdminResourceType from pyatlan.model.keycloak_events import AdminEventRequest from pyatlan.client.atlan import AtlanClient request = AdminEventRequest ( # (1) date_from = \"2023-01-01\" , # (2) date_to = \"2023-01-31\" , # (3) operation_types = [ AdminOperationType . CREATE , AdminOperationType . UPDATE ], # (4) resource_types = [ AdminResourceType . REALM_ROLE , AdminResourceType . REALM_ROLE_MAPPING ], # (5) resource_path = \"roles/connection_admins_e71551e0-7f59-44bb-989c-e434f2e5bcae\" # (6) ) client = AtlanClient () events = client . admin . get_admin_events ( request ) # (7) for event in events : # (8) # Do something with each event Begin by defining your filter criteria in a AdminEventRequest . (Optional) You can filter by events only back to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by events only up to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by one or more operations, for example focusing only on creation and updates. (Optional) You can filter by one or more resource types, for example only new roles or mappings to roles. (Optional) You can filter by a specific resource, such as the role associated with all admins for a specific connection in Atlan. From a client, call the admin.get_admin_events() method with your requested filters. Like other paginated resources, you can iterate directly through the results. The access events will be lazily-fetched from Atlan. Filter admin events 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 client // (1) . logs // (2) . getAdminEvents ( AdminEventRequest . builder () // (3) . dateFrom ( \"2023-01-01\" ) // (4) . dateTo ( \"2023-01-31\" ) // (5) . operationType ( AdminOperationType . CREATE ) // (6) . operationType ( AdminOperationType . UPDATE ) . resourceType ( AdminResourceType . REALM_ROLE ) // (7) . resourceType ( AdminResourceType . REALM_ROLE_MAPPING ) . resourcePath ( \"roles/connection_admins_e71551e0-7f59-44bb-989c-e434f2e5bcae\" ) // (8) . build ()) . stream () // (9) . limit ( 1000 ) // (10) . forEach { // (11) // Do something with each event } From a client... ... access the logs endpoints. The getAdminEvents() method allows you to filter across all admin events that are logged. (Optional) You can filter by events only back to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by events only up to a particular point in time (using the format yyyy-MM-dd ). (Optional) You can filter by one or more operations, for example focusing only on creation and updates. (Optional) You can filter by one or more resource types, for example only new roles or mappings to roles. (Optional) You can filter by a specific resource, such as the role associated with all admins for a specific connection in Atlan. Like other paginated resources, you can iterate or stream the results. The access events will be lazily-fetched from Atlan. When streaming, you can apply any additional constraints such as limiting or further filtering. And of course, you can then actually do something with each event. GET /api/service/events/main?dateFrom=2023-01-01&dateTo=2023-01-31&operationTypes=CREATE&operationTypes=UPDATE&resourceTypes=REALM_ROLE&resourceTypes=REALM_ROLE_MAPPING&resourcePath=roles%2Fconnection_admins_e71551e0-7f59-44bb-989c-e434f2e5bcae 1 // (1)! All parameters for filtering the logs are query parameters sent in the URL itself. (Note that the values should be URL-encoded.) Common resource types Some of the common resource types you might want to filter on include: Type Meaning USER An individual user. GROUP Mechanism to cluster together multiple users. GROUP_MEMBERSHIP Association between a user and a group. REALM_ROLE Object that controls access to potentially multiple resources. REALM_ROLE_MAPPING Mapping between user(s) and access control objects (roles). 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/access/purposes/",
    "content": "/api/meta/entity/bulk (DELETE) /api/meta/entity/bulk (POST) /api/meta/search/indexsearch (POST) Purposes Â¶ Purposes are a way of curating assets by a business area, or to further protect particularly sensitive data. List purposes Â¶ 0.0.14 1.4.0 4.0.0 To retrieve a listing of purposes, run a search and page the results: Java Python Kotlin Go Raw REST API List purposes 1 2 3 4 5 6 Purpose . select ( client ) // (1) . stream () // (2) . filter ( a -> a instanceof Purpose ) // (3) . forEach ( p -> { // (4) log . info ( \"Purpose: {}\" , p ); }); To start building up a query specifically for purposes, you can use the select() convenience method on Purpose itself. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. (Optional) You can do any other operations you might do on a stream, such as filtering the results to ensure they are of a certain type. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. List purposes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Purpose from pyatlan.model.fluent_search import CompoundQuery , FluentSearch client = AtlanClient () search_request = ( FluentSearch () # (1) . where ( CompoundQuery . active_assets ()) . where ( CompoundQuery . asset_type ( Purpose )) # (2) ) . to_request () # (3) results = client . asset . search ( search_request ) # (4) for asset in results : # (5) if isinstance ( asset , Purpose ): # Do something with the Purpose Begin building up a query combining multiple conditions. Ensure that we include only objects of type Purpose . Build this query into a new search request. Run the search. Page through the results (each asset in the results will be a purpose). List purposes 1 2 3 4 5 6 Purpose . select ( client ) // (1) . stream () // (2) . filter { it is Purpose } // (3) . forEach { // (4) log . info { \"Purpose: $ it \" } } To start building up a query specifically for purposes, you can use the select() convenience method on Purpose itself. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. (Optional) You can do any other operations you might do on a stream, such as filtering the results to ensure they are of a certain type. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. List purposes 1 2 3 4 5 6 7 8 9 10 11 12 13 response , atlanErr := assets . NewFluentSearch (). // (1) PageSizes ( 20 ). ActiveAssets (). AssetType ( \"Purpose\" ). // (2) Execute () // (3) if atlanErr != nil { logger . Log . Errorf ( \"Error: %v\" , atlanErr ) } for _ , entity := range response [ 0 ]. Entities { // (4) if entity . TypeName != nil && * entity . TypeName == \"Purpose\" { // Do something with the Purpose } } Begin building up a query combining multiple conditions. Ensure that we include only objects of type Purpose . Run the search. Page through the results (each asset in the results will be a purpose). POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"dsl\" : { \"query\" : { // (1) \"bool\" : { \"filter\" : [ { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"Purpose\" // (2) } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Begin building up a query combining multiple conditions. Ensure that we include only objects of type Purpose . Create a purpose Â¶ 0.0.14 2.0.0 4.0.0 To create a new purpose: Java Python Kotlin Go Raw REST API Create a purpose 1 2 3 4 5 6 Purpose toCreate = Purpose . creator ( // (1) \"Known Issues\" , // (2) List . of ( \"Issue\" )) // (3) . build (); AssetMutationResponse response = toCreate . save ( client ); // (4) Purpose purpose = ( Purpose ) response . getCreatedAssets (). get ( 0 ); // (5) Like other builder patterns in the SDK, the creator() method ensures all required information is provided for the purpose. You must provide a name for the purpose. You must provide a list of the tags that are included in the purpose. To create the purpose in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then retrieve the resulting details of the created purpose from the response (you may of course want to do some type checking first). Create a purpose 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Purpose client = AtlanClient () to_create = Purpose . creator ( # (1) name = \"Data Assets\" , # (2) atlan_tags = [ \"Issue\" ]) # (3) response = client . asset . save ( to_create ) # (4) p = response . assets_created ( asset_type = Purpose )[ 0 ] # (5) Like other builder patterns in the SDK, the create() method ensures all required information is provided for the purpose. You must provide a name for the purpose. You must provide a list of the Atlan tags that are included in the purpose. To create the purpose in Atlan, call the save() method with the object you've built. You can then retrieve the resulting details of the created purpose from the response. Create a purpose 1 2 3 4 5 6 val toCreate = Purpose . creator ( // (1) \"Known Issues\" , // (2) listOf ( \"Issue\" )) // (3) . build () val response = toCreate . save ( client ) // (4) val purpose = response . createdAssets [ 0 ] as Purpose // (5) Like other builder patterns in the SDK, the creator() method ensures all required information is provided for the purpose. You must provide a name for the purpose. You must provide a list of the tags that are included in the purpose. To create the purpose in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then retrieve the resulting details of the created purpose from the response (you may of course want to do some type checking first). Create a purpose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 toCreate := & assets . Purpose {} toCreate . Creator ( // (1) \"Data Assets\" , // (2) [] string { \"Issue\" }, // (3) ) response , err := assets . Save ( toCreate ) // (4) if err != nil { logger . Log . Errorf ( \"Error : %v\" , err ) } else { for _ , entity := range response . MutatedEntities . CREATE { // (5) fmt . Println ( \"Purpose GUID:\" , entity . Guid , \"Display Text:\" , entity . DisplayText ) // Do Something with Purpose } } Like other builder patterns in the SDK, the Creator() method ensures all required information is provided for the purpose. You must provide a name for the purpose. You must provide a list of the Atlan tags that are included in the purpose. To create the purpose in Atlan, call the Save() method with the object you've built. You can then retrieve the resulting details of the created purpose from the response. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"entities\" : [ // (1) { \"typeName\" : \"Purpose\" , // (2) \"attributes\" : { \"displayName\" : \"Known Issues\" , // (3) \"purposeClassifications\" : [ \"jRr7KmCSPliWQQSVK6dqTc\" // (4) ], \"isAccessControlEnabled\" : true , // (5) \"qualifiedName\" : \"Known Issues\" , // (6) \"name\" : \"Known Issues\" // (7) } } ] } Wrap the purpose definition in an entities array. Ensure the type of each nested object is exactly Purpose . Use the displayName to provide the name for the purpose as you want it to appear in the UI. You must specify at least one Atlan tag in the purposeClassifications array. Note that this needs to use the Atlan-internal hashed-string representation of the Atlan tag. Ensure you explicitly set the access control to enabled when creating it. You must provide a qualifiedName for the purpose, although this will be generated and overwritten by the back-end You must provide a name for the purpose, although this will also be normalized by the back-end so will be slightly different once created. Retrieve a purpose Â¶ 0.0.14 2.0.0 4.0.0 To retrieve a purpose by its name: Java Python Kotlin Go Raw REST API Retrieve a purpose 1 List < Purpose > list = Purpose . findByName ( client , \"Known Issues\" ); // (1) The findByName() method handles searching for the purpose based on its name, which could therefore return more than one result. You can also (optionally) provide a second parameter with a list of attributes to retrieve for each purpose. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve a purpose 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () result = client . asset . find_purposes_by_name ( \"Known Issues\" ) # (1) The asset.find_purposes_by_name() method handles searching for the purpose based on its name, which could therefore return more than one result. Retrieve a purpose 1 val list = Purpose . findByName ( client , \"Known Issues\" ) // (1) The findByName() method handles searching for the purpose based on its name, which could therefore return more than one result. You can also (optionally) provide a second parameter with a list of attributes to retrieve for each purpose. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve a purpose 1 2 3 4 result , atlanErr := assets . FindPurposesByName ( \"Known Issues\" ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error: %v\" , atlanErr ) } The assets.FindPurposesByName() method handles searching for the purpose based on its name, which could therefore return more than one result. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 { \"dsl\" : { \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"Purpose\" // (1) } } }, { \"term\" : { \"name.keyword\" : { \"value\" : \"Known Issues\" // (2) } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Define the search to include results for a type exactly matching Purpose , and... ... with the exact name of the purpose you want to find. Update a purpose Â¶ 0.0.14 2.0.0 4.0.0 To update a purpose: Java Python Kotlin Go Raw REST API Update a purpose 1 2 3 4 5 6 7 Purpose toUpdate = Purpose . updater ( // (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (2) \"Known Issues\" , // (3) true ) // (4) . description ( \"Now with a description!\" ) // (5) . build (); AssetMutationResponse response = toUpdate . save ( client ); // (6) Use the updater() method to update a purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then chain on any other updates, such as changing the description of the purpose. To update the purpose in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update a purpose 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Purpose client = AtlanClient () to_update = Purpose . updater ( # (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , # (2) \"Known Issues\" , # (3) True # (4) ) to_update . description = \"Now with a description!\" # (5) response = client . asset . save ( to_update ) # (6) Use the updater() method to update a purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then add on any other updates, such as changing the description of the purpose. To update the purpose in Atlan, call the save() method with the object you've built. Update a purpose 1 2 3 4 5 6 7 val toUpdate = Purpose . updater ( // (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (2) \"Known Issues\" , // (3) true ) // (4) . description ( \"Now with a description!\" ) // (5) . build () val response = toUpdate . save ( client ) // (6) Use the updater() method to update a purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then chain on any other updates, such as changing the description of the purpose. To update the purpose in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update a purpose 1 2 3 4 5 6 7 8 9 toUpdate := & assets . Purpose {} toUpdate . Updater ( // (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (2) \"Known Issues\" , // (3) true , // (4) ) Description := \"Now with a description!\" toUpdate . Description = & Description // (5) response , atlanErr := assets . Save ( toUpdate ) // (6) Use the Updater() method to update a purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then add on any other updates, such as changing the description of the purpose. To update the purpose in Atlan, call the Save() method with the object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"entities\" : [ // (1) { \"typeName\" : \"Purpose\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (3) \"name\" : \"Known Issues\" // (4) \"isAccessControlEnabled\" : true , // (5) \"description\" : \"Now with a description!\" , // (6) } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name Purpose . You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then add on any other updates, such as changing the description of the purpose. Delete a purpose Â¶ 0.0.14 1.4.0 4.0.0 To permanently delete a purpose: Java Python Kotlin Go Raw REST API Delete a purpose 1 Purpose . purge ( client , \"3886a92c-2510-40ea-a14d-803d7ac1616b\" ); // (1) To permanently delete a purpose in Atlan, call the purge() method with the GUID of the purpose. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a purpose 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . asset . purge_by_guid ( \"3886a92c-2510-40ea-a14d-803d7ac1616b\" ) # (1) To permanently delete a purpose in Atlan, call the asset.purge_by_guid() method with the GUID of the purpose. Delete a purpose 1 Purpose . purge ( client , \"3886a92c-2510-40ea-a14d-803d7ac1616b\" ) // (1) To permanently delete a purpose in Atlan, call the purge() method with the GUID of the purpose. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a purpose 1 assets . PurgeByGuid ([] string { \"3886a92c-2510-40ea-a14d-803d7ac1616b\" }) // (1) To permanently delete a purpose in Atlan, call the assets.PurgeByGuid() method with the GUID of the purpose. DELETE /api/meta/entity/bulk?guid=3886a92c-2510-40ea-a14d-803d7ac1616b&deleteType=PURGE 1 // (1) All the details for deleting the purpose are specified in the URL directly. Note that you must provide the GUID of the purpose to delete it. Activate or deactivate a purpose Â¶ 0.0.14 2.0.0 4.0.0 Alternatively, if you only want to temporarily deactivate a purpose: Java Python Kotlin Go Raw REST API Deactivate a purpose 1 2 3 4 5 6 Purpose toUpdate = Purpose . updater ( // (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (2) \"Known Issues\" , // (3) false ) // (4) . build (); AssetMutationResponse response = toUpdate . save ( client ); // (5) Use the updater() method to update the purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. Setting this to false will deactivate the purpose, while setting it to true will activate the purpose. To then apply that activation / deactivation to the purpose in Atlan, call the save() method against the object you've built. Because this operation will persist the state in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Deactivate a purpose 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Purpose client = AtlanClient () to_update = Purpose . updater ( # (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , # (2) \"Known Issues\" , # (3) False # (4) ) response = client . asset . save ( to_update ) # (5) Use the updater() method to update the purpose. You must provide the qualified_name of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. Setting this to False will deactivate the purpose, while setting it to True will activate the purpose. To then apply that activation / deactivation to the purpose in Atlan, call the save() method with the object you've built. Deactivate a purpose 1 2 3 4 5 6 val toUpdate = Purpose . updater ( // (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (2) \"Known Issues\" , // (3) false ) // (4) . build () val response = toUpdate . save ( client ) // (5) Use the updater() method to update the purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. Setting this to false will deactivate the purpose, while setting it to true will activate the purpose. To then apply that activation / deactivation to the purpose in Atlan, call the save() method against the object you've built. Because this operation will persist the state in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Deactivate a purpose 1 2 3 4 5 6 7 toUpdate := & assets . Purpose {} toUpdate . Updater ( // (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (2) \"Known Issues\" , // (3) false , // (4) ) response , atlanErr := assets . Save ( toUpdate ) // (5) Use the Updater() method to update the purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. Setting this to false will deactivate the purpose, while setting it to True will activate the purpose. To then apply that activation / deactivation to the purpose in Atlan, call the Save() method with the object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ // (1) { \"typeName\" : \"Purpose\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (3) \"name\" : \"Known Issues\" // (4) \"isAccessControlEnabled\" : false // (5) } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name Purpose . You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. Setting this to false will deactivate the purpose, while setting it to true will activate the purpose. Add policies to a purpose Â¶ Do not add policies in bulk Be careful to only add policies one-by-one to a purpose. While the SDKs will allow you to add them in bulk, currently this results in a purpose where only the final policy in the batch is active at the end of the operation. Add a metadata policy Â¶ 0.0.14 7.0.0 4.0.0 To add a metadata policy to a purpose: Java Python Kotlin Go Raw REST API Add metadata policy to purpose 1 2 3 4 5 6 7 8 9 10 AuthPolicy metadata = Purpose . createMetadataPolicy ( // (1) \"Simple read access\" , // (2) \"3886a92c-2510-40ea-a14d-803d7ac1616b\" , // (3) AuthPolicyType . ALLOW , // (4) Set . of ( PurposeMetadataAction . READ ), // (5) null , // (6) null , // (7) true ) // (8) . build (); AssetMutationResponse response = metadata . save ( client ); // (9) Use the createMetadataPolicy() method to start building a metadata policy with the minimal required information. You must give the policy a name. You must provide the GUID of the purpose to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. To include all permissions If you want to include all permissions, you can simply use Arrays.asList(PurposeMetadataAction.values()) . (Optional) Specify the internal names of groups you want the policy to apply to. At least this or the list of users, or all users must be provided. (Optional) Specify the usernames of users you want the policy to apply to. At least this or the list of groups, or all users must be provided. (Optional) Apply this policy to all users. If this is set to true it will override the previous two parameters, or if false one of the previous two parameters (users or groups) must be specified. To then add the policy to the purpose in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add metadata policy to purpose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Purpose from pyatlan.model.enums import AuthPolicyType , PurposeMetadataAction client = AtlanClient () metadata = Purpose . create_metadata_policy ( # (1) client = client , # (2) name = \"Simple read access\" , # (3) purpose_id = \"3886a92c-2510-40ea-a14d-803d7ac1616b\" , # (4) policy_type = AuthPolicyType . ALLOW , # (5) actions = { PurposeMetadataAction . READ }, # (6) all_users = True , # (7) ) response = client . asset . save ( metadata ) # (8) Use the create_metadata_policy() method to start building a metadata policy with the minimal required information. You must provide a client instance. You must give the policy a name. You must provide the GUID of the purpose to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. Specify either the internal names of groups, the usernames of users, or this all_users option to control who you want the policy to apply to. At least one of these must be provided. To then add the policy to the purpose in Atlan, call the save() method with the policy object you've built. Add metadata policy to purpose 1 2 3 4 5 6 7 8 9 10 val metadata = Purpose . createMetadataPolicy ( // (1) \"Simple read access\" , // (2) \"3886a92c-2510-40ea-a14d-803d7ac1616b\" , // (3) AuthPolicyType . ALLOW , // (4) setOf ( PurposeMetadataAction . READ ), // (5) null , // (6) null , // (7) true ) // (8) . build () val response = metadata . save ( client ) // (9) Use the createMetadataPolicy() method to start building a metadata policy with the minimal required information. You must give the policy a name. You must provide the GUID of the purpose to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. To include all permissions If you want to include all permissions, you can simply use PurposeMetadataAction.values().toList() . (Optional) Specify the internal names of groups you want the policy to apply to. At least this or the list of users, or all users must be provided. (Optional) Specify the usernames of users you want the policy to apply to. At least this or the list of groups, or all users must be provided. (Optional) Apply this policy to all users. If this is set to true it will override the previous two parameters, or if false one of the previous two parameters (users or groups) must be specified. To then add the policy to the purpose in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add metadata policy to purpose 1 2 3 4 5 6 7 8 9 10 11 12 13 purpose := & assets . Purpose {} metadata , _ := purpose . CreateMetadataPolicy ( // (1) \"Simple read access\" , // (2) \"3886a92c-2510-40ea-a14d-803d7ac1616b\" , // (3) atlan . AuthPolicyTypeAllow , // (4) [] atlan . PurposeMetadataAction { atlan . PurposeMetadataActionRead , // (5) }, nil , // (6) nil , // (7) true , // (8) ) response , atlanErr := assets . Save ( metadata ) // (9) Use the CreateMetadataPolicy() method to start building a metadata policy with the minimal required information. You must give the policy a name. You must provide the GUID of the purpose to attach this policy to. Specify the type of policy (granting or denying the actions specified next). Specify the set of permissions you want to allow (or deny) in this policy. (Optional) Specify the internal names of groups you want the policy to apply to. At least this or the list of users, or all users must be provided. (Optional) Specify the usernames of users you want the policy to apply to. At least this or the list of users, or all users must be provided. (Optional) Apply this policy to all users. If this is set to true it will override the previous two parameters, or if false one of the previous two parameters (users or groups) must be specified. To then add the policy to the purpose in Atlan, call the Save() method with the policy object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \"entities\" : [ // (1) { \"typeName\" : \"AuthPolicy\" , // (2) \"attributes\" : { \"policySubCategory\" : \"metadata\" , // (3) \"policyCategory\" : \"purpose\" , // (4) \"policyType\" : \"allow\" , // (5) \"policyServiceName\" : \"atlas_tag\" , // (6) \"name\" : \"Simple read access\" , // (7) \"qualifiedName\" : \"Simple read access\" , // (8) \"policyActions\" : [ \"entity-read\" // (9) ], \"accessControl\" : { // (10) \"typeName\" : \"Purpose\" , // (11) \"guid\" : \"3886a92c-2510-40ea-a14d-803d7ac1616b\" // (12) }, \"policyResourceCategory\" : \"TAG\" , // (13) \"policyGroups\" : [ \"public\" // (14) ] } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name AuthPolicy . You must use a policy subcategory of metadata . You must use a policy category of purpose . Specify the type of policy (granting or denying the actions specified next). You must use a policy service name of atlas_tag . You must give the policy a name. You must give the policy itself a qualifiedName , although this will be overwritten by a generated value by the back-end. Specify the set of permissions you want to allow (or deny) in this policy. To review available permissions To review the available permissions, see the SDKs â€” for example, the PurposeMetadataAction enum in the Java SDK. Use an embedded accessControl object to define the purpose to attach this policy to. The embedded type name of the accessControl object must be exactly Purpose . You must provide the GUID of the purpose to attach this policy to. You must set the policy resource category to TAG . You must specify at least one username in a policyUsers array or one internal group name in a policyGroups array. The special group public covers all users. Add a data policy Â¶ 0.0.14 7.0.0 4.0.0 To add a data policy to a purpose: Java Python Kotlin Go Raw REST API Add data policy to purpose 1 2 3 4 5 6 7 8 9 10 AuthPolicy data = Purpose . createDataPolicy ( // (1) \"Mask the data\" , // (2) \"3886a92c-2510-40ea-a14d-803d7ac1616b\" , // (3) AuthPolicyType . DATA_MASK , // (4) null , // (5) null , // (6) true ) // (7) . policyMaskType ( DataMaskingType . HASH ) // (8) . build (); AssetMutationResponse response = data . save ( client ); // (9) Use the createDataPolicy() method to start building a data policy with the minimal required information. You must give the policy a name. You must provide the GUID of the purpose to attach this policy to. Specify the type of policy (granting, denying or masking the data of assets with the tags in the purpose). (Optional) Specify the names of internal groups you want the policy to apply to. At least this or the list of users, or all users must be provided. (Optional) Specify the usernames of users you want the policy to apply to. At least this or the list of groups, or all users must be provided. (Optional) Apply this policy to all users. If this is set to true it will override the previous two parameters, or if false one of the previous two parameters (users or groups) must be specified. If you set the policy type to DATA_MASK , you also need to chain on the type of masking you want to apply. To then add the policy to the purpose in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add data policy to purpose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Purpose from pyatlan.model.enums import AuthPolicyType , DataMaskingType client = AtlanClient () data = Purpose . create_data_policy ( # (1) client = client , # (2) name = \"Mask the data\" , # (3) purpose_id = \"3886a92c-2510-40ea-a14d-803d7ac1616b\" , # (4) policy_type = AuthPolicyType . DATA_MASK , # (5) all_users = True , # (6) ) data . policy_mask_type = DataMaskingType . HASH # (7) response = client . asset . save ( data ) # (8) Use the create_data_policy() method to start building a data policy with the minimal required information. You must provide a client instance. You must give the policy a name. You must provide the GUID of the purpose to attach this policy to. Specify the type of policy (granting, denying or masking the data of assets with the tags in the purpose). Specify either the names of internal groups, the usernames of users, or this all_users option to control who you want the policy to apply to. At least one of these must be provided. If you set the policy type to DATAMASK , you also need to set the type of masking you want to apply. To then add the policy to the purpose in Atlan, call the save() method with the policy object you've built. Add data policy to purpose 1 2 3 4 5 6 7 8 9 10 val data = Purpose . createDataPolicy ( // (1) \"Mask the data\" , // (2) \"3886a92c-2510-40ea-a14d-803d7ac1616b\" , // (3) AuthPolicyType . DATA_MASK , // (4) null , // (5) null , // (6) true ) // (7) . policyMaskType ( DataMaskingType . HASH ) // (8) . build () val response = data . save ( client ) // (9) Use the createDataPolicy() method to start building a data policy with the minimal required information. You must give the policy a name. You must provide the GUID of the purpose to attach this policy to. Specify the type of policy (granting, denying or masking the data of assets with the tags in the purpose). (Optional) Specify the names of internal groups you want the policy to apply to. At least this or the list of users, or all users must be provided. (Optional) Specify the usernames of users you want the policy to apply to. At least this or the list of groups, or all users must be provided. (Optional) Apply this policy to all users. If this is set to true it will override the previous two parameters, or if false one of the previous two parameters (users or groups) must be specified. If you set the policy type to DATA_MASK , you also need to chain on the type of masking you want to apply. To then add the policy to the purpose in Atlan, call the save() method against the policy object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add data policy to purpose 1 2 3 4 5 6 7 8 9 10 11 purpose := & assets . Purpose {} data , _ := purpose . CreateDataPolicy ( // (1) \"Mask the data\" , // (2) \"3886a92c-2510-40ea-a14d-803d7ac1616b\" , // (3) atlan . AuthPolicyTypeDatamask , // (4) nil , // (5) nil , // (6) true , // (7) ) data . PolicyMaskType = & atlan . DataMaskingTypeHASH // (8) response , atlanErr := assets . Save ( data ) // (9) Use the CreateDataPolicy() method to start building a data policy with the minimal required information. You must give the policy a name. You must provide the GUID of the purpose to attach this policy to. Specify the type of policy (granting, denying or masking the data of assets with the tags in the purpose). (Optional) Specify the names of internal groups you want the policy to apply to. At least this or the list of users, or all users must be provided. (Optional) Specify the usernames of users you want the policy to apply to. At least this or the list of groups, or all users must be provided. (Optional) Apply this policy to all users. If this is set to true it will override the previous two parameters, or if false one of the previous two parameters (users or groups) must be specified. If you set the policy type to DataMask , you also need to chain on the type of masking you want to apply. To then add the policy to the purpose in Atlan, call the Save() method against the policy object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"entities\" : [ // (1) { \"typeName\" : \"AuthPolicy\" , // (2) \"attributes\" : { \"policySubCategory\" : \"data\" , // (3) \"policyCategory\" : \"purpose\" , // (4) \"policyType\" : \"dataMask\" , // (5) \"policyMaskType\" : \"MASK_HASH\" , // (6) \"policyServiceName\" : \"atlas_tag\" , // (7) \"name\" : \"Mask the data\" , // (8) \"qualifiedName\" : \"Mask the data\" , // (9) \"policyActions\" : [ \"select\" // (10) ], \"accessControl\" : { // (11) \"typeName\" : \"Purpose\" , // (12) \"guid\" : \"3886a92c-2510-40ea-a14d-803d7ac1616b\" // (13) }, \"policyResourceCategory\" : \"TAG\" , // (14) \"policyGroups\" : [ \"public\" // (15) ] } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name AuthPolicy . You must use a policy subcategory of metadata . You must use a policy category of purpose . Specify the type of policy (granting, denying or masking the data of assets with the tags in the purpose). If you set the policy type to dataMask , you also need to set the type of masking you want to apply. To review available masking options To review the available masking options, see the SDKs â€” for example, the DataMaskingType enum in the Java SDK. You must use a policy service name of atlas_tag . You must give the policy a name. You must give the policy itself a qualifiedName , although this will be overwritten by a generated value by the back-end. Specify the set of permissions you want to allow (or deny) in this policy. A data policy for a purpose can only allow or deny select permissions. Use an embedded accessControl object to define the purpose to attach this policy to. The embedded type name of the accessControl object must be exactly Purpose . You must provide the GUID of the purpose to attach this policy to. You must set the policy resource category to TAG . You must specify at least one username in a policyUsers array or one internal group name in a policyGroups array. The special group public covers all users. List policies in a purpose Â¶ 0.0.14 1.4.0 4.0.0 To list all the policies in a purpose: Java Python Kotlin Go Raw REST API List all policies in a purpose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Purpose . select ( client ) // (1) . where ( Purpose . NAME . eq ( \"Known Issues\" )) // (2) . includeOnResults ( Purpose . POLICIES ) // (3) . includeOnRelations ( AuthPolicy . NAME ) // (4) . includeOnRelations ( AuthPolicy . POLICY_TYPE ) . includeOnRelations ( AuthPolicy . POLICY_ACTIONS ) . includeOnRelations ( AuthPolicy . POLICY_USERS ) . includeOnRelations ( AuthPolicy . POLICY_GROUPS ) . stream () // (5) . filter ( a -> a instanceof Purpose ) . forEach ( p -> { // (6) Set < IAuthPolicy > policies = (( Purpose ) p ). getPolicies (); for ( IAuthPolicy policy : policies ) { // Do something with each policy } }); Start by selecting a purpose, here using a FluentSearch-based approach. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can select the purpose by whatever you like, in this example we are selecting based on its name. Include the policies for the purpose as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and users controlled by each policy. You can then directly stream the results of the search. For each result of the search (itself a Purpose), you can then retrieve its policies and iterate through them. List all policies in a purpose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from typing import cast from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AuthPolicy , Purpose from pyatlan.model.fluent_search import FluentSearch client = AtlanClient () request = ( FluentSearch () . where ( FluentSearch . asset_type ( Purpose )) # (1) . where ( Purpose . NAME . eq ( \"Known Issues\" )) # (2) . include_on_results ( Purpose . POLICIES ) # (3) . include_on_relations ( AuthPolicy . POLICY_TYPE ) # (4) . include_on_relations ( AuthPolicy . POLICY_ACTIONS ) . include_on_relations ( AuthPolicy . POLICY_USERS ) . include_on_relations ( AuthPolicy . POLICY_GROUPS ) ) . to_request () # (5) response = client . asset . search ( request ) # (6) for p in response : # (7) policies = cast ( Purpose , p ) . policies for policy in policies : # Do something with each policy Start by selecting a purpose, here using a FluentSearch-based approach. You can select the purpose by whatever you like, in this example we are selecting based on its name. Include the policies for the purpose as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and users controlled by each policy. You can then translate the FluentSearch into a search request. Run a search using the search request. For each result of the search (itself a Purpose), you can then retrieve its policies and iterate through them. List all policies in a purpose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Purpose . select ( client ) // (1) . where ( Purpose . NAME . eq ( \"Known Issues\" )) // (2) . includeOnResults ( Purpose . POLICIES ) // (3) . includeOnRelations ( AuthPolicy . NAME ) // (4) . includeOnRelations ( AuthPolicy . POLICY_TYPE ) . includeOnRelations ( AuthPolicy . POLICY_ACTIONS ) . includeOnRelations ( AuthPolicy . POLICY_USERS ) . includeOnRelations ( AuthPolicy . POLICY_GROUPS ) . stream () // (5) . filter { it is Purpose } . forEach { // (6) val policies = ( it as Purpose ). policies for ( policy in policies ) { // Do something with each policy } } Start by selecting a purpose, here using a FluentSearch-based approach. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can select the purpose by whatever you like, in this example we are selecting based on its name. Include the policies for the purpose as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and users controlled by each policy. You can then directly stream the results of the search. For each result of the search (itself a Purpose), you can then retrieve its policies and iterate through them. List all policies in a purpose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 response , atlanErr := assets . NewFluentSearch (). AssetType ( \"Purpose\" ). // (1) Where ( ctx . Purpose . NAME . Eq ( \"Known Issues\" )). // (2) IncludeOnResults ( ctx . Purpose . POLICIES . GetAtlanFieldName ()). // (3) IncludeOnRelations ( ctx . AuthPolicy . POLICY_TYPE . GetAtlanFieldName ()). // (4) IncludeOnRelations ( ctx . AuthPolicy . POLICY_ACTIONS . GetAtlanFieldName ()). IncludeOnRelations ( ctx . AuthPolicy . POLICY_USERS . GetAtlanFieldName ()). IncludeOnRelations ( ctx . AuthPolicy . POLICY_GROUPS . GetAtlanFieldName ()). Execute () // (5) if atlanErr != nil { logger . Log . Errorf ( \"Error: %v\" , atlanErr ) } for _ , entity := range response [ 0 ]. Entities { // (6) if entity . TypeName != nil && * entity . TypeName == \"Purpose\" { for _ , policy := range * entity . Policies { // Do something with each Policy } } } Start by selecting a purpose, here using a FluentSearch-based approach. You can select the purpose by whatever you like, in this example we are selecting based on its name. Include the policies for the purpose as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and users controlled by each policy. Run a search using the search request. For each result of the search (itself a Purpose), you can then retrieve its policies and iterate through them. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 { \"dsl\" : { // (1) \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"Purpose\" } } }, { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { \"name.keyword\" : { \"value\" : \"Known Issues\" // (2) } } } ] } }, \"sort\" : [ { \"__guid\" : { \"order\" : \"asc\" } } ], \"track_total_hits\" : true }, \"attributes\" : [ \"policies\" // (3) ], \"relationAttributes\" : [ // (4) \"name\" , \"policyType\" , \"policyResources\" , \"policyActions\" ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Start by running a search for purposes. You can select the purpose by whatever you like, in this example we are selecting based on its name. Include the policies for the purpose as part of the search results. Include all the attributes you want about each policy on the relations of the search results. Here we are including the name, type, actions and users controlled by each policy. Personalize the purpose Â¶ 0.0.14 2.0.0 4.0.0 To personalize which details to show for assets within a purpose: Java Python Kotlin Go Raw REST API Personalize the purpose 1 2 3 4 5 6 7 8 9 Purpose toUpdate = Purpose . updater ( // (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (2) \"Known Issues\" , // (3) true ) // (4) . denyAssetTab ( AssetSidebarTab . LINEAGE ) // (5) . denyAssetTab ( AssetSidebarTab . RELATIONS ) . denyAssetTab ( AssetSidebarTab . QUERIES ) . build (); AssetMutationResponse response = toUpdate . save ( client ); // (6) Use the updater() method to update a purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then chain preferences on which metadata tabs should be hidden when using this purpose. To update the purpose in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Personalize the purpose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Purpose from pyatlan.model.enums import AssetSidebarTab client = AtlanClient () to_update = Purpose . updater ( # (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , # (2) \"Known Issues\" , # (3) True # (4) ) to_update . deny_asset_tabs = { # (5) AssetSidebarTab . LINEAGE . value , AssetSidebarTab . RELATIONS . value , AssetSidebarTab . QUERIES . value , } response = client . asset . save ( to_update ) # (6) Use the updater() method to update a purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then set preferences on which metadata tabs should be hidden when using this purpose. To update the purpose in Atlan, call the save() method with the object you've built. Personalize the purpose 1 2 3 4 5 6 7 8 9 val toUpdate = Purpose . updater ( // (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (2) \"Known Issues\" , // (3) true ) // (4) . denyAssetTab ( AssetSidebarTab . LINEAGE ) // (5) . denyAssetTab ( AssetSidebarTab . RELATIONS ) . denyAssetTab ( AssetSidebarTab . QUERIES ) . build () val response = toUpdate . save ( client ) // (6) Use the updater() method to update a purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then chain preferences on which metadata tabs should be hidden when using this purpose. To update the purpose in Atlan, call the save() method against the object you've built. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Personalize the purpose 1 2 3 4 5 6 7 8 9 10 11 12 toUpdate := & assets . Purpose {} toUpdate . Updater ( // (1) \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (2) \"Known Issues\" , // (3) true , // (4) ) toUpdate . DenyAssetTabs = & [] string { // (5) atlan . AssetSidebarTabLineage . Name , atlan . AssetSidebarTabRelations . Name , atlan . AssetSidebarTabQueries . Name , } response , atlanErr := assets . Save ( toUpdate ) // (6) Use the Updater() method to update a purpose. You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then set preferences on which metadata tabs should be hidden when using this purpose. To update the purpose in Atlan, call the Save() method with the object you've built. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"entities\" : [ // (1) { \"typeName\" : \"Purpose\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/29LZO9Z6ipZbGT6caWTxRB\" , // (3) \"name\" : \"Known Issues\" // (4) \"isAccessControlEnabled\" : true , // (5) \"denyAssetTabs\" : [ // (6) \"Lineage\" , \"Relations\" , \"Queries\" ] } } ] } Wrap all updates in an entities array. For each embedded object, use the exact type name Purpose . You must provide the qualifiedName of the purpose. You must provide the name of the purpose. You must provide whether the purpose should be active (enabled) or deactivated after the update. You can then set preferences on which metadata tabs should be hidden when using this purpose. To review available tabs To review the available, see the SDKs â€” for example, the AssetSidebarTab enum in the Java SDK. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/",
    "content": "Asset CRUD operations Â¶ In this section you'll find details about creating, retrieving, updating, deleting, and searching for assets . Rather than covering every asset type in detail (since there are many), the subsections focus on the operations and patterns to follow for each operation. For more details on specific assets that are primarily intended to be managed programmatically, see the API-first models . Create an asset Retrieve an asset Update an asset Delete an asset Restore an asset View history of an asset Search for assets In many cases, you can also combine multiple operations into a single API call for efficiency. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/delete/",
    "content": "/api/meta/entity/bulk (DELETE) Deleting assets Â¶ Deleting an asset uses a similar pattern to the retrieval operations. For this you can use static methods provided by the Asset class. Avoid deleting connections If you want to delete a connection and all of its assets, consider using the connection delete package instead. In particular, avoid deleting a connection directly (using the methods below) without first deleting the assets contained within it. Once you delete a connection, you will be unable to delete any assets that were within it. Soft-delete an asset Â¶ Soft-deletes (also called an archive ) are a reversible operation. The status of the asset is changed to DELETED and it no longer appears in the UI, but the asset is still present in Atlan's back-end. 1.4.0 4.0.0 To soft-delete (archive) an asset, you only need to provide the GUID: Java Python Kotlin Raw REST API Soft-delete an asset 1 2 3 4 5 6 7 AssetMutationResponse response = Asset . delete ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ); // (1) Asset deleted = response . getDeletedAssets (). get ( 0 ); // (2) Glossary glossary ; if ( deleted instanceof Glossary ) { glossary = ( Glossary ) deleted ; // (3) } The delete() method returns the deleted form of the asset. Because this operation will archive the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was deleted through the getDeletedAssets() method. This lists only the assets deleted by the operation. The Asset class is a superclass of all assets. So you need to cast to more specific types (like Glossary ) after verifying the object that was actually returned. Soft-delete an asset 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary client = AtlanClient () response = client . asset . delete_by_guid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ) # (1) if deleted := response . assets_deleted ( asset_type = AtlasGlossary ): # (2) term = deleted [ 0 ] # (3) The asset.delete_by_guid() method returns the deleted form of the asset. The assets_deleted(asset_type=AtlasGlossary) method returns a list of the assets of the given type that were deleted. If an asset of the given type was deleted, then the deleted form of the asset is available. Soft-delete an asset 1 2 3 4 val response = Asset . delete ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (1) val deleted = response . deletedAssets [ 0 ] // (2) val glossary = if ( deleted is Glossary ) deleted else null // (3) The delete() method returns the deleted form of the asset. Because this operation will archive the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was deleted through the deletedAssets method. This lists only the assets deleted by the operation. The Asset class is a superclass of all assets. So you need to cast to more specific types (like Glossary ) after verifying the object that was actually returned. DELETE /api/meta/entity/bulk?guid=b4113341-251b-4adc-81fb-2420501c30e6&deleteType=SOFT 1 // (1) In the case of deleting an asset, all necessary information is included in the URL of the request. There is no payload for the body of the request. To archive an asset, use deleteType of SOFT . Hard-delete an asset Â¶ Hard-deletes (also called a purge ) are irreversible operations. The asset is removed from Atlan entirely, so no longer appears in the UI and also no longer exists in Atlan's back-end. 1.4.0 4.0.0 To hard-delete (purge) an asset, you only need to provide the GUID: Java Python Kotlin Raw REST API Hard-delete (purge) an asset 1 2 3 4 5 6 7 AssetMutationResponse response = Asset . purge ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ); // (1) Asset deleted = response . getDeletedAssets (). get ( 0 ); // (2) Glossary glossary ; if ( deleted instanceof Glossary ) { glossary = ( Glossary ) deleted ; // (3) } The purge() method returns the purged form of the asset. Because this operation will remove the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was purged through the getDeletedAssets() method. This lists only the assets deleted by the operation. The Asset class is a superclass of all assets. So you need to cast to more specific types (like Glossary ) after verifying the object that was actually returned. Hard-delete (purge) an asset 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary client = AtlanClient () response = client . asset . purge_by_guid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ) # (1) if deleted := response . assets_deleted ( asset_type = AtlasGlossary ): # (2) term = deleted [ 0 ] # (3) The asset.purge_by_guid() method returns the deleted form of the asset. The assets_deleted(asset_type=AtlasGlossary) method returns a list of the assets of the given type that were deleted. If an asset of the given type was deleted, then the deleted form of the asset is available. Hard-delete (purge) an asset 1 2 3 4 val response = Asset . purge ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (1) val deleted = response . deletedAssets [ 0 ] // (2) val glossary = if ( deleted is Glossary ) deleted else null // (3) The purge() method returns the purged form of the asset. Because this operation will remove the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was purged through the deletedAssets method. This lists only the assets deleted by the operation. The Asset class is a superclass of all assets. So you need to cast to more specific types (like Glossary ) after verifying the object that was actually returned. DELETE /api/meta/entity/bulk?guid=b4113341-251b-4adc-81fb-2420501c30e6&deleteType=PURGE 1 // (1) In the case of deleting an asset, all necessary information is included in the URL of the request. There is no payload for the body of the request. To permanently and irreversibly remove an asset, use deleteType of PURGE . Bulk-delete assets Â¶ 1.4.0 1.0.0 You can also delete a number of assets at the same time: Up to a limit You cannot send an unlimited number of assets to be deleted in a single request. As you can see from the Raw REST API tab, each GUID will be sent as a query parameter in the URI â€” so there is a maximum beyond which the URI is too long. We generally recommend sending no more than 20-50 GUIDs at a time using this approach. Java Python Kotlin Raw REST API Hard-delete (purge) multiple assets 1 2 3 4 5 6 7 AssetMutationResponse response = client . assets . delete ( // (1) List . of ( \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (2) \"21e5be62-7a0b-4547-ab7a-6ddf273d0640\" , \"a0fb35e5-690d-4a5b-8918-9ee267c8fa55\" ), AtlanDeleteType . PURGE ); // (3) List < Asset > deleted = response . getDeletedAssets (); // (4) The delete() method on the endpoint itself can be used to either archive (soft-delete) or purge (hard-delete). You can provide a list of any number of assets to delete (their GUIDs). You need to also specify whether you want to soft-delete (archive) using AtlanDeleteType.SOFT or hard-delete (purge) the assets using AtlanDeleteType.PURGE . The response will contain details of all of the assets that were deleted. Hard-delete (purge) multiple assets 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary client = AtlanClient () response = client . asset . purge_by_guid ([ # (1) \"b4113341-251b-4adc-81fb-2420501c30e6\" , \"21e5be62-7a0b-4547-ab7a-6ddf273d0640\" , \"a0fb35e5-690d-4a5b-8918-9ee267c8fa55\" ]) if deleted := response . assets_deleted ( asset_type = AtlasGlossary ): # (2) term = deleted [ 0 ] # (3) You can alternatively provide either the asset.purge_by_guid() or asset.delete_by_guid() methods with a list of any number of assets to delete (their GUIDs). The assets_deleted(asset_type=AtlasGlossary) method returns a list of the assets of the given type that were deleted. If an asset of the given type was deleted, then the deleted form of the asset is available. Hard-delete (purge) multiple assets 1 2 3 4 5 6 7 8 9 10 val response = client . assets . delete ( // (1) listOf ( \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (2) \"21e5be62-7a0b-4547-ab7a-6ddf273d0640\" , \"a0fb35e5-690d-4a5b-8918-9ee267c8fa55\" ), AtlanDeleteType . PURGE // (3) ) val deleted = response . deletedAssets // (4) The delete() method on the endpoint itself can be used to either archive (soft-delete) or purge (hard-delete). You can provide a list of any number of assets to delete (their GUIDs). You need to also specify whether you want to soft-delete (archive) using AtlanDeleteType.SOFT or hard-delete (purge) the assets using AtlanDeleteType.PURGE . The response will contain details of all of the assets that were deleted. DELETE /api/meta/entity/bulk?guid=b4113341-251b-4adc-81fb-2420501c30e6&guid=21e5be62-7a0b-4547-ab7a-6ddf273d0640&guid=a0fb35e5-690d-4a5b-8918-9ee267c8fa55&deleteType=SOFT 1 // (1) In the case of deleting multiple assets, all necessary information is included in the URL of the request. Each separate asset's GUID should be given after a guid= query parameter. There is no payload for the body of the request. Bulk deletion occurs asynchronously Be aware that bulk deleting assets occurs asynchronously. The response above will come back indicating the deleted assets; however, there can still be a delay before those assets are fully deleted in Atlan. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/create/",
    "content": "/api/meta/entity/bulk (POST) Creating an asset Â¶ All objects in the SDK that you can create within Atlan implement the builder pattern. This allows you to progressively build-up the object you want to create. In addition, each object provides a method that takes the minimal set of required fields to create that asset . Each type of asset has a different containment hierarchy Every asset in Atlan can have slightly different parent objects in which they exist. For example, a GlossaryTerm cannot exist outside a Glossary . A Column cannot exist outside a Table , View or MaterializedView ; these cannot exist outside a Schema ; which cannot exist outside a Database ; which cannot exist outside a Connection . The minimal required fields for each asset type will therefore be slightly different. Creation order is important As a result of this containment, creation order is important. Parent objects must be created (exist) before child objects can be created. Build minimal object needed Â¶ 2.0.0 1.0.0 For example, to create a glossary term you need to provide the name of the term and either the GUID or qualifiedName of the glossary in which to create the term: Java Python Kotlin Raw REST API Build minimal asset necessary for creation 1 2 3 GlossaryTermBuilder <? , ?> termCreator = GlossaryTerm . creator ( \"Example Term\" , // (1) \"b4113341-251b-4adc-81fb-2420501c30e6\" ); // (2) A name for the new term. The GUID or qualifiedName of the glossary in which to create the term. Build minimal asset necessary for creation 1 2 3 4 5 6 7 8 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm client = AtlanClient () term = AtlasGlossaryTerm . creator ( name = \"Example Term\" , # (1) glossary_guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" # (2) ) A name for the new term. The GUID of the glossary in which to create the term. Build minimal asset necessary for creation 1 2 3 4 5 val termCreator = GlossaryTerm . creator ( \"Example Term\" , // (1) \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (2) ) A name for the new term. The GUID or qualifiedName of the glossary in which to create the term. Implicit in the API calls below There is nothing specific to do for this step when using the raw APIs â€” constructing the object is simply what you place in the payload of the API calls in the steps below. Create the asset from the object Â¶ 1.4.0 4.0.0 This term object will have the minimal required information for Atlan to create it. You must then actually persist the object in Atlan 1 : Java Python Kotlin Raw REST API Create the asset 5 6 7 8 9 10 11 12 13 14 15 GlossaryTerm term = termCreator . build (); // (1) AssetMutationResponse response = term . save ( client ); // (2) Asset created = response . getCreatedAssets (). get ( 0 ); // (3) if ( created instanceof GlossaryTerm ) { term = ( GlossaryTerm ) created ; // (4) } Asset updated = response . getUpdatedAssets (). get ( 0 ); // (5) Glossary glossary ; if ( updated instanceof Glossary ) { glossary = ( Glossary ) updated ; // (6) } Before you can take actions on the builder object you've been interacting with, you need to build() it into a full object. Then you can do operations, like save() , which will either: create a new asset, if Atlan does not have a term with the same name in the same glossary update an existing asset, if Atlan already has a term with the same name in the same glossary Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: getCreatedAssets() lists assets that were created getUpdatedAssets() lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So you need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. In this example, creating the GlossaryTerm actually also updates the parent Glossary . This is why the response contains generic Asset objects rather than specific types â€” any operation could side-effect a number of different assets. Like with the GlossaryTerm , you can check and cast the generic Asset returned by the response into its more specific type ( Glossary ). Create the asset 9 10 11 12 13 14 15 response = client . asset . save ( term ) # (1) created = response . assets_created ( asset_type = AtlasGlossaryTerm ) # (2) if created : # (3) term = created [ 0 ] # (4) updated = response . assets_updated ( asset_type = AtlasGlossaryTerm ) # (5) if updated : # (6) term = updated [ 0 ] # (7) Call the save method which will create or update the asset in atlan. You can distinguish what was created or updated: assets_created(asset_type=AtlasGlossaryTerm) returns a lists assets of the specified type that were created. assets_updated(asset_type=AtlasGlossaryTerm) returns a lists assets of the specified type that were updated. Check if the list is empty to determine if an AtlasGlossaryTerm was created. Get the new AtlasGlossaryTerm that was created. In this example, creating the AtlasGlossaryTerm actually also updates the parent AtlasGlossary . This is why the response contains an AtlasGlossary . Check if the list is empty to determine if an AtlasGlossary was updated. Get the AtlasGlossary that was updated. Create the asset 6 7 8 9 10 11 12 13 var term = termCreator . build () // (1) val response = term . save ( client ) // (2) val created = response . createdAssets [ 0 ] // (3) if ( created is GlossaryTerm ) { term = created // (4) } val updated = response . updatedAssets [ 0 ] // (5) val glossary = if ( updated is Glossary ) updated else null // (6) Before you can take actions on the builder object you've been interacting with, you need to build() it into a full object. Then you can do operations, like save() , which will either: create a new asset, if Atlan does not have a term with the same name in the same glossary update an existing asset, if Atlan already has a term with the same name in the same glossary Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: getCreatedAssets() lists assets that were created getUpdatedAssets() lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So you need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. In this example, creating the GlossaryTerm actually also updates the parent Glossary . This is why the response contains generic Asset objects rather than specific types â€” any operation could side-effect a number of different assets. Like with the GlossaryTerm , you can check and cast the generic Asset returned by the response into its more specific type ( Glossary ). POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryTerm\" , // (2) \"attributes\" : { \"name\" : \"Example Term\" , // (3) \"qualifiedName\" : \"Example Term\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" } } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). For a term, this is AtlasGlossaryTerm . You must provide the exact name of the asset (case-sensitive). You must provide a qualifiedName of the asset (case-sensitive). In the case of glossary objects (like terms), this will actually be replaced in the back-end with a generated qualifiedName , but you must provide some value when creating the object. You must also specify the parent object in which this object is contained (if any). In the case of a term, it can only exist within a glossary. So here we specify the details of the parent glossary through the anchor relationship (specific to glossary assets). (Optional) Enrich before creating Â¶ 2.0.0 4.0.0 If you want to further enrich the asset before creating it, you can do this using the builder pattern: Java Python Kotlin Raw REST API Alternatively, further enrich the asset before creating it 5 6 7 8 9 10 11 GlossaryTerm term = termCreator // (1) . certificateStatus ( CertificateStatus . VERIFIED ) // (2) . announcementType ( AtlanAnnouncementType . INFORMATION ) . announcementTitle ( \"Imported\" ) . announcementMessage ( \"This term was imported from ...\" ) . build (); // (3) AssetMutationResponse response = term . save ( client ); // (4) We'll create an object you can take actions on from this creator. In this example, you're adding a certificate and announcement to the object. To persist the enrichment back to the object, you must build() the builder. You can call the save() operation against this enriched object, the same as shown earlier. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Assign the result back Remember to assign the result of the build() operation back to a variable! (In the example above this happens on line 5 with GlossaryTerm term = .) Alternatively, further enrich the asset before creating it 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary , AtlasGlossaryTerm from pyatlan.model.enums import AnnouncementType , CertificateStatus client = AtlanClient () term = AtlasGlossaryTerm . creator ( name = \"Example Term\" , glossary_guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" ) term . certificate_status = CertificateStatus . VERIFIED announcement = Announcement ( announcement_type = AnnouncementType . INFORMATION , announcement_title = \"Imported\" , announcement_message = \"This term was imported from ..\" , ) term . set_announcement ( announcement ) response = client . asset . save ( term ) # (1) You can call the save() operation against this enriched object, the same as shown earlier. Alternatively, further enrich the asset before creating it 6 7 8 9 10 11 12 val term = termCreator // (1) . certificateStatus ( CertificateStatus . VERIFIED ) // (2) . announcementType ( AtlanAnnouncementType . INFORMATION ) . announcementTitle ( \"Imported\" ) . announcementMessage ( \"This term was imported from ...\" ) . build () // (3) val response = term . save ( client ) // (4) We'll create an object you can take actions on from this creator. In this example, you're adding a certificate and announcement to the object. To persist the enrichment back to the object, you must build() the builder. You can call the save() operation against this enriched object, the same as shown earlier. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Assign the result back Remember to assign the result of the build() operation back to a variable! (In the example above this happens on line 6 with val term = .) POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryTerm\" , \"attributes\" : { \"name\" : \"Example Term\" , \"qualifiedName\" : \"Example Term\" , \"anchor\" : { \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" }, \"certificateStatus\" : \"VERIFIED\" , // (2) \"announcementType\" : \"information\" , \"announcementTitle\" : \"Imported\" , \"announcementMessage\" : \"This term was imported from...\" } } ] } You would still create the asset by wrapping it within the entities array. But you can also extend the information you store on the asset. In this example, you're adding a certificate and announcement to the object when it is created. Why no distinction between creation and update? This has to do with how Atlan detects changes â€” see the Importance of identifiers for a more detailed explanation. â†© 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/history/",
    "content": "/api/meta/entity/auditSearch (POST) Viewing the history of an asset Â¶ Accessing the history of an asset in Atlan is a flexible operation. This also makes it a bit more complex to understand than the other operations. To encapsulate the full flexibility of Atlan's search, the SDK provides a dedicated AuditSearchRequest object. Similar but not identical to searching in general Atlan's audit log that contains the history of an asset uses Elasticsearch. This makes the approach you use to access history similar to searching . However, there are differences as the audit log uses a different index than the broader search. If you're feeling brave, feel free to experiment with the more complex search mechanisms outlined in the searching section. But this should be sufficient to get you started with accessing asset history. Build the request Â¶ To retrieve an asset's history in Atlan, you need to define the request. For simplicity, we provide helper methods to retrieve a defined number of entries in reverse-chronological order (most recent entries first). By GUID Â¶ 3.0.0 4.0.0 To request the history of an asset by GUID: Java Python Kotlin Raw REST API Build the query by GUID 1 2 3 4 5 AuditSearchRequest request = AuditSearchRequest . byGuid ( // (1) client , // (2) \"6fc01478-1263-42ae-b8ca-c4a57da51392\" , // (3) 10 ) // (4) . build (); // (5) Create a request for the history of an asset, by its GUID. Because this operation will directly look up the asset's history in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Specify the GUID of the asset. Specify the amount of history (maximum number of activities). This will be in reverse-chronological order (most recent entries first). Build the request. Build the query by GUID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import logging from pyatlan.model.audit import CustomMetadataAttributesAuditDetail from pyatlan.model.assets import Table from pyatlan.model.core import AtlanTag from pyatlan.client.atlan import AtlanClient from pyatlan.client.audit import AuditSearchRequest from pyatlan.model.search import SortItem from pyatlan.model.enums import SortOrder LOGGER = logging . getLogger ( __name__ ) LOGGER . setLevel ( logging . INFO ) client = AtlanClient () request = AuditSearchRequest . by_guid ( # (1) guid = \"6fc01478-1263-42ae-b8ca-c4a57da51392\" , # (2) size = 10 , # (3) sort = [ SortItem ( \"created\" , order = SortOrder . DESCENDING )] # (4) ) Create a request for the history of an asset, by its GUID. Specify the GUID of the asset. (optional) Specify the amount of history (maximum number of activities). Defaults to 10 . (optional) Specify sorting criteria for the results.\nBy default, it sorts in reverse-chronological order, with the most recent entries first. Build the query by GUID 1 2 3 4 5 val request = AuditSearchRequest . byGuid ( // (1) client , // (2) \"6fc01478-1263-42ae-b8ca-c4a57da51392\" , // (3) 10 // (4) ). build () // (5) Create a request for the history of an asset, by its GUID. Because this operation will directly look up the asset's history in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Specify the GUID of the asset. Specify the amount of history (maximum number of activities). This will be in reverse-chronological order (most recent entries first). Build the request. Bool query contents 7 8 9 10 11 12 13 14 15 \"filter\" : [ // (1) { \"term\" : { // (2) \"entityId\" : { // (3) \"value\" : \"6fc01478-1263-42ae-b8ca-c4a57da51392\" } } } ] To retrieve history for a specific asset by that asset's GUID, start with a filter. Within the filter run a term query. And specifically filter by the field entityId in the index. By qualifiedName Â¶ 3.0.0 4.0.0 To request the history of an asset by qualifiedName: Java Python Kotlin Raw REST API Build the query by qualifiedName 1 2 3 4 5 AuditSearchRequest request = AuditSearchRequest . byQualifiedName ( // (1) client , // (2) Glossary . TYPE_NAME , \"FzCMyPR2LxkPFgr8eNGrq\" , // (3) 10 ) // (4) . build (); // (5) Create a request for the history of an asset, by its qualifiedName. Because this operation will directly look up the asset's history in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Specify the type of the asset and qualifiedName of the asset. Specify the amount of history (maximum number of activities). This will be in reverse-chronological order (most recent entries first). Build the request. Build the query by qualifiedName 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import logging from pyatlan.model.audit import CustomMetadataAttributesAuditDetail from pyatlan.model.assets import Table from pyatlan.model.core import AtlanTag from pyatlan.client.atlan import AtlanClient from pyatlan.client.audit import AuditSearchRequest from pyatlan.model.search import SortItem from pyatlan.model.enums import SortOrder LOGGER = logging . getLogger ( __name__ ) LOGGER . setLevel ( logging . INFO ) client = AtlanClient () request = AuditSearchRequest . by_qualified_name ( # (1) type_name = \"AtlasGlossary\" , # (2) qualified_name = \"FzCMyPR2LxkPFgr8eNGrq\" , # (3) size = 10 , # (4) sort = [ SortItem ( \"created\" , order = SortOrder . DESCENDING )] # (5) ) Create a request for the history of an asset, by its qualifiedName. Specify the type of the asset Specify the qualifiedName of the asset. (optional) Specify the amount of history (maximum number of activities). Defaults to 10 . (optional) Specify sorting criteria for the results.\nBy default, it sorts in reverse-chronological order, with the most recent entries first. Build the query by qualifiedName 1 2 3 4 5 val request = AuditSearchRequest . byQualifiedName ( // (1) client , // (2) Glossary . TYPE_NAME , \"FzCMyPR2LxkPFgr8eNGrq\" , // (3) 10 // (4) ). build () // (5) Create a request for the history of an asset, by its qualifiedName. Because this operation will directly look up the asset's history in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Specify the type of the asset and qualifiedName of the asset. Specify the amount of history (maximum number of activities). This will be in reverse-chronological order (most recent entries first). Build the request. Bool query contents 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \"must\" : [ // (1) { \"term\" : { // (2) \"entityQualifiedName\" : { // (3) \"value\" : \"FzCMyPR2LxkPFgr8eNGrq\" } } }, { \"term\" : { \"typeName\" : { // (4) \"value\" : \"AtlasGlossary\" } } } ] To retrieve history for a specific asset by that asset's qualifiedName, you need to combine several conditions. You need a term query for both conditions. One condition you must provide is for the entityQualifiedName , giving the qualifiedName of the asset for which you want to retrieve history. You also need to define the typeName of the asset for which you want to retrieve history, when retrieving by qualifiedName. Run the search Â¶ 3.0.0 4.0.0 To now run the search, we call the search() method against our request object: Java Python Kotlin Raw REST API Run the search 5 6 7 AuditSearchResponse response = request . search ( client ); log . info ( response . getCount ()); // (1) List < EntityAudit > results = response . getEntityAudits (); // (2) The getCount() method gives the total number of activities. Note that this could be smaller than the number requested, if fewer activities have occurred against the asset than the number used in the request. The details of each activity can be accessed through the getEntityAudits() method on the response. Run the search 17 18 response = client . audit . search ( criteria = request , bulk = False ) #(1) LOGGER . info ( response . total_count ) # (2) client.audit.search() method takes following parameters: criteria : defines the search query to execute the search. bulk ( default: False ): specifies whether to execute the search in audit bulk mode for retrieving the history of assets matching the criteria. This mode is optimized for handling large results (more than 10,000 ). When enabled ( True ), the results will be reordered based on the creation timestamp to facilitate iterating through large datasets. Note If the number of results exceeds the predefined threshold\n( 10,000 assets) audit search will be automatically converted into a bulk audit search. The total_count property gives the total number of activities. Note that this could be smaller than the number requested, if fewer activities have occurred against the asset than the number used in the request. Run the search 5 6 7 val response : AuditSearchResponse = request . search ( client ) log . info ( response . count ) // (1) val results = response . entityAudits // (2) The .count member gives the total number of activities. Note that this could be smaller than the number requested, if fewer activities have occurred against the asset than the number used in the request. The details of each activity can be accessed through the entityAudits member on the response. POST /api/meta/entity/auditSearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"dsl\" : { \"from\" : 0 , \"size\" : 10 , \"query\" : { \"bool\" : { // (1) } }, \"sort\" : [ { \"created\" : { \"order\" : \"desc\" } } ], \"track_total_hits\" : true // (2) } } Replace the contents of the bool portion of the query with the appropriate snippet from the earlier steps. You must set track_total_hits to true if you want an exact count of the number of results (in particular for pagination). Review details of each activity Â¶ Each EntityAudit entry contains details of what occurred during an activity. Contextual details Â¶ 3.0.0 1.0.0 To access contextual details about the activity: Java Python Kotlin Raw REST API Access contextual details about each activity 8 9 10 11 12 13 for ( EntityAudit result : results ) { // (1) AuditActionType action = result . getAction (); // (2) String user = result . getUser (); // (3) Long when = result . getTimestamp (); // (4) AuditDetail detail = result . getDetail (); // (5) ... You can then iterate through each activity, in reverse-chronological order (most recent first). You can access the type of activity through getAction() . This will tell you whether attributes were updated on the asset, an Atlan tag was added, custom metadata was changed, and so on. You can access who carried out the activity through getUser() . This will give you the username (or API token) that made the change. You can review when the activity occurred through getTimestamp() . This gives an epoch-based time (in milliseconds) for when the activity occurred. You can also review what specifically changed through the activity, using getDetail() . More information on what this includes is in the section below. Access contextual details about each activity 19 20 21 22 23 for result in response : # (1) action = result . action # (2) user = result . user # (3) when = result . timestamp # (4) detail = result . detail # (5) You can then iterate through each activity, in reverse-chronological order (most recent first). You can access the type of activity through the action property. This will tell you whether attributes were updated on the asset, an Atlan tag was added, custom metadata was changed, and so on. You can access who carried out the activity through the user property. This will give you the username (or API token) that made the change. You can review when the activity occurred through then timestamp property. This gives an epoch-based time (in milliseconds) for when the activity occurred. You can also review what specifically changed through the activity, using the detail property. More information on what this includes is in the section below. Access contextual details about each activity 8 9 10 11 12 13 for ( result in results ) { // (1) val action = result . action // (2) val user = result . user // (3) val timestamp = result . timestamp // (4) val detail = result . detail // (5) ... You can then iterate through each activity, in reverse-chronological order (most recent first). You can access the type of activity through .action . This will tell you whether attributes were updated on the asset, an Atlan tag was added, custom metadata was changed, and so on. You can access who carried out the activity through .user . This will give you the username (or API token) that made the change. You can review when the activity occurred through .timestamp . This gives an epoch-based time (in milliseconds) for when the activity occurred. You can also review what specifically changed through the activity, using .detail . More information on what this includes is in the section below. Response contains contextual details Each object entry in the entityAudits portion of the response will contain contextual details about a single activity on the asset. Details of the change Â¶ Each detail of each record in the activity log tells you the details of what specifically changed through one specific activity. This can be one of three kinds of objects: Action Detail type Contents ENTITY_UPDATE , ENTITY_CREATE An asset object (the specific subtype, such as Column or Table ) What was changed on the asset by the activity. CLASSIFICATION_ADD , CLASSIFICATION_DELETE , PROPAGATED_CLASSIFICATION_ADD , PROPAGATED_CLASSIFICATION_DELETE An Atlan tag object The Atlan tag that was added or removed. CUSTOM_METADATA_UPDATE A custom metadata object Which specific custom metadata attributes (and values) were set by the activity. 1.3.0 1.0.0 For example: Java Python Kotlin Raw REST API View specific changes made by an activity 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 if ( detail instanceof Table ) { // (1) Table table = ( Table ) detail ; // (2) String description = table . getDescription (); // (3) Set < String > clearedFields = table . getNullFields (); // (4) if ( clearedFields . contains ( \"description\" )) { // (5) } } if ( detail instanceof AtlanTag ) { // (6) AtlanTag tag = ( AtlanTag ) detail ; // (7) String tagName = tag . getTypeName (); // (8) if ( tagName . equals ( \"PII\" )) { // (9) ... } } if ( detail instanceof CustomMetadataAttributesAuditDetail ) { // (10) CustomMetadataAttributesAuditDetail cmad = ( CustomMetadataAttributesAuditDetail ) detail ; // (11) String cmName = cmad . getTypeName (); // (12) Map < String , Object > attributes = cmad . getAttributes (); // (13) if ( cmName . equals ( \"RACI\" ) && attributes . get ( \"Responsible\" ). equals ( \"jsmith\" )) { // (14) ... } } You can safely type-check the detailed object. You could generically use Asset here instead of Table , but if you know the type of asset you've requested the history for then the detailed object should be the same detailed type. Once you've type-checked it, you can then coerce it. From there you can access any properties. Note that only properties actually set by the activity will have values in this detail object. So in this example, only if the description was actually changed to a new value would the description variable now have any content. This also means that if a field was actually removed (or cleared) by an activity you won't be able to distinguish that by just attempting to retrieve it. (It will be null whether it was removed by the activity or simply wasn't changed by the activity.) To distinguish what was actually removed by an activity, you need to use getNullFields() . The set returned by this method will contain the names of any fields that were actually removed (cleared) by the activity. You can then take whatever action you like if a field was removed (cleared) by checking for its existence within the getNullFields() set. You can type-check the detailed object to see if it is an Atlan tag. Once you've type-checked it, you can then coerce it. You can access the Atlan tag name using getTypeName() . You can then compare this human-readable Atlan tag name to your expectations to take whatever action you like. You can type-check the detailed object to see if it details changes to custom metadata. Once you've type-checked it, you can then coerce it. You can access the name of the custom metadata using getTypeName() . You can retrieve which custom metadata attributes were changed using getAttributes() . Since the result is a map, it will only contain attributes that were changed. If an attribute was removed (cleared) it will have a null value in the map but the name of the attribute will still be a key in the map. If a custom metadata attribute was not changed by the activity, it will not be a key in this map. You can then compare these human-readable names to your expectations to take whatever action you like. View specific changes made by an activity 24 25 26 27 28 29 30 31 32 33 34 35 if isinstance ( detail , Table ): # (1) description = detail . description # (2) ... # (3) if isinstance ( detail , AtlanTag ): # (4) class_name = detail . type_name # (5) if class_name == \"PII\" : # (6) ... if isinstance ( detail , CustomMetadataAttributesAuditDetail ): # (7) cm_name = detail . type_name # (8) attributes = detail . attributes # (9) if cm_name == \"RACI\" and attributes [ \"Responsible\" ] == \"jsmith\" : # (10) ... You can safely type-check the detailed object. You could generically use Asset here instead of Table , but if you know the type of asset you've requested the history for then the detailed object should be the same detailed type. From there you can access any properties. Note that only properties actually set by the activity will have values in this detail object. So in this example, only if the description was actually changed to a new value would the description variable now have any content. You can then take whatever action you like You can type-check the detailed object to see if it is a 'AtlanTag'. You can access the 'AtlanTag' name using `type_name' property. You can then compare this human-readable Atlan tag name to your expectations to take whatever action you like. You can type-check the detailed object to see if it details changes to custom metadata. You can access the name of the custom metadata using then type_name attribute. You can retrieve which custom metadata attributes were changed using the attributes propery. Since the result is a dict, it will only contain attributes that were changed. If an attribute was removed (cleared) it will have a null value in the dict but the name of the attribute will still be a key in the map. If a custom metadata attribute was not changed by the activity, it will not be a key in this map. You can then compare these human-readable names to your expectations to take whatever action you like. View specific changes made by an activity 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 if ( detail is Table ) { // (1) val description = detail . description // (2) val clearedFields = detail . nullFields // (3) if ( clearedFields . contains ( \"description\" )) { // (4) } } if ( detail is AtlanTag ) { // (5) val tagName = detail . typeName // (6) if ( tagName == \"PII\" ) { // (7) ... } } if ( detail is CustomMetadataAttributesAuditDetail ) { // (8) val cmName = detail . typeName // (9) val attributes = detail . attributes // (10) if ( cmName == \"RACI\" && attributes [ \"Responsible\" ] == \"jsmith\" ) { // (11) ... } } You can safely type-check the detailed object. You could generically use Asset here instead of Table , but if you know the type of asset you've requested the history for then the detailed object should be the same detailed type. From there you can access any properties. Note that only properties actually set by the activity will have values in this detail object. So in this example, only if the description was actually changed to a new value would the description variable now have any content. This also means that if a field was actually removed (or cleared) by an activity you won't be able to distinguish that by just attempting to retrieve it. (It will be null whether it was removed by the activity or simply wasn't changed by the activity.) To distinguish what was actually removed by an activity, you need to use .nullFields . The set returned by this method will contain the names of any fields that were actually removed (cleared) by the activity. You can then take whatever action you like if a field was removed (cleared) by checking for its existence within the .nullFields set. You can type-check the detailed object to see if it is an Atlan tag. You can access the Atlan tag name using .typeName . You can then compare this human-readable Atlan tag name to your expectations to take whatever action you like. You can type-check the detailed object to see if it details changes to custom metadata. You can access the name of the custom metadata using .typeName . You can retrieve which custom metadata attributes were changed using .attributes . Since the result is a map, it will only contain attributes that were changed. If an attribute was removed (cleared) it will have a null value in the map but the name of the attribute will still be a key in the map. If a custom metadata attribute was not changed by the activity, it will not be a key in this map. You can then compare these human-readable names to your expectations to take whatever action you like. You will need to implement your own detection and inference The key point to note is that the format of the object within the detail of each record will vary, depending on the type of activity that occurred. You will therefore need to implement your own logic for detecting and inferring what kind of details are included when retrieving these from a raw API response. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/access/tokens/",
    "content": "/api/service/apikeys (GET) /api/service/apikeys (POST) /api/service/apikeys/{guid} (DELETE) /api/service/apikeys/{guid} (POST) API tokens Â¶ API tokens are a way to provide programmatic access to Atlan without relying on a user's own credentials or permissions. API tokens are not personal access tokens API tokens are commonly thought to be synonymous with personal access tokens (PAT). In other words, many developers assume that an API token will have the same privileges and permissions as the user who created them. This is not the case. API tokens are their own unique actor and carry entirely their own set of permissions, completely independent from the user who created or otherwise maintains them. Create an API token Â¶ 0.0.14 1.4.0 4.0.0 To create a new API token: Java Python Kotlin Go Raw REST API Create an API token 1 2 ApiToken token = ApiToken . create ( client , \"token-name\" ); // (1) String tokenValue = token . getAttributes (). getAccessToken (); // (2) You can use the ApiToken.create() method to create a new API token. Because this operation will create the token in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The actual value of the API token will only be available in this immediate response of the creation, under .getAttributes().getAccessToken() . Cannot be accessed again later You will not be able to retrieve the actual value of the API token again at a later point, for example when retrieving or updating the API token. Create an API token 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () token = client . token . create ( \"token-name\" ) # (1) token_value = token . attributes . access_token # (2) You can use the token.create() method to create a new API token. The actual value of the API token will only be available in this immediate response of the creation, under .attributes.access_token . Cannot be accessed again later You will not be able to retrieve the actual value of the API token again at a later point, for example when retrieving or updating the API token. Create an API token 1 2 val token = ApiToken . create ( client , \"token-name\" ) // (1) val tokenValue = token . attributes . accessToken // (2) You can use the ApiToken.create() method to create a new API token. Because this operation will create the token in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The actual value of the API token will only be available in this immediate response of the creation, under .attributes.accessToken . Cannot be accessed again later You will not be able to retrieve the actual value of the API token again at a later point, for example when retrieving or updating the API token. Create an API token 1 2 3 4 5 6 displayName := \"token-name\" token , atlanErr := ctx . TokenClient . Create ( & displayName , nil , nil , nil ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } tokenValue := * token . Attributes . AccessToken // (2) You can use the TokenClient.Create() method to create a new API token. The actual value of the API token will only be available in this immediate response of the creation, under .Attributes.AccessToken . Cannot be accessed again later You will not be able to retrieve the actual value of the API token again at a later point, for example when retrieving or updating the API token. POST /api/service/apikeys 1 2 3 4 5 6 7 { \"displayName\" : \"token-name\" , // (1) \"description\" : \"\" , // (2) \"personas\" : [], // (3) \"personaQualifiedNames\" : [], // (4) \"validitySeconds\" : 409968000 // (5) } You must provide a name for the token when creating it. You can optionally provide a description, but even if you do not want to provide a description must send an empty string for it in the request. You must always send an empty personas array (this is now unused, but still required). You can optionally list personas to link the API token with, but even if you do not send any must send an empty list. If you are linking the API token to one or more personas, use their qualified_name in the list. Must be complete set you want linked to the API token Any personas you leave out of the set will be removed from the API token, if they are already associated with it. (Further, if you send an empty set or no value at all for personas in the update, then ALL linked personas will be removed from the token.) You also need to specify how long the API token should last before automatically expiring. The value of 409968000 will be interpreted as never expiring. Retrieve an API token Â¶ 0.0.14 1.4.0 4.0.0 You can retrieve an API token either by its name or its client ID (the name used when it is associated with some other object). Java Python Kotlin Go Raw REST API Retrieve an API token by name 1 ApiToken token = ApiToken . retrieveByName ( client , \"token-name\" ); // (1) The retrieveByName() method handles fetching the API token based on its name. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve an API token by ID 1 ApiToken token = client . apiTokens . getById ( \"apikey-ac69de56-6529-4c8f-b53c-791cb5346308\" ); // (1) The getById() method on the apiTokens member of any client handles fetching the API token based on its client ID. This is the same as the username that will be captured when an API token is assigned to an asset without the service-account- prefix. Retrieve an API token by name 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () token = client . token . get_by_name ( \"token-name\" ) # (1) The token.get_by_name() method handles fetching the API token based on its name. Retrieve an API token by ID 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () token = client . token . get_by_id ( \"apikey-ac69de56-6529-4c8f-b53c-791cb5346308\" ) # (1) The token.get_by_id() method handles fetching the API token based on its client ID. This is the same as the username that will be captured when an API token is assigned to an asset without the service-account- prefix. Retrieve an API token by name 1 val token = ApiToken . retrieveByName ( client , \"token-name\" ) // (1) The retrieveByName() method handles fetching the API token based on its name. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve an API token by ID 1 val token = client . apiTokens . getById ( \"apikey-ac69de56-6529-4c8f-b53c-791cb5346308\" ) // (1) The getById() method on the apiTokens member of any client handles fetching the API token based on its client ID. This is the same as the username that will be captured when an API token is assigned to an asset without the service-account- prefix. Retrieve an API token by name 1 token , atlanErr := ctx . TokenClient . GetByName ( \"token-name\" ) // (1) The TokenClient.GetByName() method handles fetching the API token based on its name. Retrieve an API token by ID 1 token , atlanErr := ctx . TokenClient . GetByID ( \"apikey-ac69de56-6529-4c8f-b53c-791cb5346308\" ) // (1) The TokenClient.GetByID() method handles fetching the API token based on its client ID. This is the same as the username that will be captured when an API token is assigned to an asset without the service-account- prefix. GET /api/service/apikeys?limit=2&offset=0&sort=-createdAt&filter={\"displayName\":\"token-name\"} 1 // (1) The search criteria for retrieving an API token by its name is embedded as query parmeters in the request URL. GET /api/service/apikeys?limit=2&offset=0&sort=-createdAt&filter={\"clientId\":\"apikey-ac69de56-6529-4c8f-b53c-791cb5346308\"} 1 // (1) The search criteria for retrieving an API token by its name is embedded as query parmeters in the request URL. Update an API token Â¶ 0.0.14 1.4.0 4.0.0 There is limited information to update on an API token: Java Python Kotlin Go Raw REST API Update an API token 1 2 3 4 5 6 7 ApiToken token = ApiToken . retrieveByName ( client , \"token-name\" ); // (1) ApiToken revised = token . toBuilder () // (2) . attributes ( token . getAttributes (). toBuilder () . description ( \"Now with a description.\" ) // (3) . build ()) . build (); ApiToken updated = revised . update ( client ); // (4) You are best first retrieving the API token you want to update. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then use the builder pattern to update information within that token, such as its description. Note that some information, like description, is embedded within the attributes of the token. You can then send the revised token information to Atlan to be updated. Because this operation will persist the token in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete an API token 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient client = AtlanClient () token = client . token . get_by_name ( \"token-name\" ) # (1) client . token . update ( # (2) token . guid , token . display_name , description = \"Now with a description.\" , ) You are best first retrieving the API token you want to update. You can then send the revised token information to Atlan to be updated using the token.update() method, and passing the updated information (like the new description). Update an API token 1 2 3 4 5 6 7 val token = ApiToken . retrieveByName ( client , \"token-name\" ) // (1) val revised = token . toBuilder () // (2) . attributes ( token . attributes . toBuilder () . description ( \"Now with a description.\" ) // (3) . build ()) . build () val updated = revised . update ( client ) // (4) You are best first retrieving the API token you want to update. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then use the builder pattern to update information within that token, such as its description. Note that some information, like description, is embedded within the attributes of the token. You can then send the revised token information to Atlan to be updated. Because this operation will persist the token in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete an API token 1 2 3 4 token , _ := ctx . TokenClient . GetByName ( \"token-name\" ) // (1) displayName := \"Updated name\" description := \"Now with a description.\" ctx . TokenClient . Update ( token . GUID , & displayName , & description , nil ) // (2) You are best first retrieving the API token you want to update. You can then send the revised token information to Atlan to be updated using the TokenClient.Update method, and passing the updated information (like the new description and display name). POST /api/service/apikeys/98fb61da-eb8f-455e-b5ea-c022ee390044 1 2 3 4 5 6 { \"displayName\" : \"token-name\" , // (1) \"description\" : \"Now with a revised description.\" , // (2) \"personas\" : [], // (3) \"personaQualifiedNames\" : [] // (4) } You must provide the name for the token when updating it, in addition to its ID (GUID) in the request URL itself. You can provide updates to its description, for example. You must always send an empty personas array (this is now unused, but still required). You must also list personas to link the API token with, even if you do not send any you must send an empty list. If you do specify personas to link to the API token, use the qualified_name of the persona(s). Must be complete set you want linked to the API token Any personas you leave out of the set will be removed from the API token, if they are already associated with it. (Further, if you send an empty set or no value at all for personas in the update, then ALL linked personas will be removed from the token.) Delete an API token Â¶ 0.0.14 1.4.0 4.0.0 You can delete an API token by its GUID: Java Python Kotlin Go Raw REST API Delete an API token 1 ApiToken . delete ( client , \"98fb61da-eb8f-455e-b5ea-c022ee390044\" ); // (1) Note that the GUID of an API token is not the same as its client ID. From an API token object, the getId() method will give you its GUID. Because this operation will remove the token from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Irreversible Once deleted, the API token will be permanently removed and no longer usable. Delete an API token 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () token = client . token . purge ( \"98fb61da-eb8f-455e-b5ea-c022ee390044\" ) # (1) Note that the GUID of an API token is not the same as its client ID. From an API token object, the guid property will give you its GUID. Irreversible Once deleted, the API token will be permanently removed and no longer usable. Delete an API token 1 ApiToken . delete ( client , \"98fb61da-eb8f-455e-b5ea-c022ee390044\" ) // (1) Note that the GUID of an API token is not the same as its client ID. From an API token object, the getId() method will give you its GUID. Because this operation will remove the token from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Irreversible Once deleted, the API token will be permanently removed and no longer usable. Delete an API token 1 ctx . TokenClient . Purge ( \"a853f1d5-f1f4-4cdb-b86d-c61df3ecade6\" ) // (1) Note that the GUID of an API token is not the same as its client ID. From an API token object, the guid property will give you its GUID. Irreversible Once deleted, the API token will be permanently removed and no longer usable. DELETE /api/service/apikeys/98fb61da-eb8f-455e-b5ea-c022ee390044 1 // (1) The criteria for deleting an API token is entirely contained in the request URL. Note that the GUID of an API token is not the same as its client ID. From an API token response, the id property will give you its GUID. Irreversible Once deleted, the API token will be permanently removed and no longer usable. Give permissions to API token Â¶ As called out at the top of this page, API tokens are unique actors with their own privileges and permissions. For an API token to be able to interact with certain objects, they must be granted such permissions directly. Link an API token to a persona Â¶ 0.0.14 1.4.0 4.0.0 You can link an API token to a persona to give the API token all of the permissions granted by the policies within that persona. Java Python Kotlin Go Raw REST API Link an API token to persona(s) 1 2 3 4 5 client . apiTokens . update ( // (1) \"98fb61da-eb8f-455e-b5ea-c022ee390044\" , // (2) \"token-name\" , null , Set . of ( \"default/aQi5KHtGwZYvxGnTSAYO8J\" )); // (3) Use the update() method on the apiTokens member of any client to link an API token with personas. You will need to provide both the GUID and the display name for the API token. You must also provide the complete set of personas that should be linked to the API token, using their qualifiedName s. Must be complete set you want linked to the API token Any personas you leave out of the set will be removed from the API token, if they are already associated with it. (Further, if you send an empty set or no value at all for personas in the update, then ALL linked personas will be removed from the token.) Link an API token to persona(s) 1 2 3 4 5 6 7 8 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . token . update ( # (1) guid = \"98fb61da-eb8f-455e-b5ea-c022ee390044\" , # (2) display_name = \"token-name\" , personas = { \"default/aQi5KHtGwZYvxGnTSAYO8J\" }, # (3) ) Use token.update() to link an API token with personas. You will need to provide both the GUID and the display name for the API token. You must also provide the complete set of personas that should be linked to the API token, using their qualified_name s. Must be complete set you want linked to the API token Any personas you leave out of the set will be removed from the API token, if they are already associated with it. (Further, if you send an empty set or no value at all for personas in the update, then ALL linked personas will be removed from the token.) Link an API token to persona(s) 1 2 3 4 5 client . apiTokens . update ( // (1) \"98fb61da-eb8f-455e-b5ea-c022ee390044\" , // (2) \"token-name\" , null , setOf ( \"default/aQi5KHtGwZYvxGnTSAYO8J\" )) // (3) Use the update() method on the apiTokens member of any client to link an API token with personas. You will need to provide both the GUID and the display name for the API token. You must also provide the complete set of personas that should be linked to the API token, using their qualifiedName s. Must be complete set you want linked to the API token Any personas you leave out of the set will be removed from the API token, if they are already associated with it. (Further, if you send an empty set or no value at all for personas in the update, then ALL linked personas will be removed from the token.) Link an API token to persona(s) 1 2 3 4 5 6 7 8 displayName := \"token-name\" tokenGUID := \"98fb61da-eb8f-455e-b5ea-c022ee390044\" ctx . TokenClient . Update ( // (1) & tokenGUID , // (2) & displayName , nil , [] string { \"default/aQi5KHtGwZYvxGnTSAYO8J\" }, // (3) ) Use TokenClient.Update to link an API token with personas. You will need to provide both the GUID and the display name for the API token. You must also provide the complete set of personas that should be linked to the API token, using their qualifiedName s. Must be complete set you want linked to the API token Any personas you leave out of the set will be removed from the API token, if they are already associated with it. (Further, if you send an empty set or no value at all for personas in the update, then ALL linked personas will be removed from the token.) POST /api/service/apikeys/98fb61da-eb8f-455e-b5ea-c022ee390044 1 2 3 4 5 6 { \"displayName\" : \"token-name\" , // (1) \"description\" : \"\" , // (2) \"personas\" : [], // (3) \"personaQualifiedNames\" : [ \"default/aQi5KHtGwZYvxGnTSAYO8J\" ] // (4) } You must provide the name for the token when updating it, in addition to its ID (GUID) in the request URL itself. You must provide the description for the token. Even if you do not want it to have any description, you need to send an empty string. You must always send an empty personas array (this is now unused, but still required). You must also provide the complete set of personas that should be linked to the API token, using their qualified_name s. Must be complete set you want linked to the API token Any personas you leave out of the set will be removed from the API token, if they are already associated with it. (Further, if you send an empty set or no value at all for personas in the update, then ALL linked personas will be removed from the token.) Add an API token as a connection admin Â¶ 1.4.0 4.0.0 For any actor to manage policies for a connection, that actor must be a connection admin on the connection. You must therefore add the API token as a connection admin to any connection you want it to be able to manage policies for. 1 Must first obtain a user's bearer token To carry out this operation, you must first obtain a user's bearer token . Specifically, you must obtain the bearer token for a user who is already a connection admin on the connection. Java Python Kotlin Raw REST API Add an API token as connection admin 1 2 3 4 5 6 7 List < Connection > connections = Connection . findByName ( client , \"development\" , AtlanConnectorType . BIGQUERY ); Connection connection = connections . get ( 0 ); // (1) String impersonationToken = \"eyNnCJd2T9Y8fEsbdx...\" ; // (2) AssetMutationResponse response = connection . addApiTokenAsAdmin ( client , impersonationToken ); // (3) You will need to start by retrieving the connection you want to add the API token to as a connection admin. In this example, we use a search to retrieve the connection. You must use a user's bearer token as an impersonation token. (We would recommend capturing this in something like an environment variable rather than embedding directly in the code.) You can use the .addApiTokenAsAdmin() method to add the API token as a connection admin. Because this operation will update the connection in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Will add the API token, not impersonation token, as connection admin Note that you are providing a user's bearer token as the impersonationToken only to give sufficient privileges to add the API token configured for the SDK as a connection admin. It is the API token configured for the SDK that you're adding as connection admin, not the user's bearer token. (The user's bearer token must already have connection admin permissions on this connection for the operation to succeed.) Add an API token as connection admin 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient connections = client . asset . find_connections_by_name ( \"development\" , AtlanConnectorType . BIGQUERY ) connection = connections [ 0 ] # (1) impersonation_token = \"eyNnCJd2T9Y8fEsbdx...\" # (2) response = client . user . add_as_admin ( # (3) asset_guid = connection . guid , impersonation_token = impersonation_token , ) You will need to start by retrieving the connection you want to add the API token to as a connection admin. In this example, we use a search to retrieve the connection. You must use a user's bearer token as an impersonation token. (We would recommend capturing this in something like an environment variable rather than embedding directly in the code.) You can use the user.add_as_admin() method to add the API token as a connection admin, providing the GUID of the connection and the impersonation token. Will add the API token, not impersonation token, as connection admin Note that you are providing a user's bearer token as the impersonation_token only to give sufficient privileges to add the API token configured for the SDK as a connection admin. It is the API token configured for the SDK that you're adding as connection admin, not the user's bearer token. (The user's bearer token must already have connection admin permissions on this connection for the operation to succeed.) Add an API token as connection admin 1 2 3 4 5 6 7 val connections = Connection . findByName ( client , \"development\" , AtlanConnectorType . BIGQUERY ) val connection = connections [ 0 ] // (1) val impersonationToken = \"eyNnCJd2T9Y8fEsbdx...\" // (2) val response = connection . addApiTokenAsAdmin ( client , impersonationToken ) // (3) You will need to start by retrieving the connection you want to add the API token to as a connection admin. In this example, we use a search to retrieve the connection. You must use a user's bearer token as an impersonation token. (We would recommend capturing this in something like an environment variable rather than embedding directly in the code.) You can use the .addApiTokenAsAdmin() method to add the API token as a connection admin. Because this operation will update the connection in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Will add the API token, not impersonation token, as connection admin Note that you are providing a user's bearer token as the impersonationToken only to give sufficient privileges to add the API token configured for the SDK as a connection admin. It is the API token configured for the SDK that you're adding as connection admin, not the user's bearer token. (The user's bearer token must already have connection admin permissions on this connection for the operation to succeed.) Not recommended We do not recommend attempting this change through the Raw REST APIs as it requires a number of different API calls, carefully parsing the responses and combining elements, and has high potential for minor typos or copy/paste errors to cause mistakes. Add an API token as a collection editor Â¶ 1.4.0 4.0.0 As with connections, if you want an API token to be able to manage a query collection or the queries within it, you must add the API token as an editor on the collection. Must first obtain a user's bearer token To carry out this operation, you must first obtain a user's bearer token . Specifically, you must obtain the bearer token for a user who is already an editor on (or owner of) the query collection. Java Python Kotlin Raw REST API Add an API token as collection editor 1 2 3 4 5 6 List < AtlanCollection > collections = AtlanCollection . findByName ( client , \"My query collection\" ); AtlanCollection collection = collections . get ( 0 ); // (1) String impersonationToken = \"eyNnCJd2T9Y8fEsbdx...\" ; // (2) AssetMutationResponse response = collection . addApiTokenAsAdmin ( client , impersonationToken ); // (3) You will need to start by retrieving the query collection you want to add the API token to as an editor. In this example, we use a search to retrieve the collection. You must use a user's bearer token as an impersonation token. (We would recommend capturing this in something like an environment variable rather than embedding directly in the code.) You can use the .addApiTokenAsAdmin() method to add the API token as a collection editor. Because this operation will update the collection in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Will add the API token, not impersonation token, as collection editor Note that you are providing a user's bearer token as the impersonationToken only to give sufficient privileges to add the API token configured for the SDK as a collection editor. It is the API token configured for the SDK that you're adding as collection editor, not the user's bearer token. (The user's bearer token must already have collection editor permissions on this collection for the operation to succeed.) Add an API token as collection editor 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Collection from pyatlan.model.fluent_search import FluentSearch request = ( FluentSearch () . where ( FluentSearch . asset_type ( Collection )) . where ( Collection . NAME . eq ( \"My query collection\" )) ) . to_request () response = client . asset . search ( request ) collection = response . current_page ()[ 0 ] # (1) impersonation_token = \"eyNnCJd2T9Y8fEsbdx...\" # (2) response = client . user . add_as_admin ( # (3) asset_guid = collection . guid , impersonation_token = impersonation_token , ) You will need to start by retrieving the query collection you want to add the API token to as a collection editor. In this example, we use a search to retrieve the collection. You must use a user's bearer token as an impersonation token. (We would recommend capturing this in something like an environment variable rather than embedding directly in the code.) You can use the user.add_as_admin() method to add the API token as a collection editor, providing the GUID of the collection and the impersonation token. Will add the API token, not impersonation token, as collection editor Note that you are providing a user's bearer token as the impersonation_token only to give sufficient privileges to add the API token configured for the SDK as a collection editor. It is the API token configured for the SDK that you're adding as collection editor, not the user's bearer token. (The user's bearer token must already have collection editor permissions on this collection for the operation to succeed.) Add an API token as collection editor 1 2 3 4 5 6 val collections = AtlanCollection . findByName ( client , \"My query collection\" ) val collection = collections [ 0 ] // (1) val impersonationToken = \"eyNnCJd2T9Y8fEsbdx...\" // (2) val response = collection . addApiTokenAsAdmin ( client , impersonationToken ) // (3) You will need to start by retrieving the query collection you want to add the API token to as an editor. In this example, we use a search to retrieve the collection. You must use a user's bearer token as an impersonation token. (We would recommend capturing this in something like an environment variable rather than embedding directly in the code.) You can use the .addApiTokenAsAdmin() method to add the API token as a collection editor. Because this operation will update the collection in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Will add the API token, not impersonation token, as collection editor Note that you are providing a user's bearer token as the impersonationToken only to give sufficient privileges to add the API token configured for the SDK as a collection editor. It is the API token configured for the SDK that you're adding as collection editor, not the user's bearer token. (The user's bearer token must already have collection editor permissions on this collection for the operation to succeed.) Not recommended We do not recommend attempting this change through the Raw REST APIs as it requires a number of different API calls, carefully parsing the responses and combining elements, and has high potential for minor typos or copy/paste errors to cause mistakes. Add an API token as a collection viewer Â¶ 1.4.0 4.0.0 Alternatively, if you only want the API token to be able to view and run queries within a collection (but not change them), you can add the API token as a viewer on the collection. Must first obtain a user's bearer token To carry out this operation, you must first obtain a user's bearer token . Specifically, you must obtain the bearer token for a user who is already an editor on (or owner of) the query collection. Java Python Kotlin Raw REST API Add an API token as collection viewer 1 2 3 4 5 6 List < AtlanCollection > collections = AtlanCollection . findByName ( client , \"My query collection\" ); AtlanCollection collection = collections . get ( 0 ); // (1) String impersonationToken = \"eyNnCJd2T9Y8fEsbdx...\" ; // (2) AssetMutationResponse response = collection . addApiTokenAsViewer ( client , impersonationToken ); // (3) You will need to start by retrieving the query collection you want to add the API token to as an viewer. In this example, we use a search to retrieve the collection. You must use a user's bearer token as an impersonation token. (We would recommend capturing this in something like an environment variable rather than embedding directly in the code.) You can use the .addApiTokenAsViewer() method to add the API token as a collection viewer. Because this operation will update the collection in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Will add the API token, not impersonation token, as collection viewer Note that you are providing a user's bearer token as the impersonationToken only to give sufficient privileges to add the API token configured for the SDK as a collection viewer. It is the API token configured for the SDK that you're adding as collection viewer, not the user's bearer token. (The user's bearer token must already have collection editor permissions on this collection for the operation to succeed.) Add an API token as collection viewer 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Collection from pyatlan.model.fluent_search import FluentSearch request = ( FluentSearch () . where ( FluentSearch . asset_type ( Collection )) . where ( Collection . NAME . eq ( \"My query collection\" )) ) . to_request () response = client . asset . search ( request ) collection = response . current_page ()[ 0 ] # (1) impersonation_token = \"eyNnCJd2T9Y8fEsbdx...\" # (2) response = client . user . add_as_viewer ( # (3) asset_guid = collection . guid , impersonation_token = impersonation_token , ) You will need to start by retrieving the query collection you want to add the API token to as a collection viewer. In this example, we use a search to retrieve the collection. You must use a user's bearer token as an impersonation token. (We would recommend capturing this in something like an environment variable rather than embedding directly in the code.) You can use the user.add_as_admin() method to add the API token as a collection viewer, providing the GUID of the collection and the impersonation token. Will add the API token, not impersonation token, as collection viewer Note that you are providing a user's bearer token as the impersonation_token only to give sufficient privileges to add the API token configured for the SDK as a collection viewer. It is the API token configured for the SDK that you're adding as collection viewer, not the user's bearer token. (The user's bearer token must already have collection editor permissions on this collection for the operation to succeed.) Add an API token as collection viewer 1 2 3 4 5 6 val collections = AtlanCollection . findByName ( client , \"My query collection\" ) val collection = collections [ 0 ] // (1) val impersonationToken = \"eyNnCJd2T9Y8fEsbdx...\" // (2) val response = collection . addApiTokenAsViewer ( client , impersonationToken ) // (3) You will need to start by retrieving the query collection you want to add the API token to as an viewer. In this example, we use a search to retrieve the collection. You must use a user's bearer token as an impersonation token. (We would recommend capturing this in something like an environment variable rather than embedding directly in the code.) You can use the .addApiTokenAsViewer() method to add the API token as a collection viewer. Because this operation will update the collection in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Will add the API token, not impersonation token, as collection viewer Note that you are providing a user's bearer token as the impersonationToken only to give sufficient privileges to add the API token configured for the SDK as a collection viewer. It is the API token configured for the SDK that you're adding as collection viewer, not the user's bearer token. (The user's bearer token must already have collection editor permissions on this collection for the operation to succeed.) Not recommended We do not recommend attempting this change through the Raw REST APIs as it requires a number of different API calls, carefully parsing the responses and combining elements, and has high potential for minor typos or copy/paste errors to cause mistakes. Obtain a user's bearer token Â¶ For a few of the operations above, there is a bit of a \"catch-22\". For example: An API token cannot manage policies for a connection if it is not the connection admin. An API token cannot be used to add itself as a connection admin. 2 So how do you get an API token set up as a connection admin, without using an API token? The answer is that you must use a user's own bearer token (temporarily) â€” specifically a user who already has the appropriate authority on the object. (In this example, a user who is already a connection admin on that connection.) To do this, you'll need to 3 : Log in to Atlan as the user with the level of access required (e.g. the user who is a connection admin). Open the Developer Tools view of Chrome: Open the View menu, then Developer , then Developer Tools . Leave the Developer Tools window open, but change back to your original Atlan window. Navigate to the Assets page where you can discover assets. Return to the Developer Tools window and find any indexsearch item along the left: Right-click the indexsearch item, then Copy , then Copy as cURL . Paste what this has copied into a text editor: 4 Copied cURL 1 2 3 4 5 6 7 curl 'https://tenant.atlan.com/api/meta/search/indexsearch' \\\n  -H 'authority: tenant.atlan.com' \\\n  -H 'accept: application/json, text/plain, */*' \\\n  -H 'accept-language: en-GB,en-US;q=0.9,en;q=0.8' \\ -H 'authorization: Bearer eyNnCJd2T9Y8fEsbdxTgKQqyWFm7uNBvw55EjAIakh9Yrg3cdd1YoNMXr1LtFmreHfVrYrSRxCzUYcoJKASXovfBO5PnGZOWe8hAdxb7WqesNZS.TPFcWzwRLA2Aeb38iWBIAG3rrsTz7iyufecbPeLBLTZ2RjaweLji7PGIVz5Mj8G2bAIPM7tguCGbz...' \\ -H 'content-type: application/json' \\\n  ... The details you need are on the highligted line 5, that begins with -H 'authorization: Bearer . You specifically want everything after the word Bearer , up to the final single quote ( ' ) at the end of the line. So in this example you would copy: eyNnCJd2T9Y8fEsbdxTgKQqyWFm7uNBvw55EjAIakh9Yrg3cdd1YoNMXr1LtFmreHfVrYrSRxCzUYcoJKASXovfBO5PnGZOWe8hAdxb7WqesNZS.TPFcWzwRLA2Aeb38iWBIAG3rrsTz7iyufecbPeLBLTZ2RjaweLji7PGIVz5Mj8G2bAIPM7tguCGbz... This is the user's own bearer token. Has a limited lifespan Be aware that the user's own bearer token will have a limited lifespan. At some point the user will be automatically logged out and will be asked to log in again, at which point this bearer token will have expired and need to be re-retrieved. The non-obvious exception to this rule is where the API token was used to create a connection in the first place. If you use an API token to create a connection, the API token is automatically set up as a connection admin for that connection as part of its creation. â†© Hopefully the reason is self-apparent, but for clarity, if this were possible then API tokens could be used to bypass the security inherent in connection administration (policy management for the connection and its assets). â†© The steps shown assume you are doing this via the Chrome browser. Similar methods should exist in other browsers; if you are facing difficulty, please reach out with which browser you are using and we can help you through. â†© While this example bearer token may \"look\" real, it is entirely made up of random numbers and letters. Do not try to use this example bearer token, as it is guaranteed not to work. â†© 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/restore/",
    "content": "/api/meta/entity/bulk (POST) Restoring assets Â¶ Restoring an asset from an archived (soft-deleted) state back to active uses a similar pattern to the deletion operations. By qualifiedName Â¶ 1.4.0 4.0.0 To restore an asset by its qualifiedName : Java Python Kotlin Raw REST API Restore an asset by its qualifiedName 1 2 boolean restored = GlossaryTerm . restore ( client , \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" ); // (1) If an asset with the provided qualifiedName exists and is now active, the operation will return true. If no archived (soft-deleted) version of the asset could be found the operation will return false. Because this operation will restore the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Restore an asset by its qualifiedName 1 2 3 4 5 6 7 8 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm client = AtlanClient () restored = client . asset . restore ( asset_type = AtlasGlossaryTerm , qualified_name = \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" # (1) ) If an asset with the provided qualified_name exists and is now active, the operation will return true. If no archived (soft-deleted) version of the asset could be found the operation will return false. Restore an asset by its qualifiedName 1 2 val restored = GlossaryTerm . restore ( \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" ) // (1) If an asset with the provided qualifiedName exists and is now active, the operation will return true. If no archived (soft-deleted) version of the asset could be found the operation will return false. Because this operation will restore the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk?replaceClassifications=false&replaceBusinessAttributes=false&overwriteBusinessAttributes=false 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryTerm\" , // (2) \"attributes\" : { \"name\" : \"Example\" , // (3) \"qualifiedName\" : \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , // (6) \"guid\" : \"51a28d46-b700-4c9d-807d-397f25f2b6d6\" // (7) } }, \"status\" : \"ACTIVE\" // (8) } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). For a glossary, this is AtlasGlossary . You must provide the exact name of the existing asset (case-sensitive). You must provide the exact qualifiedName of the existing asset (case-sensitive). Must exactly match the qualifiedName of an existing asset If this does not exactly match the qualifiedName of an existing asset, the API call will instead create a new asset rather than restoring an existing one. For glossary-contained objects (terms and categories), you must also specify the glossary in which they are contained. You do this using the anchor attribute. Nested within the anchor attribute you must give the typeName as exactly AtlasGlossary . Nested within the anchor attribute you must also give the GUID of the glossary the archived object is contained within. The piece that actually triggers the restore is to change the status of the asset. When soft-deleted (archived) the status will be DELETED . Therefore, changing this to ACTIVE will restore the asset. In bulk Â¶ 2.0.0 To restore a number of assets at the same time, for example after retrieving them via a search: Java Python Kotlin Raw REST API Restore assets in bulk 1 2 3 4 5 6 7 List < Asset > toRestore = client . assets . select ( true ) // (1) . archived () . stream () // (2) . limit ( 50 ) // (3) . collect ( Collectors . toList ()); // (4) AssetMutationResponse response = client . assets . restore ( toRestore ); // (4) response . getUpdatedAssets (); // (5) You would want more to your search criteria than this, but the true sent to the select(true) will ensure archived (soft-deleted) assets are returned, while the .archived() will ensure only archived assets are returned. Run the search to retrieve such results, and page through them automatically. Limit the results to some maximum number, as you will need to limit how many you try to restore at the same time to avoid timeouts or other issues. You can collect up the assets to be restored into a list from the stream. You can then simply pass the results of the search across to the restore operation to re-activate all of them en masse. The restore will only be run on that limited set of results In this example, the restore is only running on the first set of 50 results. To restore all assets that match the search criteria, don't forget to loop over this logic or create your own batching mechanism to process all pages! You can retrieve the details of the specific assets that were restored from the bulk restore response. Restoring assets in bulk is not currently available in the Python SDK, but will be coming soon. Restore assets in bulk 1 2 3 4 5 6 7 val toRestore = client . assets . select ( true ) // (1) . archived () . stream () // (2) . limit ( 50 ) // (3) . collect ( Collectors . toList ()) // (4) val response = client . assets . restore ( toRestore ) // (4) response . updatedAssets // (5) You would want more to your search criteria than this, but the true sent to the select(true) will ensure archived (soft-deleted) assets are returned, while the .archived() will ensure only archived assets are returned. Run the search to retrieve such results, and page through them automatically. Limit the results to some maximum number, as you will need to limit how many you try to restore at the same time to avoid timeouts or other issues. You can collect up the assets to be restored into a list from the stream. You can then simply pass the results of the search across to the restore operation to re-activate all of them en masse. The restore will only be run on that limited set of results In this example, the restore is only running on the first set of 50 results. To restore all assets that match the search criteria, don't forget to loop over this logic or create your own batching mechanism to process all pages! You can retrieve the details of the specific assets that were restored from the bulk restore response. Multiple API calls required You will need to first run a search for all assets with a status of DELETED . You can then bulk-restore them by placing all of the returned assets into a single API call. POST /api/meta/entity/bulk?replaceClassifications=false&replaceBusinessAttributes=false&overwriteBusinessAttributes=false 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryTerm\" , // (2) \"attributes\" : { \"name\" : \"Example\" , // (3) \"qualifiedName\" : \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , // (6) \"guid\" : \"51a28d46-b700-4c9d-807d-397f25f2b6d6\" // (7) } }, \"status\" : \"ACTIVE\" // (8) }, { \"typeName\" : \"AtlasGlossaryTerm\" , \"attributes\" : { \"name\" : \"Another Example\" , \"qualifiedName\" : \"...\" , \"anchor\" : { \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"51a28d46-b700-4c9d-807d-397f25f2b6d6\" } }, \"status\" : \"ACTIVE\" } { ... }, { ... }, { ... } ] } All assets must be wrapped in an entities array. You must provide the exact type name for each asset (case-sensitive). You must provide the exact name of each existing asset (case-sensitive). You must provide the exact qualifiedName of each existing asset (case-sensitive). Must exactly match the qualifiedName of an existing asset If this does not exactly match the qualifiedName of an existing asset, the API call will instead create a new asset rather than restoring an existing one. For glossary-contained objects (terms and categories), you must also specify the glossary in which they are contained. You do this using the anchor attribute. Nested within the anchor attribute you must give the typeName as exactly AtlasGlossary . Nested within the anchor attribute you must also give the GUID of the glossary the archived object is contained within. The piece that actually triggers the restore is to change the status of each asset. When soft-deleted (archived) the status will be DELETED . Therefore, changing this to ACTIVE will restore each asset. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/combine/",
    "content": "/api/meta/entity/bulk (POST) Combining multiple operations Â¶ For most of the write operations covered in the sections above, there is also an approach you can use to combine multiple operations together. Optimizes changes to a single asset This is useful when you have many changes to make to an asset. Rather than making a separate API call for each change, with this approach you can make a single API call and include all the information within it. 2.0.0 4.0.0 For example, to create a term complete with a description, parent category, announcement, certificate, several owners, and several linked assets: Java Python Kotlin Raw REST API Combine multiple operations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 GlossaryTerm term = GlossaryTerm . creator ( \"Example Term\" , // (1) \"b4113341-251b-4adc-81fb-2420501c30e6\" ) . description ( \"This is only an example.\" ) // (2) . category ( GlossaryCategory . refByGuid ( // (3) \"1b736a83-207b-4269-ab92-44d77e1aca28\" , Reference . SaveSemantic . APPEND // (4) )) . announcementType ( AtlanAnnouncementType . INFORMATION ) // (5) . announcementTitle ( \"New!\" ) // (6) . announcementMessage ( \"This term is newly created.\" ) // (7) . certificateStatus ( CertificateStatus . VERIFIED ) // (8) . certificateStatusMessage ( \"For good measure!\" ) // (9) . ownerUser ( \"jdoe\" ) // (10) . ownerUser ( \"jsmith\" ) // (11) . build (); // (12) AssetMutationResponse response = term . save ( client ); // (13) assert response . getCreatedAssets (). size () == 1 // (14) assert response . getUpdatedAssets (). size () == 1 // (15) Use the creator() method to initialize the object with all necessary attributes for creating it . For a term, this is a name and the GUID of the glossary in which to create the term. (The final null is for a qualifiedName of the glossary, which could be used instead of the GUID.) Set a description for the term. Add a category for the term. (This category must already exist, or be created before this operation.) (Optional) You can specify whether this category should be APPEND ed to any categories the term is already organized within, or REMOVE d from the existing set of categories. If this argument is left out, the entire set of categories the term is linked to will be REPLACE d. Set the announcement that should be added (in this example, we're using INFORMATION ). Add a title for the announcement. Add a message for the announcement. Set the certificate for the term (in this example, we're using VERIFIED ). Add a message for the certificate. Add an owner. Add another owner. Note that we can just continue chaining single owners, we do not need to create our own list first. Call the build() method to build the enriched object. Call the save() method to actually create the term with all of these initial details. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single term that was created. The response will also include any related objects (the category) that were updated by this term being related to them. Combine multiple operations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import ( Announcement , AtlasGlossaryTerm , AtlasGlossaryCategory , AtlasGlossary , ) from pyatlan.model.enums import AnnouncementType , CertificateStatus client = AtlanClient () term = AtlasGlossaryTerm . creator ( # (1) name = \"Example Term\" , glossary_guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" ) term . description = \"This is only an example.\" # (2) term . attributes . categories = [ # (3) AtlasGlossaryCategory . ref_by_guid ( \"1b736a83-207b-4269-ab92-44d77e1aca28\" , SaveSemantic . APPEND # (4) ) ] announcement = Announcement ( # (5) announcement_type = AnnouncementType . INFORMATION , # (6) announcement_title = \"New!\" , # (7) announcement_message = \"This term is newly created.\" , # (8) ) term . set_announcement ( announcement ) # (9) term . certificate_status = CertificateStatus . VERIFIED # (10) term . certificate_status_message = \"For good measure!\" # (11) term . owner_users = { \"jdoe\" , \"jsmith\" } # (12) response = client . asset . save ( term ) # (13) assert len ( response . assets_created ( asset_type = AtlasGlossaryTerm )) == 1 # (14) assert len ( response . assets_updated ( asset_type = AtlasGlossary )) == 1 # (15) assert len ( response . assets_updated ( asset_type = AtlasGlossaryCategory )) == 1 # (16) Use the creator() method to initialize the object with all necessary attributes for creating it . For a term, this is a name and the GUID of the glossary in which to create the term. Set a description for the term. Add the categories for the term. (This parameter is a list and the categories must already exist, or be created before this operation.). (Optional) You can specify whether this category should be APPEND ed to any categories the term is already organized within, or REMOVE d from the existing set of categories. If this argument is left out, the entire set of categories the term is linked to will be REPLACE d. Create the announcement. Set the announcement that should be added (in this example, we're using INFORMATION ). Add a title for the announcement. Add a message for the announcement. Set the announcement for the term. Set the certificate for the term (in this example, we're using VERIFIED ). Add a message for the certificate. Add the owners. This parameter is a set of strings. Call the save() method to actually create the term with all of these initial details. The response will include that single term that was created. The response will include that single glossary that was updated. The response will include that single category that was updated. Combine multiple operations 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 val term = GlossaryTerm . creator ( \"Example Term\" , // (1) \"b4113341-251b-4adc-81fb-2420501c30e6\" ) . description ( \"This is only an example.\" ) // (2) . category ( GlossaryCategory . refByGuid ( // (3) \"1b736a83-207b-4269-ab92-44d77e1aca28\" , Reference . SaveSemantic . APPEND // (4) )) . announcementType ( AtlanAnnouncementType . INFORMATION ) // (5) . announcementTitle ( \"New!\" ) // (6) . announcementMessage ( \"This term is newly created.\" ) // (7) . certificateStatus ( CertificateStatus . VERIFIED ) // (8) . certificateStatusMessage ( \"For good measure!\" ) // (9) . ownerUser ( \"jdoe\" ) // (10) . ownerUser ( \"jsmith\" ) // (11) . build () // (12) val response = term . save ( client ) // (13) assert ( response . createdAssets . size == 1 ) // (14) assert ( response . updatedAssets . size == 1 ) // (15) Use the creator() method to initialize the object with all necessary attributes for creating it . For a term, this is a name and the GUID of the glossary in which to create the term. (The final null is for a qualifiedName of the glossary, which could be used instead of the GUID.) Set a description for the term. Add a category for the term. (This category must already exist, or be created before this operation.) (Optional) You can specify whether this category should be APPEND ed to any categories the term is already organized within, or REMOVE d from the existing set of categories. If this argument is left out, the entire set of categories the term is linked to will be REPLACE d. Set the announcement that should be added (in this example, we're using INFORMATION ). Add a title for the announcement. Add a message for the announcement. Set the certificate for the term (in this example, we're using VERIFIED ). Add a message for the certificate. Add an owner. Add another owner. Note that we can just continue chaining single owners, we do not need to create our own list first. Call the build() method to build the enriched object. Call the save() method to actually create the term with all of these initial details. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single term that was created. The response will also include any related objects (the category) that were updated by this term being related to them. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryTerm\" , // (2) \"attributes\" : { \"name\" : \"Example Term\" , // (3) \"qualifiedName\" : \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , \"anchor\" : { \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" }, \"description\" : \"This is only an example.\" , // (4) \"announcementType\" : \"information\" , // (5) \"announcementTitle\" : \"New!\" , // (6) \"announcementMessage\" : \"This term is newly created.\" , // (7) \"certificateStatus\" : \"VERIFIED\" , // (8) \"certificateStatusMessage\" : \"For good measure!\" , // (9) \"ownerUsers\" : [ // (10) \"jdoe\" , \"jsmith\" ] }, \"appendRelationshipAttributes\" : { \"categories\" : [ // (11) { \"typeName\" : \"AtlasGlossaryCategory\" , \"guid\" : \"1b736a83-207b-4269-ab92-44d77e1aca28\" } ] } } ] } All details must still be included in an outer entities array. You need to specify the type for each asset you are updating. You need to specify other required attributes for each asset, such as its name and qualifiedName. (And in the case of terms and categories, also the parent glossary they exist within.) Set a description for the term. Set the announcement that should be added. Add a title for the announcement. Add a message for the announcement. Set the certificate for the term (in this example, we're using VERIFIED ). Add a message for the certificate. Add multiple owners. Add one or more categories for the term. (Each category must already exist, or be created before this operation.) When you want the relationship to be appended, you must specify it within the appendRelationshipAttributes portion of the payload. If it is within the attributes portion of the payload, it will replace all such relationships on the asset. What are the available enrichments? The list of methods that are available to enrich each type of asset in Atlan should be directly visible to you within your favorite IDE (when using one of the SDKs). 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/read/",
    "content": "/api/meta/entity/guid/{guid} (GET) /api/meta/entity/uniqueAttribute/type/{typeName}/attr:qualifiedName={qualifiedName} (GET) Retrieving assets Â¶ I need to do this before I can update an asset, right? Strictly speaking, no, you do not. And in fact if you ultimately intend to update an asset you should trim it down to only what you intend to change and not send a complete asset. See Updating an asset for more details. Retrieving an asset uses a slightly different pattern from the other operations. For this you can use static methods provided by the Asset class: By GUID Â¶ 4.0.0 4.0.0 To retrieve an asset by its GUID: Java Python Kotlin Raw REST API Retrieve an asset by its GUID 1 2 Glossary glossary = Glossary . get ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ); // (1) If no exception is thrown, the returned object will be non-null and of the type requested. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Compile-time type checking This operation will type-check the asset you are retrieving is of the type requested. If it is not, you will receive a NotFoundException , even if the GUID represents some other asset. Retrieve an asset by its GUID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary , AtlasGlossaryTerm client = AtlanClient () glossary = client . asset . get_by_guid ( # (1) guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" , asset_type = AtlasGlossary , min_ext_info = False , ignore_relationships = True , attributes = [ AtlasGlossary . USER_DESCRIPTION , AtlasGlossary . TERMS ], related_attributes = [ AtlasGlossaryTerm . USER_DESCRIPTION ] ) assert glossary and glossary . user_description assert glossary . terms and len ( glossary . terms ) > 0 assert glossary . terms [ 0 ] . user_description client.asset.get_by_guid() method takes following parameters: guid : specify the (GUID) of the asset to retrieve. asset_type ( optional ): specify the type of asset to retrieve. Defaults to Asset . If no exception is thrown, the returned object will be non-null and of the type requested. min_ext_info ( optional ): minimizes additional information when set to True . Defaults to False ignore_relationships ( optional ): specify whether to include relationships ( False ) or exclude them ( True ). Defaults to True attributes ( optional ): defines the list of attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . related_attributes ( optional ): defines the list of relationship attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . Attributes and Related attributes In this example, we are retrieving the userDescription attribute for\nboth the glossary and its terms . You can also retrieve other attributes as illustrated above. Run-time type checking This operation will type-check the asset you are retrieving is of the type requested. If it is not, you will receive a NotFoundException , even if the GUID represents some other asset. Retrieve an asset by its GUID 1 2 val glossary = Glossary . get ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (1) If no exception is thrown, the returned object will be non-null and of the type requested. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Compile-time type checking This operation will type-check the asset you are retrieving is of the type requested. If it is not, you will receive a NotFoundException , even if the GUID represents some other asset. GET /api/meta/entity/guid/b4113341-251b-4adc-81fb-2420501c30e6?ignoreRelationships=false&minExtInfo=false 1 // (1) In the case of retrieving an asset, all necessary information is included in the URL of the request. There is no payload for the body of the request. By GUID (runtime typing) Â¶ 4.0.0 1.0.0 To retrieve an asset by GUID, but only resolve the type at runtime: Java Python Kotlin Raw REST API Retrieve an asset by its GUID 1 2 3 4 5 6 7 8 Asset read = Asset . get ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (1) false ); Glossary glossary ; if ( read instanceof Glossary ) { glossary = ( Glossary ) read ; // (2) } Retrieve the asset by its GUID. Since GUIDs are globally unique, you do not need to specify a type. (And this is why the operation returns a generic Asset , since the SDK can only determine the type at runtime, once it has a response back from Atlan.) Since the operation returns a generic Asset , you need to check and cast it to a more specific type if you want to access the more specific attributes of that type. Retrieve an asset by its GUID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary , AtlasGlossaryTerm client = AtlanClient () glossary = client . asset . get_by_guid ( # (1) guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" , asset_type = AtlasGlossary , min_ext_info = False , ignore_relationships = True , attributes = [ AtlasGlossary . USER_DESCRIPTION , AtlasGlossary . TERMS ], related_attributes = [ AtlasGlossaryTerm . USER_DESCRIPTION ] ) if isinstance ( asset , AtlasGlossary ): # (2) glossary = asset assert glossary and glossary . user_description assert glossary . terms and len ( glossary . terms ) > 0 assert glossary . terms [ 0 ] . user_description client.asset.get_by_guid() method takes following parameters: guid : specify the (GUID) of the asset to retrieve. asset_type ( optional ): specify the type of asset to retrieve. Defaults to Asset . If no exception is thrown, the returned object will be non-null and of the type requested. min_ext_info ( optional ): minimizes additional information when set to True . Defaults to False ignore_relationships ( optional ): specify whether to include relationships ( False ) or exclude them ( True ). Defaults to True attributes ( optional ): defines the list of attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . related_attributes ( optional ): defines the list of relationship attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . Attributes and Related attributes In this example, we are retrieving the userDescription attribute for\nboth the glossary and its terms . You can also retrieve other attributes as illustrated above. Since the operation returns a generic Asset , you need to use isinstance() to cast it to a more specific type in the block if you want an IDE to provide more specific type hints. Retrieve an asset by its GUID 1 2 3 4 5 val read : Asset = Asset . get ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (1) false ) val glossary = if ( read is Glossary ) read else null // (2) Retrieve the asset by its GUID. Since GUIDs are globally unique, you do not need to specify a type. (And this is why the operation returns a generic Asset , since the SDK can only determine the type at runtime, once it has a response back from Atlan.) Since the operation returns a generic Asset , you need to check and cast it to a more specific type if you want to access the more specific attributes of that type. Does not apply to a raw API request There is no concept of typing in a raw API request â€” all responses to the raw API will simply be JSON objects. By qualifiedName Â¶ 4.0.0 4.0.0 To retrieve an asset by its qualifiedName : Java Python Kotlin Raw REST API Retrieve an asset by its qualifiedName 1 2 3 4 Glossary glossary = Glossary . get ( client , \"FzCMyPR2LxkPFgr8eNGrq\" ); // (1) Table table = Table . get ( client , \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" ); // (2) If no exception is thrown, the returned object will be non-null and of the type requested. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Qualified name, not name You must provide the qualifiedName for glossary objects (glossaries, categories, terms) to use this method. If you only know the name, you should instead use the findByName() operations. For most objects, you can probably build-up the qualifiedName in your code directly. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Finding the connection portion The one exception is likely to be the connection portion of the name ( default/snowflake/1657037873 in this example). To find this portion, see Find connections . Retrieve an asset by its qualifiedName 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary , AtlasGlossaryTerm , Table client = AtlanClient () glossary = client . asset . get_by_qualified_name ( # (1) asset_type = AtlasGlossary , qualified_name = \"pXkf3RUvsIOIG8xnn0W3O\" , min_ext_info = False , ignore_relationships = True , attributes = [ AtlasGlossary . USER_DESCRIPTION , AtlasGlossary . TERMS ], related_attributes = [ AtlasGlossaryTerm . USER_DESCRIPTION ] ) assert glossary and glossary . user_description assert glossary . terms and len ( glossary . terms ) > 0 assert glossary . terms [ 0 ] . user_description table = client . asset . get_by_qualified_name ( asset_type = Table , qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , # (2) min_ext_info = False , ignore_relationships = True , attributes = [ Table . USER_DESCRIPTION , Table . COLUMNS ], related_attributes = [ COLUMN . USER_DESCRIPTION ] ) assert table and table . user_description assert table . columns and len ( table . columns ) > 0 assert table . columns [ 0 ] . user_description client.asset.get_by_qualified_name() method takes following parameters: qualified_name : specify the qualified name of the asset to retrieve. asset_type : specify the type of asset to retrieve. If no exception is thrown, the returned object will be non-null and of the type requested. min_ext_info ( optional ): minimizes additional information when set to True . Defaults to False ignore_relationships ( optional ): specify whether to include relationships ( False ) or exclude them ( True ). Defaults to True attributes ( optional ): defines the list of attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . related_attributes ( optional ): defines the list of relationship attributes to retrieve for the asset. Accepts either a list of strings or a list of AtlanField . Attributes and Related attributes In this example, we are retrieving the userDescription attribute for\nboth the glossary and its terms . You can also retrieve other attributes as illustrated above. For most objects, you can probably build-up the qualified_name in your code directly. Finding the connection portion The one exception is likely to be the connection portion of the name ( default/snowflake/1657037873 in this example). To find this portion, see Find connections . Retrieve an asset by its qualifiedName 1 2 3 4 val glossary = Glossary . get ( client , \"FzCMyPR2LxkPFgr8eNGrq\" ) // (1) val table = Table . get ( client , \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" ) // (2) If no exception is thrown, the returned object will be non-null and of the type requested. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Qualified name, not name You must provide the qualifiedName for glossary objects (glossaries, categories, terms) to use this method. If you only know the name, you should instead use the findByName() operations. For most objects, you can probably build-up the qualifiedName in your code directly. Because this operation will read the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Finding the connection portion The one exception is likely to be the connection portion of the name ( default/snowflake/1657037873 in this example). To find this portion, see Find connections . GET /api/meta/entity/uniqueAttribute/type/Glossary?attr:qualifiedName=FzCMyPR2LxkPFgr8eNGrq&ignoreRelationships=false&minExtInfo=false 1 // (1) In the case of retrieving an asset, all necessary information is included in the URL of the request. There is no payload for the body of the request. URL encoding may be needed Note that depending on the qualifiedName, you may need to URL-encode its value before sending. This is to replace any parts of the name that could be misinterpreted as actual URL components (like / or spaces). Full vs minimal assets Â¶ 2.0.3 4.0.0 The examples above illustrate how to retrieve: an asset with all of its relationships (a complete asset). an asset without any of its relationships (a minimal asset). You can also retrieve the opposite by explicitly requesting it: Java Python Kotlin Raw REST API Retrieve an asset by its GUID 1 2 3 4 Glossary glossary = Glossary . get ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" , true ); // (1) Retrieve the full asset, with all of its relationships, by its GUID. The last (optional) parameter being true indicates you want to retrieve the asset with all its relationships (a \"full\" asset). Similar variations exist on every asset as well as on the dynamically-typed Asset static methods. Retrieve an asset by its GUID 1 2 3 4 5 6 7 8 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary client = AtlanClient () glossary = client . asset . retrieve_minimal ( asset_type = AtlasGlossary , # (1) guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" ) Optionally, you can provide the asset type: If no exception is thrown, the returned object will be non-null and of the type requested. Retrieve an asset by its GUID 1 2 3 4 val glossary = Glossary . get ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" , true ) // (1) Retrieve the full asset, with all of its relationships, by its GUID. The last (optional) parameter being true indicates you want to retrieve the asset with all its relationships (a \"full\" asset). Similar variations exist on every asset as well as on the dynamically-typed Asset static methods. GET /api/meta/entity/guid/b4113341-251b-4adc-81fb-2420501c30e6?ignoreRelationships=true&minExtInfo=true 1 // (1) In the case of retrieving an asset, all necessary information is included in the URL of the request. Retrieving a minimal asset is a matter of setting the query parameters ignoreRelationships and minExtInfo to true . Retrieve minimal assets where possible You should retrieve minimal assets for better performance in cases where you do not need all of the relationships of the asset. Keep in mind that although the relationships will not be visible in the object after retrieving a minimal asset, this does not mean that there are no relationships on that asset (in Atlan). 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/suggestions/",
    "content": "/api/meta/search/indexsearch (POST) Find and apply suggestions to an asset Â¶ As a data team ourselves, we understand that metadata curation can be time-consuming.\nTo streamline this process, each time you fill in a metadata gap, Atlan looks\nfor opportunities to reuse that information. This is where trident suggestions ðŸ”± come into play.\nIt provides metadata suggestions for similar assets. For example: CUSTOMER (table) You add the description \"Information about customers\" to a table called CUSTOMER . Atlan searches for other tables named CUSTOMER that lack a description. Atlan then suggests \"Information about customers\" as the description for these other CUSTOMER tables. In Atlan's SDK, you can use the Suggestions object to\nfind and apply these recommendations to your assets. Find suggestions Â¶ 2.3.3 1.12.7 Find suggestions for a given asset: Java Python Kotlin Find suggestions for a given asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Table table = Table . get ( \"default/snowflake/1720661835/db/schema/table\" ); // (1) SuggestionResponse response = Suggestions . finder ( table ) // (2) . include ( Suggestions . TYPE . GroupOwners ) . include ( Suggestions . TYPE . UserDescription ) . maxSuggestions ( 5 ) . withOtherType ( View . TYPE_NAME ) . includeArchived ( false ) . get (); // (3) assertNotNull ( response ); // (4) assertNotNull ( response . getOwnerGroups ()); assertNotNull ( response . getUserDescriptions ()); First, you need to retrieve the asset for which you want to find suggestions. Start by building a Suggestions request by chaining the following methods: finder : specify the asset for which you want to find suggestions. include : add criteria to specify the types of suggestions\nto include in the search results. For this example, we're retrieving\nsuggestions for GroupOwners and UserDescription . Want to find suggestions for ALL types? To include all suggestion types\n( description , owner , tags , and terms ): Suggestions.includes(Arrays.asList(Suggestions.TYPE.values())) maxSuggestions : (Optional) specify the maximum\nnumber of suggestions to return. Defaults to 5 . includeArchived : (Optional) specify whether to include\narchived assets in the suggestions ( true ) or not ( false ). Defaults to false . withOtherType : (Optional) add a criterion\nto include another asset type in the suggestions. withOtherType By default, we will only look for suggestions on\nassets of the same type. You may want to expand this,\nfor example, by including View (s) when looking\nfor suggested metadata for Table (s). where : (Optional) add a criterion that must be present\nin every search result. ( NOTE: These are translated to filters.) whereNot : (Optional) add a criterion\nthat must not be present in any search result. Finally, to retrieve the suggestions, call the .get() method. The suggestion response contains a list of\nsuggestions for the requested types. You can access\nspecific suggestions by directly referencing the response attributes,\nsuch as response.getOwnerGroups() and response.getUserDescriptions() . Find suggestions for a given asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.suggestions import Suggestions client = AtlanClient () asset = client . asset . get_by_qualified_name ( qualified_name = \"default/snowflake/1720661835/db/schema/table\" , asset_type = Table ) # (1) response = ( Suggestions () # (2) . finder ( asset ) . include ( Suggestions . TYPE . GROUP_OWNERS ) . include ( Suggestions . TYPE . USER_DESCRIPTION ) . max_suggestion ( 5 ) . with_other_type ( \"View\" ) . include_archive ( False ) . get ( client = client ) # (3) ) assert response # (4) assert response . owner_groups and response . owner_groups [ 0 ] assert response . user_descriptions and response . user_descriptions [ 0 ] First, you need to retrieve the asset for which you want to find suggestions. Start by instantiating the Suggestions() object.\nYou can then build a suggestion request by chaining the following methods: finder : specify the asset for which you want to find suggestions. include : add criteria to specify the types of suggestions\nto include in the search results. For this example, we're retrieving\nsuggestions for GROUP_OWNERS and USER_DESCRIPTION . Want to find suggestions for ALL types? To include all suggestion types\n( description , owner , tags , and terms ),\nyou can directly pass Suggestions.TYPE.all() to the Suggestions : Suggestions(includes=Suggestions.TYPE.all()) max_suggestion : (Optional) specify the maximum\nnumber of suggestions to return. Defaults to 5 . include_archive : (Optional) specify whether to include\narchived assets in the suggestions ( True ) or not ( False ). Defaults to False . with_other_type : (Optional) add a criterion\nto include another asset type in the suggestions. with_other_type By default, we will only look for suggestions on\nassets of the same type. You may want to expand this,\nfor example, by including View (s) when looking\nfor suggested metadata for Table (s). where : (Optional) add a criterion that must be present\nin every search result. ( NOTE: These are translated to filters.) where_not : (Optional) add a criterion\nthat must not be present in any search result. Finally, to retrieve the suggestions, call the .get() method. The suggestion response contains a list of\nsuggestions for the requested types. You can access\nspecific suggestions by directly referencing the response attributes,\nsuch as response.owner_groups and response.user_descriptions . Find suggestions for a given asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 var table = Table . get ( \"default/snowflake/1720661835/db/schema/table\" ); // (1) var response = Suggestions . finder ( table ) // (2) . include ( Suggestions . TYPE . GroupOwners ) . include ( Suggestions . TYPE . UserDescription ) . maxSuggestions ( 5 ) . withOtherType ( View . TYPE_NAME ) . includeArchived ( false ) . get (); // (3) assertNotNull ( response ); // (4) assertNotNull ( response . getOwnerGroups ()); assertNotNull ( response . getUserDescriptions ()); First, you need to retrieve the asset for which you want to find suggestions. Start by building a Suggestions request by chaining the following methods: finder : specify the asset for which you want to find suggestions. include : add criteria to specify the types of suggestions\nto include in the search results. For this example, we're retrieving\nsuggestions for GroupOwners and UserDescription . Want to find suggestions for ALL types? To include all suggestion types\n( description , owner , tags , and terms ): Suggestions.includes(Suggestions.TYPE.values().toList()) maxSuggestions : (Optional) specify the maximum\nnumber of suggestions to return. Defaults to 5 . includeArchived : (Optional) specify whether to include\narchived assets in the suggestions ( true ) or not ( false ). Defaults to false . withOtherType : (Optional) add a criterion\nto include another asset type in the suggestions. withOtherType By default, we will only look for suggestions on\nassets of the same type. You may want to expand this,\nfor example, by including View (s) when looking\nfor suggested metadata for Table (s). where : (Optional) add a criterion that must be present\nin every search result. ( NOTE: These are translated to filters.) whereNot : (Optional) add a criterion\nthat must not be present in any search result. Finally, to retrieve the suggestions, call the .get() method. The suggestion response contains a list of\nsuggestions for the requested types. You can access\nspecific suggestions by directly referencing the response attributes,\nsuch as response.getOwnerGroups() and response.getUserDescriptions() . Apply suggestions Â¶ 7.0.0 1.12.7 Apply suggestions to a given asset: Java Python Kotlin Apply suggestions to a given asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Table table = Table . get ( \"default/snowflake/1720661835/db/schema/table\" ); // (1) SuggestionResponse response = Suggestions . finder ( table ) // (2) . include ( Suggestions . TYPE . GroupOwners ) . include ( Suggestions . TYPE . UserDescription ) . maxSuggestions ( 5 ) . withOtherType ( View . TYPE_NAME ) . includeArchived ( false ) . apply ( true ); // (3) assertNotNull ( response ); // (4) assertNotNull ( response . getUpdatedAssets ()) First, retrieve the asset for which you want to apply suggestions. Start by building Suggestions request in the same\nway as described in Find suggestions section ,\nsince here we first find the suggestions and then apply them. To apply the suggestions, call the .apply() method.\nOptionally, you can set allowMultiple to true to allow\nmultiple suggestions to be applied to the asset (up to the maxSuggestions requested), such as for owners , terms , and tags . The AssetMutationResponse will contain the updated asset entities. Apply suggestions to a given asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.suggestions import Suggestions client = AtlanClient () asset = client . asset . get_by_qualified_name ( qualified_name = \"default/snowflake/1720661835/db/schema/table\" , asset_type = Table ) # (1) response = ( Suggestions () # (2) . finder ( asset ) . include ( Suggestions . TYPE . GROUP_OWNERS ) . include ( Suggestions . TYPE . USER_DESCRIPTION ) . max_suggestion ( 5 ) . with_other_type ( \"View\" ) . include_archive ( False ) . apply ( client = client , allow_multiple = True ) # (3) ) assert response and response . mutated_entities # (4) assert response . mutated_entities . UPDATE assert response . mutated_entities . UPDATE [ 0 ] First, retrieve the asset for which you want to apply suggestions. Start by instantiating the Suggestions() object.\nYou can then build a suggestion request in the same\nway as described in Find suggestions section ,\nsince here we first find the suggestions and then apply them. To apply the suggestions, call the .apply() method.\nOptionally, you can set allow_multiple to True to allow\nmultiple suggestions to be applied to the asset (up to the max_suggestions requested), such as for owners , terms , and tags . The AssetMutationResponse will contain the mutated asset entities. Apply suggestions to a given asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 var table = Table . get ( \"default/snowflake/1720661835/db/schema/table\" ); // (1) var response = Suggestions . finder ( table ) // (2) . include ( Suggestions . TYPE . GroupOwners ) . include ( Suggestions . TYPE . UserDescription ) . maxSuggestions ( 5 ) . withOtherType ( View . TYPE_NAME ) . includeArchived ( false ) . apply ( true ); // (3) assertNotNull ( response ); // (4) assertNotNull ( response . getUpdatedAssets ()) First, retrieve the asset for which you want to apply suggestions. Start by building Suggestions request in the same\nway as described in Find suggestions section ,\nsince here we first find the suggestions and then apply them. To apply the suggestions, call the .apply() method.\nOptionally, you can set allowMultiple to true to allow\nmultiple suggestions to be applied to the asset (up to the maxSuggestions requested), such as for owners , terms , and tags . The AssetMutationResponse will contain the updated asset entities. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/update/",
    "content": "/api/meta/entity/bulk (POST) Updating an asset Â¶ All objects in the SDK that you can update within Atlan implement the builder pattern. This allows you to progressively build-up the object you want to update. In addition, each object provides a method that takes the minimal set of required fields to update that asset , when it already exists in Atlan. Include only your intended changes, nothing more When enriching an asset in Atlan, you only need to specify the information you want to change. Any information you do not include in your update will be left untouched on the asset in Atlan. This way you do not need to: try to reproduce the complete asset in your request to do targeted updates to specific attributes worry about other changes that may be made to the asset in parallel to the changes you will be making to the asset Build minimal object needed Â¶ 2.0.0 1.0.0 For example, to update a glossary term we need to provide the qualifiedName and name of the term, and the GUID of the glossary in which it exists: Java Python Kotlin Raw REST API Build minimal asset necessary for update 1 2 3 4 GlossaryTermBuilder <? , ?> termUpdater = GlossaryTerm . updater ( \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , // (1) \"Example Term\" , // (2) \"b4113341-251b-4adc-81fb-2420501c30e6\" ); // (3) The qualifiedName of the existing term, which must match exactly (case-sensitive). Note that for some assets (like terms), this may be a strange-looking Atlan-internal string. The name of the existing term. This must match exactly (case-sensitive). The GUID of the glossary in which the term exists. Build minimal asset necessary for update 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Announcement , AtlasGlossaryTerm from pyatlan.model.enums import AnnouncementType , CertificateStatus client = AtlanClient () term = AtlasGlossaryTerm . updater ( qualified_name = \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , # (1) name = \"Example Term\" , # (2) glossary_guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" , # (3) ) The qualifiedName of the existing term, which must match exactly (case-sensitive). Note that for some assets (like terms), this may be a strange-looking Atlan-internal string. The name of the existing term. This must match exactly (case-sensitive). The GUID of the glossary in which the term exists. Build minimal asset necessary for update 1 2 3 4 5 6 val termUpdater = GlossaryTerm . updater ( \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , // (1) \"Example Term\" , // (2) \"b4113341-251b-4adc-81fb-2420501c30e6\" // (3) ) The qualifiedName of the existing term, which must match exactly (case-sensitive). Note that for some assets (like terms), this may be a strange-looking Atlan-internal string. The name of the existing term. This must match exactly (case-sensitive). The GUID of the glossary in which the term exists. Implicit in the API calls below There is nothing specific to do for this step when using the raw APIs â€” constructing the object is simply what you place in the payload of the API calls in the steps below. What if I already have an asset (for example, from a search)? Since you should only include your intended changes, and nothing more, the SDKs provide a convenience method to reduce an asset down to its minimal properties. You should use this method to trim an object in your code down to a starting point for the changes you want to make to that asset: Java Python Trim existing asset to minimal properties 4 GlossaryTermBuilder <? , ?> termUpdater = existing . trimToRequired (); // (1) Assuming you have an existing asset in a variable called existing , you can call trimToRequired() to reduce it to a builder with the minimal properties needed to update that asset. Trim existing asset to minimal properties 10 term = existing . trim_to_required () # (1) Assuming you have an existing asset in a variable called existing , you can call trim_to_required() to reduce it to an object with the minimal properties needed to update that asset. Enrich before updating Â¶ The term object now has the minimal required information for Atlan to update it. Without any additional enrichment, though, there isn't really anything to update... 1.4.0 1.0.0 So first you should enrich the object: Java Python Kotlin Raw REST API Enrich the asset before updating it 5 6 7 8 9 10 GlossaryTerm term = termUpdater // (1) . certificateStatus ( CertificateStatus . VERIFIED ) // (2) . announcementType ( AtlanAnnouncementType . INFORMATION ) // (3) . announcementTitle ( \"Imported\" ) . announcementMessage ( \"This term was imported from ...\" ) . build (); // (4) We'll create an object we can take actions on from this updater. In this example, we're adding a certificate to the object. Note that you can chain any number of enrichments together. Here we are also adding an announcement to the asset. To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) Enrich the asset before updating it 11 12 13 14 15 16 17 term . certificate_status = CertificateStatus . VERIFIED # (1) announcement = Announcement ( announcement_type = AnnouncementType . INFORMATION , announcement_title = \"Imported\" , announcement_message = \"This term was imported from ..\" , ) term . set_announcement ( announcement ) # (2) In this example, we're adding a certificate to the object. In this example, we're adding an announcement to the object. Enrich the asset before updating it 7 8 9 10 11 12 val term = termUpdater // (1) . certificateStatus ( CertificateStatus . VERIFIED ) // (2) . announcementType ( AtlanAnnouncementType . INFORMATION ) // (3) . announcementTitle ( \"Imported\" ) . announcementMessage ( \"This term was imported from ...\" ) . build () // (4) We'll create an object we can take actions on from this updater. In this example, we're adding a certificate to the object. Note that you can chain any number of enrichments together. Here we are also adding an announcement to the asset. To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) Implicit in the API calls below There is nothing specific to do for this step when using the raw APIs â€” constructing the object is simply what you place in the payload of the API calls in the steps below. Update the asset from the object Â¶ 1.4.0 4.0.0 You can then actually update the object in Atlan 1 : Java Python Kotlin Raw REST API Update (or create) the asset 11 12 13 14 15 16 AssetMutationResponse response = term . save ( client ); // (1) Asset updated = response . getUpdatedAssets (). get ( 0 ); // (2) GlossaryTerm term ; if ( updated instanceof GlossaryTerm ) { term = ( GlossaryTerm ) updated ; // (3) } The save() method will either update an existing asset (if Atlan already has a term with the same name and qualifiedName in the same glossary) or create a new asset (if Atlan does not have a term with the same name in the same glossary). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: getCreatedAssets() lists assets that were created getUpdatedAssets() lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. Strictly update the asset 11 12 13 14 15 16 17 18 19 20 try { AssetMutationResponse response = term . updateMergingCM ( client , false ); // (1) Asset updated = response . getUpdatedAssets (). get ( 0 ); // (2) GlossaryTerm term ; if ( updated instanceof GlossaryTerm ) { term = ( GlossaryTerm ) updated ; // (3) } } catch ( NotFoundException e ) { // (4) log . warn ( \"No existing asset to update, so nothing changed or created.\" , e ); } The updateMergingCM() method will only update an existing asset (if Atlan already has an asset of the same type with the same name qualifiedName ).  Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Depending on the update behavior you want, you could also use: updateMergingCM(false) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while leaving any Atlan tags on the existing asset untouched updateMergingCM(true) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while replacing any Atlan tags on the existing asset updateReplacingCM(false) to overwrite all custom metadata on the existing asset with what you're providing in your update, while leaving any Atlan tags on the existing asset untouched updateReplacingCM(true) to overwrite all custom metadata on the existing asset with what you're providing in your update, while replacing any Atlan tags on the existing asset You can distinguish what was created or updated: getUpdatedAssets() lists assets that were updated Note that the update...() methods always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. Since the update...() methods strictly update (and never create) an asset, if the asset you are trying to update does not exist the operation will throw a NotFoundException . Update (or create) the asset 18 19 20 response = client . asset . save ( term ) # (1) if updated := response . assets_updated ( asset_type = AtlasGlossaryTerm ): # (2) term = updated [ 0 ] # (3) The save(term) method will either update an existing asset (if Atlan already has a term with the same name and qualifiedName in the same glossary) or (create a new asset, if Atlan does not have a term with the same name in the same glossary). You can distinguish what was created or updated: assets_created(asset_type = AtlasGlossaryType) returns a list assets of the specified type that were created. assets_updated(asset_type = AtlasGlossaryType) returns a list assets of the specified type that were updated. If the returned list is not empty, get the term that was updated. Strictly update the asset 18 19 20 21 22 23 try : response = client . asset . update_merging_cm ( term ) # (1) if updated := response . assets_updated ( asset_type = AtlasGlossaryTerm ): # (2) term = updated [ 0 ] except NotFoundError : # (3) print ( \"No existing asset to update, so nothing changed or created.\" ) The update_merging_cm() method will only update an existing asset (if Atlan already has an asset of the same type with the same name qualified_name ). Depending on the update behavior you want, you could also use: update_merging_cm(replace_atlan_tags=False) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while leaving any Atlan tags on the existing asset untouched update_merging_cm(replace_atlan_tags=True) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while replacing any Atlan tags on the existing asset update_replacing_cm(replace_atlan_tags=False) to overwrite all custom metadata on the existing asset with what you're providing in your update, while leaving any Atlan tags on the existing asset untouched update_replacing_cm(replace_atlan_tags=True) to overwrite all custom metadata on the existing asset with what you're providing in your update, while replacing any Atlan tags on the existing asset You can distinguish what was created or updated: assets_updated(asset_type = AtlasGlossaryType) returns a list assets of the specified type that were updated. Since the update...() methods strictly update (and never create) an asset, if the asset you are trying to update does not exist the operation will throw a NotFoundError . Update (or create) the asset 13 14 15 val response = term . save ( client ) // (1) val updated = response . updatedAssets [ 0 ] // (2) val term = if ( updated is GlossaryTerm ) updated else null // (3) The save() method will either update an existing asset (if Atlan already has a term with the same name and qualifiedName in the same glossary) or create a new asset (if Atlan does not have a term with the same name in the same glossary). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: createdAssets lists assets that were created updatedAssets lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. Strictly update the asset 13 14 15 16 17 18 19 try { val response = term . updateMergingCM ( client , false ) // (1) val updated = response . updatedAssets [ 0 ] // (2) val term = if ( updated is GlossaryTerm ) updated else null // (3) } catch ( e : NotFoundException ) { // (4) log . warn ( \"No existing asset to update, so nothing changed or created.\" , e ) } The updateMergingCM() method will only update an existing asset (if Atlan already has an asset of the same type with the same name qualifiedName ). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Depending on the update behavior you want, you could also use: updateMergingCM(false) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while leaving any Atlan tags on the existing asset untouched updateMergingCM(true) to only overwrite any custom metadata provided in your update (leaving anything you don't provide untouched on the existing asset), while replacing any Atlan tags on the existing asset updateReplacingCM(false) to overwrite all custom metadata on the existing asset with what you're providing in your update, while leaving any Atlan tags on the existing asset untouched updateReplacingCM(true) to overwrite all custom metadata on the existing asset with what you're providing in your update, while replacing any Atlan tags on the existing asset You can distinguish what was created or updated: getUpdatedAssets() lists assets that were updated Note that the update...() methods always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. Since the update...() methods strictly update (and never create) an asset, if the asset you are trying to update does not exist the operation will throw a NotFoundException . POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryTerm\" , // (2) \"attributes\" : { \"name\" : \"Example Term\" , // (3) \"qualifiedName\" : \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" }, \"certificateStatus\" : \"VERIFIED\" , // (6) \"announcementType\" : \"information\" , // (7) \"announcementTitle\" : \"Imported\" , \"announcementMessage\" : \"This term was imported from ...\" } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). For a term, this is AtlasGlossaryTerm . You must provide the exact name of the existing asset (case-sensitive). (Unless you want to change its name, in which case you can provide the new name instead.) You must provide the exact qualifiedName of the existing asset (case-sensitive). Must exactly match the qualifiedName of an existing asset If this does not exactly match the qualifiedName of an existing asset, the API call will instead create a new asset rather than updating an existing one. For most assets, you do not need to re-specify the parent object for an update. However, for glossary assets (like terms), you are required to re-specify the parent glossary. In this example, we're adding a certificate to the object. Note that you can include any number of enrichments together. Here we are also adding an announcement to the asset. Case-sensitive, exact match If you use a different capitalization or spelling for the qualifiedName , you may accidentally create a new asset rather than updating the existing one. 2 Remove information from an asset Â¶ 1.4.0 4.0.0 As mentioned in Enrich before updating section, only the information in your request will be updated on the object. But what if you want to remove some information that already exists on the asset in Atlan? Java Python Kotlin Raw REST API Enrich and update the asset 5 6 7 8 9 10 11 12 13 14 GlossaryTerm term = termUpdater // (1) . removeCertificate () // (2) . removeAnnouncement () // (3) . build (); // (4) AssetMutationResponse response = term . save ( client ); // (5) Asset updated = response . getUpdatedAssets (). get ( 0 ); // (6) GlossaryTerm term ; if ( updated instanceof GlossaryTerm ) { term = ( GlossaryTerm ) updated ; // (7) } We'll create an object we can take actions on from this updater. In this example, we'll remove any existing certificate from the object in Atlan. Note that you can chain any number of enrichments together. Here we are also removing any announcement from the asset. To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) The save() method will either: Update an existing asset, if Atlan already has a term with the same name and qualifiedName in the same glossary. Create a new asset, if Atlan does not have a term with the same name in the same glossary. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: getCreatedAssets() lists assets that were created getUpdatedAssets() lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. Enrich and update the asset 11 12 13 14 15 term . remove_certificate () # (1) term . remove_announcement () # (2) response = client . asset . save ( term ) if updated := response . assets_updated ( asset_type = AtlasGlossaryTerm ): term = updated [ 0 ] In this example we will remove an existing certificate from any existing certificate from the object. In this example we will remove any existing announcement from the object. Enrich and update the asset 7 8 9 10 11 12 13 var term = termUpdater // (1) . removeCertificate () // (2) . removeAnnouncement () // (3) . build () // (4) val response = term . save ( client ) // (5) val updated = response . updatedAssets [ 0 ] // (6) term = if ( updated is GlossaryTerm ) updated else null // (7) We'll create an object we can take actions on from this updater. In this example, we'll remove any existing certificate from the object in Atlan. Note that you can chain any number of enrichments together. Here we are also removing any announcement from the asset. To persist the enrichment back to the object, we must build() the builder. Assign the result back Remember to assign the result of the build() operation back to your original object. Otherwise the result is not persisted back into any variable! (In this case we're assigning to the term variable back on line 5.) The save() method will either: Update an existing asset, if Atlan already has a term with the same name and qualifiedName in the same glossary. Create a new asset, if Atlan does not have a term with the same name in the same glossary. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was created or updated: createdAssets lists assets that were created updatedAssets lists assets that were updated Note that the save() method always returns objects of type Asset , though. The Asset class is a superclass of all assets. So we need to cast to more specific types (like GlossaryTerm ) after verifying the object that was actually returned. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryTerm\" , // (2) \"attributes\" : { \"name\" : \"Example Term\" , // (3) \"qualifiedName\" : \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" }, \"certificateStatus\" : null , // (6) \"certificateStatusMessage\" : null , \"announcementType\" : null , // (7) \"announcementTitle\" : null , \"announcementMessage\" : null } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). For a term, this is AtlasGlossaryTerm . You must provide the exact name of the existing asset (case-sensitive). (Unless you want to change its name, in which case you can provide the new name instead.) You must provide the exact qualifiedName of the existing asset (case-sensitive). Must exactly match the qualifiedName of an existing asset If this does not exactly match the qualifiedName of an existing asset, the API call will instead create a new asset rather than updating an existing one. For most assets, you do not need to re-specify the parent object for an update. However, for glossary assets (like terms), you are required to re-specify the parent glossary. In this example, we're removing any existing certificate information from the object in Atlan (by sending null ). Note that you can include any number of enrichments together. Here we are also removing any announcement from the asset. Atlan automatically detects changes to determine whether to create or update an asset â€” see the Importance of identifiers for a more detailed explanation. To strictly update (and avoid creating) an asset, you must first look for the existing asset and only if found proceed with your update. When the SDKs provide such strict update functionality, this is what they are doing behind-the-scenes. Be aware that this will impact performance, so you should only do this where strictly necessary for your logic. â†© This is because Atlan uses the exact qualifiedName to determine whether it should do an update. For a more detailed explanation, see the Importance of identifiers . â†© 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/advanced-examples/search/",
    "content": "/api/meta/search/indexsearch (POST) Searching for assets Â¶ Searching is a very flexible operation in Atlan. This also makes it a bit more complex to understand than the other operations. To encapsulate the full flexibility of Atlan's search, the SDK provides a dedicated IndexSearchRequest object and a FluentSearch class for configuring such a request using a fluent builder pattern. More details on the power and flexibility of searching See the dedicated Searching section of this site for more details on Atlan's search. This covers the various kinds of searches you can run, and the detailed attributes you can search against. Build the query Â¶ 0.1.0 1.0.0 2.0.0 To run a search in Atlan, you need to define the query using Elastic's structures. While you can always use Elastic's own structures to make use of its full power, for the vast majority of cases you may find it easier to use the helpers built-in to the SDK: Java Python Kotlin Go Raw REST API Build the query 1 2 3 FluentSearch . FluentSearchBuilder <? , ?> builder = client . assets . select () // (1) . active () // (2) . where ( Asset . TYPE_NAME . eq ( GlossaryTerm . TYPE_NAME )); // (3) You can start building a query across all assets using the select() method on the assets member of any client. You can chain as many conditions as you want: where() is mandatory inclusion whereNot() is mandatory exclusion whereSome() for conditions where some of them must match This helper provides a query that ensures results are active (not archived) assets. Equivalent Elastic query Query beActive = TermQuery . of ( m -> m . field ( \"__state\" ) . value ( AtlanStatus . ACTIVE . getValue ())) . _toQuery (); This condition provides a query that restricts results to a specific type of assets (glossary terms in this example). Equivalent Elastic query Query beTerm = TermQuery . of ( m -> m . field ( \"__typeName.keyword\" ) . value ( GlossaryTerm . TYPE_NAME )) . _toQuery (); Build the query 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Asset , AtlasGlossaryTerm from pyatlan.model.fluent_search import CompoundQuery , FluentSearch builder = ( FluentSearch () # (1) . where ( CompoundQuery . active_assets ()) # (2) . where ( CompoundQuery . asset_type ( AtlasGlossaryTerm )) # (3) ) You can start building a query using a FluentSearch object. You can have as many mandatory ( where() ) conditions, mandatory exclusion ( where_not() ) conditions, and set of conditions some of which must match ( where_some() ) as you want. This helper provides a query that ensures results are active (not archived) assets. This helper provides a query that restricts results to a specific type of assets (glossary terms in this example). Build the query 1 2 3 val builder = client . assets . select () // (1) . active () // (2) . where ( Asset . TYPE_NAME . eq ( GlossaryTerm . TYPE_NAME )) // (3) You can start building a query across all assets using the select() method on the assets member of any client. You can chain as many mandatory ( where() ) conditions, mandatory exclusion ( whereNot() ) conditions, and set of conditions some of which must match ( whereSome() ) as you want. This helper provides a query that ensures results are active (not archived) assets. Equivalent Elastic query val beActive = TermQuery . of ( m -> m . field ( \"__state\" ) . value ( AtlanStatus . ACTIVE . getValue ())) . _toQuery () This helper provides a query that restricts results to a specific type of assets (glossary terms in this example). Equivalent Elastic query val beTerm = TermQuery . of ( m -> m . field ( \"__typeName.keyword\" ) . value ( GlossaryTerm . TYPE_NAME )) . _toQuery () Build the query 1 2 3 builder := assets . NewFluentSearch (). // (1) ActiveAssets (). // (2) AssetType ( \"AtlasGlossaryTerm\" ). // (3) You can start building a query using a FluentSearch object. You can have as many mandatory ( where() ) conditions, mandatory exclusion ( WhereNot() ) conditions, and set of conditions some of which must match ( WhereSome() ) as you want. This helper provides a query that ensures results are active (not archived) assets. This helper provides a query that restricts results to a specific type of assets (glossary terms in this example). Query contents 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 \"bool\" : { // (1) \"filter\" : [ // (2) { \"term\" : { // (3) \"__state\" : { // (4) \"value\" : \"ACTIVE\" // (5) } } }, { \"term\" : { \"__typeName.keyword\" : { // (6) \"value\" : \"AtlasGlossaryTerm\" // (7) } } } ] } A bool query combines together multiple conditions. A filter clause exactly matches all of the conditions, without scoring (so can be slightly faster than other scoring-based combination mechanisms). Term queries are generally used to exactly match values. The __state field will match the status of an asset in Atlan. So in this example you will only match assets that are currently ACTIVE (not archived or soft-deleted). You will also only match assets that are of a specific type, since __typeName.keyword will match the type of asset. Note these names do not exactly match attribute names Note that these names are field names in the search index, and may vary from the attribute names of the assets in Atlan. To find the appropriate field name and how it relates to an attribute name, use the full model reference . In this example, you will only match terms. Build the request Â¶ 0.0.17 1.0.0 1.1.0 Once the query is defined, we can then build up the search request. The request includes not only the query, but also parameters like paging and which attributes to include in the response: Java Python Kotlin Go Raw REST API Build the request 4 5 6 7 8 IndexSearchRequest index = builder // (1) . pageSize ( 100 ) // (2) . includeOnResults ( GlossaryTerm . ANCHOR ) // (3) . includeOnRelations ( Asset . CERTIFICATE_STATUS ) // (4) . toRequest (); // (5) You can then chain additional parameters onto the fluent search. (You could of course do this all directly as part of the same chain, you do not need to store the interim builder variable.) The number of results to include (per page). You can chain as many attributes as you want to include in each result. In this case we will return the anchor attribute for terms, which gives the relationship from the term to its parent glossary. You can chain as many attributes to include on each related asset to each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. You can now build all of this search configuration into a request. Build the request 10 11 12 13 14 15 index = ( builder # (1) . page_size ( 100 ) # (2) . include_on_results ( AtlasGlossaryTerm . ANCHOR ) # (3) . include_on_relations ( Asset . CERTIFICATE_STATUS ) # (4) ) . to_request () # (5) You can then chain additional parameters onto the fluent search. (You could of course do this all directly as part of the same chain, you do not need to store the interim builder variable.) The number of results to include (per page). You can chain as many attributes as you want to include in each result. In this case we will return the anchor attribute for terms, which gives the relationship from the term to its parent glossary. You can chain as many attributes to include on each related asset to each result. Since we are returning anchor relationships, this will ensure that the certificate_status of those related glossaries is also included in each result. You can now build all of this search configuration into a request. Build the request 4 5 6 7 8 val index = builder // (1) . pageSize ( 100 ) // (2) . includeOnResults ( GlossaryTerm . ANCHOR ) // (3) . includeOnRelations ( Asset . CERTIFICATE_STATUS ) // (4) . toRequest () // (5) You can then chain additional parameters onto the fluent search. (You could of course do this all directly as part of the same chain, you do not need to store the interim builder variable.) The number of results to include (per page). You can chain as many attributes as you want to include in each result. In this case we will return the anchor attribute for terms, which gives the relationship from the term to its parent glossary. You can chain as many attributes to include on each related asset to each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. You can now build all of this search configuration into a request. Build the request 4 5 6 7 8 index := builder . // (1) PageSize ( 100 ). // (2) IncludeOnResults ( \"anchor\" ). // (3) IncludeOnRelations ( \"certificateStatus\" ). // (4) ToRequest () // (5) You can then chain additional parameters onto the fluent search. (You could of course do this all directly as part of the same chain, you do not need to store the interim builder variable.) The number of results to include (per page). You can chain as many attributes as you want to include in each result. In this case we will return the anchor attribute for terms, which gives the relationship from the term to its parent glossary. You can chain as many attributes to include on each related asset to each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. You can now build all of this search configuration into a request for executing. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"dsl\" : { // (1) \"from\" : 0 , // (2) \"size\" : 100 , \"query\" : { // (3) }, \"track_total_hits\" : true // (4) }, \"attributes\" : [ // (5) \"anchor\" ], \"relationAttributes\" : [ // (6) \"certificateStatus\" ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , // (7) \"excludeClassifications\" : false } A query should always be defined within the dsl portion of the request. In addition to the query, you can specify from and size parameters for pagination. The query itself should be provided within the query portion of the dsl . Here you would use the query body provided in the earlier step. You must set track_total_hits to true if you want an exact count of the number of results (in particular for pagination). The list of attributes to include in each result. In this case we will return the anchor attribute for terms, which gives the relationship from the term to its parent glossary. The list of attributes to include on each relationship that is included in each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. You can also choose whether to include other related information, such as the terms and Atlan tags assigned to each result. In general, only include the information you require â€” this will provide the best performance. (Optional) Build request directly from JSON Â¶ 5.0.0 While we recommend using FluentSearch to build search requests, there might be scenarios where quick prototyping or testing of search queries is needed. For such use cases, we also support directly constructing search requests from raw JSON. Java Python Kotlin Raw REST API Coming soon Build request directly from JSON 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 from pyatlan.model.search import IndexSearchRequest raw_json_payload = r \"\"\" { \"dsl\": { \"from_\": 0, \"size\": 100, \"aggregations\": {} , \"track_total_hits\": true, \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"__state\": { \"value\": \"ACTIVE\", \"case_insensitive\": false } } }, { \"term\": { \"__typeName.keyword\": { \"value\": \"AtlasGlossaryTerm\", \"case_insensitive\": false } } } ] } }, \"sort\": [ { \"__guid\": { \"order\": \"asc\" } } ] }, \"attributes\": [ \"anchor\" ], \"relation_attributes\": [ \"certificateStatus\" ], \"suppress_logs\": true, \"show_search_score\": false, \"exclude_meanings\": false } \"\"\" index = IndexSearchRequest . parse_raw ( raw_json_payload ) # (1) You can use Pydantic's .parse_raw(raw_json_payload) method to parse a JSON string and construct an SDK IndexSearchRequest object. If you're using a dictionary instead of a JSON string, use .parse_obj(dict_payload) instead. Coming soon POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"dsl\" : { // (1) \"from\" : 0 , // (2) \"size\" : 100 , \"query\" : { // (3) }, \"track_total_hits\" : true // (4) }, \"attributes\" : [ // (5) \"anchor\" ], \"relationAttributes\" : [ // (6) \"certificateStatus\" ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , // (7) \"excludeClassifications\" : false } A query should always be defined within the dsl portion of the request. In addition to the query, you can specify from and size parameters for pagination. The query itself should be provided within the query portion of the dsl . Here you would use the query body provided in the earlier step. You must set track_total_hits to true if you want an exact count of the number of results (in particular for pagination). The list of attributes to include in each result. In this case we will return the anchor attribute for terms, which gives the relationship from the term to its parent glossary. The list of attributes to include on each relationship that is included in each result. Since we are returning anchor relationships, this will ensure that the certificateStatus of those related glossaries is also included in each result. You can also choose whether to include other related information, such as the terms and Atlan tags assigned to each result. In general, only include the information you require â€” this will provide the best performance. Run the search Â¶ 0.1.0 1.4.0 4.0.0 To now run the search, we call the search() method against our request object: Java Python Kotlin Go Raw REST API Run the search 9 10 IndexSearchResponse response = index . search ( client ); log . info ( response . getApproximateCount ()) // (1) The getApproximateCount() method gives the total number of results overall (not restricted by page). Run the search 16 17 results = client . asset . search ( index ) print ( results . count ) # (1) The count property gives the total number of results overall (not restricted by page). Run the search 9 10 val response = index . search ( client ) log . info ( response . approximateCount ); // (1) The .approximateCount member gives the total number of results overall (not restricted by page). Run the search 9 10 11 12 13 14 15 16 17 18 19 20 results , _ := assets . Search ( * index ) // (1) fmt . Printf ( \"Found %d assets\\n\" , results . Count ()) // (2) entities , errIter := results . Iter () // (3) for asset := range entities { // (4) fmt . Println ( \"asset name:\" , * asset . Name ) } if err := <- errIter ; err != nil { // (5) fmt . Println ( \"Error during iteration:\" , err ) } This will execute the search and return the first page of results. The ApproximateCount property gives the total number of results overall (not restricted by page). When working with paginated search results, the .Iter() method\nprovides a convenient way to iterate over all results without manually handling pagination. You can then directly iterate through the search results.\nThe SDK handles pagination for you, fetching each page lazily as needed. If an error occurs during iteration (e.g: a failed API request),\nit is sent to the errIter channel. The iteration stops, and you can handle the error accordingly. Implicit in the previous step Actually running the search is implicit in the example above for the previous raw API step. Iterate through results Â¶ One page of results Â¶ 0.1.0 1.0.0 1.1.0 To iterate through one page of results, loop through the list of assets: Java Python Kotlin Go Raw REST API Iterate through one page of results 11 12 13 14 15 16 List < Asset > results = response . getAssets (); // (1) for ( Asset result : results ) { // (2) if ( result instanceof GlossaryTerm ) { GlossaryTerm term = ( GlossaryTerm ) result ; // (3) } } The page of results itself can be accessed through the getAssets() method on the response. You can then iterate through these results from a single page. Remember that each result is a generic Asset . In our example we searched for a specific type, but another example may search for any asset with a given name (or Atlan tag) â€” so each result could be a different type. So again we should check and cast the results as-needed. Iterate through one page of results 18 19 20 for asset in results . current_page (): # (1) if isinstance ( asset , AtlasGlossaryTerm ): # (2) term = asset You can iterate through the results from a single page. Remember per the type hints each result is a generic Asset . In our example we searched for a specific type, but another example may search for any asset with a given name (or Atlan tags) - so each result could be a different type. So if we want to take allow an IDE to provide better code completion, we need include an if isinstance(asset, asset_type) where asset_type is the type of the asset we want the IDE to know about. Inside the IDE will know the object is of the specified type. It's also a good practice that will prevent run-time errors if an asset is not of the expected type. Iterate through one page of results 11 12 13 14 15 16 val results = response . assets // (1) for ( result in results ) { // (2) if ( result is GlossaryTerm ) { val term = result // (3) } } The page of results itself can be accessed through the .assets member on the response. You can then iterate through these results from a single page. Remember that each result is a generic Asset . In our example we searched for a specific type, but another example may search for any asset with a given name (or Atlan tag) â€” so each result could be a different type. So again we should check and cast the results as-needed. Iterate through one page of results 14 15 16 17 18 19 20 current , _ := results . CurrentPage () // (1) for _ , asset := range current . Entities { if * asset . TypeName == \"AtlasGlossaryTerm\" { // (2) // Do something with AtlasGlossaryTerm } } You can iterate through the results from a single page. You can filter the asset based on the typename and perform operations on the asset. Each object in entities is a matching asset Each item in the entities array of the response will give details about a matching asset. Multiple pages of results Â¶ 0.0.17 1.0.0 1.1.0 To iterate through multiple pages of results: Java Python Kotlin Go Raw REST API Iterate through multiple pages of results 11 12 13 for ( Asset result : response ) { // (1) // Do something with each result in the page of results... } You can simply iterate over the reponse itself. This will lazily load and loop through each page of results until the loop finishes or you break out of it. (You could also use response.forEach() , which uses the same iteratable-based implementation behind-the-scenes.) Iterate through multiple pages of results (streaming) 11 12 13 14 response . stream () // (1) . limit ( 100 ) // (2) . filter ( a -> ! ( a instanceof ILineageProcess )) // (3) . forEach ( a -> log . info ( \"Do something with each result: {}\" , a )); // (4) Alternatively, you can also stream the results direct from the response. This will also lazily load and loop through each page of results. Can be chained without creating a request in-between You can actually chain the stream() method directly onto the end of your query and request construction, without creating a request or response object in-between. With streaming, you can apply your own limits to the maximum number of results you want to process. Independent of page size Note that this is independent of page size. You could page through results 50 at a time, but only process a maximum of 100 total results this way. Since the results are lazily-loaded when streaming, only the first two pages of results would be retrieved in such a scenario. You can also apply your own logical filters to the results. Push-down as much as you can to the query You should of course push-down as many of the filters as you can to the query itself, but if you have a particular complex check to apply that cannot be encoded in the query this can be a useful secondary filter over the results. The forEach() on the resulting stream will then apply whatever actions you want with the results that come through. Iterate through multiple pages of results one page at a time 18 19 20 21 22 23 while results . current_page (): # (1) for asset in results . current_page (): # (2) if isinstance ( asset , AtlasGlossaryTerm ): # (3) term = asset if not results . next_page (): # (4) break # (5) The current_page() method returns a list of the assets for the current page. If there are none then an empty list will be returned. Iterate through the assets in the current page. Remember per the type hints each result is a generic Asset . In our example we searched for a specific type, but another example may search for any asset with a given name (or classifications) - so each result could be a different type. So if we want to take allow an IDE to provide better code completion, we need include an if isinstance(asset, asset_type) where asset_type is the type of the asset we want the IDE to know about. Inside the IDE will know the object is of the specified type. It's also a good practice that will prevent run-time errors if an asset is not of the expected type. The next_pages() method retrieves the next page of results and return True if more assets are available and False if they are not. Break out of the While loop if no more assets are available. Alternatively iterate through all the pages of results 18 19 20 for asset in results : # (1) if isinstance ( asset , AtlasGlossaryTerm ): # (2) term = asset This will iterate through all the results without the need to be concerned with pages. Iterating over results produces a Generator This means that results are retrieved from the backend a page at time. This also means that you can only iterate over the results once. Remember that each result is a generic Asset . In our example we searched for a specific type, but another example may search for any asset with a given name (or classification) â€” so each result could be a different type. So again we should check and cast the results as-needed. Iterate through multiple pages of results 11 12 13 for ( result in response ) { // (1) // Do something with each result in the page of results... } You can simply iterate over the reponse itself. This will lazily load and loop through each page of results until the loop finishes or you break out of it. (You could also use response.forEach{ } , which uses the same iteratable-based implementation behind-the-scenes.) Iterate through multiple pages of results (streaming) 11 12 13 14 response . stream () // (1) . limit ( 100 ) // (2) . filter { it !is ILineageProcess } // (3) . forEach { log . info ( \"Do something with each result: {}\" , it ) } // (4) Alternatively, you can also stream the results direct from the response. This will also lazily load and loop through each page of results. Can be chained without creating a request in-between You can actually chain the stream() method directly onto the end of your query and request construction, without creating a request or response object in-between. With streaming, you can apply your own limits to the maximum number of results you want to process. Independent of page size Note that this is independent of page size. You could page through results 50 at a time, but only process a maximum of 100 total results this way. Since the results are lazily-loaded when streaming, only the first two pages of results would be retrieved in such a scenario. You can also apply your own logical filters to the results. Push-down as much as you can to the query You should of course push-down as many of the filters as you can to the query itself, but if you have a particular complex check to apply that cannot be encoded in the query this can be a useful secondary filter over the results. The forEach{ } on the resulting stream will then apply whatever actions you want with the results that come through. Iterate through multiple pages of results one page at a time 14 15 16 17 18 19 20 21 22 entities , errIter := results . Iter () // (1) for asset := range entities { // (2) fmt . Println ( \"asset name:\" , * asset . Name ) } if err := <- errIter ; err != nil { // (3) fmt . Println ( \"Error during iteration:\" , err ) } When working with paginated search results, the .Iter() method\nprovides a convenient way to iterate over all results without manually handling pagination. You can then directly iterate through the search results.\nThe SDK handles pagination for you, fetching each page lazily as needed. If an error occurs during iteration (e.g: a failed API request),\nit is sent to the errIter channel. The iteration stops, and you can handle the error accordingly. Use the searchParameters.query of the response Each search response includes a searchParameters with a nested query string. This query string gives the details of the query that was run to produce the response â€” so to get a next page you can: Use this query string from the response to start building a new query using the same logic. Add the page size to the from parameter embedded in that query string, to give the starting point for the next page of results. Re-include any attributes or relationAttributes from the query string into the new query. Send this new query to retrieve the next page of results. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/announcements/",
    "content": "/api/meta/entity/bulk (POST) Manage announcements Â¶ Add to an existing asset Â¶ 1.4.0 4.0.0 To add an announcement to an existing asset : dbt Java Python Kotlin Raw REST API Add announcement to existing assets 1 2 3 4 5 6 7 8 9 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : attributes : # (2) announcementType : warning # (3) announcementTitle : \"Caution!\" # (4) announcementMessage : >- # (5) This table was changed. You must of course give the name of the object. The details for the announcement must be nested within the meta . atlan . attributes structure. You must provide a valid status for the announcement ( information , warning or issue ). (Optional) You can also provide a title for the announcement. (Optional) You can also provide a message to include in the announcement. Add announcement to existing assets 1 2 3 4 5 6 Table result = Table . updateAnnouncement ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) AtlanAnnouncementType . WARNING , // (4) \"Caution!\" , // (5) \"This table was changed.\" ); // (6) Use the updateAnnouncement() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the update operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the object. The type of the announcement (the AtlanAnnouncementType enumeration gives the valid values). A title for the announcement. A message to include in the announcement. Add announcement to existing assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.core import Announcement from pyatlan.model.enums import AnnouncementType client = AtlanClient () announcement = Announcement ( # (1) announcement_title = \"Caution\" , announcement_message = \"This table was changed.\" , announcement_type = AnnouncementType . WARNING , # (2) ) table = client . asset . update_announcement ( # (3) asset_type = Table , # (4) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , announcement = announcement , ) Create the announcement. The type of the announcement (the AnnouncementType enum gives the valid values). Use the update_announcment() method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the update operation all-in-one. Specify the type of asset to be updated. Add announcement to existing assets 1 2 3 4 5 6 val result = Table . updateAnnouncement ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) AtlanAnnouncementType . WARNING , // (4) \"Caution!\" , // (5) \"This table was changed.\" ) // (6) Use the updateAnnouncement() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the update operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the object. The type of the announcement (the AtlanAnnouncementType enumeration gives the valid values). A title for the announcement. A message to include in the announcement. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"announcementType\" : \"warning\" , // (5) \"announcementTitle\" : \"Caution!\" , // (6) \"announcementMessage\" : \"This table was changed.\" } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must provide a valid type for the announcement. (Optional) You can also provide a title and message for the announcement. Remove from an existing asset Â¶ 1.4.0 4.0.0 To remove an announcement from an existing asset: dbt Java Python Kotlin Raw REST API It is currently not possible to remove an announcement from an asset via dbt. Remove announcement from existing asset 1 2 3 4 Column column = Column . removeAnnouncement ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS/USER_ID\" , // (3) \"USER_ID\" ); // (4) Use the removeAnnouncement() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the column (this is generally needed on all assets). The name of the column (this varies by asset, but most assets need the name specified). Remove announcement from existing asset 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = client . asset . remove_announcement ( # (1) asset_type = Table , # (2) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS/USER_ID\" , name = \"USER_ID\" , ) Use the asset.remove_announcement() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. Specify the type of asset from which to remove the announcement. Remove announcement from existing asset 1 2 3 4 val column = Column . removeAnnouncement ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS/USER_ID\" , // (3) \"USER_ID\" ) // (4) Use the removeAnnouncement() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the column (this is generally needed on all assets). The name of the column (this varies by asset, but most assets need the name specified). POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"announcementType\" : null , // (5) \"announcementTitle\" : null , \"announcementMessage\" : null } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must provide the value null for the type of the announcement, its title, and its message. When creating an asset Â¶ 2.0.0 4.0.0 To add an announcement when creating an asset: dbt Java Python Kotlin Raw REST API Add announcement when creating asset 1 2 3 4 5 6 7 8 9 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : attributes : # (2) announcementType : information # (3) announcementTitle : \"New!\" # (4) announcementMessage : >- # (5) This table is newly created. You must of course give the name of the object. The details for the announcement must be nested within the meta . atlan . attributes structure. You must provide a valid status for the announcement ( information , warning or issue ). (Optional) You can also provide a title for the announcement. (Optional) You can also provide a message to include in the announcement. Add announcement when creating asset 1 2 3 4 5 6 7 8 9 Table table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . announcementType ( AtlanAnnouncementType . INFORMATION ) // (2) . announcementTitle ( \"New!\" ) // (3) . announcementMessage ( \"This table is newly created.\" ) // (4) . build (); // (5) AssetMutationResponse response = table . save ( client ); // (6) assert response . getCreatedAssets (). size () == 1 // (7) Use the creator() method to initialize the object with all necessary attributes for creating it . Set the announcement that should be added (in this example, we're using INFORMATION ). Add a title for the announcement. Add a message for the announcement. Call the build() method to build the enriched object. Call the save() method to actually create the asset with this announcement. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created. Add announcement when creating asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.core import Announcement from pyatlan.model.enums import AnnouncementType from pyatlan.model.response import AssetMutationResponse client = AtlanClient () table = Table . creator ( # (1) name = \"TOP_BEVERAGE_USERS\" , schema_qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" , ) announcement = Announcement ( # (2) announcement_title = \"New!\" , announcement_message = \"This table is newly created.\" , announcement_type = AnnouncementType . INFORMATION , ) table . set_announcement ( announcement = announcement ) response = client . asset . save ( table ) # (3) assert response . assets_created ( Table ) # (4) table = response . assets_created ( Table )[ 0 ] # (5) Use the create() method to initialize the object with all [necessary attributes for creating it. Create the Announcement to be used. Invoke the save() method with asset. This method will return an AssetMutationResponse object that encapsulates the results. Since a save can add, update, delete or partially update multiple assets the assets_created() method can be used to return a list of the assets of the specified type that were added. The assert statement is present to ensure a Table asset was created. Since only one Table should have been created we use an index of 0 to retrieve the newly created table. Add announcement when creating asset 1 2 3 4 5 6 7 8 9 val table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . announcementType ( AtlanAnnouncementType . INFORMATION ) // (2) . announcementTitle ( \"New!\" ) // (3) . announcementMessage ( \"This table is newly created.\" ) // (4) . build () // (5) val response = table . save ( client ) // (6) assert ( response . createdAssets . size == 1 ) // (7) Use the creator() method to initialize the object with all necessary attributes for creating it . Set the announcement that should be added (in this example, we're using INFORMATION ). Add a title for the announcement. Add a message for the announcement. Call the build() method to build the enriched object. Call the save() method to actually create the asset with this announcement. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"atlanSchema\" : { // (5) \"typeName\" : \"Schema\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" } }, \"announcementType\" : \"information\" , // (6) \"announcementTitle\" : \"New!\" , // (7) \"announcementMessage\" : \"This table is newly created.\" } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide a name for the asset. In the case of a table, the qualifiedName must be the concatenation of the parent schema's qualifiedName and the name of the table. When creating a table, you must specify the schema to create it within. This is defined by the atlanSchema attribute. You must specify both the type (must be Schema ) and qualifiedName of the schema within the atlanSchema attribute â€” and the schema must already exist. You must provide a valid status for the announcement. (Optional) You can also provide a title and message for the announcement. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/owners/",
    "content": "/api/meta/entity/bulk (POST) Change owners Â¶ There are actually two kinds of owners per asset There are actually two fields in Atlan that capture the owners of an asset: ownerUsers and ownerGroups . The examples below illustrate how to change individual (user) owners. To change group owners, replace ownerUsers with ownerGroups . Change an existing asset Â¶ Could create a new asset Remember that Atlan matches the provided qualifiedName to determine whether to update or create the asset . 2.0.0 4.0.0 To change owners on an existing asset : dbt Java Python Kotlin Raw REST API Change owners on existing asset 1 2 3 4 5 6 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : attributes : # (2) ownerUsers : [ \"jsmith\" , \"jdoe\" ] # (3) You must of course give the name of the object. The usernames must be nested within the meta . atlan . attributes structure. You must provide valid usernames, or email addresses, as a list. Users must be valid If the user does not exist in Atlan, there will be no updates to the asset. Please verify the usernames or email addresses in Atlan before assigning them to assets. Change owners on existing asset 1 2 3 4 5 6 7 Table table = Table . updater ( // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (2) \"TOP_BEVERAGE_USERS\" ) // (3) . ownerUsers ( List . of ( \"jsmith\" , \"jdoe\" )) // (4) . build (); // (5) AssetMutationResponse response = table . save ( client ); // (6) assert response . getUpdatedAssets (). size () == 1 // (7) Use the updater() helper method to create the minimal object necessary to do an update. The qualifiedName of the object. The name of the object. Provide the new owners. Note that this is a list of the usernames of the users. Build the updater into an object. Send the update to Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was updated. Change owners on existing asset 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) table . owner_users = [ \"jsmith\" , \"jdoe\" ] # (2) response = client . asset . save ( table ) # (3) assert 1 == len ( response . assets_updated ( asset_type = Table )) # (4) Use the updater() method to create an asset suitable for modification i.e. with all the requisite attributes. Provide the new owners. Note that this is a list of the usernames of the users. Send the update to Atlan. The response should only include that single asset that was updated. Change owners on existing asset 1 2 3 4 5 6 7 val table = Table . updater ( // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (2) \"TOP_BEVERAGE_USERS\" ) // (3) . ownerUsers ( listOf ( \"jsmith\" , \"jdoe\" )) // (4) . build () // (5) val response = table . save ( client ) // (6) assert ( response . updatedAssets . size == 1 ) // (7) Use the updater() helper method to create the minimal object necessary to do an update. The qualifiedName of the object. The name of the object. Provide the new owners. Note that this is a list of the usernames of the users. Build the updater into an object. Send the update to Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was updated. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"ownerUsers\" : [ \"jsmith\" , \"jdoe\" ] // (5) } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). Provide the new owners, as a list of usernames of users. Remove from an existing asset Â¶ 2.0.0 4.0.0 To remove owners from an existing asset: dbt Java Python Kotlin Raw REST API Remove owners from existing asset 1 2 3 4 5 6 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : attributes : # (2) ownerUsers : [ \"jdoe\" ] # (3) You must of course give the name of the object. The details for the owners must be nested within the meta . atlan . attributes structure. Specify only the usernames or email addresses of the users you want to keep as owners. (Compared to the other examples, this would remove jsmith and keep jdoe .) Users must be valid If the user does not exist in Atlan, there will be no updates to the asset. Please verify the usernames or email addresses in Atlan before assigning them to assets. Remove owners from existing asset 1 2 3 4 Table table = Table . removeOwners ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) \"TOP_BEVERAGE_USERS\" ); // (4) Use the removeOwners() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the object. The name of the object. Remove owners from existing asset 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) table . owner_users = None # (2) response = client . asset . save ( table ) # (3) assert 1 == len ( response . assets_updated ( asset_type = Table )) # (4) Use the updater() method to create an asset suitable for modification i.e. with all the requisite attributes. Set the owners to None . Send the update to Atlan. The response should only include that single asset that was updated (again, removing owners is an update to the asset â€” we are not deleting the asset itself). Remove owners from existing asset 1 2 3 4 val table = Table . removeOwners ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) \"TOP_BEVERAGE_USERS\" ) // (4) Use the removeOwners() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the object. The name of the object. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"ownerUsers\" : [], // (5) \"ownerGroups\" : [] } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must set the ownerUsers and ownerGroups to an empty list. When creating an asset Â¶ 2.0.0 4.0.0 To add owners when creating an asset: dbt Java Python Kotlin Raw REST API Add owners when creating asset 1 2 3 4 5 6 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : attributes : # (2) ownerUsers : [ \"jsmith\" , \"jdoe\" ] # (3) You must of course give the name of the object. The usernames must be nested within the meta . atlan . attributes structure. You must provide valid usernames, or email addresses, as a list. Users must be valid If the user does not exist in Atlan, there will be no updates to the asset. Please verify the usernames or email addresses in Atlan before assigning them to assets. Add owners when creating asset 1 2 3 4 5 6 7 Table table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . ownerUsers ( List . of ( \"jsmith\" , \"jdoe\" )) // (2) . build (); // (3) AssetMutationResponse response = table . save ( client ); // (4) assert response . getCreatedAssets (). size () == 1 // (5) Use the creator() method to initialize the object with all necessary attributes for creating it](../advanced-examples/create.md#build-minimal-object-needed). Set the owners that should be added. Note that this is a list of the usernames of the users. Call the build() method to build the enriched object. Call the save() method to actually create the asset with these owners. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created. Add owners when creating asset 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = Table . creator ( # (1) name = \"TOP_BEVERAGE_USERS\" , schema_qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" , ) table . owner_users = [ \"jsmith\" , \"jdoe\" ] # (2) response = client . asset . save ( table ) # (3) assert 1 == len ( assets_created := response . assets_created ( asset_type = Table )) # (4) table = assets_created [ 0 ] # (5) Use the creator() method to initialize the object with all necessary attributes for creating it. Set the owners that should be added. Note that this is a list of the usernames of the users. Call the save() method to actually create the asset with these owners. Since a save can add, update, delete or partially update multiple assets the assets_created() method can be used to return a list of the assets of the specified type that were added. The assert statement is present to ensure a Table asset was created. Since only one Table has been created we use an index of 0 to retrieve the newly created table. Add owners when creating asset 1 2 3 4 5 6 7 val table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . ownerUsers ( listOf ( \"jsmith\" , \"jdoe\" )) // (2) . build () // (3) val response = table . save ( client ) // (4) assert ( response . createdAssets . size == 1 ) // (5) Use the creator() method to initialize the object with all necessary attributes for creating it](../advanced-examples/create.md#build-minimal-object-needed). Set the owners that should be added. Note that this is a list of the usernames of the users. Call the build() method to build the enriched object. Call the save() method to actually create the asset with these owners. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"atlanSchema\" : { // (5) \"typeName\" : \"Schema\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" } }, \"ownerUsers\" : [ \"jsmith\" , \"jdoe\" ] // (6) } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide a name for the asset. In the case of a table, the qualifiedName must be the concatenation of the parent schema's qualifiedName and the name of the table. When creating a table, you must specify the schema to create it within. This is defined by the atlanSchema attribute. You must specify both the type (must be Schema ) and qualifiedName of the schema within the atlanSchema attribute â€” and the schema must already exist. Provide the owners, as a list of usernames of users. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/certificates/",
    "content": "/api/meta/entity/bulk (POST) Certify assets Â¶ Add to an existing asset Â¶ 1.4.0 4.0.0 To add a certificate to an existing asset : dbt Java Python Kotlin Raw REST API Add certificate to existing assets 1 2 3 4 5 6 7 8 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : attributes : # (2) certificateStatus : VERIFIED # (3) certificateStatusMessage : >- # (4) Verified through automation. You must of course give the name of the object. The details for the certificate must be nested within the meta . atlan . attributes structure. You must provide a valid status for the certificate ( DRAFT , VERIFIED or DEPRECATED ). (Optional) You can also provide a message to associate with the certificate. Add certificate to existing assets 1 2 3 4 5 Table result = Table . updateCertificate ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) CertificateStatus . VERIFIED , // (4) \"Verified through automation.\" ); // (5) Use the updateCertificate() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the update operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the object. The type of certificate (the CertificateStatus enumeration gives the valid values). (Optional) A message to include in the certificate. Add certificate to existing assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.enums import CertificateStatus client = AtlanClient () table = client . asset . update_certificate ( # (1) asset_type = Table , qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , certificate_status = CertificateStatus . VERIFIED , message = \"Verified through automation.\" , ) if table is None : # (2) print ( \"Certificate status did not change\" ) else : # (3) print ( \"Certificate status updated\" ) Use the asset.update_certificate() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the update operation all-in-one. If no change occurs to the asset then None will be returned. If the asset is updated then the asset will be returned. Add certificate to existing assets 1 2 3 4 5 val result = Table . updateCertificate ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) CertificateStatus . VERIFIED , // (4) \"Verified through automation.\" ) // (5) Use the updateCertificate() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the update operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the object. The type of certificate (the CertificateStatus enumeration gives the valid values). (Optional) A message to include in the certificate. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"certificateStatus\" : \"VERIFIED\" , // (5) \"certificateStatusMessage\" : \"Verified through automation.\" // (6) } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must provide a valid status for the certificate. (Optional) You can also provide a status message for the certificate. Remove from an existing asset Â¶ 1.4.0 4.0.0 To remove a certificate from an existing asset: dbt Java Python Kotlin Raw REST API It is currently not possible to remove a certificate from an asset via dbt. Remove certificate from existing asset 1 2 3 4 Column column = Column . removeCertificate ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS/USER_ID\" , // (3) \"USER_ID\" ); // (4) Use the removeCertificate() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the column (this is generally needed on all assets). The name of the column (this varies by asset, but most assets need the name specified). Remove certificate from existing asset 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Column client = AtlanClient () column = client . asset . remove_certificate ( # (1) asset_type = Column , qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS/USER_ID\" , name = \"USER_ID\" , ) if column is None : # (2) print ( \"Certificate was not present\" ) else : # (3) print ( \"Certificate was removed\" ) Use the asset.remove_certificate() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. If no change occurs to the asset because the certificate is not present then None will be returned. If certificate is removed from the asset then the asset will be returned. Remove certificate from existing asset 1 2 3 4 val column = Column . removeCertificate ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS/USER_ID\" , // (3) \"USER_ID\" ) // (4) Use the removeCertificate() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the column (this is generally needed on all assets). The name of the column (this varies by asset, but most assets need the name specified). POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"USER_ID\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS/USER_ID\" , // (4) \"certificateStatus\" : null , // (5) \"certificateStatusMessage\" : null } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). To remove the certificate, set its status and message to null . When creating an asset Â¶ 2.0.0 4.0.0 To add a certificate when creating an asset: dbt Java Python Kotlin Raw REST API Add certificate when creating asset 1 2 3 4 5 6 7 8 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : attributes : # (2) certificateStatus : VERIFIED # (3) certificateStatusMessage : >- # (4) Verified at creation. You must of course give the name of the object. The details for the certificate must be nested within the meta . atlan . attributes structure. You must provide a valid status for the certificate ( DRAFT , VERIFIED or DEPRECATED ). (Optional) You can also provide a message to associate with the certificate. Add certificate when creating asset 1 2 3 4 5 6 7 8 Table table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . certificateStatus ( CertificateStatus . VERIFIED ) // (2) . certificateStatusMessage ( \"Verified at creation.\" ) // (3) . build (); // (4) AssetMutationResponse response = table . save ( client ); // (5) assert response . getCreatedAssets (). size () == 1 // (6) Use the creator() method to initialize the object with all necessary attributes for creating it . Set the certificate that should be added (in this example, we're using VERIFIED ). (Optional) Add a message for the certificate. Call the build() method to build the enriched object. Call the save() method to actually create the asset with this certificate. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created. Add certificate when creating asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.enums import CertificateStatus client = AtlanClient () table = Table . creator ( # (1) name = \"TOP_BEVERAGE_USERS\" , schema_qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" , ) table . certificate_status = CertificateStatus . VERIFIED # (2) table . certificate_status_message = \"Verified at creation.\" # (3) response = client . asset . save ( table ) # (4) assert response . assets_created ( Table ) # (5) table = response . assets_created ( Table )[ 0 ] # (6) Use the create() method to initialize the object with all necessary attributes for creating it. Set the certificate that should be added (in this example, we're using VERIFIED ). (Optional) Add a message for the certificate. Invoke the save() method with asset. This method will return an AssetMutationResponse object that encapsulates the results. Since a save can add, update, delete or partially update multiple assets the assets_created() method can be used to return a list of the assets of the specified type that were added. The assert statement is present to ensure a Table asset was created. Since only one Table should have been created we use an index of 0 to retrieve the newly created table. Add certificate when creating asset 1 2 3 4 5 6 7 8 val table : Table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . certificateStatus ( CertificateStatus . VERIFIED ) // (2) . certificateStatusMessage ( \"Verified at creation.\" ) // (3) . build () // (4) val response = table . save ( client ) // (5) assert ( response . createdAssets . size == 1 ) // (6) Use the creator() method to initialize the object with all necessary attributes for creating it . Set the certificate that should be added (in this example, we're using VERIFIED ). (Optional) Add a message for the certificate. Call the build() method to build the enriched object. Call the save() method to actually create the asset with this certificate. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"atlanSchema\" : { // (5) \"typeName\" : \"Schema\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" } }, \"certificateStatus\" : \"VERIFIED\" , // (6) \"certificateStatusMessage\" : \"Verified at creation.\" // (7) } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide a name for the asset. In the case of a table, the qualifiedName must be the concatenation of the parent schema's qualifiedName and the name of the table. When creating a table, you must specify the schema to create it within. This is defined by the atlanSchema attribute. You must specify both the type (must be Schema ) and qualifiedName of the schema within the atlanSchema attribute â€” and the schema must already exist. You must provide a valid status for the certificate. (Optional) You can also provide a status message for the certificate. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/",
    "content": "Examples of common actions with assets Â¶ In this section you'll find examples of the most common actions dealing with assets . All assets share a common set of attributes and relationships: such as certificates, announcements, term assignments, classifications, custom metadata, READMEs, and more. These common examples cover interacting with those common attributes and relationships â€” focusing primarily on enriching assets that already exist. Each page includes variations on the topic they cover: Certify assets Manage announcements Change description Change owners Classify assets Change custom metadata Link terms to assets Manage asset READMEs Add asset links 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/domain-assignment/",
    "content": "/api/meta/entity/bulk (POST) Link domain and assets Â¶ Link your asset to a domain for easy discovery and governance. Add an asset to a domain Â¶ 3.0.0 4.0.0 You can add an asset to a domain or update an existing domain by updating the asset's domainGUIDs .\nIn the example below, we're adding a Table ( MARKETING_SALES ) to the domain ( Marketing ). dbt Java Python Kotlin Raw REST API Add an asset to a domain 1 2 3 4 5 models : - name : MARKETING_SALES # (1) meta : atlan : domainName : \"Marketing\" # (2) You must give the name of the object. You can specify the domain as a human-readable name. Each asset can be assigned to only one domain. You can also replace an existing domain assignment by updating the domainName to a different domain. Add an asset to a domain 1 2 3 4 5 6 7 8 9 DataDomain domain = DataDomain . findByName ( client , \"Marketing\" ). get ( 0 ); // (1) Table table = Table . updater ( // (2) \"default/snowflake/1726834662/RAW/WIDEWORLDIMPORTERS/MARKETING_SALES\" , \"MARKETING_SALES\" ) . domainGUID ( domain . getGuid ()) // (3) . build (); AssetMutationResponse response = table . save ( client ); // (4) You can retrieve a data domain by its human-readable name using the findByName() method. Because this operation will look up the domain in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Use the updater() method of an asset by providing its qualifiedName and name . To add the asset to the domain, assign the guid of the domain to the domainGUID attribute. Finally, call the save() method to apply the changes in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add an asset to a domain 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table , client = AtlanClient () domain = client . asset . find_domain_by_name ( \"Marketing\" ) # (1) table = Table . updater ( # (2) qualified_name = \"default/snowflake/1726834662/RAW/WIDEWORLDIMPORTERS/MARKETING_SALES\" , name = \"MARKETING_SALES\" , ) table . domain_g_u_i_ds = [ domain . guid ] # (3) client . asset . save ( table ) # (4) You can retrieve a data domain by its human-readable name using the find_domain_by_name() method. Use the updater() method of an asset by providing its qualifiedName and name . To add the asset to the domain, assign the guid of the domain to the asset.domain_g_u_i_ds attribute. Finally, call the save() method to apply the changes in Atlan. Add an asset to a domain 1 2 3 4 5 6 7 8 9 val domain = DataDomain . findByName ( client , \"Marketing\" ) [ 0 ] // (1) val table = Table . updater ( // (2) \"default/snowflake/1726834662/RAW/WIDEWORLDIMPORTERS/MARKETING_SALES\" , \"MARKETING_SALES\" ) . domainGUID ( domain . getGuid ()) // (3) . build () val response = table . save ( client ) // (4) You can retrieve a data domain by its human-readable name using the findByName() method. Because this operation will look up the domain in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Use the updater() method of an asset by providing its qualifiedName and name . To add the asset to the domain, assign the guid of the domain to the asset.domainGUID attribute. Finally, call the save() method to apply the changes in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"entities\" : [ { \"typeName\" : \"Table\" , // (1) \"attributes\" : { \"qualifiedName\" : \"default/snowflake/1726834662/RAW/WIDEWORLDIMPORTERS/MARKETING_SALES\" , // (2) \"name\" : \"MARKETING_SALES\" , // (3) \"domainGUIDs\" : [ \"db711647-99a9-4c50-93c5-fab0b992a0cc\" // (4) ] } } ] } You need to specify the typeName of the asset;\nfor this example, we're updating the domainGUIDs for a Snowflake table. You need to specify the qualifiedName of the asset. You need to specify the name of the asset. To add the asset to the domain, assign the guid of the domain to the domainGUIDs property. Retrieve Assets by Domain Â¶ 3.0.0 4.0.0 You can retrieve all assets associated with a domain by filtering on the domainGUIDs . \nIn the example below, we retrieve all assets linked to the ( Marketing ) domain. Java Python Kotlin Retrieve assets from a domain 1 2 3 4 5 6 7 8 9 10 11 12 13 14 String domainName = \"Marketing\" ; DataDomain domain = DataDomain . findByName ( client , domainName ). get ( 0 ); // (1) String domainGuid = domain . getGuid (); List < Asset > result = client . assets . select () . where ( Asset . DOMAIN_GUIDS . eq ( domainGuid )) // (2) . includeOnResults ( Asset . NAME ) . includeOnResults ( Asset . QUALIFIED_NAME ) // (3) . stream () . toList (); result . forEach ( asset -> { // (4) System . out . println ( \"Asset Name: \" + asset . getName ()); System . out . println ( \"Qualified Name: \" + asset . getQualifiedName ()); }); You can retrieve a data domain by its human-readable name using the findByName() method. Because this  operation will look up the domain in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Query all assets linked to the domain using client.assets.select().where(Asset.DOMAIN_GUIDS.eq(domainGuid)). Include specific attributes (e.g., Asset.NAME, Asset.QUALIFIED_NAME) in the results using .includeOnResults(). Process the retrieved assets and print any specific attributes you need, such as name and qualifiedName in this example. Retrieve assets from a domain 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Asset , Readme from pyatlan.model.fluent_search import CompoundQuery , FluentSearch client = AtlanClient () domain_name = \"Marketing\" domain = client . asset . find_domain_by_name ( domain_name ) # (1) domain_guid = domain . guid response = ( # (2) FluentSearch () . select () . where ( Asset . DOMAIN_GUIDS . eq ( domain_guid )) . include_on_results ( Asset . NAME ) # (3) . include_on_results ( Asset . QUALIFIED_NAME ) . execute ( client = client ) ) for asset in response : # (4) print ( f \"Name: { asset . name } , Qualified Name: { asset . qualified_name } \" ) You can retrieve a data domain by its human-readable name using the find_domain_by_name() method. Query all assets linked to the domain using FluentSearch.select().where(Asset.DOMAIN_GUIDS.eq(domain_guid)). Include specific attributes (e.g., Asset.NAME, Asset.QUALIFIED_NAME) in the results using .include_on_results(). Process the retrieved assets and print any specific attributes you need, such as name and qualifiedName in this example. Retrieve assets from a domain 1 2 3 4 5 6 7 8 9 10 11 12 13 14 val domainName = \"Marketing\" val domain = DataDomain . findByName ( client , domainName ) [ 0 ] // (1) val domainGuid = domain . guid val result = client . assets //(2) . select () . where ( Asset . DOMAIN_GUIDS . eq ( domainGuid )) . includeOnResults ( Asset . NAME ) // (3) . includeOnResults ( Asset . QUALIFIED_NAME ) . stream () . toList () for ( assets in result ) { //(4) println ( \"Asset Name:\" + assets . name ) println ( \"Asset Qualified Name:\" + assets . qualifiedName ) } You can retrieve a data domain by its human-readable name using the findByName() method. Because this operation will look up the domain in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Query all assets linked to the domain using FluentSearch.search().where(Asset.DOMAIN_GUIDS.eq(domainGuid)). Include specific attributes (e.g., Asset.NAME, Asset.QUALIFIED_NAME) in the results using .includeOnResults(). Process the retrieved assets and print any specific attributes you need, such as name and qualifiedName in this example. Remove an asset from a domain Â¶ 3.0.0 4.0.0 You can remove an asset from a domain by updating the asset's domainGUIDs .\nIn the example below, we're removing a table ( MARKETING_SALES ) asset from the existing linked domain. dbt Java Python Kotlin Raw REST API Remove an asset from a domain 1 2 3 4 5 models : - name : MARKETING_SALES # (1) meta : atlan : domainName : \"\" # (2) You must give the name of the object. To remove the asset from the domain, set the domainName to an empty string. Remove an asset from a domain 1 2 3 4 5 6 7 Table table = Table . updater ( // (1) \"default/snowflake/1726834662/RAW/WIDEWORLDIMPORTERS/MARKETING_SALES\" , \"MARKETING_SALES\" ) . nullField ( Table . DOMAIN_GUIDS . getAtlanFieldName ()) // (2) . build (); AssetMutationResponse response = table . save ( client ); // (3) Use the updater() method of an asset by providing its qualifiedName and name . To remove the asset from the domain, set the domainGUIDs field as a nullField on the builder. Finally, call the save() method to apply the changes in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Remove an asset from a domain 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table , client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1726834662/RAW/WIDEWORLDIMPORTERS/MARKETING_SALES\" , name = \"MARKETING_SALES\" , ) table . domain_g_u_i_ds = [] # (2) client . asset . save ( table ) # (3) Use the updater() method of an asset by providing its qualifiedName and name . To remove the asset from the domain, assign the domain_g_u_i_ds of the asset to an empty list ie. [] . Finally, call the save() method to apply the changes in Atlan. Remove an asset from a domain 1 2 3 4 5 6 7 val table = Table . updater ( // (1) \"default/snowflake/1726834662/RAW/WIDEWORLDIMPORTERS/MARKETING_SALES\" , \"MARKETING_SALES\" ) . nullField ( Table . DOMAIN_GUIDS . atlanFieldName ) // (2) . build () val response = table . save ( client ) // (3) Use the updater() method of an asset by providing its qualifiedName and name . To remove the asset from the domain, set the domainGUIDs field as a nullField on the builder. Finally, call the save() method to apply the changes in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ { \"typeName\" : \"Table\" , // (1) \"attributes\" : { \"qualifiedName\" : \"default/snowflake/1726834662/RAW/WIDEWORLDIMPORTERS/MARKETING_SALES\" , // (2) \"name\" : \"MARKETING_SALES\" , // (3) \"domainGUIDs\" : [] // (4) } } ] } You need to specify the typeName of the asset;\nfor this example, we're updating the domainGUIDs for a Snowflake table. You need to specify the qualifiedName of the asset. You need to specify the name of the asset. To remove the asset from the domain, assign the domainGUIDs property of the asset to the empty array. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/custom-metadata/",
    "content": "/api/meta/entity/bulk (POST) /api/meta/entity/guid/{guid}/businessmetadata (POST) /api/meta/entity/guid/{guid}/businessmetadata/{name} (POST) Change custom metadata on assets Â¶ Structures must exist before setting values on assets Remember that you must first create the custom metadata structures before you will be able to set or change any values for custom metadata on an asset. Value representation Â¶ To update custom metadata values on an asset, you need to use the correct representation: Property type Value representation Example Text String \"a value\" Integer Number: without decimals 42 Decimal Number: with decimals 42.0 Boolean Boolean: either true or false true Date Number: milliseconds since epoch (January 1, 1970), to a specific day 1681171200000 Options String: exact value from the options list \"success\" Users String: username of the user \"jsmith\" Groups String: unique name of the group, which appears under the name of the group in the UI list of groups \"finance\" URL String: starting with http[s]:// https://www.google.com SQL String \"SELECT *\\nFROM somewhere;\" Multi-value attributes Custom attributes in Atlan can be configured to allow multiple values. For these, you must wrap all values in a multi-valued collection, even if there is only a single value you are setting . Each value in that collection needs to follow the appropriate representation as indicated in the table above. For example, if you want to set just a single group in a field that allows multiple values: dbt Java Python Kotlin Raw REST API Multi-valued attribute 1 2 3 4 5 6 7 8 models : - name : TOP_BEVERAGE_USERS meta : atlan : businessAttributes : MNJ8mpLsIOaP4OQnLNhRta : F8XI9GzcBpdBdfi4cLiPEz : - \"finance\" # (1) You must provide the value(s) as a list in YAML: each value on a new line, indented below the attribute, and prefixed with - . Multi-valued attribute 1 2 3 CustomMetadataAttributes cmRACI = CustomMetadataAttributes . builder () . attribute ( \"Consulted\" , List . of ( \"finance\" )) // (1) . build (); You must provide the value(s) as a collection (List, Set, etc). Multi-valued attribute 1 2 3 4 from pyatlan.model.custom_metadata import CustomMetadataDict cm_RACI = CustomMetadataDict ( client = client , name = \"RACI\" ) # (1) cm_RACI [ \"Consulted\" ] = [ \"finance\" ] # (2) Provide the client instance and name of the custom metadata set. Name will be validated The name will be validated at runtime to ensure that a custom metadata set with the given name exists. For any property that can be multi-valued, we need to send a list of values. Name will be validated The metadata property name will be validated at runtime to ensure that a property with the given name exists in the custom metadata set. Multi-valued attribute 1 2 3 val cmRACI = CustomMetadataAttributes . builder () . attribute ( \"Consulted\" , listOf ( \"finance\" )) // (1) . build () You must provide the value(s) as a collection (List, Set, etc). POST /api/meta/entity/guid/a89ff15b-f5e6-48bc-870b-acfa11e212ae/businessmetadata/MNJ8mpLsIOaP4OQnLNhRta 1 2 3 4 5 6 7 { \"MNJ8mpLsIOaP4OQnLNhRta\" : { \"F8XI9GzcBpdBdfi4cLiPEz\" : [ // (1) \"finance\" ] } } You must provide the value(s) in a JSON-style array. Add to existing assets Â¶ Update only some custom metadata attributes Â¶ 7.0.0 4.0.0 To update only some custom metadata attributes (leaving all others unchanged): dbt Java Python Kotlin Raw REST API This is currently not possible via dbt, custom metadata is replaced rather than selectively updated. Update only some custom metadata attributes 1 2 3 4 5 6 7 8 9 CustomMetadataAttributes cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Responsible\" , \"jsmith\" ) // (2) . attribute ( \"Consulted\" , List . of ( \"finance\" , \"risk\" )) // (3) . build (); Table . updateCustomMetadataAttributes ( // (4) client , // (5) \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (6) \"RACI\" , // (7) cmRACI ); // (8) Create a custom metadata attributes object that will contain only the attributes and values for custom metadata that you want to update on the asset . All other custom metadata attributes (those not specified in this object) will remain unchanged on the asset. For each attribute, use the attribute() method and pass: the name of the attribute within that set the value for that attribute The value can be any object valid for the attribute: a string, a boolean, or a number. (Note that dates are sent as long (epoch) numbers.) For any attribute that can be multi-valued, we can send a list of values. Use the updateCustomMetadataAttributes() method to update only the Responsible and Consulted attributes in the RACI custom metadata on the existing asset. Any other custom metadata attributes in RACI and all other custom metadata will be left unchanged. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Note that for this operation you must know the GUID of the asset you want to apply the custom metadata to. Also, the operation returns no result: if there is an error it will throw an exception, but the result of the operation must be determined by retrieving the asset through a separate API call, if you want to confirm it. Provide the name for the custom metadata you want to update. Provide the custom metadata attributes object with the attributes and values you want to update for that custom metadata. Update only some custom metadata attributes 1 2 3 4 5 6 7 8 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" , name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" , ) Use the updater() method to create an asset suitable for modifiaction i.e. with all the requisite attributes. Update the custom metadata on the table 10 11 12 cm_raci = table . get_custom_metadata ( client = client , name = \"RACI\" ) # (1) cm_raci [ \"Responsible\" ] = \"jsmith\" # (2) cm_raci [ \"Consulted\" ] = [ \"finance\" , \"risk\" ] # (3) Get the custom metadata set from the table via the get_custom_metadata method by specifying the client and name of the custom metadata set. Name will be validated The name will be validated at runtime to ensure that a custom metadata set with the given name exists. For each property of the metadata set you wish to update specify the name of the property. Name will be validated The metadata property name will be validated at runtime to ensure that a property with the given name exists in the custom metadata set. For any attribute that can be multi-valued, we need to send a list of values. Alternatively, create a custom metadata set and add it to the table 10 11 12 13 cm_raci = CustomMetadataDict ( client = client , name = \"RACI\" ) # (1) cm_raci [ \"Responsible\" ] = \"jsmith\" # (2) cm_raci [ \"Consulted\" ] = [ \"finance\" , \"risk\" ] # (3) table . set_custom_metadata ( client = client , custom_metadata = cm_raci ) # (4) Create an empty custom metadata set by specifying the client instance and name of an existing custom metadata set. Name will be validated The name will be validated at runtime to ensure that a custom metadata set with the given name exists. For each property of the metadata set you wish to update specify the name of the property. Name will be validated The metadata property name will be validated at runtime to ensure that a property with the given name exists in the custom metadata set. For any attribute that can be multi-valued, we need to send a list of values. Use the set_custom_metadata method to set the custom metadata set on the table. Update the model object on the server 14 15 16 17 response = client . asset . save_merging_cm ( # (1) table ) assert ( tables := response . assets_updated ( asset_type = Table )) # (2) Use the save_merging_cm() method to update the model object on the server. Assert that a Table asset has been updated. Update only some custom metadata attributes 1 2 3 4 5 6 7 8 9 val cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Responsible\" , \"jsmith\" ) // (2) . attribute ( \"Consulted\" , listOf ( \"finance\" , \"risk\" )) // (3) . build () Table . updateCustomMetadataAttributes ( // (4) client , // (5) \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (6) \"RACI\" , // (7) cmRACI ) // (8) Create a custom metadata attributes object that will contain only the attributes and values for custom metadata that you want to update on the asset . All other custom metadata attributes (those not specified in this object) will remain unchanged on the asset. For each attribute, use the attribute() method and pass: the name of the attribute within that set the value for that attribute The value can be any object valid for the attribute: a string, a boolean, or a number. (Note that dates are sent as long (epoch) numbers.) For any attribute that can be multi-valued, we can send a list of values. Use the updateCustomMetadataAttributes() method to update only the Responsible and Consulted attributes in the RACI custom metadata on the existing asset. Any other custom metadata attributes in RACI and all other custom metadata will be left unchanged. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Note that for this operation you must know the GUID of the asset you want to apply the custom metadata to. Also, the operation returns no result: if there is an error it will throw an exception, but the result of the operation must be determined by retrieving the asset through a separate API call, if you want to confirm it. Provide the name for the custom metadata you want to update. Provide the custom metadata attributes object with the attributes and values you want to update for that custom metadata. POST /api/meta/entity/guid/a89ff15b-f5e6-48bc-870b-acfa11e212ae/businessmetadata?isOverwrite=false 1 2 3 4 5 6 7 8 9 { // (1) \"MNJ8mpLsIOaP4OQnLNhRta\" : { // (2) \"fWMB77RSjRGNYoFeD4FcGi\" : \"jsmith\" , // (3) \"F8XI9GzcBpdBdfi4cLiPEz\" : [ // (4) \"finance\" , \"risk\" ] } } You must pass the GUID of the asset to change for this operation. There is no alternative that works with the qualifiedName. Each custom metadata set you want to add or update must be given using its hashed-string representation . Each custom metadata attribute you want to update must be given using its hashed-string representation . For multivalued custom metadata attributes, specify the value as an array. Replace some custom metadata on an asset Â¶ 7.0.0 4.0.0 You can also add/replace an entire set of custom metadata to existing assets . If you do this individually, you can selectively update individual sets of custom metadata (leaving any others unchanged): dbt Java Python Kotlin Raw REST API Replace some custom metadata on an asset 1 2 3 4 5 6 7 8 9 10 11 12 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : businessAttributes : # (2) MNJ8mpLsIOaP4OQnLNhRta : # (3) fWMB77RSjRGNYoFeD4FcGi : jsmith # (4) F8XI9GzcBpdBdfi4cLiPEz : [ \"finance\" , \"risk\" ] # (5) businessAttributeNames : # (6) RACI : Informed : - \"marketing\" You must of course give the name of the object. The custom metadata must be nested within the meta . atlan . businessAttributes structure. Each custom metadata set you want to add or update must be given using its hashed-string representation . Each custom metadata attribute you want to update must be given using its hashed-string representation . For multivalued custom metadata attributes, specify the value as an array. You can use displayNames instead of hashed-string representations by nesting custom metadata within meta . atlan . businessAttributeNames structure. Replace some custom metadata on an asset 1 2 3 4 5 6 7 8 9 CustomMetadataAttributes cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Responsible\" , \"jsmith\" ) // (2) . attribute ( \"Consulted\" , List . of ( \"finance\" , \"risk\" )) // (3) . build (); Table . replaceCustomMetadata ( // (4) client , // (5) \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (6) \"RACI\" , // (7) cmRACI ); // (8) Create a custom metadata attributes object that will contain the attributes and values for custom metadata you want to add to the asset. For each attribute, use the attribute() method and pass: the name of the attribute within that set the value for that attribute The value can be any object valid for the attribute: a string, a boolean, or a number. (Note that dates are sent as long (epoch) numbers.) For any attribute that can be multi-valued, we can send a list of values. Use the replaceCustomMetadata() method to replace only this named RACI custom metadata on the existing asset. Any other custom metadata will be left unchanged. Note that any attributes in RACI that are not included in the custom metadata attributes object we send will be removed from the custom metadata on that asset. (In our examples, this means any existing values in the Accountable and Informed attributes of RACI on this asset would be removed.) Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Note that for this operation you must know the GUID of the asset you want to apply the custom metadata to. Also, the operation returns no result: if there is an error it will throw an exception, but the result of the operation must be determined by retrieving the asset through a separate API call, if you want to confirm it. Provide the name for the custom metadata you want to add/replace. Provide the custom metadata attributes object with the attributes and values you want to be the complete set for that custom metadata. Replace some custom metadata on an asset 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.custom_metadata import CustomMetadataDict client = AtlanClient () raci = CustomMetadataDict ( client = client , name = \"RACI\" ) # (1) raci [ \"Responsible\" ] = [ \"jsmith\" ] # (2) raci [ \"Consulted\" ] = [ \"finance\" , \"risk\" ] # (3) client . asset . replace_custom_metadata ( # (4) guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" , #(5) custom_metadata = raci # (6) ) Create an empty custom metadata set by specifying the client instance and name of an existing custom metadata set. Name will be validated The name will be validated at runtime to ensure that a custom metadata set with the given name exists. For each property of the metadata set you wish to update specify the name of the property. Name will be validated The metadata property name will be validated at runtime to ensure that a property with the given name exists in the custom metadata set. For any property that can be multi-valued, we need to send a list of values. Use the asset.replace_custom_metadata() method to replace only this named RACI custom metadata on the existing asset. Any other custom metadata will be left unchanged. Note that any properties in RACI that are not included in the custom metadata object we send will be removed from the custom metadata on that asset. (In our examples, this means any existing values in the Accountable and Informed properties of RACI on this asset would be removed.) Note that for this operation you must know the GUID of the asset you want to apply the custom metadata to. Also, the operation returns no result: if there is an error it will throw an exception, but the result of the operation must be determined by retrieving the asset through a separate API call, if you want to confirm it. Provide the custom metadata set object with the properties and values you want to be the complete set for that custom metadata. Replace some custom metadata on an asset 1 2 3 4 5 6 7 8 9 val cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Responsible\" , \"jsmith\" ) // (2) . attribute ( \"Consulted\" , listOf ( \"finance\" , \"risk\" )) // (3) . build () Table . replaceCustomMetadata ( // (4) client , // (5) \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (6) \"RACI\" , // (7) cmRACI ) // (8) Create a custom metadata attributes object that will contain the attributes and values for custom metadata you want to add to the asset. For each attribute, use the attribute() method and pass: the name of the attribute within that set the value for that attribute The value can be any object valid for the attribute: a string, a boolean, or a number. (Note that dates are sent as long (epoch) numbers.) For any attribute that can be multi-valued, we can send a list of values. Use the replaceCustomMetadata() method to replace only this named RACI custom metadata on the existing asset. Any other custom metadata will be left unchanged. Note that any attributes in RACI that are not included in the custom metadata attributes object we send will be removed from the custom metadata on that asset. (In our examples, this means any existing values in the Accountable and Informed attributes of RACI on this asset would be removed.) Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Note that for this operation you must know the GUID of the asset you want to apply the custom metadata to. Also, the operation returns no result: if there is an error it will throw an exception, but the result of the operation must be determined by retrieving the asset through a separate API call, if you want to confirm it. Provide the name for the custom metadata you want to add/replace. Provide the custom metadata attributes object with the attributes and values you want to be the complete set for that custom metadata. POST /api/meta/entity/guid/a89ff15b-f5e6-48bc-870b-acfa11e212ae/businessmetadata/MNJ8mpLsIOaP4OQnLNhRta 1 2 3 4 5 6 7 8 9 { // (1) \"MNJ8mpLsIOaP4OQnLNhRta\" : { // (2) \"fWMB77RSjRGNYoFeD4FcGi\" : \"jsmith\" , // (3) \"F8XI9GzcBpdBdfi4cLiPEz\" : [ // (4) \"finance\" , \"risk\" ] } } You must pass the GUID of the asset to change for this operation. There is no alternative that works with the qualifiedName. Note that you also need the hashed-string representation of the custom metadata set in the URL itself. Each custom metadata set you want to replace must be given using its hashed-string representation . Each custom metadata attribute you want to include in the replacement must be given using its hashed-string representation . For multivalued custom metadata attributes, specify the value as an array. Defining custom metadata by names in dbt You can iteratively migrate from using businessAttributes to businessAttributeNames with dbt. But remember to not define the same attribute under both. Replace all custom metadata on an asset Â¶ Could create a new asset Remember that Atlan matches the provided qualifiedName to determine whether to update or create the asset . 7.0.0 4.0.0 You can also replace all the custom metadata on one or many existing assets at the same time. Replaces any existing custom metadata This approach will replace all existing custom metadata (across all attributes) on the asset. If you have only a few custom metadata attributes defined in the update, this will remove any other custom metadata attributes that are already set on the asset within Atlan. dbt Java Python Kotlin Raw REST API Replace all custom metadata on existing assets 1 2 3 4 5 6 7 8 9 10 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : businessAttributes : # (2) MNJ8mpLsIOaP4OQnLNhRta : # (3) fWMB77RSjRGNYoFeD4FcGi : jsmith # (4) F8XI9GzcBpdBdfi4cLiPEz : [ \"finance\" , \"risk\" ] # (5) foMg7yOwUajucuya0JEF4J : # (6) uTmK5o0J8jHTH3KWFXXeZi : example # (7) You must of course give the name of the object. The custom metadata must be nested within the meta . atlan . businessAttributes structure. Each custom metadata set you want to add or update must be given using its hashed-string representation . Each custom metadata attribute you want to update must be given using its hashed-string representation . For multivalued custom metadata attributes, specify the value as an array. Additional custom metadata sets would be listed as additional sub-objects of the businessAttributes object. (Still using a hashed-string representation .) ...and custom metadata attributes within those sets would be listed as sub-objects of the custom metadata set object. (Still using a hashed-string representation .) Replace all custom metadata on existing assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 CustomMetadataAttributes cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Responsible\" , \"jsmith\" ) . attribute ( \"Consulted\" , List . of ( \"finance\" , \"risk\" )) . build (); CustomMetadataAttributes cmOther = CustomMetadataAttributes . builder () // (2) . attribute ( \"Another\" , \"example\" ) . build (); Table table = Table . updater ( \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) \"TOP_BEVERAGE_USERS\" ) . customMetadata ( \"RACI\" , cmRACI ) // (4) . customMetadata ( \"Other\" , cmOther ) // (5) . build (); // (6) AssetMutationResponse response = table . saveReplacingCM ( client , false ); // (7) assert response . getUpdatedAssets (). size () == 1 // (8) Create one or more custom metadata attributes objects that will contain all the custom metadata you want the asset to have. You can create as many custom metadata attributes objects as you have named sets of custom metadata. Use the updater() method to initialize the object with all necessary attributes for updating it . Directly chain the custom metadata attributes onto the updater() method's result. Note that the first parameter needs to be the name of the custom metadata that contains these attributes. Continue chaining custom metadata attributes onto each other, if you have multiple sets of custom metadata you want to include in the replacement. Call the build() method to build the enriched object. Call the saveReplacingCM() method to replace the custom metadata for the asset in Atlan. (If you use save() then no custom metadata updates will be made; while using saveMergingCM() will only update any new or changed values.) Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include all assets that were updated. Replace all custom metadata on existing assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.custom_metadata import CustomMetadataDict client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) cm_raci = CustomMetadataDict ( # (2) client = client , name = \"RACI\" , # (3) ) cm_raci [ \"Responsible\" ] = \"jsmith\" # (4) cm_raci [ \"Consulted\" ] = [ \"finance\" , \"risk\" ] # (5) cm_other = CustomMetadataDict ( client = client , name = \"Other\" , ) cm_other [ \"Another\" ] = \"example\" table . set_custom_metadata ( client = client , custom_metadata = cm_raci ) # (6) table . set_custom_metadata ( client = client , custom_metadata = cm_other ) # (7) response = client . asset . save_replacing_cm ( # (8) table ) assert ( tables := response . assets_updated ( asset_type = Table )) # (9) Use the updater() method to create an asset suitable for modifiaction i.e. with all the required attributes. Create a new instance of CustomMetadataDict. Provide the name of an existing custom metadata set. Name will be validated The name will be validated at runtime to ensure that a custom metadata set with the given name exists. For each property that you want to set, specify the property name. Name will be validated The metadata property name will be validated at runtime to ensure that a property with the given name exists in the custom metadata set. For any property that can be multi-valued, we need to send a list of values. Use the set_custom_metadata() method to add the custom metadata to the model object. You must call set_custom_metadata() for each set of custom metadata. Use the save_replacing_cm() method to update the model object on the server. Assert that a Table asset has been updated. Replace all custom metadata on existing assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 val cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Responsible\" , \"jsmith\" ) . attribute ( \"Consulted\" , listOf ( \"finance\" , \"risk\" )) . build () val cmOther = CustomMetadataAttributes . builder () // (2) . attribute ( \"Another\" , \"example\" ) . build () val table = Table . updater ( \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) \"TOP_BEVERAGE_USERS\" ) . customMetadata ( \"RACI\" , cmRACI ) // (4) . customMetadata ( \"Other\" , cmOther ) // (5) . build () // (6) val response = table . saveReplacingCM ( client , false ) // (7) assert ( response . updatedAssets . size == 1 ) // (8) Create one or more custom metadata attributes objects that will contain all the custom metadata you want the asset to have. You can create as many custom metadata attributes objects as you have named sets of custom metadata. Use the updater() method to initialize the object with all necessary attributes for updating it . Directly chain the custom metadata attributes onto the updater() method's result. Note that the first parameter needs to be the name of the custom metadata that contains these attributes. Continue chaining custom metadata attributes onto each other, if you have multiple sets of custom metadata you want to include in the replacement. Call the build() method to build the enriched object. Call the saveReplacingCM() method to replace the custom metadata for the asset in Atlan. (If you use save() then no custom metadata updates will be made; while using saveMergingCM() will only update any new or changed values.) Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include all assets that were updated. POST /api/meta/entity/bulk?replaceClassifications=false&replaceBusinessAttributes=true&overwriteBusinessAttributes=true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { // (1) \"entities\" : [ // (2) { \"typeName\" : \"Table\" , // (3) \"attributes\" : { \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"name\" : \"TOP_BEVERAGE_USERS\" // (5) }, \"businessAttributes\" : { // (6) \"MNJ8mpLsIOaP4OQnLNhRta\" : { // (7) \"fWMB77RSjRGNYoFeD4FcGi\" : \"jsmith\" , // (8) \"F8XI9GzcBpdBdfi4cLiPEz\" : [ \"finance\" , \"risk\" ] }, \"foMg7yOwUajucuya0JEF4J\" : { // (9) \"uTmK5o0J8jHTH3KWFXXeZi\" : \"example\" // (10) } } } ] } Note that the query parameters replaceBusinessAttributes and overwriteBusinessAttributes must both equal true in the request. This is what causes the replacement behavior. All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). Each custom metadata set you want to include in the replacement must be a sub-object of the businessAttributes object. Each custom metadata set must be specified using its hashed-string representation . Each custom metadata attribute you want to update must be given using its hashed-string representation . Additional custom metadata sets would be listed as additional sub-objects of the businessAttributes object. (Still using a hashed-string representation .) ...and custom metadata attributes within those sets would be listed as sub-objects of the custom metadata set object. (Still using a hashed-string representation .) Remove from an existing asset Â¶ Remove only some custom metadata attributes Â¶ 7.0.0 4.0.0 To remove only some custom metadata attributes (leaving all others unchanged): dbt Java Python Kotlin Raw REST API This currently isn't possible via dbt. Remove only some custom metadata attributes 1 2 3 4 5 6 7 8 CustomMetadataAttributes cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Accountable\" , Removable . NULL ) // (2) . build (); Table . updateCustomMetadataAttributes ( // (3) client , // (4) \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (5) \"RACI\" , // (6) cmRACI ); // (7) Create a custom metadata attributes object that will contain only the attributes and values for custom metadata that you want to remove from the asset. All other custom metadata attributes (those not specified in this object) will remain unchanged on the asset. For each attribute, use the attribute() method and pass: the name of the attribute within that set a special value of Removable.NULL This special value will ensure that the custom metadata attribute ( Accountable in this example) is removed from the asset. Use the updateCustomMetadataAttributes() method to update only the Accountable attribute in the RACI custom metadata on the existing asset. Since we're sending a special value to null this attribute, it will be removed by the update. Any other custom metadata attributes in RACI and all other custom metadata will be left unchanged. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Note that for this operation you must know the GUID of the asset you want to remove the custom metadata from. Also, the operation returns no result: if there is an error it will throw an exception, but the result of the operation must be determined by retrieving the asset through a separate API call, if you want to confirm it. Provide the name for the custom metadata you want to remove. Provide the custom metadata attributes object with the attributes and special Removable.NULL values you want to remove for that custom metadata. Remove only some custom metadata attributes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.custom_metadata import CustomMetadataDict client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) cm_raci = CustomMetadataDict ( # (2) client = client , name = \"RACI\" , # (3) ) cm_raci [ \"Accountable\" ] = None # (4) table . set_custom_metadata ( client = client , custom_metadata = cm_raci ) # (5) response = client . asset . save_merging_cm ( # (6) table ) assert ( tables := response . assets_updated ( asset_type = Table )) # (7) Use the updater() method to create an asset suitable for modifiaction i.e. with all the required attributes. Create an instance of CustomMetadataDict . Provide the name of an existing of an existing custom metadata set. Name will be validated The name will be validated at runtime to ensure that a custom metadata set with the given name exists. Set the value of the property you wish to replace to None . Name will be validated The metadata property name will be validated at runtime to ensure that a property with the given name exists in the custom metadata set. Use the set_custom_metadata() method to add the custom metadata to the model object. Use the save_merging_cm() method to update the model object on the server. Assert that a Table asset has been updated. Remove only some custom metadata attributes 1 2 3 4 5 6 7 8 val cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Accountable\" , Removable . NULL ) // (2) . build () Table . updateCustomMetadataAttributes ( // (3) client , // (4) \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (5) \"RACI\" , // (6) cmRACI ) // (7) Create a custom metadata attributes object that will contain only the attributes and values for custom metadata that you want to remove from the asset. All other custom metadata attributes (those not specified in this object) will remain unchanged on the asset. For each attribute, use the attribute() method and pass: the name of the attribute within that set a special value of Removable.NULL This special value will ensure that the custom metadata attribute ( Accountable in this example) is removed from the asset. Use the updateCustomMetadataAttributes() method to update only the Accountable attribute in the RACI custom metadata on the existing asset. Since we're sending a special value to null this attribute, it will be removed by the update. Any other custom metadata attributes in RACI and all other custom metadata will be left unchanged. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Note that for this operation you must know the GUID of the asset you want to remove the custom metadata from. Also, the operation returns no result: if there is an error it will throw an exception, but the result of the operation must be determined by retrieving the asset through a separate API call, if you want to confirm it. Provide the name for the custom metadata you want to remove. Provide the custom metadata attributes object with the attributes and special Removable.NULL values you want to remove for that custom metadata. POST /api/meta/entity/guid/a89ff15b-f5e6-48bc-870b-acfa11e212ae/businessmetadata?isOverwrite=false 1 2 3 4 5 { // (1) \"MNJ8mpLsIOaP4OQnLNhRta\" : { // (2) \"xDUCZllc4JyTKhwqSDkWK4\" : null // (3) } } Note that the query parameters isOverwrite must be false in the request. This is what allows the removal of only the attributes provided in the request (and leaving all others unchanged). Also note that you must provide the GUID of the asset â€” there is no equivalent operation using the qualifiedName. Each custom metadata set you want to include in the partial removal must be specified using its hashed-string representation . Each custom metadata attribute you want to remove must be given using its hashed-string representation , with a value of null . You would either need to first retrieve the list of custom metadata definitions via API to determine this value, or look through the development console of your browser while opening the custom metadata in the Atlan UI. Remove some custom metadata from an asset Â¶ 7.0.0 4.0.0 You can also remove an entire set of custom metadata from existing assets. If you do this individually, you can selectively remove individual sets of custom metadata: dbt Java Python Kotlin Raw REST API Remove some custom metadata from an existing asset 1 2 3 4 5 6 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : businessAttributes : MNJ8mpLsIOaP4OQnLNhRta : {} # (2) You must of course give the name of the object. The custom metadata must be nested within the meta . atlan . businessAttributes structure. To remove all properties for some custom metadata, send an explicit empty dictionary {} to the custom metadata's hashed-string representation . Remove some custom metadata from an asset 1 2 3 4 Table . removeCustomMetadata ( // (1) client , // (2) \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (3) \"RACI\" ); // (4) Use the removeCustomMetadata() method to remove an entire named set of custom metadata from an asset. Any other custom metadata in other named sets will be left unchanged. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Note that for this operation you must know the GUID of the asset you want to remove the custom metadata from. Also, the operation returns no result: if there is an error it will throw an exception, but the result of the operation must be determined by retrieving the asset through a separate API call, if you want to confirm it. Provide the name for the custom metadata you want to remove. Remove some custom metadata from an asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.custom_metadata import CustomMetadataDict client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) cm_raci = CustomMetadataDict ( # (2) client = client , name = \"RACI\" , # (3) ) table . set_custom_metadata ( client = client , custom_metadata = cm_raci ) # (4) response = client . asset . save_merging_cm ( # (5) table ) assert ( tables := response . assets_updated ( asset_type = Table )) # (6) Use the updater() method to create an asset suitable for modifiaction i.e. with all the required attributes. Create a new instance of CustomMetadataDict . Provide the name of an existing custom metadata set. Name will be validated The name will be validated at runtime to ensure that a custom metadata set with the given name exists. Use the set_custom_metadata() method to add the custom metadata to the model object. Use the save_merging_cm() method to update the model object on the server. Assert that a Table asset has been updated. Remove some custom metadata from an asset 1 2 3 4 Table . removeCustomMetadata ( // (1) client , // (2) \"b4113341-251b-4adc-81fb-2420501c30e6\" , // (3) \"RACI\" ) // (4) Use the removeCustomMetadata() method to remove an entire named set of custom metadata from an asset. Any other custom metadata in other named sets will be left unchanged. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Note that for this operation you must know the GUID of the asset you want to remove the custom metadata from. Also, the operation returns no result: if there is an error it will throw an exception, but the result of the operation must be determined by retrieving the asset through a separate API call, if you want to confirm it. Provide the name for the custom metadata you want to remove. POST /api/meta/entity/guid/a89ff15b-f5e6-48bc-870b-acfa11e212ae/businessmetadata?isOverwrite=false 1 2 3 4 5 6 7 8 { // (1) \"MNJ8mpLsIOaP4OQnLNhRta\" : { // (2) \"F8XI9GzcBpdBdfi4cLiPEz\" : null , // (3) \"xDUCZllc4JyTKhwqSDkWK4\" : null , \"fWMB77RSjRGNYoFeD4FcGi\" : null , \"rN6H6xMQpyHvo639SXER83\" : null } } Note that the query parameters isOverwrite must be false in the request. This is what allows the removal of only the custom metadata set provided in the request (and leaving all others unchanged). Also note that you must provide the GUID of the asset â€” there is no equivalent operation using the qualifiedName. The custom metadata set you want to remove must be specified using its hashed-string representation . Each custom metadata attribute in that custom metadata set must be specified using its hashed-string representation , with a value of null . You would either need to first retrieve the list of custom metadata definitions via API to determine this value, or look through the development console of your browser while opening the custom metadata in the Atlan UI. Remove all custom metadata from an asset Â¶ Could create a new asset Remember that Atlan matches the provided qualifiedName to determine whether to update or create the asset . 7.0.0 4.0.0 To remove all custom metadata from an existing asset: dbt Java Python Kotlin Raw REST API Remove all custom metadata from an existing asset 1 2 3 4 5 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : businessAttributes : {} # (2) You must of course give the name of the object. The custom metadata must be nested within the meta . atlan . businessAttributes structure. To remove all custom metadata, send an explicit empty dictionary {} . Remove all custom metadata from an existing asset 1 2 3 4 5 Table table = Table . updater ( \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (1) \"TOP_BEVERAGE_USERS\" ). build (); AssetMutationResponse response = table . saveReplacingCM ( client , false ); // (2) assert response . getUpdatedAssets (). size () == 1 ; // (3) Use the updater() method to initialize the object with all necessary attributes for updating it . (Removing the custom metadata is still an update to the asset, we are not deleting the asset itself.) Call the saveReplacingCM() method to actually update the asset, and overwrite custom metadata. Since we have not provided any custom metadata in our object, this will replace the existing custom metadata on the asset with no custom metadata. (In other words, it will remove all custom metadata from the asset.) Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was updated (again, removing custom metadata is an update to the asset â€” we are not deleting the asset itself). Remove all custom metadata from an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) response = client . asset . save_replacing_cm ( # (2) table ) assert ( tables := response . assets_updated ( asset_type = Table )) # (3) Use the updater() method to initialize the object with all necessary attributes for updating it . (Removing the custom metadata is still an update to the asset, we are not deleting the asset itself.) Call the save_replacing_cm() method to actually update the asset. Assert that a Table asset has been updated. Remove all custom metadata from an existing asset 1 2 3 4 5 val table = Table . updater ( \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (1) \"TOP_BEVERAGE_USERS\" ). build () val response = table . saveReplacingCM ( client , false ) // (2) assert ( response . updatedAssets . size == 1 ) // (3) Use the updater() method to initialize the object with all necessary attributes for updating it . (Removing the custom metadata is still an update to the asset, we are not deleting the asset itself.) Call the saveReplacingCM() method to actually update the asset, and overwrite custom metadata. Since we have not provided any custom metadata in our object, this will replace the existing custom metadata on the asset with no custom metadata. (In other words, it will remove all custom metadata from the asset.) Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was updated (again, removing custom metadata is an update to the asset â€” we are not deleting the asset itself). POST /api/meta/entity/bulk?replaceClassifications=false&replaceBusinessAttributes=true&overwriteBusinessAttributes=true 1 2 3 4 5 6 7 8 9 10 11 { // (1) \"entities\" : [ // (2) { \"typeName\" : \"Table\" , // (3) \"attributes\" : { \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"name\" : \"TOP_BEVERAGE_USERS\" // (5) } // (6) } ] } Note that the query parameters replaceBusinessAttributes and overwriteBusinessAttributes must both equal true in the request. This is what causes the replacement behavior. All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). By not providing any businessAttributes in the request, you will replace whatever custom metadata is on the asset with no custom metadata â€” equivalent to removing all custom metadata. When creating an asset Â¶ 7.0.0 4.0.0 To add custom metadata when creating one or many assets: dbt Java Python Kotlin Raw REST API Add custom metadata when creating asset 1 2 3 4 5 6 7 8 9 10 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : businessAttributes : # (2) MNJ8mpLsIOaP4OQnLNhRta : # (3) fWMB77RSjRGNYoFeD4FcGi : jsmith # (4) xDUCZllc4JyTKhwqSDkWK4 : jdoe F8XI9GzcBpdBdfi4cLiPEz : [ \"finance\" , \"risk\" ] # (5) rN6H6xMQpyHvo639SXER83 : [ \"operations\" ] You must of course give the name of the object. The custom metadata must be nested within the meta . atlan . businessAttributes structure. Each custom metadata set you want to add or update must be given using its hashed-string representation . Each custom metadata attribute you want to update must be given using its hashed-string representation . For multivalued custom metadata attributes, specify the value as an array. Add custom metadata when creating asset 1 2 3 4 5 6 7 8 9 10 11 12 13 CustomMetadataAttributes cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Responsible\" , \"jsmith\" ) // (2) . attribute ( \"Accountable\" , \"jdoe\" ) . attribute ( \"Consulted\" , List . of ( \"finance\" , \"risk\" )) // (3) . attribute ( \"Informed\" , List . of ( \"operations\" )) . build (); Table table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (4) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . customMetadata ( \"RACI\" , cmRACI ) // (5) . build (); // (6) AssetMutationResponse response = table . saveReplacingCM ( client , false ); // (7) assert response . getCreatedAssets (). size () == 1 // (8) Create a custom metadata attributes object that will contain the attributes and values for custom metadata you want to add to the asset. For each attribute, use the attribute() method and pass: the name of the attribute within that set the value for that attribute The value can be any object valid for the attribute: a string, a boolean, or a number. (Note that dates are sent as long (epoch) numbers.) For any attribute that can be multi-valued, we can send a list of values. Use the creator() method to initialize the object with all necessary attributes for creating it . Set the custom metadata that should be added (using the custom metadata attributes object you built earlier). Note that the first parameter to this method is the name of the custom metadata for which you're providing the attributes and values. You can chain this customMetadata() method as many times as you like to add other custom metadata and attributes, but you should only call it once per named custom metadata set. (If you call it multiple times for the same named custom metadata, only the last one will be applied.) Call the build() method to build the enriched object. Call the saveReplacingCM() method to create the asset, including its custom metadata. (During creation you could also use saveMergingCM() , but if you use only save() then no custom metadata will be attached to the assets.) Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include all assets that were created. Add custom metadata when creating asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.custom_metadata import CustomMetadataDict client = AtlanClient () table = Table . creator ( # (1) schema_qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" , name = \"TOP_BEVERAGE_USERS\" , ) cm_raci = CustomMetadataDict ( # (2) client = client , name = \"RACI\" , # (3) ) # cm_raci [ \"Accountable\" ] = \"jdoe\" # (4) cm_raci [ \"Responsible\" ] = [ \"jsmith\" ] cm_raci [ \"Consulted\" ] = [ \"finance\" , \"risk\" ] # (5) cm_raci [ \"Informed\" ] = [ \"operations\" ] table . set_custom_metadata ( client = client , custom_metadata = cm_raci ) # (6) response = client . asset . save ( table ) # (7) assert ( created := response . assets_created ( asset_type = Table ) # (8) Use the creator() method to initialize the object with all necessary attributes for creating it . Create a new instance of CustomMetadataDict . Provide the name of and existing custom metadata set. Name will be validated The name will be validated at runtime to ensure that a custom metadata set with the given name exists. For each property that you want to set, specify the property name. Name will be validated The metadata property name will be validated at runtime to ensure that a property with the given name exists in the custom metadata set. For any attribute that can be multi-valued, we need to send a list of values. Use the set_custom_metadata() method to add the custom metadata to the model object. Use the save() method to update the model object on the server. assert that a Table asset was created. Add custom metadata when creating asset 1 2 3 4 5 6 7 8 9 10 11 12 13 val cmRACI = CustomMetadataAttributes . builder () // (1) . attribute ( \"Responsible\" , \"jsmith\" ) // (2) . attribute ( \"Accountable\" , \"jdoe\" ) . attribute ( \"Consulted\" , listOf ( \"finance\" , \"risk\" )) // (3) . attribute ( \"Informed\" , listOf ( \"operations\" )) . build () val table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (4) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . customMetadata ( \"RACI\" , cmRACI ) // (5) . build () // (6) val response = table . saveReplacingCM ( client , false ) // (7) assert ( response . createdAssets . size == 1 ) // (8) Create a custom metadata attributes object that will contain the attributes and values for custom metadata you want to add to the asset. For each attribute, use the attribute() method and pass: the name of the attribute within that set the value for that attribute The value can be any object valid for the attribute: a string, a boolean, or a number. (Note that dates are sent as long (epoch) numbers.) For any attribute that can be multi-valued, we can send a list of values. Use the creator() method to initialize the object with all necessary attributes for creating it . Set the custom metadata that should be added (using the custom metadata attributes object you built earlier). Note that the first parameter to this method is the name of the custom metadata for which you're providing the attributes and values. You can chain this customMetadata() method as many times as you like to add other custom metadata and attributes, but you should only call it once per named custom metadata set. (If you call it multiple times for the same named custom metadata, only the last one will be applied.) Call the build() method to build the enriched object. Call the saveReplacingCM() method to create the asset, including its custom metadata. (During creation you could also use saveMergingCM() , but if you use only save() then no custom metadata will be attached to the assets.) Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include all assets that were created. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"atlanSchema\" : { // (5) \"typeName\" : \"Schema\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" } } }, \"businessAttributes\" : { // (6) \"MNJ8mpLsIOaP4OQnLNhRta\" : { // (7) \"fWMB77RSjRGNYoFeD4FcGi\" : \"jsmith\" , // (8) \"xDUCZllc4JyTKhwqSDkWK4\" : \"jdoe\" , \"F8XI9GzcBpdBdfi4cLiPEz\" : [ // (9) \"finance\" , \"risk\" ], \"rN6H6xMQpyHvo639SXER83\" : [ \"operations\" ] } } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide a name for the asset. In the case of a table, the qualifiedName must be the concatenation of the parent schema's qualifiedName and the name of the table. When creating a table, you must specify the schema to create it within. This is defined by the atlanSchema attribute. You must specify both the type (must be Schema ) and qualifiedName of the schema within the atlanSchema attribute â€” and the schema must already exist. Each custom metadata set you want to include on the asset must be a sub-object of the businessAttributes object. Each custom metadata set must be specified using its hashed-string representation . Each custom metadata attribute you want to add must be given using its hashed-string representation . For multivalued custom metadata attributes, specify the value as an array. Find hashed-string names Â¶ When using either the raw APIs or specifying businessAttributes with dbt, you must provide the classification names using Atlan's hashed-string representation. Not necessary for SDKs Note that this is not needed when using the SDKs, which translate these for you! To look up the hashed-string representations: GET /api/meta/types/typedefs?type=business_metadata The response will include displayName and name , both at overall custom metadata level and for each attribute (property). The displayName is what you see in Atlan's UI, and the name is the hashed-string representation: Simplified response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 { \"enumDefs\" : [], \"structDefs\" : [], \"classificationDefs\" : [], \"entityDefs\" : [], \"relationshipDefs\" : [], \"businessMetadataDefs\" : [ { \"category\" : \"BUSINESS_METADATA\" , \"guid\" : \"e5cc3476-9cd9-4ed7-89a7-18dfde86f827\" , \"name\" : \"MNJ8mpLsIOaP4OQnLNhRta\" , \"displayName\" : \"RACI\" , \"options\" : { \"logoType\" : \"emoji\" , \"emoji\" : \"ðŸ‘ª\" }, \"attributeDefs\" : [ { \"name\" : \"fWMB77RSjRGNYoFeD4FcGi\" , \"displayName\" : \"Responsible\" , \"cardinality\" : \"SINGLE\" , \"typeName\" : \"string\" , \"description\" : \"\" , \"options\" : { \"customType\" : \"users\" , \"showInOverview\" : \"false\" , \"allowFiltering\" : \"true\" , \"isEnum\" : \"false\" , \"multiValueSelect\" : \"false\" , \"primitiveType\" : \"users\" } }, { \"name\" : \"xDUCZllc4JyTKhwqSDkWK4\" , \"displayName\" : \"Accountable\" , \"cardinality\" : \"SINGLE\" , \"typeName\" : \"string\" , \"description\" : \"\" , \"options\" : { \"customType\" : \"users\" , \"showInOverview\" : \"false\" , \"allowFiltering\" : \"true\" , \"isEnum\" : \"false\" , \"multiValueSelect\" : \"false\" , \"primitiveType\" : \"users\" } }, { \"name\" : \"F8XI9GzcBpdBdfi4cLiPEz\" , \"displayName\" : \"Consulted\" , \"typeName\" : \"array<string>\" , \"cardinality\" : \"SET\" , \"description\" : \"\" , \"options\" : { \"customType\" : \"groups\" , \"showInOverview\" : \"false\" , \"allowFiltering\" : \"true\" , \"isEnum\" : \"false\" , \"multiValueSelect\" : \"true\" , \"primitiveType\" : \"groups\" } }, { \"name\" : \"rN6H6xMQpyHvo639SXER83\" , \"displayName\" : \"Informed\" , \"typeName\" : \"array<string>\" , \"cardinality\" : \"SET\" , \"description\" : \"\" , \"options\" : { \"customType\" : \"groups\" , \"showInOverview\" : \"false\" , \"allowFiltering\" : \"true\" , \"isEnum\" : \"false\" , \"multiValueSelect\" : \"true\" , \"primitiveType\" : \"groups\" } }, { \"name\" : \"okm7BDXjTQx4iYPT5u7ilu\" , \"displayName\" : \"Extra\" , \"typeName\" : \"string\" , \"cardinality\" : \"SINGLE\" , \"description\" : \"\" , \"options\" : { \"showInOverview\" : \"false\" , \"allowFiltering\" : \"true\" , \"isEnum\" : \"false\" , \"multiValueSelect\" : \"false\" , \"primitiveType\" : \"string\" } } ] }, { \"category\" : \"BUSINESS_METADATA\" , \"guid\" : \"389c0f8a-5d68-407c-8b5c-45a19f2cc7e0\" , \"name\" : \"foMg7yOwUajucuya0JEF4J\" , \"displayName\" : \"Other\" , \"options\" : { \"logoType\" : \"emoji\" , \"emoji\" : \"â“\" }, \"attributeDefs\" : [ { \"name\" : \"uTmK5o0J8jHTH3KWFXXeZi\" , \"displayName\" : \"Another\" , \"typeName\" : \"string\" , \"cardinality\" : \"SINGLE\" , \"description\" : \"\" , \"options\" : { \"showInOverview\" : \"false\" , \"allowFiltering\" : \"true\" , \"isEnum\" : \"false\" , \"multiValueSelect\" : \"false\" , \"primitiveType\" : \"string\" } } ] } ] } 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/descriptions/",
    "content": "/api/meta/entity/bulk (POST) Change description Â¶ There are actually two descriptions per asset There are actually two fields in Atlan that capture the description of an asset: description and userDescription . In the UI, userDescription will take precedence. This is the field that is updated when a user updates the description through the UI. When a system updates a description, it will populate the description field. This field is only shown in the UI when the userDescription field is empty. The examples below therefore all update the description field, to allow a user to still override this value through the UI. If you want to actually override any users' descriptions, however, replace description in the examples below with userDescription . Change an existing asset Â¶ Could create a new asset Remember that Atlan matches the provided qualifiedName to determine whether to update or create the asset . 2.0.0 4.0.0 To change a description on an existing asset : dbt Java Python Kotlin Raw REST API Change description on existing asset 1 2 3 4 models : - name : TOP_BEVERAGE_USERS # (1) description : >- # (2) My new description You must of course give the name of the object. You just use the normal dbt description field to provide a description â€” no need for the meta . atlan . attributes structure. Change description on existing asset 1 2 3 4 5 6 7 Table table = Table . updater ( // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (2) \"TOP_BEVERAGE_USERS\" ) // (3) . description ( \"My new description\" ) // (4) . build (); // (5) AssetMutationResponse response = table . save ( client ); // (6) assert response . getUpdatedAssets (). size () == 1 // (7) Use the updater() helper method to create the minimal object necessary to do an update. The qualifiedName of the object. The name of the object. Provide the new description. Build the updater into an object. Send the update to Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was updated. Change description on existing an asset 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) table . description = \"My new description\" # (2) response = client . asset . save ( table ) # (3) assert 1 == len ( response . assets_updated ( asset_type = Table )) # (4) Use the updater() method to create an asset suitable for modification i.e. with all the requisite attributes. Provide the new description. Send the update to Atlan. The response should only include that single asset that was updated. Change description on existing asset 1 2 3 4 5 6 7 val table = Table . updater ( // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (2) \"TOP_BEVERAGE_USERS\" ) // (3) . description ( \"My new description\" ) // (4) . build () // (5) val response = table . save ( client ) // (6) assert ( response . updatedAssets . size == 1 ) // (7) Use the updater() helper method to create the minimal object necessary to do an update. The qualifiedName of the object. The name of the object. Provide the new description. Build the updater into an object. Send the update to Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was updated. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"description\" : \"My new description\" // (5) } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). Provide the new description. Remove from an existing asset Â¶ 2.0.0 4.0.0 To remove a description from an existing asset: dbt Java Python Kotlin Raw REST API It is currently not possible to remove a description from an asset via dbt. Remove description from existing asset 1 2 3 4 Table table = Table . removeDescription ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) \"TOP_BEVERAGE_USERS\" ); // (4) Use the removeDescription() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the object. The name of the object. Remove description from an existing asset 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) table . description = None # (2) response = client . asset . save ( table ) # (3) assert 1 == len ( response . assets_updated ( asset_type = Table )) # (4) Use the updater() method to create an asset suitable for modification i.e. with all the requisite attributes. Set the description to None . Send the update to Atlan. The response should only include that single asset that was updated (again, removing owners is an update to the asset â€” we are not deleting the asset itself). Remove description from existing asset 1 2 3 4 val table = Table . removeDescription ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) \"TOP_BEVERAGE_USERS\" ) // (4) Use the removeDescription() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request, call the necessary API(s), and return with the result of the removal operation all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the object. The name of the object. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"description\" : null // (5) } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must set the description to null . When creating an asset Â¶ 2.0.0 4.0.0 To add a description when creating an asset: dbt Java Python Kotlin Raw REST API Add description when creating an asset 1 2 3 4 models : - name : TOP_BEVERAGE_USERS # (1) description : >- # (2) My description of the asset You must of course give the name of the object. You just use the normal dbt description field to provide a description â€” no need for the meta . atlan . attributes structure. Add description when creating asset 1 2 3 4 5 6 7 Table table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . description ( \"My description of the asset\" ) // (2) . build (); // (3) AssetMutationResponse response = table . save ( client ); // (4) assert response . getCreatedAssets (). size () == 1 // (5) Use the creator() method to initialize the object with all necessary attributes for creating it (../advanced-examples/create.md#build-minimal-object-needed). Set the description that should be added. Call the build() method to build the enriched object. Call the save() method to actually create the asset with this description. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created. Add description when creating asset 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = Table . creator ( # (1) name = \"TOP_BEVERAGE_USERS\" , schema_qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" , ) table . description = \"My description of the asset\" # (2) response = client . asset . save ( table ) # (3) assert 1 == len ( assets_created := response . assets_created ( asset_type = Table )) # (4) table = assets_created [ 0 ] # (5) Use the creator() method to initialize the object with all necessary attributes for creating it. Set the description. Call the save() method to actually create the asset with these owners. Since a save can add, update, delete or partially update multiple assets the assets_created() method can be used to return a list of the assets of the specified type that were added. The assert statement is present to ensure a Table asset was created. Since only one Table has been created we use an index of 0 to retrieve the newly created table. Add description when creating asset 1 2 3 4 5 6 7 val table = Table . creator ( \"TOP_BEVERAGE_USERS\" , // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" ) . description ( \"My description of the asset\" ) // (2) . build () // (3) val response = table . save ( client ) // (4) assert ( response . createdAssets . size == 1 ) // (5) Use the creator() method to initialize the object with all necessary attributes for creating it (../advanced-examples/create.md#build-minimal-object-needed). Set the description that should be added. Call the build() method to build the enriched object. Call the save() method to actually create the asset with this description. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (4) \"atlanSchema\" : { // (5) \"typeName\" : \"Schema\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV\" } }, \"description\" : \"My description of the asset\" // (6) } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide a name for the asset. In the case of a table, the qualifiedName must be the concatenation of the parent schema's qualifiedName and the name of the table. When creating a table, you must specify the schema to create it within. This is defined by the atlanSchema attribute. You must specify both the type (must be Schema ) and qualifiedName of the schema within the atlanSchema attribute â€” and the schema must already exist. Provide the description. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/relationship-attributes/",
    "content": "/api/meta/entity/bulk (POST) Manage asset relationships with attributes Â¶ Atlan supports relationships between assets that can include attributes, similar to how assets themselves have attributes. These relationship-level attributes provide additional context and metadata about the connection between assets. The SDK enables you to create, retrieve, and delete these attributed relationships programmatically. Relationships with attribute support Â¶ The following relationship types support attributes that you can set via the SDK: Available attributed relationships AtlasGlossaryAntonym AtlasGlossarySynonym AtlasGlossaryReplacementTerm AtlasGlossarySemanticAssignment AtlasGlossaryPreferredTerm AtlasGlossaryRelatedTerm AtlasGlossaryTermCategorization AtlasGlossaryTranslation AtlasGlossaryValidValue AtlasGlossaryIsARelationship CustomParentEntityCustomChildEntities CustomRelatedFromEntitiesCustomRelatedToEntities UserDefRelationship Add user-defined relationship Â¶ 7.1.0 This example demonstrates how to add UserDefRelationship between glossary terms. While this relationship type is currently visible in the Atlan UI for glossary terms, you can create any other supported relationship types that have attributes ( as listed above ) between any asset types using similar steps shown in the snippet below. All these relationships will be persisted in the backend (metastore) even if they're not currently visible in the Atlan UI. Python Java Raw REST API Add user-defined relationship between terms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm from pyatlan.model.assets.relations import UserDefRelationship client = AtlanClient () # Create updater for the source term term1_to_update = AtlasGlossaryTerm . updater ( # (1) qualified_name = \"FpWBfqOfP0qZQ6tCpK10n@nrrnNyRABZTc6CEKwHh73\" , name = \"Policy\" , glossary_guid = \"55aaaa56-d8cd-4f19-8026-10d511a9b071\" ) # Create reference to the target term term2 = AtlasGlossaryTerm . ref_by_guid ( \"f558a01a-2e16-440c-ba2d-fed2099e540a\" ) # (2) # Create the user-defined relationship with attributes udr = UserDefRelationship ( # (3) from_type_label = \"Sold by\" , to_type_label = \"Sells\" ) # Build and assign the relationship term1_to_update . user_def_relationship_to = [ # (4) udr . user_def_relationship_to ( term2 ) ] # Save the relationship response = client . asset . save ( term1_to_update ) # (5) Create an updater for the source term using .updater() with the qualified_name , name , and glossary_guid of the term you want to add the relationship to. Create a reference to the target term using ref_by_guid() or ref_by_qualified_name() . Define the relationship by creating a UserDefRelationship object with attributes like from_type_label and to_type_label . Build the relationship using the user_def_relationship_to() method and assign it to the user_def_relationship_to attribute. Save the changes using client.asset.save() to persist the relationship in Atlan. Coming soon POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"entities\" : [ { \"typeName\" : \"AtlasGlossaryTerm\" , \"attributes\" : { \"name\" : \"Policy\" , // (1) \"qualifiedName\" : \"FpWBfqOfP0qZQ6tCpK10n@nrrnNyRABZTc6CEKwHh73\" , \"anchor\" : { \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"55aaaa56-d8cd-4f19-8026-10d511a9b071\" }, \"userDefRelationshipTo\" : [ { \"typeName\" : \"AtlasGlossaryTerm\" , // (2) \"guid\" : \"f558a01a-2e16-440c-ba2d-fed2099e540a\" , \"relationshipAttributes\" : { \"typeName\" : \"UserDefRelationship\" , // (3) \"attributes\" : { \"toTypeLabel\" : \"Sells\" , \"fromTypeLabel\" : \"Sold by\" } }, \"relationshipType\" : \"UserDefRelationship\" } ] } } ] } Provide source asset details: Include the required attributes name , qualifiedName , and glossary guid for the term you want to add the relationship to. Reference the target asset: Create a reference to the target term using its guid . Define relationship attributes: Provide the necessary attributes for the UserDefRelationship , including toTypeLabel and fromTypeLabel . Relationship visibility User-defined relationships between glossary terms are visible in the Atlan UI. For other asset types, relationships are stored in the backend but may not be visible in the UI until support is added. Remove user-defined relationship Â¶ 7.1.0 This example demonstrates how to remove UserDefRelationship between glossary terms. You can use the same approach to remove any attributed relationship type ( as listed above ) between any asset types using the steps shown in the snippet below. Python Java Raw REST API Remove user-defined relationship between terms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm client = AtlanClient () # Create updater for the source term term1_to_update = AtlasGlossaryTerm . updater ( # (1) qualified_name = \"FpWBfqOfP0qZQ6tCpK10n@nrrnNyRABZTc6CEKwHh73\" , name = \"Policy\" , glossary_guid = \"55aaaa56-d8cd-4f19-8026-10d511a9b071\" ) # Remove all outgoing relationships by setting to empty list term1_to_update . user_def_relationship_to = [] # (2) # Save the changes to remove the relationship response = client . asset . save ( term1_to_update ) # (3) Create an updater for the source term using .updater() with the qualified_name , name , and glossary_guid of the term you want to remove relationships from. Clear relationships by assigning an empty list [] to user_def_relationship_to to remove all existing outgoing relationships. Save the changes using client.asset.save() to persist the removal in Atlan. Coming soon POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"entities\" : [ { \"typeName\" : \"AtlasGlossaryTerm\" , \"attributes\" : { \"anchor\" : { // (1) \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"55aaaa56-d8cd-4f19-8026-10d511a9b071\" }, \"name\" : \"Policy\" , \"qualifiedName\" : \"FpWBfqOfP0qZQ6tCpK10n@nrrnNyRABZTc6CEKwHh73\" , \"userDefRelationshipTo\" : [] // (2) } } ] } Provide source asset details: Include the required attributes name , qualifiedName , and glossary guid for the term you want to add the relationship to. Clear relationships by assigning an empty array [] to userDefRelationshipTo to remove all existing outgoing relationships. Complete removal Setting user_def_relationship_to = [] removes all outgoing user-defined relationships from the asset. To remove only specific relationships while keeping others, you would need to retrieve the existing relationships first and reconstruct the list without the unwanted ones. Retrieve user-defined relationship Â¶ 7.1.0 This example demonstrates how to retrieve UserDefRelationship between glossary terms. You can use the same approach to retrieve any attributed relationship type ( as listed above ) between any asset types. While UserDefRelationship is visible in the UI for glossary terms, all relationships are persisted in the backend regardless of UI visibility. Python Java Raw REST API Retrieve user-defined relationships between terms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm from pyatlan.model.assets.relations import UserDefRelationship from pyatlan.model.query import FluentSearch client = AtlanClient () # Build search request for both terms in the relationship request = ( FluentSearch () . select () . where_some ( AtlasGlossaryTerm . GUID . eq ( \"f558a01a-2e16-440c-ba2d-fed2099e540a\" )) # (1) . where_some ( AtlasGlossaryTerm . GUID . eq ( \"540ee0f6-bb26-4b1a-88b7-31cfc26746b4\" )) # (2) . include_on_results ( AtlasGlossaryTerm . USER_DEF_RELATIONSHIP_TO ) # (3) . include_on_results ( AtlasGlossaryTerm . USER_DEF_RELATIONSHIP_FROM ) # (4) . include_relationship_attributes ( True ) # (5) . to_request () ) # Execute the search results = client . asset . search ( request ) # (6) assert results and results . count == 2 # (7) # Access the source term (with outgoing relationship) source_term = results . current_page ()[ 0 ] # (8) if source_term . user_def_relationship_to : relationship = source_term . user_def_relationship_to [ 0 ] print ( f \"Source term GUID: { relationship . guid } \" ) print ( f \"Relationship type: { relationship . type_name } \" ) print ( f \"Relationship attributes: { relationship . attributes . relationship_attributes . attributes } \" ) # Access the target term (with incoming relationship) target_term = results . current_page ()[ 1 ] if target_term . user_def_relationship_from : relationship = target_term . user_def_relationship_from [ 0 ] print ( f \"Target term GUID: { relationship . guid } \" ) print ( f \"Relationship type: { relationship . type_name } \" ) print ( f \"Relationship attributes: { relationship . attributes . relationship_attributes . attributes } \" ) Search for source term: Provide the GUID of the source term (relationship origin - USER_DEF_RELATIONSHIP_TO ). Search for target term: Provide the GUID of the target term (relationship destination - USER_DEF_RELATIONSHIP_FROM ). Include outgoing relationships: Ensure results include the USER_DEF_RELATIONSHIP_TO attribute. Include incoming relationships: Ensure results include the USER_DEF_RELATIONSHIP_FROM attribute. Include relationship attributes: Set to True to retrieve attributes for each relationship. Execute search: Run the search request using client.asset.search() . Verify results: Since we're retrieving two specific terms with relationships, results.count should be 2 . Access results: Iterate through results or use current_page()[index] to access specific terms. Coming soon POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 { \"attributes\" : [ \"userDefRelationshipTo\" , // (1) \"userDefRelationshipFrom\" // (2) ], \"dsl\" : { \"from\" : 0 , \"size\" : 100 , \"aggregations\" : {}, \"track_total_hits\" : true , \"query\" : { \"bool\" : { \"must\" : [ { \"term\" : { \"__superTypeNames.keyword\" : { \"value\" : \"Referenceable\" } } } ], \"should\" : [ { \"term\" : { \"__guid\" : { \"value\" : \"f558a01a-2e16-440c-ba2d-fed2099e540a\" , // (3) \"case_insensitive\" : false } } }, { \"term\" : { \"__guid\" : { \"value\" : \"540ee0f6-bb26-4b1a-88b7-31cfc26746b4\" , // (4) \"case_insensitive\" : false } } } ], \"filter\" : [ { \"term\" : { \"__superTypeNames.keyword\" : { \"value\" : \"Asset\" } } }, { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { \"__superTypeNames.keyword\" : { \"value\" : \"Referenceable\" } } } ], \"minimum_should_match\" : 1 } }, \"sort\" : [ { \"__guid\" : { \"order\" : \"asc\" } } ] }, \"relationAttributes\" : [ \"name\" ], \"includeRelationshipAttributes\" : true // (5) } Include outgoing relationships: Ensure results include the USER_DEF_RELATIONSHIP_TO attribute. Include incoming relationships: Ensure results include the USER_DEF_RELATIONSHIP_FROM attribute. Search for source term: Provide the GUID of the source term (relationship origin - USER_DEF_RELATIONSHIP_TO ). Search for target term: Provide the GUID of the target term (relationship destination - USER_DEF_RELATIONSHIP_FROM ). Include relationship attributes: Set to true to retrieve attributes for each relationship. Relationship direction USER_DEF_RELATIONSHIP_TO : Outgoing relationships from this asset USER_DEF_RELATIONSHIP_FROM : Incoming relationships to this asset Accessing attributes Relationship attributes are nested under relationship.attributes.relationship_attributes.attributes . Make sure to include include_relationship_attributes(True) in your search to retrieve these values. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/resources/",
    "content": "/api/meta/entity/bulk (POST) Add asset resources / links Â¶ Resources (links) can only be added to assets after an asset exists. (The asset itself must be created first.) Add to an existing asset Â¶ 2.0.0 4.0.0 Each resource can be assigned to only a single asset. To create a resource and assign it to an asset: dbt Java Python Kotlin Raw REST API Managing resources for assets is currently not possible via dbt. Add to an existing asset 1 2 3 4 5 6 7 8 9 Table table = Table . refByQualifiedName ( \"default/snowflake/1234567890/reln_db/reln_schema/customers\" ); // (1) Link link = Link . creator ( // (2) table , // (3) \"Definition\" , // (4) \"https://en.wikipedia.org/wiki/Customer\" ) // (5) . build (); AssetMutationResponse response = link . save ( client ); // (6) assert response . getCreatedAssets (). size () == 1 // (7) assert response . getUpdatedAssets (). size () == 1 // (8) Set up a reference to the asset you want to assign the resource to from somewhere. In this example, we just create a reference based on the qualifiedName of the asset we want, but this could also be from the result of a search, for example. Use the creator() method to initialize the link with all necessary attributes for creating it . In the case of a link, you need to give a reference to an asset to which you want to attach the link. And the title Atlan should display for the link. And finally the URL for the link itself. Call the save() method to actually create the link and attach it to the asset. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created (the link). The response will also include a single asset that was updated (the asset to which you've attached the link). Add to an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Link , Table client = AtlanClient () table = Table . ref_by_qualified_name ( \"default/snowflake/1234567890/reln_db/reln_schema/customers\" ) # (1) link = Link . creator ( # (2) asset = table , # (3) name = \"Definition\" , # (4) link = \"https://en.wikipedia.org/wiki/Customer\" ) # (5) response = client . asset . save ( link ) # (6) assert ( links := response . assets_created ( asset_type = Link )) # (7) assert ( tables := response . assets_updated ( asset_type = Table )) # (8) Set up a reference to the asset you want to assign the resource to from somewhere. In this example, we just create a reference based on the qualified_name of the asset we want, but this could also be from the result of a search, for example. Use the create() method to build a link with all necessary attributes for creating it . In the case of a link, you need to give a reference to an asset to which you want to attach the link. And the title Atlan should display for the link. And finally the URL for the link itself. Call the save() method to actually create the link and attach it to the asset. Assert that the link was created. Assert a table was updated (the asset to which you've attached the link). Add to an existing asset 1 2 3 4 5 6 7 8 9 val table = Table . refByQualifiedName ( \"default/snowflake/1234567890/reln_db/reln_schema/customers\" ) // (1) val link = Link . creator ( // (2) table , // (3) \"Definition\" , // (4) \"https://en.wikipedia.org/wiki/Customer\" ) // (5) . build () val response = link . save ( client ) // (6) assert ( response . createdAssets . size == 1 ) // (7) assert ( response . updatedAssets . size == 1 ) // (8) Set up a reference to the asset you want to assign the resource to from somewhere. In this example, we just create a reference based on the qualifiedName of the asset we want, but this could also be from the result of a search, for example. Use the creator() method to initialize the link with all necessary attributes for creating it . In the case of a link, you need to give a reference to an asset to which you want to attach the link. And the title Atlan should display for the link. And finally the URL for the link itself. Call the save() method to actually create the link and attach it to the asset. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created (the link). The response will also include a single asset that was updated (the asset to which you've attached the link). Note that you are actually creating a new link asset When adding a link through the API, you are really creating a new instance of a link asset. At the same time, you're attaching this new object to an existing asset. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \"entities\" : [ // (1) { \"typeName\" : \"Link\" , // (2) \"attributes\" : { \"name\" : \"Definition\" , // (3) \"qualifiedName\" : \"540182b5-f47b-4ba1-a247-6ca9ffd9f37a\" , // (4) \"link\" : \"https://en.wikipedia.org/wiki/Customer\" , // (5) \"asset\" : [ // (6) { \"typeName\" : \"Table\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1234567890/reln_db/reln_schema/customers\" } } ] } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the link asset, which will always be Link (case-sensitive). You must also provide a name for the link. This will show up on the UI as the title of the link. You must also provide a unique qualifiedName for the link. This will not show up on the UI, and must be a unique UUID (generated yourself). The URL for the link should be provided in the link field. Finally, you need to include the reference information for the asset the link should be attached to. Remove from an existing asset Â¶ To remove a link from an existing asset you only need to delete the link itself. (The link is itself an asset.) See Deleting an asset . The link will have its own GUID, separate from the asset to which it is attached When deleting the link, you need to use the link's GUID, not the GUID of the asset to which it is attached. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/term-assignment/",
    "content": "/api/meta/entity/bulk (POST) Link terms and assets Â¶ Append terms to an asset Â¶ 1.4.0 4.0.0 To append new terms to an asset , without changing any of the existing terms on the asset: dbt Java Python Kotlin Raw REST API This is currently not possible via dbt, term assignments are replaced rather than appended. Append terms to an asset 1 2 3 4 5 6 Column column = Column . appendTerms ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (3) List . of ( // (4) GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ), GlossaryTerm . refByGuid ( \"b267858d-8316-4c41-a56a-6e9b840cef4a\" ))); Use the appendTerms() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to add the terms all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset on which to add the terms. A list of term references. Each reference can be to either a term by its GUID or its qualifiedName . At the completion of this code, the terms in this list will be added to any other terms that are already linked to the asset. Append terms to an asset 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm , Column client = AtlanClient () column = client . asset . append_terms ( # (1) asset_type = Column , # (2) qualified_name = \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , # (3) terms = [ AtlasGlossaryTerm . ref_by_guid ( guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" ), # (4) AtlasGlossaryTerm . ref_by_guid ( guid = \"b267858d-8316-4c41-a56a-6e9b840cef4a\" )] ) # (5) Use the asset.append_terms() method, which will construct the necessary\nrequest and call the necessary API(s) to add the terms all-in-one. The asset_type of the asset on which to add the terms. The qualified_name of the asset on which to add the terms. Note: Alternatively the parameter name guid could have been\nspecified along with the guid of the asset on which to add the terms. A list of term references. Each reference can be to either a term by its GUID\nor its qualified_name . At the completion of this code, the terms in this list will\nbe added to any other terms that are already linked to the asset. The asset returned by this call will be a mininmal asset and will not contain\nany terms . If you need an asset which contains the terms retrieve it via\nthe asset.get_by_guid or asset.get_by_qualified_name methods. Append terms to an asset 1 2 3 4 5 6 val column = Column . appendTerms ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (3) listOf ( // (4) GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ), GlossaryTerm . refByGuid ( \"b267858d-8316-4c41-a56a-6e9b840cef4a\" ))) Use the appendTerms() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to add the terms all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset on which to add the terms. A list of term references. Each reference can be to either a term by its GUID or its qualifiedName . At the completion of this code, the terms in this list will be added to any other terms that are already linked to the asset. Requires multiple steps through the raw REST API Retrieve the existing asset . Iterate through the assigned terms that exist on the asset to build up a temporary list, including only those that are not deleted. Add the terms you want to append to the list built-up in (2). Send through the resulting complete list, as in the example below. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"entities\" : [ // (1) { \"typeName\" : \"Column\" , // (2) \"attributes\" : { \"name\" : \"CUSTOMER_NAME\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (4) \"meanings\" : [ // (5) { ... }, // (6) { \"typeName\" : \"AtlasGlossaryTerm\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" }, { \"typeName\" : \"AtlasGlossaryTerm\" , \"guid\" : \"b267858d-8316-4c41-a56a-6e9b840cef4a\" } ] } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). Provide the details of the terms to assign to the asset in the meanings array. Each reference to a term must include the typeName (always AtlasGlossaryTerm ) and guid for the term. Remember you will need to first retrieve the existing asset to retrieve the full set of existing term assignments to append onto. Replace terms on an asset Â¶ 1.4.0 4.0.0 To replace all the terms on an asset, meaning any not specified in the request will be removed from the asset: dbt Java Python Kotlin Raw REST API Replace terms on an asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 models : - name : ORDER_ANALYSIS # (1) columns : - name : CUSTOMER_NAME # (2) meta : atlan : termGUIDs : # (3) - \"b4113341-251b-4adc-81fb-2420501c30e6\" - \"b267858d-8316-4c41-a56a-6e9b840cef4a\" termQualifiedNames : # (4) - \"SepizCqzgygmdTvVk5a9i@yJuFhD0LiU1QDl5YwXiQy\" - \"BfoxTP4209kT5zZFKPKqZ@yJuFhD0LiU1QDl5YwXiQy\" termNames : # (5) - \"Customer Name\" - \"Data Governance@Full Name\" - \"Finance@Marketing Budget\" You must of course give the name of the object. If you are applying the terms to a column, you need to give the name of the column as well. You can either specify the terms as a list of GUIDs (each GUID refers to one term). Or you can specify the terms as a list of qualifiedNames. Or you can specify the terms as a list of human-readable names. Handling duplicate term names You can disambiguate terms with the same name across different glossaries using the format glossaryName@termName (e.g., \"Data Governance@Full Name\" ). Use glossaryName@termName in termNames , when term names aren't unique across glossaries. Replace terms on an asset 1 2 3 4 5 6 7 Column column = Column . replaceTerms ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (3) \"CUSTOMER_NAME\" , // (4) List . of ( // (5) GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ), GlossaryTerm . refByGuid ( \"b267858d-8316-4c41-a56a-6e9b840cef4a\" ))); Use the replaceTerms() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to replace the terms on the asset all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset for which to replace the terms. The human-readable name of the asset for which to replace the terms. A list of term references. Each reference can be to either a term by its GUID or its qualifiedName . After the completion of this code, only the terms in this list will be assigned to the asset. Replace terms to an asset 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm , Column client = AtlanClient () column = client . asset . replace_terms ( # (1) asset_type = Column , # (2) qualified_name = \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , # (3) terms = [ AtlasGlossaryTerm . ref_by_guid ( guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" ), # (4) AtlasGlossaryTerm . ref_by_guid ( guid = \"b267858d-8316-4c41-a56a-6e9b840cef4a\" )] ) # (5) Use the asset.replace_terms() method, which will construct the necessary\nrequest and call the necessary API(s) to replace the terms all-in-one. The asset_type of the asset on which to replace the terms. The qualified_name of the asset on which to replace the terms. Note: Alternatively the parameter name guid could have been\nspecified along with the guid of the asset on which to replace the terms. A list of term references. Each reference can be to either a term by its GUID\nor its qualified_name . At the completion of this code, the terms in this list wil replace any other terms that are already linked to the asset. The asset returned by this call will be a mininmal asset and will not contain\nany terms . If you need an asset which contains the terms retrieve it via the asset.get_by_guid or asset.get_by_qualified_name methods. Replace terms on an asset 1 2 3 4 5 6 7 val column = Column . replaceTerms ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (3) \"CUSTOMER_NAME\" , // (4) listOf ( // (5) GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ), GlossaryTerm . refByGuid ( \"b267858d-8316-4c41-a56a-6e9b840cef4a\" ))) Use the replaceTerms() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to replace the terms on the asset all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset for which to replace the terms. The human-readable name of the asset for which to replace the terms. A list of term references. Each reference can be to either a term by its GUID or its qualifiedName . After the completion of this code, only the terms in this list will be assigned to the asset. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"entities\" : [ // (1) { \"typeName\" : \"Column\" , // (2) \"attributes\" : { \"name\" : \"CUSTOMER_NAME\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (4) \"meanings\" : [ // (5) { \"typeName\" : \"AtlasGlossaryTerm\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" }, { \"typeName\" : \"AtlasGlossaryTerm\" , \"guid\" : \"b267858d-8316-4c41-a56a-6e9b840cef4a\" } ] } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). Provide the details of the terms to assign to the asset in the meanings array. Each reference to a term must include the typeName (always AtlasGlossaryTerm ) and guid for the term. After the completion of this code, only the terms in this list will be assigned to the asset. Remove terms from an asset Â¶ 1.4.0 4.0.0 To remove some terms from an asset, without removing all of the terms on the asset: dbt Java Python Kotlin Raw REST API This is currently not possible via dbt, term assignments are replaced rather than selectively removed. Remove terms from an asset 1 2 3 4 5 Column column = Column . removeTerms ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (3) List . of ( // (4) GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ))); Use the removeTerms() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to remove the terms all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset from which to remove the terms. A list of term references. Each reference must be to a term by its GUID. At the completion of this code, the terms in this list will be removed from those linked to the asset. Remove terms from an asset 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm , Column client = AtlanClient () column = client . asset . remove_terms ( # (1) asset_type = Column , # (2) qualified_name = \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , # (3) terms = [ AtlasGlossaryTerm . ref_by_guid ( guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" )] # (4) ) Use the asset.remove_terms() method, which will construct the necessary\nrequest and call the necessary API(s) to remove the terms on the asset all-in-one. The asset_type of the asset on which to remove the terms. The qualified_name of the asset on which to remove the terms. Note: Alternatively the parameter name guid could have been \nspecified along with the guid of the asset on which to remove the terms. A list of term references. Each reference must be to a term by its GUID.\nAt the completion of this code, the terms in this list will be removed from those linked to the asset. Remove terms from an asset 1 2 3 4 5 val column = Column . removeTerms ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (3) listOf ( // (4) GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ))) Use the removeTerms() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to remove the terms all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset from which to remove the terms. A list of term references. Each reference must be to a term by its GUID. At the completion of this code, the terms in this list will be removed from those linked to the asset. Requires multiple steps through the raw REST API Retrieve the existing asset . Iterate through the assigned terms that exist on the asset to build up a temporary list, excluding any that you want to remove. Send through the resulting reduced list, as in the example below. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \"entities\" : [ // (1) { \"typeName\" : \"Column\" , // (2) \"attributes\" : { \"name\" : \"CUSTOMER_NAME\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (4) \"meanings\" : [ // (5) { ... }, // (6) { \"typeName\" : \"AtlasGlossaryTerm\" , \"guid\" : \"b267858d-8316-4c41-a56a-6e9b840cef4a\" } ] } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). Provide the details of the terms to assign to the asset in the meanings array. Each reference to a term must include the typeName (always AtlasGlossaryTerm ) and guid for the term. Remember you will need to first retrieve the existing asset to retrieve the reduced set of term assignemnts that should remain, which should not include the ones you want to remove. Remove all terms from an asset Â¶ 1.4.0 4.0.0 To remove all terms linked to an asset: dbt Java Python Kotlin Raw REST API Remove all terms from an asset 1 2 3 4 5 6 7 models : - name : ORDER_ANALYSIS # (1) columns : - name : CUSTOMER_NAME # (2) meta : atlan : termGUIDs : [] # (3) You must of course give the name of the object. If you are applying the terms to a column, you need to give the name of the column as well. If you send an explicit empty list ( [] ) as the list of GUIDs this will remove all terms from the asset. Remove all terms from an asset 1 2 3 4 5 Column column = Column . replaceTerms ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (3) \"CUSTOMER_NAME\" , // (4) null ); // (5) Use the replaceTerms() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to replace (remove) the terms on the asset all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset from which to remove the terms. The human-readable name of the asset from which to remove the terms. A null as the list of term references. This will replace any existing terms on the asset with no terms at all â€” in other words, it will remove all terms from the asset. Remove all terms from an asset 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm , Column client = AtlanClient () column = client . asset . replace_terms ( # (1) asset_type = Column , # (2) qualified_name = \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , # (3) terms = [] # (4) ) Use the asset.replace_terms() method, which will construct the necessary\nrequest and call the necessary API(s) to replace (remove) the terms on the asset all-in-one. The asset_type of the asset on which to remove the terms. The qualified_name of the asset on which to remove the terms. Note: Alternatively the parameter name guid could have been\nspecified along with the guid of the asset on which to remove the terms. An empty list of term references. This will replace any existing terms\non the asset with no terms at all â€” in other words, it will remove all terms from the asset. Remove all terms from an asset 1 2 3 4 5 val column = Column . replaceTerms ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (3) \"CUSTOMER_NAME\" , // (4) null ) // (5) Use the replaceTerms() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to replace (remove) the terms on the asset all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset from which to remove the terms. The human-readable name of the asset from which to remove the terms. A null as the list of term references. This will replace any existing terms on the asset with no terms at all â€” in other words, it will remove all terms from the asset. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ // (1) { \"typeName\" : \"Column\" , // (2) \"attributes\" : { \"name\" : \"CUSTOMER_NAME\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DATA/FOOD_BEVERAGE/ORDER_ANALYSIS/CUSTOMER_NAME\" , // (4) \"meanings\" : [] // (5) } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). To remove all terms, send an empty array ( [] ) for the meanings array. When creating an asset Â¶ 2.0.0 4.0.0 To link terms when creating an asset: dbt Java Python Kotlin Raw REST API Link terms when creating asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 models : - name : ORDER_ANALYSIS # (1) columns : - name : CUSTOMER_NAME # (2) meta : atlan : termGUIDs : # (3) - \"b4113341-251b-4adc-81fb-2420501c30e6\" - \"b267858d-8316-4c41-a56a-6e9b840cef4a\" termQualifiedNames : # (4) - \"SepizCqzgygmdTvVk5a9i@yJuFhD0LiU1QDl5YwXiQy\" - \"BfoxTP4209kT5zZFKPKqZ@yJuFhD0LiU1QDl5YwXiQy\" termNames : # (5) - \"Customer Name\" - \"Data Governance@Full Name\" - \"Finance@Marketing Budget\" You must of course give the name of the object. If you are applying the terms to a column, you need to give the name of the column as well. You can either specify the terms as a list of GUIDs (each GUID refers to one term). Or you can specify the terms as a list of qualifiedNames. Or you can specify the terms as a list of human-readable names. Handling duplicate term names You can disambiguate terms with the same name across different glossaries using the format glossaryName@termName (e.g., \"Data Governance@Full Name\" ). Use glossaryName@termName in termNames , when term names aren't unique across glossaries. Link terms when creating asset 1 2 3 4 5 6 7 8 9 10 S3Object s3Object = S3Object . creator ( \"sample-file.csv\" , // (1) S3Bucket . refByGuid ( \"8aa53eb2-3630-4de0-81e1-d57922f43336\" ), \"aws::test:samples-bucket:a/prefix/sample-file.csv\" ) . assignedTerm ( GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" )) // (2) . assignedTerm ( GlossaryTerm . refByGuid ( \"b267858d-8316-4c41-a56a-6e9b840cef4a\" )) . build (); AssetMutationResponse response = s3Object . save ( client ); // (3) assert response . getCreatedAssets (). size () == 1 ; // (4) assert response . getUpdatedAssets (). size () == 3 ; // (5) Use the creator() method to initialize the object with all necessary attributes for creating it . Directly chain the assignedTerm enrichment methods to add the linked terms. Note that we only need a Reference to the linked term, in these examples the type and GUID of the term. Call the save() method to actually create the asset (with its linked terms). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created... ... and the two linked terms that were updated, along with the bucket the object is created within. Link terms when creating asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm , S3Object client = AtlanClient () s3_object = S3Object . creator ( # (1) name = \"sample-file.csv\" , connection_qualified_name = \"default/s3/1661068484\" , aws_arn = \"aws::test:samples-bucket:a/prefix/sample-file.csv\" ) s3_object . assigned_terms = [ AtlasGlossaryTerm . ref_by_guid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ), # (2) AtlasGlossaryTerm . ref_by_guid ( \"b267858d-8316-4c41-a56a-6e9b840cef4a\" )] response = client . asset . save ( s3_object ) # (3) assert ( s3_objects_added := response . assets_created ( S3Object )) assert len ( s3_objects_added ) == 1 # (4) assert ( terms_updated := response . assets_updated ( AtlasGlossaryTerm )) assert len ( terms_updated ) == 2 # (5) Use the create() method to initialize the object with all necessary attributes for creating it . Provide a list of the terms to be linked to the asset . Note that we only need a Reference to the linked term, in these examples the type and GUID of the term. Call the save() method to actually create the asset (with its linked terms). The response will include the single asset created ... and the two linked terms that were updated. Link terms when creating asset 1 2 3 4 5 6 7 8 9 10 val s3Object = S3Object . creator ( \"sample-file.csv\" , // (1) S3Bucket . refByGuid ( \"8aa53eb2-3630-4de0-81e1-d57922f43336\" ), \"aws::test:samples-bucket:a/prefix/sample-file.csv\" ) . assignedTerm ( GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" )) // (2) . assignedTerm ( GlossaryTerm . refByGuid ( \"b267858d-8316-4c41-a56a-6e9b840cef4a\" )) . build () val response = s3Object . save ( client ) // (3) assert ( response . createdAssets . size == 1 ) // (4) assert ( response . updatedAssets . size == 3 ) // (5) Use the creator() method to initialize the object with all necessary attributes for creating it . Directly chain the assignedTerm enrichment methods to add the linked terms. Note that we only need a Reference to the linked term, in these examples the type and GUID of the term. Call the save() method to actually create the asset (with its linked terms). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created... ... and the two linked terms that were updated, along with the bucket the object is created within. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { \"entities\" : [ // (1) { \"typeName\" : \"S3Object\" , // (2) \"attributes\" : { \"name\" : \"sample-file.csv\" , // (3) \"qualifiedName\" : \"default/s3/1661068484/aws::test:samples-bucket:a/prefix/sample-file.csv\" , // (4) \"awsArn\" : \"aws::test:samples-bucket:a/prefix/sample-file.csv\" , \"meanings\" : [ // (5) { \"typeName\" : \"AtlasGlossaryTerm\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" }, { \"typeName\" : \"AtlasGlossaryTerm\" , \"guid\" : \"b267858d-8316-4c41-a56a-6e9b840cef4a\" } ] } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). Provide the details of the terms to assign to the asset in the meanings array. Each reference to a term must include the typeName (always AtlasGlossaryTerm ) and guid for the term. After the completion of this code, only the terms in this list will be assigned to the asset. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/tags/",
    "content": "/api/meta/entity/bulk (POST) /api/meta/entity/uniqueAttribute/type/{typeName}/classification/{name}?attr:qualifiedName={qualifiedName} (DELETE) /api/meta/entity/uniqueAttribute/type/{typeName}/classifications?attr:qualifiedName={qualifiedName} (POST) Tag (classify) assets Â¶ Atlan tags must exist before tagging assets Remember that you must first create the Atlan tag before you will be able to tag any assets. Cannot add tags when creating assets Currently it is not possible to add tags when creating assets , other than via dbt. Add to an existing asset Â¶ 6.2.0 4.0.0 To add tags to an existing asset: dbt Java Python Kotlin Raw REST API Add tags to an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : classificationNames : # (2) - PII # (3) - Marketing Analysis classificationNames : # (4) - name : PII # (5) propagate : true # (6) removePropagationsOnEntityDelete : true # (7) restrictPropagationThroughLineage : false # (8) restrictPropagationThroughHierarchy : false # (9) - name : Marketing Analysis propagate : true removePropagationsOnEntityDelete : true restrictPropagationThroughLineage : false restrictPropagationThroughHierarchy : false classifications : # (10) - typeName : yQBDoKHdTLJhqAsdR3RMq6 # (11) propagate : true removePropagationsOnEntityDelete : true restrictPropagationThroughLineage : false restrictPropagationThroughHierarchy : false - typeName : WCVjmgKnW40G151dESXZ03 propagate : true removePropagationsOnEntityDelete : true restrictPropagationThroughLineage : false restrictPropagationThroughHierarchy : false You must of course give the name of the object. The simplest way to tag an asset, using the default values for propagation (those shown below), is to use the meta . atlan . classificationNames structure. When using this simplified form, you can give the normal human-readable name of the tags rather than the hashed-string representation . Alternatively, if you want to override the propagation settings, you can use this more detailed structure. Each listed item must itself be a YAML object consisting of the human-readable name of the tag and the propagation setting overrides: (Optional) You can decide whether to propagate this tag (true) or not (false). If you choose false, no propagation of this tag from the asset will occur â€” neither through lineage nor parent-child relationships. (Optional) If propagation is allowed, you can then define whether propagated tags should be removed if this asset is deleted (true) or not (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for lineage (true) or still allow it through lineage (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for hierarchy (true) or still allow it through hierarchy (false). Alternatively, you can specify tags nested within the meta . atlan . classifications structure. In this structure, each tag you want to add must be given using its hashed-string representation . Its propagation settings can be overridden using the same options described above. Replaces all tags Unlike the examples for the SDKs and raw APIs, dbt will always replace all tags on the asset. Any tags that already exist on the asset that are not specified here will be removed. Add tags to an existing asset 1 2 3 4 Table . appendAtlanTags ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) List . of ( \"PII\" , \"Marketing Analysis\" )); // (4) Use the appendAtlanTags() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to add the tags all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset. A list of the tags (the names as you set them up in the UI) to add to the asset. Add tags to an existing asset 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () client . asset . add_atlan_tags ( # (1) asset_type = Table , qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , atlan_tag_names = [ \"PII\" , \"Marketing Analysis\" ], # (2) ) Use the asset.add_atlan_tags() method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to add the tags all-in-one. A list of the tags (the names as you set them up in the UI) to add to the asset. Add tags to an existing asset 1 2 3 4 val table = Table . appendAtlanTags ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) listOf ( \"PII\" , \"Marketing Analysis\" )) // (4) Use the appendAtlanTags() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to add the tags all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset. A list of the tags (the names as you set them up in the UI) to add to the asset. POST /api/meta/entity/bulk?replaceTags=false&appendTags=true&replaceBusinessAttributes=false&overwriteBusinessAttributes=false 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/snowflake/1746022526/WIDE_WORLD_IMPORTERS/BRONZE_WAREHOUSE/FIVETRAN_AUDIT\" , // (3) \"name\" : \"FIVETRAN_AUDIT\" // (4) }, \"addOrUpdateClassifications\" : [ { \"typeName\" : \"VfsfmLbnuxc2vdNJ0Ysh\" , // (5) \"propagate\" : false , // (6) \"removePropagationsOnEntityDelete\" : true , // (7) \"restrictPropagationThroughLineage\" : false , // (8) \"restrictPropagationThroughHierarchy\" : true // (9) }, { \"typeName\" : \"RsCmLbnuxc2vdNJ234Ysh\" , \"propagate\" : false , \"removePropagationsOnEntityDelete\" : true , \"restrictPropagationThroughLineage\" : false , \"restrictPropagationThroughHierarchy\" : true } ] } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). Each tag you want to add must be given using its hashed-string representation . (Optional) You can decide whether to propagate this tag (true) or not (false). If you choose false, no propagation of this tag from the asset will occur â€” neither through lineage nor parent-child relationships. (Optional) If propagation is allowed, you can then define whether propagated tags should be removed if this asset is deleted (true) or not (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for lineage (true) or still allow it through lineage (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for hierarchy (true) or still allow it through hierarchy (false). Update on an existing asset Â¶ 6.2.0 4.0.0 To update tags on an existing asset: dbt Java Python Kotlin Raw REST API Not possible through dbt In dbt, the tags will be replaced in their entirety. It is not possible to just update a single tag through dbt. Update tags on an existing asset 1 2 3 4 5 6 7 8 9 Table . updateAtlanTags ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) List . of ( \"PII\" , \"Marketing Analysis\" ), // (4) true , // (5) true , // (6) false , // (7) false // (8) ); Use the updateAtlanTags() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to update tags for an asset, all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset. A list of the tags (the names as you set them up in the UI) to update for the asset. (Optional) You can decide whether to propagate these tags (true) or not (false). If you choose false, no propagation for these tags from the asset will occur â€” neither through lineage nor parent-child relationships. (Optional) If propagation is allowed, you can then define whether propagated tags should be removed if this asset is deleted (true) or not (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tags only for lineage (true) or still allow it through lineage (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for hierarchy (true) or still allow it through hierarchy (false). Update tags on an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () client . asset . update_atlan_tags ( # (1) asset_type = Table , qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , atlan_tag_names = [ \"PII\" , \"Marketing Analysis\" ], # (2) True , # (3) True , # (4) False , # (5) False , # (6) ) Use the asset.update_atlan_tags() method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to update tags for an asset, all-in-one. A list of the tags (the names as you set them up in the UI) to update for the asset. (Optional) You can decide whether to propagate these tags (True) or not (False). If you choose False, no propagation for these tags from the asset will occur â€” neither through lineage nor parent-child relationships. (Optional) If propagation is allowed, you can then define whether propagated tags should be removed if this asset is deleted (True) or not (False). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tags only for lineage (True) or still allow it through lineage (False). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for hierarchy (True) or still allow it through hierarchy (False). Update tags on an existing asset 1 2 3 4 5 6 7 8 9 val table = Table . updateAtlanTags ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) listOf ( \"PII\" , \"Marketing Analysis\" ), // (4) true , // (5) true , // (6) false , // (7) false // (8) ) Use the updateAtlanTags() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to update tags for an asset, all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset. A list of the tags (the names as you set them up in the UI) to update for the asset. (Optional) You can decide whether to propagate these tags (true) or not (false). If you choose false, no propagation for these tags from the asset will occur â€” neither through lineage nor parent-child relationships. (Optional) If propagation is allowed, you can then define whether propagated tags should be removed if this asset is deleted (true) or not (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tags only for lineage (true) or still allow it through lineage (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for hierarchy (true) or still allow it through hierarchy (false). POST /api/meta/entity/bulk?replaceTags=false&appendTags=true&replaceBusinessAttributes=false&overwriteBusinessAttributes=false 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/snowflake/1746022526/WIDE_WORLD_IMPORTERS/BRONZE_WAREHOUSE/FIVETRAN_AUDIT\" , // (3) \"name\" : \"FIVETRAN_AUDIT\" // (4) }, \"addOrUpdateClassifications\" : [ { \"typeName\" : \"VfsfmLbnuxc2vdNJ0Ysh\" , // (5) \"propagate\" : false , // (6) \"removePropagationsOnEntityDelete\" : false , // (7) \"restrictPropagationThroughLineage\" : false , // (8) \"restrictPropagationThroughHierarchy\" : false // (9) }, { \"typeName\" : \"RsCmLbnuxc2vdNJ234Ysh\" , \"propagate\" : true , \"removePropagationsOnEntityDelete\" : true , \"restrictPropagationThroughLineage\" : false , \"restrictPropagationThroughHierarchy\" : false } ] } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). Each tag you want to add must be given using its hashed-string representation . (Optional) You can decide whether to propagate this tag (true) or not (false). If you choose false, no propagation of this tag from the asset will occur â€” neither through lineage nor parent-child relationships. (Optional) If propagation is allowed, you can then define whether propagated tags should be removed if this asset is deleted (true) or not (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for lineage (true) or still allow it through lineage (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for hierarchy (true) or still allow it through hierarchy (false). Remove from existing assets Â¶ Remove a single tag Â¶ 6.2.0 4.0.0 To remove a single tag from an existing asset: dbt Java Python Kotlin Raw REST API Remove one tag from an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : classificationNames : # (2) - PII # (3) classifications : # (4) - typeName : yQBDoKHdTLJhqAsdR3RMq6 # (5) propagate : true removePropagationsOnEntityDelete : true restrictPropagationThroughLineage : false restrictPropagationThroughHierarchy : false You must of course give the name of the object. You can use the meta . atlan . classificationNames structure, as above. When using this simplified form, you can give the normal human-readable name of the tags you want to remain, rather than the hashed-string representation . The tag being removed is no longer present You are removing the tag by not specifying it anymore here in dbt. The tags must be nested within the meta . atlan . classifications structure. Each tag you want to remain must be given using its hashed-string representation . The tag being removed is no longer present You are removing the tag by not specifying it anymore here in dbt. Remove one tag from an existing asset 1 2 3 4 Table . removeAtlanTag ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) \"Marketing Analysis\" ); // (4) Use the removeAtlanTag() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to remove a tag from an asset, all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset. The tag (the name you set up in the UI) to remove from the asset. Remove one tag from an existing asset 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () client . asset . remove_atlan_tag ( # (1) asset_type = Table , qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , atlan_tag_name = \"Marketing Analysis\" , # (2) ) Use the asset.remove_atlan_tag() method, which for most objects requires a minimal set of information. This method will construct the necessary request and call the necessary API(s) to remove a tag from an asset, all-in-one. The tag (the name you set up in the UI) to remove from the asset. Remove one tag from an existing asset 1 2 3 4 Table . removeAtlanTag ( // (1) client , // (2) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , // (3) \"Marketing Analysis\" ) // (4) Use the removeAtlanTag() helper method, which for most objects requires a minimal set of information. This helper method will construct the necessary request and call the necessary API(s) to remove a tag from an asset, all-in-one. Because this operation will directly change the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The qualifiedName of the asset. The tag (the name you set up in the UI) to remove from the asset. DELETE /api/meta/entity/uniqueAttribute/type/Table/classification/WCVjmgKnW40G151dESXZ03?attr:qualifiedName=default%2Fsnowflake%2F1657037873%2FSAMPLE_DB%2FFOOD_BEV%2FTOP_BEVERAGE_USERS 1 // (1) Note that all of the details for the deletion are embedded in the URL or query parameters to the request. Once again the hashed-string representation of the tag is required. You would either need to first retrieve the list of tags via API to determine this value, or look through the development console of your browser while opening the tag in the Atlan UI. Remove multiple tags Â¶ 6.2.0 To remove one or more tags from an existing asset: dbt Java Python Kotlin Raw REST API Coming soon Coming soon Remove multiple tags from an existing asset 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () client . asset . remove_atlan_tags ( # (1) asset_type = Table , qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , atlan_tag_names = [ \"Marketing Analysis\" , \"PII\" ], # (2) ) Use the asset.remove_atlan_tags() method, which for most objects requires a minimal set of information. This method will construct the necessary request and call the necessary API(s) to remove a tag from an asset, all-in-one. A list of the tags (the names as you set them up in the UI) to remove for the asset. Coming soon POST /api/meta/entity/bulk?replaceTags=false&appendTags=true&replaceBusinessAttributes=false&overwriteBusinessAttributes=false 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"qualifiedName\" : \"default/snowflake/1746022526/WIDE_WORLD_IMPORTERS/BRONZE_WAREHOUSE/FIVETRAN_AUDIT\" , // (3) \"name\" : \"FIVETRAN_AUDIT\" // (4) }, \"removeClassifications\" : [ { \"typeName\" : \"VfsfmLbnuxc2vdNJ0Ysh\" // (5) }, { \"typeName\" : \"RsCmLbnuxc2vdNJ234Ysh\" } ] } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). Each tag you want to add must be given using its hashed-string representation . Remove all tags Â¶ Could create a new asset Remember that Atlan matches the provided qualifiedName to determine whether to update or create the asset . 2.0.0 4.0.0 To remove all tags from an existing asset, you need to specify no tags in your object: dbt Java Python Kotlin Raw REST API Remove all tags from an existing asset 1 2 3 4 5 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : classifications : [] # (2) You must of course give the name of the object. The tags must be nested within the meta . atlan . classifications structure. To remove all of them, send an explicit empty list. Remove all tags from an existing asset 1 2 3 4 5 Table table = Table . updater ( // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , \"TOP_BEVERAGE_USERS\" ). build (); AssetMutationResponse response = table . save ( client , true ); // (2) assert response . getUpdatedAssets (). size () == 1 ; // (3) Use the updater() method to initialize the object with all necessary attributes for updating it(../advanced-examples/update.md#build-minimal-object-needed). (Removing the tags is still an update to the asset, we are not deleting the asset itself.) Call the save() method to actually update the asset, using true as the second argument to overwrite tags. Since we have not provided any tags in our object, this will replace the existing tags on the asset with no tags. (In other words, it will remove all tags from the asset.) Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was updated (again, removing tags is an update to the asset â€” we are not deleting the asset itself). Remove all tags from an existing asset 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) response = client . asset . save ( table , replace_atlan_tags = True ) #  (2) assert 1 == len ( response . assets_updated ( asset_type = Table )) # (3) Use the updater() method to create an asset suitable for modifiaction i.e. with all the requisite attributes. Call the save() method to actually update the asset, using True for the replace_atlan_tags argument will cause the tags to be overwritten. Since we have not provided any tags in our object,  this will replace the existing tags on the asset with no tags. (In other words, it will remove all tags from the asset.) The response should only include that single asset that was updated (again, removing tags is an update to the asset â€” we are not deleting the asset itself). Remove all tags from an existing asset 1 2 3 4 5 val table = Table . updater ( // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , \"TOP_BEVERAGE_USERS\" ). build () val response = table . save ( client , true ) // (2) assert ( response . updatedAssets . size == 1 ) // (3) Use the updater() method to initialize the object with all necessary attributes for updating it(../advanced-examples/update.md#build-minimal-object-needed). (Removing the tags is still an update to the asset, we are not deleting the asset itself.) Call the save() method to actually update the asset, using true as the second argument to overwrite tags. Since we have not provided any tags in our object, this will replace the existing tags on the asset with no tags. (In other words, it will remove all tags from the asset.) Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was updated (again, removing tags is an update to the asset â€” we are not deleting the asset itself). POST /api/meta/entity/bulk?replaceTags=true&replaceBusinessAttributes=false&overwriteBusinessAttributes=false 1 2 3 4 5 6 7 8 9 10 11 { // (1) \"entities\" : [ // (2) { \"typeName\" : \"Table\" , // (3) \"attributes\" : { \"qualifiedName\" : \"default/snowflake/1665450065/DBT_FOOD_BEVERAGE/PUBLIC/INSTACART_ORDER_PRODUCTS_USERS_TIME_MASTER\" , // (4) \"name\" : \"INSTACART_ORDER_PRODUCTS_USERS_TIME_MASTER\" // (5) } } ] } Note that the query parameter replaceTags is equal to true in the request. This is what causes the override behavior â€” so when you do not specify any tags in the request body those overwrite any existing tags on the asset. (The result being that there are then no tags on the asset.) All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). In bulk Â¶ 7.0.0 4.0.0 You can modify many tags, for many assets, at the same time. Operates as a replace Applying tags in bulk can currently only be done as a replacement. All tags on the asset(s) you upate will be replaced with the tags you specify. This means any tags that already exist on the asset in Atlan that are not in your update will be removed from that asset. dbt Java Python Kotlin Raw REST API Replace tags on multiple assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 models : - name : TOP_BEVERAGE_USERS # (1) meta : atlan : classificationNames : # (2) - PII # (3) - Marketing Analysis classificationNames : # (4) - name : PII # (5) propagate : true # (6) removePropagationsOnEntityDelete : true # (7) restrictPropagationThroughLineage : false # (8) restrictPropagationThroughHierarchy : false # (9) - name : Marketing Analysis propagate : true removePropagationsOnEntityDelete : true restrictPropagationThroughLineage : false restrictPropagationThroughHierarchy : false classifications : # (10) - typeName : yQBDoKHdTLJhqAsdR3RMq6 # (11) propagate : true removePropagationsOnEntityDelete : true restrictPropagationThroughLineage : false restrictPropagationThroughHierarchy : false - typeName : WCVjmgKnW40G151dESXZ03 propagate : true removePropagationsOnEntityDelete : true restrictPropagationThroughLineage : false restrictPropagationThroughHierarchy : false - name : ANOTHER_ASSET # (12) meta : atlan : classificationNames : - ... You must of course give the name of the object. The simplest way to tag an asset, using the default values for propagation (those shown below), is to use the meta . atlan . classificationNames structure. When using this simplified form, you can give the normal human-readable name of the tags rather than the hashed-string representation . Alternatively, if you want to override the propagation settings, you can use this more detailed structure. Each listed item must itself be a YAML object consisting of the human-readable name of the tag and the propagation setting overrides: (Optional) You can decide whether to propagate this tag (true) or not (false). If you choose false, no propagation of this tag from the asset will occur â€” neither through lineage nor parent-child relationships. (Optional) If propagation is allowed, you can then define whether propagated tags should be removed if this asset is deleted (true) or not (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for lineage (true) or still allow it through lineage (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for hierarchy (true) or still allow it through hierarchy (false). Alternatively, you can specify tags nested within the meta . atlan . classifications structure. In this structure, each tag you want to add must be given using its hashed-string representation . Its propagation settings can be overridden using the same options described above. To apply tags to multiple assets, just list all of the assets in the model file. Replace tags on multiple assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Table table = Table . updater ( // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , \"TOP_BEVERAGE_USERS\" ) . atlanTag ( AtlanTag . of ( \"PII\" )) // (2) . atlanTag ( AtlanTag . builder () // (3) . typeName ( \"Marketing Analysis\" ) . propagate ( true ) . removePropagationsOnEntityDelete ( true ) . restrictPropagationThroughLineage ( false ) . restrictPropagationThroughHierarchy ( false ) . build ()) . atlanTag ( AtlanTag . of ( \"Sensitivity\" , // (4) SourceTagAttachment . byName ( client , // (5) SourceTagCache . SourceTagName ( \"snowflake/development@@DB/SCH/SENSITIVITY\" ), // (6) List . of ( SourceTagAttachmentValue . of ( null , \"Restricted\" ))))) // (7) . build (); AtlanMutationResponse response = table . save ( client , true ); // (8) Use the updater() helper method, which for most objects requires a minimal set of information. This helper method will construct a builder onto which you can chain any tag details. (You can also do this at creation time, using the creator() helper method, which will also return a builder.) You can chain simple Atlan tags using .atlanTag() and the AtlanTag.of() helper. You can chain a fully-configured Atlan tag using .atlanTag() and the AtlanTag.builder() helper to specify the exact propagation options. Or you can chain a source-synced tag using .atlanTag() and the AtlanTag.of() helper that takes a SourceTagAttachment object. Because creating a source tag attachment may need to look up information in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can build a source tag attachment by name or qualifiedName of the source tag. To build by name, you need to specify the source tag in the format: {{connectorType}}/{{connectionName}}@@DB_NAME/SCHEMA_NAME/TAG_NAME . You can then also specify the value(s) for that source tag, either by key (first argument to SourceTagAttachemntValue.of() ) or value (second argument to SourceTagAttachmentValue.of() ). When you save the object, you must send true to the optional parameter to replace tags (otherwise all tags in your request will be ignored). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Replace tags on multiple assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.core import AtlanTag , AtlanTagName from pyatlan.cache.source_tag_cache import SourceTagName from pyatlan.model.structs import SourceTagAttachment , SourceTagAttachmentValue client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , name = \"TOP_BEVERAGE_USERS\" , ) table . atlan_tags = [ # (2) AtlanTag . of ( atlan_tag_name = AtlanTagName ( \"PII\" )), AtlanTag ( type_name = AtlanTagName ( \"Marketing Analysis\" ), # (3) remove_propagations_on_entity_delete = True , restrict_propagation_through_lineage = False , restrict_propagation_through_hierarchy = False , ), AtlanTag . of ( # (4) atlan_tag_name = AtlanTagName ( \"Sensitivity\" ), source_tag_attachment = SourceTagAttachment . by_name ( client = client , name = SourceTagName ( client = client , tag = \"snowflake/development@@DB/SCH/SENSITIVITY\" ), # (5) source_tag_values = [ SourceTagAttachmentValue ( tag_attachment_key = \"\" , tag_attachment_value = \"Restricted\" ) # (6) ], ), ), ] response = client . asset . save ( table , replace_atlan_tags = True ) # (7) Use the updater() helper method, which typically requires\nonly a minimal set of information for most objects. You can assign Atlan tags directly to table.atlan_tags . To create a fully-configured Atlan tag, use the AtlanTag() model,\nallowing you to specify precise propagation options. Alternatively, create a source-synced tag using the AtlanTag.of() helper,\nwhich takes a SourceTagAttachment object. Build a source tag attachment using either the name or qualified_name of the source tag. To build by name, specify the source tag in this format: {{connectorType}}/{{connectionName}}@@DB_NAME/SCHEMA_NAME/TAG_NAME . Specify the value(s) for the source tag using either the key\n( tag_attachment_key ) or the value ( tag_attachment_value ). When saving the object, include the optional parameter replace_atlan_tags=true to replace the tags\n(otherwise, all tags in the request will be ignored). Add tags to an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 val table = Table . updater ( // (1) \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" , \"TOP_BEVERAGE_USERS\" ) . atlanTag ( AtlanTag . of ( \"PII\" )) // (2) . atlanTag ( AtlanTag . builder () // (3) . typeName ( \"Marketing Analysis\" ) . propagate ( true ) . removePropagationsOnEntityDelete ( true ) . restrictPropagationThroughLineage ( false ) . restrictPropagationThroughHierarchy ( false ) . build ()) . atlanTag ( AtlanTag . of ( \"Sensitivity\" , // (4) SourceTagAttachment . byName ( client , // (5) SourceTagCache . SourceTagName ( \"snowflake/development@@DB/SCH/SENSITIVITY\" ), // (6) listOf ( SourceTagAttachmentValue . of ( null , \"Restricted\" ))))) // (7) . build () val response = table . save ( client , true ) // (8) Use the updater() helper method, which for most objects requires a minimal set of information. This helper method will construct a builder onto which you can chain any tag details. (You can also do this at creation time, using the creator() helper method, which will also return a builder.) You can chain simple Atlan tags using .atlanTag() and the AtlanTag.of() helper. You can chain a fully-configured Atlan tag using .atlanTag() and the AtlanTag.builder() helper to specify the exact propagation options. Or you can chain a source-synced tag using .atlanTag() and the AtlanTag.of() helper that takes a SourceTagAttachment object. Because creating a source tag attachment may need to look up information in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can build a source tag attachment by name or qualifiedName of the source tag. To build by name, you need to specify the source tag in the format: {{connectorType}}/{{connectionName}}@@DB_NAME/SCHEMA_NAME/TAG_NAME . You can then also specify the value(s) for that source tag, either by key (first argument to SourceTagAttachemntValue.of() ) or value (second argument to SourceTagAttachmentValue.of() ). When you save the object, you must send true to the optional parameter to replace tags (otherwise all tags in your request will be ignored). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 { \"entities\" : [ // (1) { \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"TOP_BEVERAGE_USERS\" , // (3) \"qualifiedName\" : \"default/snowflake/1657037873/SAMPLE_DB/FOOD_BEV/TOP_BEVERAGE_USERS\" // (4) }, \"classifications\" : [ // (5) { \"typeName\" : \"yQBDoKHdTLJhqAsdR3RMq6\" , // (6) \"propagate\" : true , // (7) \"removePropagationsOnEntityDelete\" : true , // (8) \"restrictPropagationThroughLineage\" : false , // (9) \"restrictPropagationThroughHierarchy\" : false // (10) }, { \"typeName\" : \"WCVjmgKnW40G151dESXZ03\" , \"propagate\" : true , \"removePropagationsOnEntityDelete\" : true , \"restrictPropagationThroughLineage\" : false , \"restrictPropagationThroughHierarchy\" : false }, { \"typeName\" : \"Z96sGJrF0S68PxYTUdKG6b\" , \"propagate\" : true , \"removePropagationsOnEntityDelete\" : true , \"restrictPropagationThroughLineage\" : false , \"restrictPropagationThroughHierarchy\" : false , \"attributes\" : { // (11) \"rt5N3mHZTcxXafuu6ZPpyL\" : [ // (12) { \"sourceTagName\" : \"CONFIDENTIAL\" , // (13) \"sourceTagQualifiedName\" : \"default/snowflake/1726834662/ANALYTICS/WIDE_WORLD_IMPORTERS/CONFIDENTIAL\" , // (14) \"sourceTagGuid\" : \"c28c08a8-320b-4a1a-b52e-75d120b4a8cc\" , // (15) \"sourceTagConnectorName\" : \"snowflake\" , // (16) \"isSourceTagSynced\" : false , \"sourceTagSyncTimestamp\" : 0 , \"sourceTagValue\" : [ // (17) { \"tagAttachmentValue\" : \"Highly Restricted\" // (18) } ] } ] } } ] } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). Each tag you want to apply to the asset must be wrapped in a classifications array. Each tag you want to add must be given using its hashed-string representation . (Optional) You can decide whether to propagate this tag (true) or not (false). If you choose false, no propagation of this tag from the asset will occur â€” neither through lineage nor parent-child relationships. (Optional) If propagation is allowed, you can then define whether propagated tags should be removed if this asset is deleted (true) or not (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for lineage (true) or still allow it through lineage (false). (Optional) If propagation is allowed, you can also decide whether to disable propagation of the tag only for hierarchy (true) or still allow it through hierarchy (false). For source tags, you must also specify an embedded attributes in the tag. You must give the hashed-string representation of the attribute whose displayName is sourceTagAttachment . You must give the name of the tag in the source. You must give the qualifiedName of the Tag asset representing that tag in Atlan. You must give the guid of the Tag asset representing that tag in Atlan. You must give the name of the connector for the source where the tag comes from. To specify a value for the tag, you must wrap it in a sourceTagValue array. You can then specify the value using tagAttachmentValue or its key using tagAttachmentKey . Find hashed-string names Â¶ When using either the raw APIs or dbt, you must provide the custom metadata names using Atlan's hashed-string representation. Not necessary for SDKs Note that this is not needed when using the SDKs, which translate these for you! To look up the hashed-string representations: GET /api/meta/types/typedefs?type=classification The response will include displayName and name for each tag. The displayName is what you see in Atlan's UI, and the name is the hashed-string representation: Simplified response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 { \"enumDefs\" : [], \"structDefs\" : [], \"classificationDefs\" : [ { \"category\" : \"CLASSIFICATION\" , \"guid\" : \"c43e2f52-975f-40b6-88fa-93697fb54f52\" , \"name\" : \"WCVjmgKnW40G151dESXZ03\" , // (1) \"displayName\" : \"Marketing Analysis\" , \"description\" : \"Assets relevant to the marketing domain\" }, { \"category\" : \"CLASSIFICATION\" , \"guid\" : \"ec641061-d8fa-4090-9145-a5f23c9c3e99\" , \"name\" : \"yQBDoKHdTLJhqAsdR3RMq6\" , // (2) \"displayName\" : \"PII\" , \"description\" : \"Personally-identifiable information can be used to uniquely identify an individual person.\" }, { \"category\" : \"CLASSIFICATION\" , \"guid\" : \"70211696-f3fb-4a4a-a81a-db589e29f436\" , \"name\" : \"Z96sGJrF0S68PxYTUdKG6b\" , // (3) \"displayName\" : \"Sensitivity\" , \"attributeDefs\" : [ { \"name\" : \"rt5N3mHZTcxXafuu6ZPpyL\" , // (4) \"displayName\" : \"sourceTagAttachment\" , \"description\" : \"\" , \"typeName\" : \"array<SourceTagAttachment>\" , \"isDefaultValueNull\" : false , \"isOptional\" : true , \"cardinality\" : \"SET\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 2147483647 , \"isUnique\" : false , \"isIndexable\" : false , \"includeInNotification\" : false , \"skipScrubbing\" : false , \"searchWeight\" : -1 , \"isNew\" : true } ], } ], \"entityDefs\" : [], \"relationshipDefs\" : [], \"businessMetadataDefs\" : [] } Hashed-string name for the tag named Marketing Analysis . Hashed-string name for the tag named PII . Hashed-string name for the tag named Sensitivity . Hashed-string name for the sourceTagAttachment attribute in the Sensitivity classification. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/readme/",
    "content": "/api/meta/entity/bulk (POST) Manage asset READMEs Â¶ READMEs can only be added to assets after an asset exists. (The asset itself must be created first.) README content is written in HTML The content of a README needs to be HTML. The HTML should be everything that would be inside the <body></body> tags, but not include the <body></body> tags themselves. (So it should also exclude the outer <html></html> tags.) Add to an existing asset Â¶ 2.0.0 4.0.0 Each README can be assigned to only a single asset. To create a README and assign it to an asset: Java Python Kotlin Raw REST API Add to an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 final String readmeContent = \"<h1>Overview</h1><p>Details about this term...</p>\" ; // (1) GlossaryTerm term = GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (2) . toBuilder () . name ( \"Example Term\" ) // (3) . build (); Readme readme = Readme . creator ( // (4) term , // (5) readmeContent ) . build (); AssetMutationResponse response = readme . save ( client ); // (6) assert response . getCreatedAssets (). size () == 1 // (7) assert response . getUpdatedAssets (). size () == 1 // (8) Pick up your HTML content from somewhere (here it is defined directly in the code). The README must be attached to some asset. You could either first search for or retrieve that asset, or build up a reference directly (as in this example). The asset you send to the README creation must have its name populated, not only its GUID or qualifiedName. Use the creator() method to initialize the README with all necessary attributes for creating it . For a README, you need to send the asset to attach it to and the content for the README itself (the HTML). Call the save() method to actually create the README and attach it to the asset. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created (the README). The response will also include a single asset that was updated (the asset to which we've attached the README). Add to an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Readme , AtlasGlossaryTerm client = AtlanClient () content = \"<h1>Overview</h1><p>More Details about this term...</p>\" # (1) readme = Readme . creator ( # (2) asset = AtlasGlossaryTerm . ref_by_guid ( guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" ), # (3) content = content , # (4) asset_name = \"Example Term\" ) # (5) response = client . asset . save ( readme ) # (6) assert ( readmes := response . assets_created ( asset_type = Readme )) # (7) assert ( glossaries := response . assets_updated ( asset_type = AtlasGlossaryTerm )) # (8) Pick up your HTML content from somewhere (here it is defined directly in the code). Use the create() method to initialize the README with all necessary attributes for creating it . We need to give the asset to attach the README to. The content for the README itself (the HTML). The name of the asset to which we want to attach the README. Note: The name is only required because we are using the ref_by_guid method to create the AtlasGlossaryTerm consequently it will not have a name. If we had an asset we had previosly retrieved via a search or using the asset.get_by_guid method we could leave the asset_name parameter out and the name from the given asset would be used. Call the save() method to actually create the README and attach it to the asset. Assert that the README was created. Assert a GlossaryTerm was updated (the asset to which we've attached the README). Add to an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 val readmeContent = \"<h1>Overview</h1><p>Details about this term...</p>\" // (1) val term = GlossaryTerm . refByGuid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (2) . toBuilder () . name ( \"Example Term\" ) // (3) . build () val readme = Readme . creator ( // (4) term , // (5) readmeContent ) . build () val response = readme . save ( client ) // (6) assert ( response . createdAssets . size == 1 ) // (7) assert ( response . updatedAssets . size == 1 ) // (8) Pick up your HTML content from somewhere (here it is defined directly in the code). The README must be attached to some asset. You could either first search for or retrieve that asset, or build up a reference directly (as in this example). The asset you send to the README creation must have its name populated, not only its GUID or qualifiedName. Use the creator() method to initialize the README with all necessary attributes for creating it . For a README, you need to send the asset to attach it to and the content for the README itself (the HTML). Call the save() method to actually create the README and attach it to the asset. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single asset that was created (the README). The response will also include a single asset that was updated (the asset to which we've attached the README). Note that you are actually creating a new README asset When adding a README through the API, you are really creating a new instance of a README asset. At the same time, you're attaching this new object to an existing asset. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"entities\" : [ // (1) { \"typeName\" : \"Readme\" , // (2) \"attributes\" : { \"name\" : \"Example Term Readme\" , // (3) \"qualifiedName\" : \"b4113341-251b-4adc-81fb-2420501c30e6/readme\" , // (4) \"description\" : \"%3Ch1%3EOverview%3C%2Fh1%3E%3Cp%3EDetails%20about%20this%20term...%3C%2Fp%3E\" , // (5) \"asset\" :{ // (6) \"typeName\" : \"AtlasGlossaryTerm\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" } } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the README asset, which will always be Readme (case-sensitive). You must also provide a name for the README. This will not show up on the UI, but should be a concatenation of the name of the asset the README will be attached to and Readme . You must also provide a unique qualifiedName for the README. This will not show up on the UI, but should be a concatenation of the GUID of the asset the README will be attached to and /readme . The content of the README should be provided in the description field. Note that this must be HTML, which must further be url-encoded. Use a library Most languages will provide a library to url-encode and url-decode strings. Use this, wherever possible. For an example of translating raw HTML into url-encoded form (or decoding an encoded form) you can also try urlencoder.org and urldecoder.org , respectively. Finally, you need to include the reference information for the asset the README should be attached to. Retrieve a README from an existing asset Â¶ 2.0.0 4.0.0 To retrieve a README and its content for an existing asset: Java Python Kotlin Raw REST API Retrieve README's content from an existing asset 1 2 3 4 5 6 7 8 String termQn = \"fb45981203221-atlan\" ; // (1) var results = client . assets . select () // (2) . where ( Asset . QUALIFIED_NAME . eq ( termQn )) . includeOnResults ( Asset . README ) . includeOnRelations ( Readme . DESCRIPTION ) . stream () . toList (); System . out . println ( results . get ( 0 ). getReadme (). getDescription ()); // (3) Store the qualified name of the asset (GlossaryTerm) connected to the README in the termQn variable. Configure the search to match the qualified name, include the README, and fetch its description. Extract and print the README's content. Retrieve README's content from an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm , Readme from pyatlan.model.fluent_search import CompoundQuery , FluentSearch client = AtlanClient () term_qn = \"fb45981203221-atlan\" # (1) response = ( # (2) FluentSearch () . select () . where ( CompoundQuery . asset_type ( AtlasGlossaryTerm )) . where ( AtlasGlossaryTerm . QUALIFIED_NAME . eq ( term_qn )) . include_on_results ( AtlasGlossaryTerm . README ) . include_on_relations ( Readme . DESCRIPTION ) . execute ( client = client ) ) if first := response . current_page (): readme_content = first [ 0 ] . readme . description # (3) print ( readme_content ) Store the asset's qualified name in the term_qn variable. Filter by asset type, match the qualified name, include the README, and fetch its description. Extract and print the README's content. Retrieve README's content from an existing asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 val assetQualifiedName = \"fb45981203221-atlan\" // (1) val description = client . assets . select () // (2) . where ( Asset . QUALIFIED_NAME . eq ( assetQualifiedName )) . includeOnResults ( Asset . README ) . includeOnRelations ( Readme . DESCRIPTION ) . stream () . toList () . firstOrNull () ?. readme ?. description ?: \"README description not found.\" println ( \"README Description: $ description \" ) // (3) Store the qualified name of the asset in the assetQualifiedName variable. Search for the asset, include the README, and fetch its description. Extract and print the README's content. GET /api/meta/entity/bulk?guid=b4113341-251b-4adc-81fb-2420501c30e6 1 // (1) When retrieving the README, you need to use the README's GUID, not the GUID of the asset to which it is attached. Update a README attached to an existing asset Â¶ 2.0.0 4.0.0 To update a README and its content for an existing asset: Java Python Kotlin Raw REST API Updating README's content 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 String termQn = \"fb45981203221-atlan\" ; // (1) List < Asset > assets = client . assets . select () . where ( Asset . QUALIFIED_NAME . eq ( termQn )) . includeOnResults ( Asset . README ) . includeOnRelations ( Readme . DESCRIPTION ) . includeOnRelations ( Readme . NAME ) . stream () . toList (); Asset asset = assets . get ( 0 ); String newDescription = \"<p>This is the updated README description</p>\" ; Readme updatedReadme = Readme . updater ( asset . getGuid (), asset . getName ()) //(2) . description ( newDescription ) . build (); AssetMutationResponse response = updatedReadme . save ( client ); // (3) Store the qualified name of the asset (GlossaryTerm) connected to the README in the termQn variable. Use Readme.updater() to update the README's description. Save the updated README. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Updating README's content 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm , Readme from pyatlan.model.fluent_search import CompoundQuery , FluentSearch client = AtlanClient () term_qn = \"fb45981203221-atlan\" # (1) response = ( FluentSearch () . select () . where ( CompoundQuery . asset_type ( AtlasGlossaryTerm )) . where ( AtlasGlossaryTerm . QUALIFIED_NAME . eq ( term_qn )) . include_on_results ( AtlasGlossaryTerm . README ) . include_on_relations ( Readme . DESCRIPTION ) . execute ( client = client ) ) if first := response . current_page (): current_content = first [ 0 ] . readme . description updated_content = \"<p>Added new information to the Readme.</p>\" updated_readme = Readme . creator ( # (2) asset = first [ 0 ], content = updated_content ) save_response = client . asset . save ( updated_readme ) # (3) Store the asset's qualified name in the term_qn variable. Use Readme.creator() to create a new README for the same asset (AtlasGlossaryTerm). Save the updated README. Updating README's content 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 val assetQualifiedName = \"fb45981203221-atlan\" // (1) val assets : List < Asset > = client . assets . select () . where ( Asset . QUALIFIED_NAME . eq ( termQualifiedName )) . includeOnResults ( Asset . README ) . includeOnRelations ( Readme . DESCRIPTION ) . includeOnRelations ( Readme . NAME ) . stream () . toList () val asset = assets . firstOrNull () val newDescription = \"<p>Final Changes</p>\" val updatedReadme = Readme . updater ( asset . guid , asset . name ) // (2) . description ( newDescription ) . build () val response = updatedReadme . save ( client ) // (3) Store the asset's qualified name in the assetQualifiedName variable. Use Readme.updater() to update the README's description. Save the updated README. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. 1 // (1) REST API for updating a README is not available Remove a README from an Existing Asset Â¶ To remove a README from an existing asset, delete the README itself. (A README is treated as a separate asset with its own GUID.) 2.0.0 4.0.0 To hard-delete (purge) a README, provide the README's GUID: Java Python Kotlin Raw REST API Hard-delete (purge) a README asset 1 2 3 4 5 6 7 AssetMutationResponse response = Asset . purge ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ); // (1) Asset deleted = response . getDeletedAssets (). get ( 0 ); // (2) Readme readme ; if ( deleted instanceof Readme ) { readme = ( Readme ) deleted ; // (3) } Call the purge() method with the README's GUID to remove it permanently. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was purged through the getDeletedAssets() method. This lists only the assets deleted by the operation. If the deleted asset is a README, cast it to the Readme type. Hard-delete (purge) a README asset 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Readme client = AtlanClient () response = client . asset . purge_by_guid ( \"b4113341-251b-4adc-81fb-2420501c30e6\" ) # (1) if deleted := response . assets_deleted ( asset_type = Readme ): # (2) Readme = deleted [ 0 ] # (3) Use the asset.purge_by_guid() method with the README's GUID to perform the hard-delete. Use the assets_deleted(asset_type=Readme) method to filter for deleted READMEs. If a README was deleted, access its details through the returned response. Hard-delete (purge) a README asset 1 2 3 4 val response = Asset . purge ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (1) val deleted = response . deletedAssets [ 0 ] // (2) val readme = if ( deleted is Readme ) deleted else null // (3) Call the purge() method with the README's GUID to permanently remove. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can distinguish what was purged through the deletedAssets method. This lists only the assets deleted by the operation. Verify and cast the deleted asset to the README type DELETE /api/meta/entity/bulk?guid=b4113341-251b-4adc-81fb-2420501c30e6&deleteType=PURGE 1 // (1) When deleting a README via the API, specify its GUID in the URL and use a deleteType of PURGE. For more options on deleting README assets: Deleting an asset . The README will have its own GUID, separate from the asset to which it is attached When deleting the README, you need to use the README's GUID, not the GUID of the asset to which it is attached. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/finding/",
    "content": "Get all assets that... Â¶ One of the most common starting points for an algorithm is to retrieve all assets that meet certain criteria. To contrast approaches, let's start with an example: Imagine you might want to do something with all the columns in a particular schema (irrespective of the table or view they are in). As a traversal Â¶ You might logically consider the problem as a traversal: Traversal algorithm (pseudocode) 1. Retrieve the schema. 2. Retrieve all the tables in that schema. a. For each table, retrieve all the columns. What you'll get from the table is actually just a reference â€” the GUID and qualifiedName of the column, but no details. i. For each column reference, retrieve its details. Now do something with the column. 3. Retrieve all the views in that schema. a. For each view, retrieve all the columns. What you'll get from the table is actually just a reference â€” the GUID and qualifiedName of the column, but no details. i. For each column reference, retrieve its details. Now do something with the column. While logical, this will be resource-intensive and time-consuming Such an algorithm is certainly logical. However, you need to consider what's really happening behind-the-scenes. As the layout of the pseudocode above hopefully illustrates, there are a number of nested loops: (2) and (3) are loops, and within those loops you are making an API call per asset (table or view) to retrieve other assets. At first glance, this creates an algorithm whose runtime will grow roughly linearly with the number of tables and views in the schema . However, when retrieving the columns through the relationships on a table or view, you only get a reference to the column, not the full details of the column. So in reality, you then need to retrieve each column. The linear time complexity is now approaching quadratic . This will become much slower as volumes grow. As a search Â¶ In almost all cases, you can more quickly accomplish your goal by using search. For the example above: Search-based algorithm (pseudocode) 1. Run a search. Using the following conditions: Limit assets by type , to only columns. Limit results by status , to only active (non-archived) assets. Search by prefix using the qualifiedName of the schema. Request only the attributes you need to be included in each column result. 2. Iterate through the results. Now do something with the column. Less code (fewer loops), and faster to run With this algorithm, you'll only make as many API calls as there are pages of results. (So if you have a page size of 100 and there are 10,000 columns, that's 100 API calls â€” compared to the other algorithm's 10,000+ API calls with one per column.) In general when you want to get many assets, think search first This was only one example to show the approach. The sections below illustrate a number of them, but you may have many, many others. Each of these can be accomplished through a search â€” you just need to define the appropriate criteria! To do that, you might want to read up a bit more on search in general . It's incredibly powerful, but we know it is not trivial to understand when you're first getting started. Top tip : you can combine these examples together to form an even more powerful query, using compound queries . 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/glossary/",
    "content": "Glossary introduction Â¶ Operations on glossary assets (glossaries, categories, terms). Glossaries are a container for both terms and categories Terms and categories can only exist within a glossary. So a glossary must first exist, before you can create a term or category. This also means that if you delete a glossary, all of the terms and categories within that glossary are also deleted. Note as well that terms are not contained within categories. Categories are simply a mechanism to organize terms in Atlan. This means that deleting a category will not delete the terms related to it. erDiagram\n    Glossary ||--o{ GlossaryCategory : contains\n    Glossary ||--o{ GlossaryTerm : contains Each glossary object mostly behaves like other assets in Atlan. You can create , retrieve , update , delete , restore , view history , search , and combine operations using the same patterns as for any other asset. Unique characteristics of glossary objects There are, however, a few points that are unique to glossary objects compared to other asset types: The qualifiedNames of all glossary objects are a hashed string. This is not human-readable. All other asset types have qualifiedNames that are human-readable. We have provided helper methods specifically for retrieval of glossary assets by their human-readable names. When updating a contained glossary object (term or category), you must provide details about the parent glossary of that object. You can update all other asset types without re-specifying their parent object. As a result, the creator() and updater() builder methods for terms and categories require extra parameters to specify the glossary. The other difference that exists in interacting with glossaries is the common need to traverse their hierarchy of categories. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/glossary/create/",
    "content": "/api/meta/entity/bulk (POST) Creating glossary objects Â¶ You can create objects in glossaries in the same way as all other objects in the SDK . Each object provides a method that takes the minimal set of required fields to create that asset . Create a glossary Â¶ 2.0.0 4.0.0 To create a glossary: Java Python Kotlin Raw REST API Create a glossary 1 2 3 4 5 Glossary glossary = Glossary . creator ( \"Example Glossary\" ) // (1) . assetIcon ( AtlanIcon . BOOK_OPEN_TEXT ) // (2) . build (); // (3) AssetMutationResponse response = glossary . save ( client ); // (4) A name for the new glossary. You can chain any other enrichment onto the creator, such as an icon for the glossary in this example. You then build the object (in-memory). And finally you can save the glossary back to Atlan (and the result of that save is returned). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a glossary 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossary from pyatlan.model.enums import AtlanIcon client = AtlanClient () glossary = AtlasGlossary . creator ( name = \"Example Glossary\" # (1) ) glossary . asset_icon = AtlanIcon . BOOK_OPEN_TEXT . value # (2) response = client . asset . save ( glossary ) # (3) A name for the new glossary. You can chain any other enrichment onto the creator, such as an icon for the glossary in this example. You then build the object (in-memory). And finally you can save the glossary back to Atlan (and the result of that save is returned). Create a glossary 1 2 3 4 5 val glossary = Glossary . creator ( \"Example Glossary\" ) // (1) . assetIcon ( AtlanIcon . BOOK_OPEN_TEXT ) // (2) . build () // (3) val response = glossary . save ( client ) // (4) A name for the new glossary. You can chain any other enrichment onto the creator, such as an icon for the glossary in this example. You then build the object (in-memory). And finally you can save the glossary back to Atlan (and the result of that save is returned). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossary\" , // (2) \"attributes\" : { \"name\" : \"Example Glossary\" , // (3) \"qualifiedName\" : \"Example Glossary\" , // (4) \"assetIcon\" : \"PhBookOpenText\" // (5) } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). For a glossary, this is AtlasGlossary . You must provide the exact name of the asset (case-sensitive). You must provide a qualifiedName of the asset (case-sensitive). In the case of glossaries, this will actually be replaced in the back-end with a generated qualifiedName , but you must provide some value when creating the object. You can also provide other enrichment, such as an icon for the glossary in this example. Create a category Â¶ 2.0.0 4.0.0 To create a category: Java Python Kotlin Raw REST API Create a category 1 2 3 4 5 GlossaryCategory category = GlossaryCategory . creator ( \"Example Category\" , // (1) \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (2) . build (); // (3) AssetMutationResponse response = category . save ( client ); // (4) You must provide a name for the new category. You must provide the ID of the glossary in which the category should be created (GUID or qualifiedName). You then build the object (in-memory). And finally you can save the category back to Atlan (and the result of that save is returned). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a category 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryCategory client = AtlanClient () category = AtlasGlossaryCategory . creator ( name = \"Example Category\" , # (1) glossary_guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" # (2) ) response = client . asset . save ( category ) # (3) You must provide a name for the new category. You must provide the ID of the glossary (GUID) in which the category should be created. And finally you can save the category back to Atlan (and the result of that save is returned). Create a category 1 2 3 4 5 val category = GlossaryCategory . creator ( \"Example Category\" , // (1) \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (2) . build () // (3) val response = category . save ( client ) // (4) You must provide a name for the new category. You must provide the ID of the glossary in which the category should be created (GUID or qualifiedName). You then build the object (in-memory). And finally you can save the category back to Atlan (and the result of that save is returned). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryCategory\" , // (2) \"attributes\" : { \"name\" : \"Example Category\" , // (3) \"qualifiedName\" : \"Example Category\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" } } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). For a category, this is AtlasGlossaryCategory . You must provide the exact name of the asset (case-sensitive). You must provide a qualifiedName of the asset (case-sensitive). In the case of categories, this will actually be replaced in the back-end with a generated qualifiedName , but you must provide some value when creating the object. You must also specify the parent glossary in which the category should be created. This must be placed in an anchor property, which itself has an embedded typeName (of AtlasGlossary ) and the GUID of the glossary. Create a term Â¶ 2.0.0 4.0.0 To create a term: Java Python Kotlin Raw REST API Create a term 1 2 3 4 5 GlossaryTerm term = GlossaryTerm . creator ( \"Example Term\" , // (1) \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (2) . build (); // (3) AssetMutationResponse response = term . save ( client ); // (4) You must provide a name for the new term. You must provide the ID of the glossary in which the term should be created (GUID or qualifiedName). You then build the object (in-memory). And finally you can save the term back to Atlan (and the result of that save is returned). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a term 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm client = AtlanClient () term = AtlasGlossaryTerm . creator ( name = \"Example Term\" , # (1) glossary_guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" # (2) ) response = client . asset . save ( term ) # (3) You must provide a name for the new term. You must provide the ID of the glossary (GUID) in which the term should be created. And finally you can save the term back to Atlan (and the result of that save is returned). Create a term 1 2 3 4 5 val term = GlossaryTerm . creator ( \"Example Term\" , // (1) \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (2) . build () // (3) val response = term . save ( client ) // (4) You must provide a name for the new term. You must provide the ID of the glossary in which the term should be created (GUID or qualifiedName). You then build the object (in-memory). And finally you can save the term back to Atlan (and the result of that save is returned). Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryTerm\" , // (2) \"attributes\" : { \"name\" : \"Example Term\" , // (3) \"qualifiedName\" : \"Example Term\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" } } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the asset (case-sensitive). For a term, this is AtlasGlossaryTerm . You must provide the exact name of the asset (case-sensitive). You must provide a qualifiedName of the asset (case-sensitive). In the case of terms, this will actually be replaced in the back-end with a generated qualifiedName , but you must provide some value when creating the object. You must also specify the parent glossary in which the term should be created. This must be placed in an anchor property, which itself has an embedded typeName (of AtlasGlossary ) and the GUID of the glossary. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/finding/examples/",
    "content": "/api/meta/search/indexsearch (POST) Search examples Â¶ Connection by name and type Â¶ You may have noticed that connections in Atlan have qualifiedName s that include a timestamp. As a result, they are not trivial to directly construct. 1.4.0 4.0.0 However, you can search for them by name and type to resolve their qualifiedName : Java Python Kotlin Raw REST API Find a connection by name and type 1 2 3 4 List < Connection > connections = Connection . findByName ( // (1) client , // (2) \"production\" , // (3) AtlanConnectorType . SNOWFLAKE ); // (4) Use the findByName static method on the Connection class to search for connections by name and type. If you name your connections uniquely (by type), this should only return a single-item list. Because this operation will directly search for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Provide the name of the connection (this will be exact-matched). Provide the type of the connection. You can also (optionally) provide a list of extra attributes you want to retrieve for the connection. Core attributes like qualifiedName and its GUID are already included. Find a connection by name and type 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import AtlanConnectorType client = AtlanClient () connections = client . asset . find_connections_by_name ( # (1) name = \"production\" , # (2) connector_type = AtlanConnectorType . SNOWFLAKE , # (3) attributes = [] # (4) ) Use the asset.find_connections_by_name method on the AtlanClient class to search for connections by name and type. If you name your connections uniquely (by type), this should only return a single-item list. Provide the name of the connection (this will be exact-matched). Provide the type of the connection. You can also (optionally) provide a list of extra attributes you want to retrieve for the connection. Core attributes like qualifiedName and its GUID are already included. Find a connection by name and type 1 2 3 4 val connections = Connection . findByName ( // (1) client , // (2) \"production\" , // (3) AtlanConnectorType . SNOWFLAKE ) // (4) Use the findByName static method on the Connection class to search for connections by name and type. If you name your connections uniquely (by type), this should only return a single-item list. Because this operation will directly search for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Provide the name of the connection (this will be exact-matched). Provide the type of the connection. You can also (optionally) provide a list of extra attributes you want to retrieve for the connection. Core attributes like qualifiedName and its GUID are already included. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 { \"dsl\" : { // (1) \"query\" : { \"bool\" : { // (2) \"filter\" : [ // (3) { \"term\" : { // (4) \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { // (5) \"__typeName.keyword\" : { \"value\" : \"Connection\" } } }, { \"term\" : { // (6) \"name.keyword\" : { \"value\" : \"production\" } } }, { \"term\" : { // (7) \"connectorName\" : { \"value\" : \"snowflake\" } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the connections. To start building up a query with multiple conditions, you can use a bool query in Elasticsearch. You can use the filter criteria to define all the conditions the search results must match in a binary way (either matches or doesn't). This avoids the need to calculate a score for each result. Searches by default will return all assets that are found â€” whether active or archived (soft-deleted). In most cases, you probably only want the active ones. Since there could be tables, views, materialized views, columns, databases, schemas, etc in this connection â€” but you only want the connection itself â€” you can use an exact match on the type to restrict results to only connections. Exact match search (case-sensitive) on the name of the connection. Exact match search on the type of the connector for the connection. All connections Â¶ 1.4.0 4.0.0 On the other hand, you may want to find all the connections that exist in the environment: Java Python Kotlin Raw REST API Find all connections 1 2 3 4 5 6 7 Connection . select ( client ) // (1) . pageSize ( 100 ) // (2) . stream () // (3) . filter ( a -> a instanceof Connection ) // (4) . forEach ( c -> { // (5) log . info ( \"Connection: {}\" , c ); }); To start building up a query to include all connections, you can use the select() convenience method on Connection itself. This will already limit results to only active (non-archived) connections. Because this operation will directly search for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. (Optional) You can chain a pageSize() method to control the page sizes, to further limit API calls by retrieving more results per page. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. (Optional) You can do any other operations you might do on a stream, such as filtering the results to ensure they are of a certain type. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Find all connections 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Connection from pyatlan.model.fluent_search import FluentSearch , CompoundQuery client = AtlanClient () # (1) request = ( FluentSearch () # (2) . where ( CompoundQuery . asset_type ( Connection )) # (3) . where ( CompoundQuery . active_assets ()) # (4) . page_size ( 100 ) # (5) ) . to_request () # (6) for result in client . asset . search ( request ): # (7) if isinstance ( result , Connection ): # (8) print ( result ) Start with a client to run the search through. For the default client, you can always use AtlanClient() . To search across all assets, you can use a FluentSearch object. The .where() method allows you to limit to only certain assets. In this example, we are looking for connections, so use the CompoundQuery.asset_type() helper to narrow to only connections. You can chain additional .where() methods to add further conditions, like this example where we limit to only active (non-archived) assets. (Optional) You can chain a pageSize() method to control the page sizes, to further limit API calls by retrieving more results per page. You can then translate the fluent search into an index search request. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Use the isinstance method to ensure that the asset is of the desired type. This will also allow an IDE to provide specific type hints for this asset type. Find all connections 1 2 3 4 5 6 7 Connection . select ( client ) // (1) . pageSize ( 100 ) // (2) . stream () // (3) . filter { it is Connection } // (4) . forEach { // (5) log . info { \"Connection: $ it \" } } To start building up a query to include all connections, you can use the select() convenience method on Connection itself. This will already limit results to only active (non-archived) connections. Because this operation will directly search for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. (Optional) You can chain a pageSize() method to control the page sizes, to further limit API calls by retrieving more results per page. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. (Optional) You can do any other operations you might do on a stream, such as filtering the results to ensure they are of a certain type. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 { \"dsl\" : { // (1) \"from\" : 0 , \"size\" : 100 , \"query\" : { \"bool\" : { // (2) \"filter\" : [ // (3) { \"term\" : { // (4) \"__typeName.keyword\" : { \"value\" : \"Connection\" } } }, { \"term\" : { \"__state\" : { // (5) \"value\" : \"ACTIVE\" } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the connections. To start building up a query with multiple conditions, you can use a bool query in Elasticsearch. You can use the filter criteria to define all the conditions the search results must match in a binary way (either matches or doesn't). This avoids the need to calculate a score for each result. You can use an exact match on the type to restrict results to only connections. Searches by default will return all assets that are found â€” whether active or archived (soft-deleted). In most cases, you probably only want the active ones. Columns in a schema Â¶ 1.4.0 4.0.0 This example finds all columns that exist in a particular schema â€” irrespective of the table, view, or materialized view they exist within. Java Python Kotlin Raw REST API Get all columns in a schema 1 2 3 4 5 6 7 8 9 10 String schemaQN = \"default/snowflake/1662194632/MYDB/MY_SCH\" ; // (1) Column . select ( client ) // (2) . where ( Asset . QUALIFIED_NAME . startsWith ( schemaQN )) // (3) . pageSize ( 100 ) // (4) . includeOnResults ( Asset . DESCRIPTION ) // (5) . stream () // (6) . filter ( a -> a instanceof Column ) // (7) . forEach ( c -> { // (8) log . info ( \"Column: {}\" , c ); }); Part of the trick here is that the qualifiedName of a column starts with the qualifiedName of its parent (table, view or materialized view). Similarly, the qualifiedName of the table, view or materialized view starts with the qualifiedName of its parent schema. (And so on, all the way up to the connection itself.) To start building up a query specifically for columns, you can use the select() convenience method on Column itself. Because this operation will directly search for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can use the where() method to define all the conditions the search results must match. For this example, use the Asset.QUALIFIED_NAME constant to limit to only those assets whose qualifiedName starts with the qualifiedName of the schema (by using the startsWith() predicate). In this example, that means only assets that are within this particular schema will be returned as results. (Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page. Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every column will include its description. (Limit these attributes to the minimum you need about each column to do your intended work.) The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. (Optional) You can do any other operations you might do on a stream, such as filtering the results to ensure they are of a certain type. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all columns in a schema 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Column from pyatlan.model.fluent_search import FluentSearch , CompoundQuery schema_qn = \"default/snowflake/1646836521/ATLAN_SAMPLE_DATA/PUBLIC\" # (1) client = AtlanClient () # (2) request = ( FluentSearch () # (3) . where ( CompoundQuery . asset_type ( Column )) # (4) . where ( CompoundQuery . active_assets ()) # (5) . where ( Column . QUALIFIED_NAME . startswith ( schema_qn )) # (6) ) . to_request () # (7) for result in client . asset . search ( request ): # (8) if isinstance ( result , Column ): # (9) print ( result ) Part of the trick here is that the qualified_name of a column starts with the qualified_name of its parent (table, view or materialized view). Similarly, the qualified_name of the table, view or materialized view starts with the qualified_name of its parent schema. (And so on, all the way up to the connection itself.) Start with a client to run the search through. For the default client, you can always use AtlanClient() . To search across all assets, you can use a FluentSearch object. The .where() method allows you to limit to only certain assets. In this example, we are looking for columns, so use the CompoundQuery.asset_type() helper to narrow to only columns. You can chain additional .where() methods to add further conditions, like this example where we limit to only active (non-archived) assets. For this example, use the Column.QUALIFIED_NAME constant to limit to only those columns whose qualified_name starts with the qualified_name of the schema (by using the startswith() predicate). In this example, that means only columns that are within this particular schema will be returned as results. You can then translate the fluent search into an index search request. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Use the isinstance method to ensure that the asset is of the desired type. This will also allow an IDE to provide specific type hints for this asset type. Get all columns in a schema 1 2 3 4 5 6 7 8 9 10 val schemaQN = \"default/snowflake/1662194632/MYDB/MY_SCH\" // (1) Column . select ( client ) // (2) . where ( Asset . QUALIFIED_NAME . startsWith ( schemaQN )) // (3) . pageSize ( 100 ) // (4) . includeOnResults ( Asset . DESCRIPTION ) // (5) . stream () // (6) . filter { it is Column } // (7) . forEach { // (8) log . info { \"Column: $ it \" } }); Part of the trick here is that the qualifiedName of a column starts with the qualifiedName of its parent (table, view or materialized view). Similarly, the qualifiedName of the table, view or materialized view starts with the qualifiedName of its parent schema. (And so on, all the way up to the connection itself.) To start building up a query specifically for columns, you can use the select() convenience method on Column itself. Because this operation will directly search for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can use the where() method to define all the conditions the search results must match. For this example, use the Asset.QUALIFIED_NAME constant to limit to only those assets whose qualifiedName starts with the qualifiedName of the schema (by using the startsWith() predicate). In this example, that means only assets that are within this particular schema will be returned as results. (Optional) You can play around with different page sizes, to further limit API calls by retrieving more results per page. Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every column will include its description. (Limit these attributes to the minimum you need about each column to do your intended work.) The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. (Optional) You can do any other operations you might do on a stream, such as filtering the results to ensure they are of a certain type. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 { \"dsl\" : { // (1) \"query\" : { \"bool\" : { // (2) \"filter\" : [ // (3) { \"prefix\" : { // (4) \"qualifiedName\" : { \"value\" : \"default/snowflake/1662194632/MYDB/MY_SCH\" } } }, { \"term\" : { // (5) \"__typeName.keyword\" : { \"value\" : \"Column\" } } }, { \"term\" : { // (6) \"__state\" : { \"value\" : \"ACTIVE\" } } } ] } }, \"from\" : 0 , // (7) \"size\" : 100 , \"track_total_hits\" : true }, \"attributes\" : [ // (8) \"description\" ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the columns. To start building up a query with multiple conditions, you can use a bool query in Elasticsearch. You can use the filter criteria to define all the conditions the search results must match in a binary way (either matches or doesn't). This avoids the need to calculate a score for each result. Part of the trick here is that the qualifiedName of a column starts with the qualifiedName of its parent (table, view or materialized view). Similarly, the qualifiedName of the table, view or materialized view starts with the qualifiedName of its parent schema. (And so on, all the way up to the connection itself.) Since there could be tables, views, materialized views and columns in this schema â€” but you only want columns â€” you can use an exact match on the type to restrict results to only columns. Searches by default will return all assets that are found â€” whether active or archived (soft-deleted). In most cases, you probably only want the active ones. Here you can play around with different page sizes, to further limit API calls by retrieving more results per page. Add as many attributes as needed. Each attribute you add here will ensure that detail is included in each search result. So in this example, every column will include its description. (Limit these attributes to the minimum you need about each column to do your intended work.) Assets with custom metadata Â¶ 7.0.0 1.1.0 This example finds all assets with a particular custom metadata attribute populated â€” irrespective of the specific value of the attribute. Java Python Kotlin Raw REST API Get all assets with a custom metadata attribute populated 1 2 3 4 5 6 7 client . assets . select () // (1) . where ( CustomMetadataField . of ( client , \"RACI\" , \"Responsible\" ). hasAnyValue ()) // (2) . _includesOnResults ( client . getCustomMetadataCache (). getAttributesForSearchResults ( \"RACI\" )) // (3) . stream () // (4) . forEach ( a -> { // (5) log . info ( \"Asset: {}\" , a ); }); To search across all assets, you can use the assets.select() convenience method on a client. When searching for custom metadata attributes, you can construct a CustomMetadataField to start a clause that will match a custom metadata property. Since you are searching for the custom metadata attribute itself, there is no enum for the custom metadata or its property names, so these must be provided as strings. (The CustomMetadataField will handle translating these from their human-readable values to the Atlan-internal ID strings needed for the search.) The hasAnyValue() predicate allows you to limit to assets that have any value for this custom metadata attribute. Since you are searching for custom metadata, you probably want to include the values for custom metadata in each search result. This getAttributesForSearchResults() helper method will return all of the custom attributes within the RACI custom metadata structure. These will be encoded in the specific form required by the search for you. Note the use of _includesOnResults Since the getAttributesForSearchResults() helper will return a list of strings, you'll need to use the special _includesOnResults() method to add these for inclusion. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all assets with a custom metadata attribute populated 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.fields.atlan_fields import CustomMetadataField from pyatlan.model.fluent_search import FluentSearch client = AtlanClient () # (1) request = ( FluentSearch ( _includes_on_results = client . custom_metadata_cache . get_attributes_for_search_results ( \"RACI\" )) # (2) . where ( CustomMetadataField ( client = client , set_name = \"RACI\" , attribute_name = \"Responsible\" ) . has_any_value ()) # (3) ) . to_request () # (4) for result in client . asset . search ( request ): # (5) print ( result ) Start with a client to run the search through. For the default client, you can always use AtlanClient() . To search across all assets, you can use a FluentSearch object. Since you are searching for custom metadata, you probably want to include the values for custom metadata in each search result. This get_attributes_for_search_results() helper method will return all of the custom attributes within the RACI custom metadata structure. These will be encoded in the specific form required by the search for you. Note the use of _includes_on_results Since the get_attributes_for_search_results() helper will return a list of strings, you'll need to use the special _includes_on_results parameter to add these for inclusion. When searching for custom metadata attributes, you can construct a CustomMetadataField to start a clause that will match a custom metadata property. Since you are searching for the custom metadata attribute itself, there is no enum for the custom metadata or its property names, so these must be provided as strings. (The CustomMetadataField will handle translating these from their human-readable values to the Atlan-internal ID strings needed for the search.) The has_any_value() predicate allows you to limit to assets that have any value for this custom metadata attribute. You can then translate the fluent search into an index search request. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all assets with a custom metadata attribute populated 1 2 3 4 5 6 7 client . assets . select () // (1) . where ( CustomMetadataField . of ( client , \"RACI\" , \"Responsible\" ). hasAnyValue ()) // (2) . _includesOnResults ( client . customMetadataCache . getAttributesForSearchResults ( \"RACI\" )) // (3) . stream () // (4) . forEach { // (5) log . info { \"Asset: $it\" } } To search across all assets, you can use the assets.select() convenience method on a client. When searching for custom metadata attributes, you can construct a CustomMetadataField to start a clause that will match a custom metadata property. Since you are searching for the custom metadata attribute itself, there is no enum for the custom metadata or its property names, so these must be provided as strings. (The CustomMetadataField will handle translating these from their human-readable values to the Atlan-internal ID strings needed for the search.) The hasAnyValue() predicate allows you to limit to assets that have any value for this custom metadata attribute. Since you are searching for custom metadata, you probably want to include the values for custom metadata in each search result. This getAttributesForSearchResults() helper method will return all of the custom attributes within the RACI custom metadata structure. These will be encoded in the specific form required by the search for you. Note the use of _includesOnResults Since the getAttributesForSearchResults() helper will return a list of strings, you'll need to use the special _includesOnResults() method to add these for inclusion. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Requires multiple API operations Before you can search for custom metadata, you first need to have the Atlan-internal hashed-string representation of the custom metadata property. You will likely need to first retrieve the hashed-string representation . POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \"dsl\" : { // (1) \"query\" : { // (2) \"exists\" : { // (3) \"field\" : \"omrIzGB4oYlZrFKfTIUz6D\" // (4) } }, \"track_total_hits\" : true }, \"attributes\" : [ \"UQot6bU4XcGcIx8gAQ1dsW.omrIzGB4oYlZrFKfTIUz6D\" // (5) ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the assets. For a search with only a single condition, we can directly provide the condition. You can use the exists criteria to match any assets that have some value (no matter what that value is) for a given field. Use the Atlan-internal hashed-string representation of the custom metadata field name. Include the Atlan-internal hashed-string representation of the custom metadata field name in the attributes list, so you can see the value of the custom metadata on each result. In this attributes list it needs to be written as <CustomMetadata>.<Attribute> , using the hashed-string representation for both pieces. Assets with specific custom metadata value Â¶ 7.0.0 1.1.0 This example finds all assets with a particular custom metadata attribute populated â€” with a specific value for the attribute. Java Python Kotlin Raw REST API Get all assets with a specific custom metadata attribute value 1 2 3 4 5 6 7 client . assets . select () // (1) . where ( CustomMetadataField . of ( client , \"RACI\" , \"Responsible\" ). eq ( \"This exact value\" , false )) // (2) . _includesOnResults ( client . getCustomMetadataCache (). getAttributesForSearchResults ( \"RACI\" )) // (3) . stream () // (4) . forEach ( a -> { // (5) log . info ( \"Asset: {}\" , a ); }); To search across all assets, you can use the assets.select() convenience method on a client. When searching for custom metadata attributes, you can construct a CustomMetadataField to start a clause that will match a custom metadata property. Since you are searching for the custom metadata attribute itself, there is no enum for the custom metadata or its property names, so these must be provided as strings. (The CustomMetadataField will handle translating these from their human-readable values to the Atlan-internal ID strings needed for the search.) The eq() predicate allows you to limit to assets that have only the exact value provided for this custom metadata attribute (and in the case of a string value, you must supply a second parameter indicating whether the search should be case-sensitive (false) or case-insensitive (true)). Since you are searching for custom metadata, you probably want to include the values for custom metadata in each search result. This getAttributesForSearchResults() helper method will return all of the custom attributes within the RACI custom metadata structure. These will be encoded in the specific form required by the search for you. Note the use of _includesOnResults Since the getAttributesForSearchResults() helper will return a list of strings, you'll need to use the special _includesOnResults() method to add these for inclusion. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all assets with a custom metadata attribute populated 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.fields.atlan_fields import CustomMetadataField from pyatlan.model.fluent_search import FluentSearch client = AtlanClient () # (1) request = ( FluentSearch ( _includes_on_results = client . custom_metadata_cache . get_attributes_for_search_results ( \"RACI\" )) # (2) . where ( CustomMetadataField ( client = client , set_name = \"RACI\" , attribute_name = \"Responsible\" ) . eq ( \"This exact value\" )) # (3) ) . to_request () # (4) for result in client . asset . search ( request ): # (5) print ( result ) Start with a client to run the search through. For the default client, you can always use AtlanClient() . To search across all assets, you can use a FluentSearch object. Since you are searching for custom metadata, you probably want to include the values for custom metadata in each search result. This get_attributes_for_search_results() helper method will return all of the custom attributes within the RACI custom metadata structure. These will be encoded in the specific form required by the search for you. Note the use of _includes_on_results Since the get_attributes_for_search_results() helper will return a list of strings, you'll need to use the special _includes_on_results parameter to add these for inclusion. When searching for custom metadata attributes, you can construct a CustomMetadataField to start a clause that will match a custom metadata property. Since you are searching for the custom metadata attribute itself, there is no enum for the custom metadata or its property names, so these must be provided as strings. (The CustomMetadataField will handle translating these from their human-readable values to the Atlan-internal ID strings needed for the search.) The eq() predicate allows you to limit to assets that have only the exact value provided for this custom metadata attribute. You can then translate the fluent search into an index search request. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all assets with a specific custom metadata attribute value 1 2 3 4 5 6 7 client . assets . select () // (1) . where ( CustomMetadataField . of ( client , \"RACI\" , \"Responsible\" ). eq ( \"This exact value\" , false )) // (2) . _includesOnResults ( client . customMetadataCache . getAttributesForSearchResults ( \"RACI\" )) // (3) . stream () // (4) . forEach { // (5) log . info { \"Asset: $ it \" } } To search across all assets, you can use the assets.select() convenience method on a client. When searching for custom metadata attributes, you can construct a CustomMetadataField to start a clause that will match a custom metadata property. Since you are searching for the custom metadata attribute itself, there is no enum for the custom metadata or its property names, so these must be provided as strings. (The CustomMetadataField will handle translating these from their human-readable values to the Atlan-internal ID strings needed for the search.) The eq() predicate allows you to limit to assets that have only the exact value provided for this custom metadata attribute (and in the case of a string value, you must supply a second parameter indicating whether the search should be case-sensitive (false) or case-insensitive (true)). Since you are searching for custom metadata, you probably want to include the values for custom metadata in each search result. This getAttributesForSearchResults() helper method will return all of the custom attributes within the RACI custom metadata structure. These will be encoded in the specific form required by the search for you. Note the use of _includesOnResults Since the getAttributesForSearchResults() helper will return a list of strings, you'll need to use the special _includesOnResults() method to add these for inclusion. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Requires multiple API operations Before you can search for custom metadata, you first need to have the Atlan-internal hashed-string representation of the custom metadata property. You will likely need to first retrieve the hashed-string representation . POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"dsl\" : { // (1) \"query\" : { // (2) \"term\" : { // (3) \"omrIzGB4oYlZrFKfTIUz6D\" : { // (4) \"value\" : \"This exact value\" // (5) } } }, \"track_total_hits\" : true }, \"attributes\" : [ \"UQot6bU4XcGcIx8gAQ1dsW.omrIzGB4oYlZrFKfTIUz6D\" // (6) ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the assets. For a search with only a single condition, we can directly provide the condition. You can use the term query to exactly match a value on assets, for a given field. Use the Atlan-internal hashed-string representation of the custom metadata field name. Provide the exact value you want to match in that custom metadata property. Include the Atlan-internal hashed-string representation of the custom metadata field name in the attributes list, so you can see the value of the custom metadata on each result. In this attributes list it needs to be written as <CustomMetadata>.<Attribute> , using the hashed-string representation for both pieces. Assets linked to a term Â¶ This example finds all assets that are linked to a specific glossary term. (And could be extended to do find assets linked to any one of a number of glossary terms.) In this specific example we will find any assets linked to a glossary term named Revenue in a glossary named Metrics . You'll need the qualifiedName of the glossary term To find the assets linked to the glossary term, you'll need to search using the qualifiedName of the term. This is not the human-readable name you see in the UI. So this example is split into two parts: Finding the qualifiedName of the glossary term from its human-readable name and the result of (1). Finding all assets linked to that glossary term. 1.4.0 4.0.0 For example: Java Python Kotlin Raw REST API Find qualifiedName of the term 1 2 GlossaryTerm term = GlossaryTerm . findByName ( client , \"Revenue\" , \"Concepts\" ); // (1) String termQualifiedName = term . getQualifiedName (); // (2) The GlossaryTerm.findByName() helper method will retrieve the glossary term by its human-readable name, given the name of the glossary in which it should exist. If the term does not exist (within that glossary), it will throw a NotFoundException . Because this operation will directly search for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. If no exception was thrown, you can retrieve the qualifiedName of the glossary term. Get all assets linked to a specific term 3 4 5 6 7 8 client . assets . select () // (1) . where ( Asset . ASSIGNED_TERMS . in ( List . of ( termQualifiedName ))) // (2) . stream () // (3) . forEach ( a -> { // (4) log . info ( \"Asset: {}\" , a ); }); To search across all assets, you can use the assets.select() convenience method on a client. When searching for assets linked to one or more terms, you need to use the qualifiedName of the term(s). (This example shows searching for just one term, but you could search for any number of them in the list. The search will find assets that are assigned at least one of those terms in the list.) The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Find qualifiedName of the term 1 2 3 4 5 6 from pyatlan.client.atlan import AtlanClient from pyatlan.model.fluent_search import FluentSearch , CompoundQuery client = AtlanClient () # (1) term = client . asset . find_term_by_name ( \"Revenue\" , \"Concepts\" ) # (2) term_qualified_name = term . qualified_name # (3) Start with a client to run the search through. For the default client, you can always use AtlanClient() . The asset.find_term_by_name() helper method will retrieve the glossary term by its human-readable name, given the name of the glossary in which it should exist. If the term does not exist (within that glossary), it will throw a NotFoundError . If no exception was thrown, you can retrieve the qualified_name of the glossary term. Get all assets linked to a specific term 7 8 9 10 11 12 request = ( FluentSearch () # (1) . where ( CompoundQuery . assigned_term ([ term_qualified_name ])) # (2) ) . to_request () # (3) for result in client . asset . search ( request ): # (4) print ( result ) To search across all assets, you can use a FluentSearch object. When searching for assets linked to a given term, you need to use the qualified_name of the term. You can then translate the fluent search into an index search request. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Find qualifiedName of the term 1 2 val term = GlossaryTerm . findByName ( client , \"Revenue\" , \"Concepts\" ) // (1) val termQualifiedName = term . qualifiedName // (2) The GlossaryTerm.findByName() helper method will retrieve the glossary term by its human-readable name, given the name of the glossary in which it should exist. If the term does not exist (within that glossary), it will throw a NotFoundException . Because this operation will directly search for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. If no exception was thrown, you can retrieve the qualifiedName of the glossary term. Get all assets linked to a specific term 3 4 5 6 7 8 client . assets . select () // (1) . where ( Asset . ASSIGNED_TERMS . `in` ( listOf ( termQualifiedName ))) // (2) . stream () // (3) . forEach { // (4) log . info { \"Asset: $ it \" } } To search across all assets, you can use the assets.select() convenience method on a client. When searching for assets linked to one or more terms, you need to use the qualifiedName of the term(s). (This example shows searching for just one term, but you could search for any number of them in the list. The search will find assets that are assigned at least one of those terms in the list.) The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Requires multiple API operations Before you can search for assets linked to a term, you first need to have the qualifiedName of the term. You will likely need to first find the term by its name . POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"dsl\" : { // (1) \"query\" : { // (2) \"terms\" : { // (3) \"__meanings\" : [ // (4) \"5h2wMbSbWtRN1V1b05Mtb@LD5Tb30qbuYCZKsmFRpmS\" // (5) ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the assets. For a search with only a single condition, we can directly provide the condition. You can use the terms query to exactly match a value on assets, for a given field, against a list of possible matches. To find terms, match against the __meanings field. Provide the exact value of the qualifiedName for the term for which you want to find linked assets. Assets with an Atlan tag Â¶ 7.0.0 2.0.0 This example finds all assets that are assigned a specific Atlan tag â€” irrespective of whether they were directly assigned the tag or it was propagated. Java Python Kotlin Raw REST API Get all assets with a specific tag 1 2 3 4 5 6 client . assets . select () // (1) . tagged ( List . of ( \"PII\" )) // (2) . stream () // (3) . forEach ( a -> { // (4) log . info ( \"Asset: {}\" , a ); }); To search across all assets, you can use the assets.select() convenience method on a client. The .tagged() helper method allows us to limit to assets that match at least one of potentially multiple values (since there could be many tags on an asset). The SDK will translate the provided Atlan tag into the necessary internal representation required for the search â€” you can just provide the human-readable names of the Atlan tags. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all assets with a specific tag 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.fluent_search import FluentSearch , CompoundQuery client = AtlanClient () # (1) request = ( FluentSearch () # (2) . where ( CompoundQuery . tagged ( client = client , with_one_of = [ \"PII\" ])) # (3) ) . to_request () # (4) for result in client . asset . search ( request ): # (5) print ( result ) Start with a client to run the search through. For the default client, you can always use AtlanClient() . To search across all assets, you can use a FluentSearch object. The CompoundQuery.tagged() helper method allows us to limit to assets that match at least one of potentially multiple values (since there could be many tags on an asset). The SDK will translate the provided Atlan tag into the necessary internal representation required for the search â€” you can just provide the human-readable names of the Atlan tags. You can then translate the fluent search into an index search request. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all assets with a specific tag 1 2 3 4 5 6 client . assets . select () // (1) . tagged ( listOf ( \"PII\" )) // (2) . stream () // (3) . forEach { // (4) log . info { \"Asset: $ it \" } } Start with a client to run the search through. For the default client, you can always use Atlan.getDefaultClient() . To search across all assets, you can use the assets.select() convenience method on a client. The .tagged() helper method allows us to limit to assets that match at least one of potentially multiple values (since there could be many tags on an asset). The SDK will translate the provided Atlan tag into the necessary internal representation required for the search â€” you can just provide the human-readable names of the Atlan tags. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Requires multiple API operations Before you can search for Atlan tags, you first need to have the Atlan-internal hashed-string representation of the tags. You will likely need to first retrieve the hashed-string representation . POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"dsl\" : { // (1) \"query\" : { \"bool\" : { // (2) \"minimum_should_match\" : \"1\" , // (3) \"should\" : [ // (4) { \"terms\" : { \"__traitNames\" : [ // (4) \"wAI4bROOqCQzES8HCNso9F\" // (5) ] } }, { \"terms\" : { \"__propagatedTraitNames\" : [ // (6) \"wAI4bROOqCQzES8HCNso9F\" // (7) ] } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the assets. To match both assets that are directly assigned the Atlan tag and those that were propagated the Atlan tag, use a bool query for multiple conditions. Define the minimum number of conditions that need to match on an asset to be included in the results. In this example, you want either a direct or propagated Atlan tag, so should match at least one of the conditions provided. Use __traitNames to match directly-classified assets. Use the Atlan-internal hashed-string representation of the Atlan tag. Use __propagatedTraitNames to match assets that have been propagated this Atlan tag. Once again, use the Atlan-internal hashed-string representation of the Atlan tag. Assets with a source tag Â¶ 7.0.0 2.0.0 This example finds all assets that are assigned a specific source tag. Java Python Kotlin Raw REST API Get all assets with a specific tag 1 2 3 4 5 6 7 8 9 10 client . assets . select () // (1) . taggedWithValue ( // (2) \"Sensitivity\" , // (3) \"Highly Restricted\" , // (4) true // (5) ) . stream () // (6) . forEach ( a -> { // (7) log . info ( \"Asset: {}\" , a ); }); To search across all assets, you can use the assets.select() convenience method on a client. The .taggedWithValue() helper method allows us to limit to assets that match having this particular tag and value combination. You must specify the human-readable name of the Atlan tag that is mapped to a source tag. The SDK will translate the provided Atlan tag into the necessary internal representation required for the search â€” you can just provide the human-readable names of the Atlan tags. You must also provide the value you want to match. (Optional) You can restrict the search to only directly-tagged assets with the value using true , or look for all assets tagged with the value (whether directly or propagated). The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all assets with a specific tag 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets.core.asset import Asset from pyatlan.model.fluent_search import CompoundQuery , FluentSearch client = AtlanClient () # (1) request = ( FluentSearch () . select () # (2) . where ( CompoundQuery . tagged_with_value ( # (3) client = client , # (4) atlan_tag_name = \"Confidential\" , # (5) value = \"Highly Restricted\" , # (6) directly = True , # (7) source_tag_qualified_name = \"default/snowflake/1711669993/ANALYTICS/PRODUCTION/CONFIDENTIAL\" , # (8) ) ) ) . to_request () # (9) response = client . asset . search ( request ) # (10) for asset in response : # (11) ... Start with a client to run the search through. To search across all active assets, you can use the FluentSearch.select() method. The CompoundQuery.tagged_with_value() helper method allows.\nus to limit to assets that match having this particular tag and value combination. You must provide a client instance. You must specify the human-readable name of the Atlan tag that is mapped to a source tag.\nThe SDK will translate the provided Atlan tag into the necessary internal representation\nrequired for the search â€” you can just provide the human-readable names of the Atlan tags. You must also provide the value you want to match. (Optional) You can restrict the search to only directly-tagged assets\nwith the value using True , or look for all assets tagged with the value\n(whether directly or propagated) ( False ). (Optional) You can specify which source tag qualified name to use when multiple mapped source-synced tags are found. Now convert the given FluentSearch into an IndexSearchRequest object for the search. The search will only run when you call the client.asset.search() method. This is the pattern for iterating through all results (across pages)\ncovered in the Searching for assets portion of the SDK documentation. Get all assets with a specific tag 1 2 3 4 5 6 7 8 9 10 client . assets . select () // (1) . taggedWithValue ( // (2) \"Sensitivity\" , // (3) \"Highly Restricted\" , // (4) true // (5) ) . stream () // (6) . forEach { // (7) log . info { \"Asset: $ it \" } } To search across all assets, you can use the assets.select() convenience method on a client. The .taggedWithValue() helper method allows us to limit to assets that match having this particular tag and value combination. You must specify the human-readable name of the Atlan tag that is mapped to a source tag. The SDK will translate the provided Atlan tag into the necessary internal representation required for the search â€” you can just provide the human-readable names of the Atlan tags. You must also provide the value you want to match. (Optional) You can restrict the search to only directly-tagged assets with the value using true , or look for all assets tagged with the value (whether directly or propagated). The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Requires multiple API operations Before you can search for Atlan tags, you first need to have the Atlan-internal hashed-string representation of the tags. You will likely need to first retrieve the hashed-string representation . POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 { \"dsl\" : { // (1) \"query\" : { \"bool\" : { // (2) \"filter\" : [ { \"term\" : { \"__traitNames\" : { // (3) \"value\" : \"Z96sGJrF0S68PxYTUdKG6b\" , // (4) \"case_insensitive\" : false } } }, { \"span_within\" : { // (5) \"big\" : { \"span_near\" : { \"clauses\" : [ { \"span_term\" : { \"__classificationsText.text\" : { // (6) \"value\" : \"Z96sGJrF0S68PxYTUdKG6b\" } } }, { \"span_term\" : { \"__classificationsText.text\" : { // (7) \"value\" : \"default/snowflake/1726834662/ANALYTICS/WIDE_WORLD_IMPORTERS/CONFIDENTIAL\" } } } ], \"in_order\" : true , \"slop\" : 10000000 } }, \"little\" : { \"span_near\" : { \"clauses\" : [ { \"span_term\" : { \"__classificationsText.text\" : { // (8) \"value\" : \"tagAttachmentValue\" } } }, { \"span_term\" : { \"__classificationsText.text\" : { \"value\" : \"Highly\" // (9) } } }, { \"span_term\" : { \"__classificationsText.text\" : { \"value\" : \"Restricted\" } } }, { \"span_term\" : { \"__classificationsText.text\" : { \"value\" : \"tagAttachmentKey\" // (10) } } } ], \"in_order\" : true , \"slop\" : 0 } } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the assets. To match assets that are directly assigned the Atlan tag, use a bool query for multiple conditions. Use __traitNames to match directly-classified assets. Use the Atlan-internal hashed-string representation of the Atlan tag. To match a source tag, you must use a span_within query that has this exact structure. Once again, everywhere you specify the Atlan tag you must use the Atlan-internal hashed-string representation of the Atlan tag. You must also specify the qualifiedName of the source Tag asset. When matching a value, you must specify these little.span_near clauses in exactly the order shown in this example, starting with tagAttachmentValue . The value itself should come next, but note that if there are any spaces in the value then you must specify a span_term for each individual word of the value. Finally you must specify a span_term of tagAttachmentKey as the final clause. Deprecated assets Â¶ 1.4.0 1.1.0 This example finds all assets that are marked as deprecated. Java Python Kotlin Raw REST API Get all deprecated assets 1 2 3 4 5 6 client . assets . select () // (1) . where ( Asset . CERTIFICATE_STATUS . eq ( CertificateStatus . DEPRECATED )) // (2) . stream () // (3) . forEach ( a -> { // (4) log . info ( \"Asset: {}\" , a ); }); To search across all assets, you can use the assets.select() convenience method on a client. The .where() method allows you to limit to only assets that have a particular value in a particular field. In this example, we are looking for values for the certificate status, so use Asset.CERTIFICATE_STATUS . Since we only want assets that are deprecated, we will query where that certificate is set to the CertificateStatus.DEPRECATED value. (No need to try to remember or ever even know what the precise string values for the certificates are â€” we've provided enums for them in the SDK.) The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all deprecated assets 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Asset from pyatlan.model.enums import CertificateStatus from pyatlan.model.fluent_search import FluentSearch client = AtlanClient () # (1) request = ( FluentSearch () # (2) . where ( Asset . CERTIFICATE_STATUS . eq ( CertificateStatus . DEPRECATED . value )) # (3) ) . to_request () # (4) for result in client . asset . search ( request ): # (5) print ( result ) Start with a client to run the search through. For the default client, you can always use AtlanClient() . To search across all assets, you can use a FluentSearch object. The .where() method allows you to limit to only assets that have a particular value in a particular field. In this example, we are looking for values for the certificate status, so use Asset.CERTIFICATE_STATUS . Since we only want assets that are deprecated, we will query where that certificate is set to the CertificateStatus.DEPRECATED value. (No need to try to remember or ever even know what the precise string values for the certificates are â€” we've provided enums for them in the SDK.) You can then translate the fluent search into an index search request. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all deprecated assets 1 2 3 4 5 6 client . assets . select () // (1) . where ( Asset . CERTIFICATE_STATUS . eq ( CertificateStatus . DEPRECATED )) // (2) . stream () // (3) . forEach { // (4) log . info { \"Asset: $ it \" } } To search across all assets, you can use the assets.select() convenience method on a client. The .where() method allows you to limit to only assets that have a particular value in a particular field. In this example, we are looking for values for the certificate status, so use Asset.CERTIFICATE_STATUS . Since we only want assets that are deprecated, we will query where that certificate is set to the CertificateStatus.DEPRECATED value. (No need to try to remember or ever even know what the precise string values for the certificates are â€” we've provided enums for them in the SDK.) The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Requires multiple API operations Before you can search for classifications, you first need to have the Atlan-internal hashed-string representation of the classification. You will likely need to first retrieve the hashed-string representation . POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"dsl\" : { // (1) \"query\" : { // (2) \"term\" : { // (3) \"certificateStatus\" : { // (4) \"value\" : \"DEPRECATED\" // (5) } } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the assets. For a search with only a single condition, we can directly provide the condition. You can use the term query to exactly match a value on assets, for a given field. Use the name of the field you want to match. In this example, since you want to match a specific certificate, you can use the certificateStatus field. Provide the exact value you want to match in that field. In this example, you will be matching only assets with a certificate of DEPRECATED . Certified but incomplete assets Â¶ 1.4.0 1.1.0 This example finds all assets that are marked as verified, but are missing a description â€” suggesting they are in fact incomplete. Java Python Kotlin Raw REST API Get all verified assets that have no description 1 2 3 4 5 6 7 8 9 10 client . assets . select () // (1) . where ( Asset . CERTIFICATE_STATUS . eq ( CertificateStatus . VERIFIED )) // (2) . whereNot ( Asset . DESCRIPTION . hasAnyValue ()) // (3) . whereNot ( Asset . USER_DESCRIPTION . hasAnyValue ()) . includeOnResults ( Asset . OWNER_USERS ) // (4) . includeOnResults ( Asset . OWNER_GROUPS ) // (5) . stream () // (6) . forEach ( a -> { // (7) log . info ( \"Asset: {}\" , a ); }); To search across all assets, you can use the assets.select() convenience method on a client. The where() helper method allows us to limit to only assets that meet a a particular condition. In this example, we are looking for values for the certificate status, so use Asset.CERTIFICATE_STATUS . (No need to try to remember or ever even know what the precise string value is for the name of this field â€” we've provided enums for them in the SDK.) Since we only want assets that are verified, we will query where that certificate is set to the CertificateStatus.VERIFIED value. (No need to try to remember or ever even know what the precise string values for the certificates are â€” we've provided enums for them in the SDK.) You can use the whereNot() method to do the opposite â€” define all the conditions the search results must not match. Here we are limiting to only assets that have a description populated. The hasAnyValue() predicate method allows us to limit to only assets that have a user-defined description populated. In Atlan you have both description (crawled from source) and userDescription (user-defined or overridden). For this example use case, you probably want to check that both of these are empty. As part of the search, you may want certain details included in every result. In this use case, you may want to know the asset owner â€” someone to confirm this should really be certified when there is no description. In Atlan you have both users and groups that can own assets. For this example use case, you probably want to retrieve both of these for every result. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all verified assets that have no description 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Asset from pyatlan.model.enums import CertificateStatus from pyatlan.model.fluent_search import FluentSearch client = AtlanClient () # (1) request = ( FluentSearch () # (2) . where ( Asset . CERTIFICATE_STATUS . eq ( CertificateStatus . VERIFIED . value )) # (3) . where_not ( Asset . DESCRIPTION . has_any_value ()) # (4) . where_not ( Asset . USER_DESCRIPTION . has_any_value ()) . include_on_results ( Asset . OWNER_USERS ) # (5) . include_on_results ( Asset . OWNER_GROUPS ) # (6) ) . to_request () # (7) for result in client . asset . search ( request ): # (8) print ( result ) Start with a client to run the search through. For the default client, you can always use AtlanClient() . To search across all assets, you can use a FluentSearch object. The .where() method allows you to limit to only assets that have a particular value in a particular field. In this example, we are looking for values for the certificate status, so use Asset.CERTIFICATE_STATUS . Since we only want assets that are verified, we will query where that certificate is set to the CertificateStatus.VERIFIED value. (No need to try to remember or ever even know what the precise string values for the certificates are â€” we've provided enums for them in the SDK.) You can use the .where_not() method to do the opposite â€” define all the conditions the search results must not match. Here we are limiting to only assets that have a description populated. The has_any_value() predicate method allows us to limit to only assets that have a user-defined description populated. In Atlan you have both description (crawled from source) and userDescription (user-defined or overridden). For this example use case, you probably want to check that both of these are empty. As part of the search, you may want certain details included in every result. In this use case, you may want to know the asset owner â€” someone to confirm this should really be certified when there is no description. In Atlan you have both users and groups that can own assets. For this example use case, you probably want to retrieve both of these for every result. You can then translate the fluent search into an index search request. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Get all verified assets that have no description 1 2 3 4 5 6 7 8 9 10 client . assets . select () // (1) . where ( Asset . CERTIFICATE_STATUS . eq ( CertificateStatus . VERIFIED )) // (2) . whereNot ( Asset . DESCRIPTION . hasAnyValue ()) // (3) . whereNot ( Asset . USER_DESCRIPTION . hasAnyValue ()) . includeOnResults ( Asset . OWNER_USERS ) // (4) . includeOnResults ( Asset . OWNER_GROUPS ) // (5) . stream () // (6) . forEach { // (7) log . info { \"Asset: $ it \" } } To search across all assets, you can use the assets.select() convenience method on a client. The where() helper method allows us to limit to only assets that meet a a particular condition. In this example, we are looking for values for the certificate status, so use Asset.CERTIFICATE_STATUS . (No need to try to remember or ever even know what the precise string value is for the name of this field â€” we've provided enums for them in the SDK.) Since we only want assets that are verified, we will query where that certificate is set to the CertificateStatus.VERIFIED value. (No need to try to remember or ever even know what the precise string values for the certificates are â€” we've provided enums for them in the SDK.) You can use the whereNot() method to do the opposite â€” define all the conditions the search results must not match. Here we are limiting to only assets that have a description populated. The hasAnyValue() predicate method allows us to limit to only assets that have a user-defined description populated. In Atlan you have both description (crawled from source) and userDescription (user-defined or overridden). For this example use case, you probably want to check that both of these are empty. As part of the search, you may want certain details included in every result. In this use case, you may want to know the asset owner â€” someone to confirm this should really be certified when there is no description. In Atlan you have both users and groups that can own assets. For this example use case, you probably want to retrieve both of these for every result. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \"dsl\" : { // (1) \"query\" : { \"bool\" : { // (2) \"filter\" : [ // (3) { \"term\" : { \"certificateStatus\" : { // (4) \"value\" : \"VERIFIED\" } } } ], \"must_not\" : [ // (5) { \"exists\" : { \"field\" : \"description\" } }, { \"exists\" : { \"field\" : \"userDescription\" } } ] } }, \"track_total_hits\" : true }, \"attributes\" : [ \"ownerUsers\" , // (6) \"ownerGroups\" // (7) ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } Run a search to find the columns. To start building up a query with multiple conditions, you can use a bool query in Elasticsearch. You can use the filter criteria to define all the conditions the search results must match in a binary way (either matches or doesn't). This avoids the need to calculate a score for each result. In this example, you are looking for verified assets. So you can begin by filtering only those assets with a certificateStatus of VERIFIED . Since you want to find assets that specifically do not have other characteristics, use the must_not criteria to specify these. Specifically, match assets that do not have either a description or userDescription populated. As part of the search, you may want certain details included in every result. In this use case, you may want to know the asset owner â€” someone to confirm this should really be certified when there is no description. Where did ownerUsers come from? The Models section of the site details all the attributes that exist in each different type of asset, and therefore which ones you can retrieve as additional details in each search result, like ownerUsers . In Atlan you have both users and groups that can own assets. For this example use case, you probably want to retrieve both of these for every result. Where did ownerGroups come from? The Models section of the site details all the attributes that exist in each different type of asset, and therefore which ones you can retrieve as additional details in each search result, like ownerGroups . 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/glossary/categorize-terms/",
    "content": "/api/meta/entity/bulk (POST) Categorize terms Â¶ Terms in a glossary can also be organized in one (or many) categories. Remember the glossary is the container, not the category Remember that the glossary is what contains the term, not the category. Therefore a term must have a glossary, but is not required to have a category. Also, a term can (optionally) be organized in many categories, but can only exist in one glossary. Category must exist before creating or updating the term Remember: each category must already exist before you create or update a term that refers to the category. During term creation Â¶ 2.0.0 4.0.0 To categorize a term during its creation: Java Python Kotlin Raw REST API Categorize during creation 1 2 3 4 5 6 7 GlossaryTerm term = GlossaryTerm . creator ( \"Example Term\" , // (1) glossary ) // (2) . category ( GlossaryCategory . refByGuid ( \"dc4c0a08-a902-402b-bf24-cf935aecc343\" )) // (3) . category ( GlossaryCategory . refByQualifiedName ( anotherCategoryQN )) // (4) . build (); // (5) AssetMutationResponse response = term . save ( client ); // (6) A name for the new term. The glossary in which to create the term. You can then add any number of categories using the category() builder method. In this example the term will be categorized in a category with a GUID of dc4c0a08-a902-402b-bf24-cf935aecc343 ... ...in this example the term will also be categorized in a category with a qualifiedName given by the anotherCategoryQN variable. You need to build the object you've just defined. You then only need to save() 1 the object to create it in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Categorize during creation 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm client = AtlanClient () term = AtlasGlossaryTerm . creator ( name = \"Example Term\" , # (1) anchor = glossary , # (2) categories = [ AtlasGlossaryCategory . ref_by_guid ( \"dc4c0a08-a902-402b-bf24-cf935aecc343\" ), # (3) AtlasGlossaryCategory . ref_by_qualified_name ( another_category_qn ) # (4) ] response = client . asset . save ( term ); # (5) A name for the new term. The glossary in which to create the term. You can then add any number of categories through the categories named argument. In this example the term will be categorized in a category with a GUID of dc4c0a08-a902-402b-bf24-cf935aecc343 ... ...in this example the term will also be categorized in a category with a qualified_name given by the another_category_qn variable. You then only need to save() 1 the object to create it in Atlan. Categorize during creation 1 2 3 4 5 6 7 val term = GlossaryTerm . creator ( \"Example Term\" , // (1) glossary ) // (2) . category ( GlossaryCategory . refByGuid ( \"dc4c0a08-a902-402b-bf24-cf935aecc343\" )) // (3) . category ( GlossaryCategory . refByQualifiedName ( anotherCategoryQN )) // (4) . build () // (5) val response = term . save ( client ) // (6) A name for the new term. The glossary in which to create the term. You can then add any number of categories using the category() builder method. In this example the term will be categorized in a category with a GUID of dc4c0a08-a902-402b-bf24-cf935aecc343 ... ...in this example the term will also be categorized in a category with a qualifiedName given by the anotherCategoryQN variable. You need to build the object you've just defined. You then only need to save() 1 the object to create it in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryTerm\" , // (2) \"attributes\" : { \"name\" : \"Example Term\" , // (3) \"displayName\" : \"Example Term\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , // (6) \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" // (7) }, \"categories\" : [ // (8) { \"typeName\" : \"AtlasGlossaryCategory\" , // (9) \"guid\" : \"dc4c0a08-a902-402b-bf24-cf935aecc343\" // (10) }, { \"typeName\" : \"AtlasGlossaryCategory\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"...\" // (11) } } ] } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the term (case-sensitive), which for a term is AtlasGlossaryTerm . You must provide the exact name of the term (case-sensitive). You must provide the exact name of the term (case-sensitive) as you want it to appear in the UI. You must provide an anchor relationship. Within the anchor relationship you must provide the exact type name for a glossary: AtlasGlossary . Within the anchor relationship you must provide the GUID of the glossary the category should be created within. When you want to place the term into one or more categories, you must provide the categories relationship. Within a categories relationship, you must provide the exact type name for a category: AtlasGlossaryCategory . Within a categories relationship, you must provide either the GUID of the category (in this example)... ...or the qualifiedName of the category, which itself must be further nested within a uniqueAttributes object. Updating an existing term Â¶ 2.0.0 4.0.0 To create a child category, the steps are very similar but you add in the reference to the parent category: Java Python Kotlin Raw REST API Update an existing term 1 2 3 4 5 6 7 8 GlossaryTerm term = GlossaryTerm . updater ( \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , // (1) \"Example Term\" , // (2) \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (3) . category ( GlossaryCategory . refByGuid ( \"dc4c0a08-a902-402b-bf24-cf935aecc343\" )) // (4) . category ( GlossaryCategory . refByQualifiedName ( anotherCategoryQN )) // (5) . build (); // (6) AssetMutationResponse response = child . save ( client ); // (7) The qualifiedName of the existing term. The name of the existing term. The GUID of the glossary in which the term exists. You can then add any number of categories using the category() builder method. In this example the term will be categorized in a category with a GUID of dc4c0a08-a902-402b-bf24-cf935aecc343 ... ...in this example the term will also be categorized in a category with a qualifiedName given by the anotherCategoryQN variable. You need to build the object you've just defined. You then only need to save() 1 the object to update it in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update an existing term 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryTerm term = AtlasGlossaryTerm . updater ( qualified_name = \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , # (1) name = \"Example Term\" , # (2) glossary_guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" ) # (3) term . categories = [ AtlasGlossaryCategory . ref_by_guid ( \"dc4c0a08-a902-402b-bf24-cf935aecc343\" ), # (4) AtlasGlossaryCategory . ref_by_qualified_name ( another_category_qn ) # (5) ] response = client . asset . save ( term ); # (6) The qualified_name of the existing term. The name of the existing term. The GUID of the glossary in which the term exists. You can then add any number of categories using the categories property. In this example the term will be categorized in a category with a GUID of dc4c0a08-a902-402b-bf24-cf935aecc343 ... ...in this example the term will also be categorized in a category with a qualified_name given by the another_category_qn variable. You then only need to save() 1 the object to update it in Atlan. Update an existing term 1 2 3 4 5 6 7 8 val term = GlossaryTerm . updater ( \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , // (1) \"Example Term\" , // (2) \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (3) . category ( GlossaryCategory . refByGuid ( \"dc4c0a08-a902-402b-bf24-cf935aecc343\" )) // (4) . category ( GlossaryCategory . refByQualifiedName ( anotherCategoryQN )) // (5) . build () // (6) val response = child . save ( client ) // (7) The qualifiedName of the existing term. The name of the existing term. The GUID of the glossary in which the term exists. You can then add any number of categories using the category() builder method. In this example the term will be categorized in a category with a GUID of dc4c0a08-a902-402b-bf24-cf935aecc343 ... ...in this example the term will also be categorized in a category with a qualifiedName given by the anotherCategoryQN variable. You need to build the object you've just defined. You then only need to save() 1 the object to update it in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryCategory\" , // (2) \"attributes\" : { \"name\" : \"Example Term\" , // (3) \"qualifiedName\" : \"gsNccqJraDZqM6WyGP3ea@FzCMyPR2LxkPFgr8eNGrq\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , // (6) \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" // (7) }, \"categories\" : [ // (8) { \"typeName\" : \"AtlasGlossaryCategory\" , // (9) \"guid\" : \"dc4c0a08-a902-402b-bf24-cf935aecc343\" // (10) }, { \"typeName\" : \"AtlasGlossaryCategory\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"...\" // (11) } } ] } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the term (case-sensitive), which for a category is AtlasGlossaryCategory . You must provide the exact name of the term (case-sensitive). You must provide the exact qualifiedName of the term (case-sensitive), as it already exists in Atlan. You must provide an anchor relationship. Within the anchor relationship you must provide the exact type name for a glossary: AtlasGlossary . Within the anchor relationship you must provide the GUID of the glossary the category should be created within. When you want to place the term into one or more categories, you must provide the categories relationship. Within a categories relationship, you must provide the exact type name for a category: AtlasGlossaryCategory . Within a categories relationship, you must provide either the GUID of the category (in this example)... ...or the qualifiedName of the category, which itself must be further nested within a uniqueAttributes object. Why no distinction between create and update? This has to do with how Atlan detects changes â€” see the Importance of identifiers for a more detailed explanation. â†© â†© â†© â†© â†© â†© 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/glossary/create-hierarchy/",
    "content": "/api/meta/entity/bulk (POST) Creating a hierarchy Â¶ Categories in a glossary can be organized within another category, to create a hierarchy of categories. To do this, you need to create the upper levels of the hierarchy before the lower levels. Each level you create should refer to its parent, and therefore its parent must first exist. Create a root-level category Â¶ 2.0.0 4.0.0 To create a root- or top-level category (no parent): Java Python Kotlin Raw REST API Create a top-level category 1 2 3 4 5 GlossaryCategory top = GlossaryCategory . creator ( \"Top\" , // (1) glossary ) // (2) . build (); // (3) AssetMutationResponse response = top . save ( client ); // (4) A name for the new category. The glossary in which to create the category. Note that we do not specify any parent category anywhere, since this will be a top-level category. You need to build the object you've just defined. You then only need to save() 1 the object to create it in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a top-level category 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryCategory client = AtlanClient () top = AtlasGlossaryCategory . creator ( name = \"Top\" , # (1) anchor = glossary # (2) ) response = client . asset . save ( top ) # (3) A name for the new category. The glossary in which to create the category. Note that we do not specify any parent category anywhere, since this will be a top-level category. You then only need to save() 1 the object to create it in Atlan. Create a top-level category 1 2 3 4 5 val top = GlossaryCategory . creator ( \"Top\" , // (1) glossary ) // (2) . build () // (3) val response = top . save ( client ) // (4) A name for the new category. The glossary in which to create the category. Note that we do not specify any parent category anywhere, since this will be a top-level category. You need to build the object you've just defined. You then only need to save() 1 the object to create it in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryCategory\" , // (2) \"attributes\" : { \"name\" : \"Top\" , // (3) \"displayName\" : \"Top\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , // (6) \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" // (7) } } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the category (case-sensitive), which for a category is AtlasGlossaryCategory . You must provide the exact name of the category (case-sensitive). You must provide the exact name of the category (case-sensitive) as you want it to appear in the UI. You must provide an anchor relationship. Within the anchor relationship you must provide the exact type name for a glossary: AtlasGlossary . Within the anchor relationship you must provide the GUID of the glossary the category should be created within. Create a child category Â¶ Parent must exist before creating the child Remember: the parent category must exist before you create the child category. 2.0.0 4.0.0 To create a child category, the steps are very similar but you add in the reference to the parent category: Java Python Kotlin Raw REST API Create a child category 6 7 8 9 10 11 GlossaryCategory child = GlossaryCategory . creator ( \"Middle\" , // (1) glossary ) // (2) . parentCategory ( top . trimToReference ()) // (3) . build (); // (4) AssetMutationResponse response = child . save ( client ); // (5) A name for the new category. The glossary in which to create the category. Now you add in the reference to the parent category. There are multiple ways you can reference the category: If you have the parent category already, you can use trimToReference() to obtain the minimal reference to it. If you only know the GUID, you can use GlossaryCategory.refByGuid() to create a minimal reference to it. If you only know the qualifiedName, you can use GlossaryCategory.refByQualifiedName() to create a minimal reference to it. You need to build the object you've just defined. You then only need to save() 1 the object to create it in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a child category 10 11 12 13 14 15 16 17 18 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import AtlasGlossaryCategory client = AtlanClient () child = AtlasGlossaryCategory . creator ( name = \"Middle\" , # (1) anchor = glossary , # (2) parent_category = top ) # (3) response = client . asset . save ( child ); # (4) A name for the new category. The glossary in which to create the category. Now you add in the reference to the parent category. There are multiple ways you can reference the category: If you have the parent category already, you can send it through as-is. If you only know the GUID, you can use AtlasGlossaryCategory.ref_by_guid() to create a minimal reference to it. If you only know the qualifiedName, you can use GlossaryCategory.ref_by_qualified_name() to create a minimal reference to it. You then only need to save() 1 the object to create it in Atlan. Create a child category 6 7 8 9 10 11 val child = GlossaryCategory . creator ( \"Middle\" , // (1) glossary ) // (2) . parentCategory ( top . trimToReference ()) // (3) . build () // (4) val response = child . save ( client ) // (5) A name for the new category. The glossary in which to create the category. Now you add in the reference to the parent category. There are multiple ways you can reference the category: If you have the parent category already, you can use trimToReference() to obtain the minimal reference to it. If you only know the GUID, you can use GlossaryCategory.refByGuid() to create a minimal reference to it. If you only know the qualifiedName, you can use GlossaryCategory.refByQualifiedName() to create a minimal reference to it. You need to build the object you've just defined. You then only need to save() 1 the object to create it in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"entities\" : [ // (1) { \"typeName\" : \"AtlasGlossaryCategory\" , // (2) \"attributes\" : { \"name\" : \"Middle\" , // (3) \"displayName\" : \"Middle\" , // (4) \"anchor\" : { // (5) \"typeName\" : \"AtlasGlossary\" , // (6) \"guid\" : \"b4113341-251b-4adc-81fb-2420501c30e6\" // (7) }, \"parentCategory\" : { // (8) \"typeName\" : \"AtlasGlossaryCategory\" , // (9) \"guid\" : \"dc4c0a08-a902-402b-bf24-cf935aecc343\" // (10) } } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for the category (case-sensitive), which for a category is AtlasGlossaryCategory . You must provide the exact name of the category (case-sensitive). You must provide the exact name of the category (case-sensitive) as you want it to appear in the UI. You must provide an anchor relationship. Within the anchor relationship you must provide the exact type name for a glossary: AtlasGlossary . Within the anchor relationship you must provide the GUID of the glossary the category should be created within. You must provide a parentCategory relationship to define the parent category. Within the parentCategory relationship you must provide the exact type name for a category: AtlasGlossaryCategory . Within the parentCategory relationship you must provide the GUID of the category this category should be organized within. Why no distinction between create and update? This has to do with how Atlan detects changes â€” see the Importance of identifiers concept for a more detailed explanation. â†© â†© â†© â†© â†© â†© 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/lineage/",
    "content": "Lineage overview Â¶ Lineage in Atlan is defined by Process entities. These link assets together by inputs and outputs , and there can be any number of each of these: graph LR\n    s1[(Source 1)]\n    s2[(Source 2)]\n    s3[(Source 3)]\n    t1[(Target 1)]\n    t2[(Target 2)]\n    p([Process])\n    s1 & s2 & s3-->|inputs|p-->|outputs|t1 & t2 Through Atlan's APIs, you can create your own lineage as well as traverse lineage programmatically. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/glossary/hierarchy/",
    "content": "/api/meta/search/indexsearch (POST) Traverse categories Â¶ You can populate glossaries in Atlan with arbitrarily deep category hierarchies. graph\n    g([Glossary])\n    c1([Category 1])\n    c2([Category 2])\n    c1a([Category 1a])\n    c1b([Category 1b])\n    c1ai([\"Category 1a(i)\"])\n    c1aii([\"Category 1a(ii)\"])\n    tA([Term A])\n    tB([Term B])\n    g-->c1-->c1a\n    g-->c2\n    c1-->c1b\n    c1a-->c1ai\n    c1a-->c1aii\n    c1b-->tB\n    c1ai-->tA To traverse these categories efficiently (without retrieving each level through a separate API call) you need to search for all categories in a glossary and reconstruct the hierarchy in-memory. This reconstruction can be cumbersome, so we've provided a helper method for that in the SDKs. Retrieve the hierarchy Â¶ 1.6.2 4.0.0 To retrieve a traversable hierarchy for a glossary: Java Python Kotlin Raw REST API Retrieve traversable hierarchy 1 2 Glossary glossary = Glossary . findByName ( client , \"Concepts\" ); // (1) Glossary . CategoryHierarchy tree = glossary . getHierarchy ( client ); // (2) Start by retrieving the glossary itself, for example using Glossary.findByName() . The glossary object used must have its qualifiedName present, so if you already know the qualifiedName you could also use Glossary._internal().qualifiedName(\"...\").build(); as a shortcut, which does not require making any API call. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Call the .getHierarchy() method on the glossary to retrieve a traversable Glossary.CategoryHierarchy object. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. More details The .getHierarchy() method will only retrieve the bare minimum information about each category (its GUID, qualifiedName and name). If you want to retrieve additional details, such as the terms in that category or certificate for the category, you need to pass these as an additional argument. To do this, use the .getHierarchy(AtlanClient, List<String>) method, and pass a list of strings giving the names of any additional attributes you want to retrieve for each category. (For example, to retrieve terms you would use terms , for certificates you would use certificateStatus .) Retrieve traversable hierarchy 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () glossary = client . asset . find_glossary_by_name ( \"Concepts\" ) # (1) hierarchy = client . asset . get_hierarchy ( glossary ) # (2) Start by retrieving the glossary itself, for example using find_glossary_by_name() . The glossary object used must have its qualified_name present. Call the get_hierarchy() to retrieve a traversable AtlasGlossary.CategoryHierarchy object. More details The .get_hierarchy() method will only retrieve the bare minimum information about each category (its GUID, qualifiedName and name). If you want to retrieve additional details, such as the terms in that category or certificate for the category, you need to pass these as an additional argument. To do this, add the additional attributes parameter and pass a list of strings giving the names of any additional attributes ou want to retrieve for each category. (For example, to retrieve terms you would use terms , for certificates you would use certificateStatus .) Retrieve traversable hierarchy 1 2 val glossary = Glossary . findByName ( client , \"Concepts\" ) // (1) val tree = glossary . getHierarchy ( client ) // (2) Start by retrieving the glossary itself, for example using Glossary.findByName() . The glossary object used must have its qualifiedName present, so if you already know the qualifiedName you could also use Glossary._internal().qualifiedName(\"...\").build(); as a shortcut, which does not require making any API call. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Call the .getHierarchy() method on the glossary to retrieve a traversable Glossary.CategoryHierarchy object. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. More details The .getHierarchy() method will only retrieve the bare minimum information about each category (its GUID, qualifiedName and name). If you want to retrieve additional details, such as the terms in that category or certificate for the category, you need to pass these as an additional argument. To do this, use the .getHierarchy(AtlanClient, List<String>) method, and pass a list of strings giving the names of any additional attributes you want to retrieve for each category. (For example, to retrieve terms you would use terms , for certificates you would use certificateStatus .) Requires multiple API operations and non-API logic To retrieve all categories in a glossary could require multiple API operations, to page through results. You would do this by incrementing the from in each subsequent call (in increments equal to the size ) to get the next page of results. Each page of results from the search will return a flat list of categories. You will need to use the parentCategory relationship within each result to reverse-engineer the hierarchical structure of the categories from the flat lists. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 { \"dsl\" : { // (1) \"from\" : 0 , // (2) \"size\" : 20 , // (3) \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { // (4) \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { // (5) \"__typeName.keyword\" : { \"value\" : \"AtlasGlossaryCategory\" } } }, { \"term\" : { // (6) \"__glossary\" : { \"value\" : \"LD5Tb30qbuYCZKsmFRpmS\" } } } ] } }, \"sort\" : [ // (7) { \"name.keyword\" : { \"order\" : \"asc\" } } ], \"track_total_hits\" : true }, \"attributes\" : [ \"parentCategory\" // (8) ], \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } You should run a search to efficiently retrieve many categories at the same time. Use the from parameter to define the start of each page. If you have many categories in the glossary, page through them rather than trying to retrieve them all in a single request. The from should be incremented in multiples of the size , so in this example would be 0 , 20 , 40 , and so on. The size parameter controls how many categories you will try to retrieve per search request. You will probably want to filter the categories to only those that are active (excluding any archived or soft-deleted categories). You should filter the search by a specific type, in this example AtlasGlossaryCategory is the name of the type in Atlan for categories. Finally, you should also filter the search for the specific glossary in which to find the categories. Requires qualifiedName of the glossary Note that this requires the `qualifiedName of the glossary, which therefore must first be known or found by an earlier search on glossaries. When you expect to page through results, it is always a good idea to sort the results so that each page returns them in a consistent order. Since we want to be able to understand the hierarchy of categories, we also need to include the parentCategory in each result. Traverse the hierarchy Â¶ To traverse the hierarchy of categories you then have a few options. Depth-first traversal Â¶ 1.6.2 1.0.0 To list every category in the hierarchy in depth-first order: Java Python Kotlin Raw REST API Traverse the hierarchy depth-first 3 4 5 6 7 List < IGlossaryCategory > dfs = tree . depthFirst (); // (1) for ( GlossaryCategory category : dfs ) { // (2) // Do something with the category... // Order: [1, 1a, 1a(i), 1a(ii), 1b, 2] } The .depthFirst() method will return an ordered list of all the categories in the glossary, ordered by a depth-first traversal. You can then iterate through them in this particular order. Traverse the hierarchy depth-first 6 7 8 for category in hierarchy . depth_first : # (1) ... # Do something with the category ... # Order [1, 1a, 1a(i), 1a(ii), 1b, 2] The depth_first property will return an ordered list of all the categories in the glossary, ordered by a depth-first traversal. You can then iterate through them in this particular order. Traverse the hierarchy depth-first 3 4 5 6 7 val dfs = tree . depthFirst () // (1) for ( category in dfs ) { // (2) // Do something with the category... // Order: [1, 1a, 1a(i), 1a(ii), 1b, 2] } The .depthFirst() method will return an ordered list of all the categories in the glossary, ordered by a depth-first traversal. You can then iterate through them in this particular order. Non-API logic Once you have retrieved the categories using the search approach outlined above, traversing them becomes an operation entirely in your own program (does not interact with Atlan APIs). For a depth-first traversal: Start by listing a single top-level category (those whose parentCategory relationship is empty). Then output a single child category of that top-level category. Then output a single child category of (2). Continue in this way down the hierarchy. Once exhausted, then move on to the next (grand-)child category and exhaust its (grand-)children. Continue in this way until all categories are listed. Breadth-first traversal Â¶ 1.6.2 1.0.0 To list every category in the hierarchy in breadth-first order: Java Python Kotlin Raw REST API Traverse the hierarchy breadth-first 3 4 5 6 7 List < IGlossaryCategory > bfs = tree . breadthFirst (); // (1) for ( GlossaryCategory category : bfs ) { // (2) // Do something with the category... // Order: [1, 2, 1a, 1b, 1a(i), 1a(ii)] } The .breadthFirst() method will return an ordered list of all the categories in the glossary, ordered by a breadth-first traversal. You can then iterate through them in this particular order. Traverse the hierarchy breadth-first 6 7 8 for category in hierarchy . breadth_first : # (1) ... # Do something with the category ... # Order [1, 1a, 1a(i), 1a(ii), 1b, 2] The breadth-first property will return an ordered list of all the categories in the glossary, ordered by a depth-first traversal. You can then iterate through them in this particular order. Traverse the hierarchy breadth-first 3 4 5 6 7 val bfs = tree . breadthFirst () // (1) for ( category in bfs ) { // (2) // Do something with the category... // Order: [1, 2, 1a, 1b, 1a(i), 1a(ii)] } The .breadthFirst() method will return an ordered list of all the categories in the glossary, ordered by a breadth-first traversal. You can then iterate through them in this particular order. Non-API logic Once you have retrieved the categories using the search approach outlined above, traversing them becomes an operation entirely in your own program (does not interact with Atlan APIs). For a breadth-first traversal: Start by listing the top-level categories (those whose parentCategory relationship is empty). For each of these categories, then list all of its children. Continue the logic from (1) for each child category. Build-your-own traversal Â¶ 1.6.2 1.0.0 Alternatively, you may want to iterate through the hierarchy in your own order. From the traversable hierarchy you can retrieve the top-level categories, and then decide what to do from there: Java Python Kotlin Raw REST API Traverse the hierarchy as you like, starting from the top 3 4 5 6 7 8 9 10 11 12 for ( IGlossaryCategory top : tree . getRootCategories ()) { // (1) // Do something with the top-level categories [1, 2] for ( IGlossaryCategory child : top . getChildrenCategories ()) { // (2) // Do something with the child categories [1a, 1b] for ( IGlossaryCategory gc : child . getChildrenCategories ()) { // Do something with the grand-children categories [1a(i), 1a(ii)] // ... and so on } } } The .getRootCategories() method will return a list of only those categories at the root of the glossary. (The categories that have no parent categories themselves.) You can then retrieve the child categories using .getChildrenCategories() . And you can do this iteratively as you traverse down the hierarchy. Traverse the hierarchy as you like, starting from the top 6 7 8 9 10 for top in hierarchy . root_categories : # (1) for child in top . children_categories or []: # (2) for gc in child . children_categories or []: ... # Do something with the grand-children categories [1a(i), 1a(ii)] ... # ... and so on The root_categories property will return a list of only those categories at the root of the glossary. (The categories that have no parent categories themselves.) You can then retrieve the child categories using children_categories property. And you can do this iteratively as you traverse down the hierarchy. Traverse the hierarchy as you like, starting from the top 3 4 5 6 7 8 9 10 11 12 for ( top in tree . rootCategories ) { // (1) // Do something with the top-level categories [1, 2] for ( child in top . childrenCategories ) { // (2) // Do something with the child categories [1a, 1b] for ( gc in child . childrenCategories ) { // Do something with the grand-children categories [1a(i), 1a(ii)] // ... and so on } } } The .rootCategories member will return a list of only those categories at the root of the glossary. (The categories that have no parent categories themselves.) You can then retrieve the child categories using .childrenCategories . And you can do this iteratively as you traverse down the hierarchy. Non-API logic Once you have retrieved the categories using the search approach outlined above, traversing them becomes an operation entirely in your own program (does not interact with Atlan APIs). 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/glossary/retrieve-by-name/",
    "content": "/api/meta/search/indexsearch (POST) Retrieving glossary objects by name Â¶ Glossary objects (terms, categories and even glossaries themselves) in Atlan have complicated qualifiedName s. This makes retrieving them using the get() operation less than ideal. To address this, the SDKs provide helper methods to retrieve glossary objects based on their human-readable names. Retrieve a glossary by name Â¶ 1.4.0 4.0.0 To retrieve a glossary by its human-readable name: Java Python Kotlin Raw REST API Retrieve glossary by name 1 2 3 Glossary glossary = Glossary . findByName ( // (1) client , // (2) \"Concepts\" ) // (3) The findByName() helper method retrieves the glossary based on its human-readable name. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You must provide the human-readable name of the glossary. The method will only include a bare minimum set of attributes about the glossary â€” you can request additional attributes by providing a list of them as an (optional) second parameter to this method. Retrieve category by name 1 2 3 4 5 6 from pyatlan.client.atlan import AtlanClient client = AtlanClient () glossary = client . asset . find_glossary_by_name ( # (1) name = \"Concepts\" , # (2) attributes = None ) # (3) The asset.find_glossary_by_name() method retrieves the glossary based on its human-readable name. You must provide the human-readable name of the glossary. The method will only include a bare minimum set of attributes about the glossary â€” you can request additional attributes by providing a list of them as the second parameter to this method. Retrieve glossary by name 1 2 3 val glossary = Glossary . findByName ( // (1) client , // (2) \"Concepts\" ) // (3) The findByName() helper method retrieves the glossary based on its human-readable name. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You must provide the human-readable name of the glossary. The method will only include a bare minimum set of attributes about the glossary â€” you can request additional attributes by providing a list of them as an (optional) second parameter to this method. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \"dsl\" : { // (1) \"from\" : 0 , \"size\" : 2 , \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { // (2) \"__typeName.keyword\" : { \"value\" : \"AtlasGlossary\" } } }, { \"term\" : { // (3) \"name.keyword\" : { \"value\" : \"Concepts\" } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } You actually need to run a search to retrieve glossary objects by name. You should filter the search by a specific type, in this example AtlasGlossary is the name of the type in Atlan for glossaries. You then must also filter by the name of the glossary you want to find. This example does an exact match against the provided Concepts value (case-sensitive). Retrieve a category by name Â¶ 1.4.0 4.0.0 To retrieve a category by its human-readable name: Java Python Kotlin Raw REST API Retrieve category by name 1 2 3 4 GlossaryCategory category = GlossaryCategory . findByName ( // (1) client , // (2) \"Finance\" , // (3) \"Concepts\" ); // (4) The findByName() helper method retrieves the category based on its human-readable name. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You must provide the human-readable name of the category. You must also provide the human-readable name of the glossary for that category. (A category with the same name can exist in different glossaries, but not in the same glossary.) The method will only include a bare minimum set of attributes about the category â€” you can request additional attributes by providing a list of them as an (optional) third parameter to this method. Retrieve category by name 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () category = client . asset . find_category_by_name ( # (1) name = \"Finance\" , # (2) glossary_name = \"Concepts\" , #(3) attributes = None ) # (4) The asset.find_category_by_name() method retrieves the category based on its human-readable name. You must provide the human-readable name of the category. You must also provide the human-readable name of the glossary for that category. (A category with the same name can exist in different glossaries, but not in the same glossary.) The method will only include a bare minimum set of attributes about the category â€” you can request additional attributes by providing a list of them as the third parameter to this method. Retrieve category by name 1 2 3 4 val category = GlossaryCategory . findByName ( // (1) client , // (2) \"Finance\" , // (3) \"Concepts\" ) // (4) The findByName() helper method retrieves the category based on its human-readable name. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You must provide the human-readable name of the category. You must also provide the human-readable name of the glossary for that category. (A category with the same name can exist in different glossaries, but not in the same glossary.) The method will only include a bare minimum set of attributes about the category â€” you can request additional attributes by providing a list of them as an (optional) third parameter to this method. Requires multiple API operations To find a category by its name, using the name of the glossary it exists within (rather than the qualifiedName of the glossary), you must first find the glossary by name. (See above example.) Then use the returned qualifiedName of the glossary to run the search below for the category within that glossary. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 { \"dsl\" : { // (1) \"from\" : 0 , \"size\" : 2 , \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { // (2) \"__typeName.keyword\" : { \"value\" : \"AtlasGlossaryCategory\" } } }, { \"term\" : { // (3) \"name.keyword\" : { \"value\" : \"Finance\" } } }, { \"term\" : { // (4) \"__glossary\" : { \"value\" : \"LD5Tb30qbuYCZKsmFRpmS\" } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } You actually need to run a search to retrieve category objects by name. You should filter the search by a specific type, in this example AtlasGlossaryCategory is the name of the type in Atlan for categories. You then must also filter by the name of the category you want to find. This example does an exact match against the provided Finance value (case-sensitive). Finally, you should also filter the search for the specific glossary in which to find the category. (Since the same category name could exist in many glossaries.) Requires qualifiedName of the glossary Note that this requires the `qualifiedName of the glossary, which therefore must first be known or found by an earlier search on glossaries. Retrieve a term by name Â¶ 1.4.0 4.0.0 To retrieve a term by its human-readable name: Java Python Kotlin Raw REST API Retrieve term by name 1 2 3 4 GlossaryTerm term = GlossaryTerm . findByName ( // (1) client , // (2) \"Revenue\" , // (3) \"Concepts\" ); // (4) The findByName() helper method retrieves the term based on its human-readable name. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You must provide the human-readable name of the term. You must also provide the human-readable name of the glossary for that term. (A term with the same name can exist in different glossaries, but not in the same glossary.) The method will only include a bare minimum set of attributes about the term â€” you can request additional attributes by providing a list of them as an (optional) third parameter to this method. Retrieve term by name 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () term = client . asset . find_term_by_name ( # (1) name = \"Revenue\" , # (2) glossary_name = \"Concepts\" , #(3) attributes = None ) # (4) The asset.find_term_by_name() method retrieves the term based on its human-readable name. You must provide the human-readable name of the term. You must also provide the human-readable name of the glossary for that term. (A term with the same name can exist in different glossaries, but not in the same glossary.) The method will only include a bare minimum set of attributes about the term â€” you can request additional attributes by providing a list of them as the third parameter to this method. Retrieve term by name 1 2 3 4 val term = GlossaryTerm . findByName ( // (1) client , // (2) \"Revenue\" , // (3) \"Concepts\" ) // (4) The findByName() helper method retrieves the term based on its human-readable name. Because this operation will lookup the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You must provide the human-readable name of the term. You must also provide the human-readable name of the glossary for that term. (A term with the same name can exist in different glossaries, but not in the same glossary.) The method will only include a bare minimum set of attributes about the term â€” you can request additional attributes by providing a list of them as an (optional) third parameter to this method. Requires multiple API operations To find a term by its name, using the name of the glossary it exists within (rather than the qualifiedName of the glossary), you must first find the glossary by name. (See above example.) Then use the returned qualifiedName of the glossary to run the search below for the term within that glossary. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 { \"dsl\" : { // (1) \"from\" : 0 , \"size\" : 2 , \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" } } }, { \"term\" : { // (2) \"__typeName.keyword\" : { \"value\" : \"AtlasGlossaryTerm\" } } }, { \"term\" : { // (3) \"name.keyword\" : { \"value\" : \"Revenue\" } } }, { \"term\" : { // (4) \"__glossary\" : { \"value\" : \"LD5Tb30qbuYCZKsmFRpmS\" } } } ] } }, \"track_total_hits\" : true }, \"suppressLogs\" : true , \"showSearchScore\" : false , \"excludeMeanings\" : false , \"excludeClassifications\" : false } You actually need to run a search to retrieve term objects by name. You should filter the search by a specific type, in this example AtlasGlossaryTerm is the name of the type in Atlan for terms. You then must also filter by the name of the term you want to find. This example does an exact match against the provided Revenue value (case-sensitive). Finally, you should also filter the search for the specific glossary in which to find the term. (Since the same term name could exist in many glossaries.) Requires qualifiedName of the glossary Note that this requires the `qualifiedName of the glossary, which therefore must first be known or found by an earlier search on glossaries. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/lineage/parse-sql/",
    "content": "Parse SQL queries Â¶ You can parse your own SQL queries to assist with managing lineage. Unsupported Unless you have a specific agreement that states otherwise, this feature is provided as-is without any support. Also note that this does not create lineage directly â€” the results of the SQL parser could be used to programmatically build your own lineage. To parse a SQL query: Java Python Raw REST API Parse SQL query 1 2 3 4 5 6 7 QueryParserRequest request = QueryParserRequest . creator ( // (1) \"INSERT INTO orders (order_name, customer_id, product_id) VALUES(SELECT 'test_order', id, 21 FROM customers)\" , // (2) QueryParserSourceType . SNOWFLAKE ) // (3) . defaultDatabase ( \"ORDERS\" ) // (4) . defaultSchema ( \"PRODUCTION\" ) // (5) . build (); ParsedQuery parsedQuery = Atlan . getDefaultClient (). queryParser . parse ( request ); // (6) Use the creator() method to initialize the object with all necessary attributes. Provide the SQL code itself. Specify the data store for the SQL code, so it can be appropriately interpreted. (Optional) Provide a name to use for the database for any objects in the query that are not qualified. (Optional) Provide a name to use for the schema for any objects in the query that are not qualified. Call the parse() method to actually parse the query. Parse SQL query 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import QueryParserSourceType from pyatlan.model.query import QueryParserRequest request = QueryParserRequest . create ( # (1) sql = \"INSERT INTO orders (order_name, customer_id, product_id)\" # (2) \" VALUES(SELECT 'test_order', id, 21 FROM customers)\" , source = QueryParserSourceType . SNOWFLAKE , # (3) ) request . default_database = \"ORDERS\" # (4) request . default_schema = \"PRODUCTION\" # (5) client = AtlanClient () response = client . parse_query ( request ) # (6) Use the create() method to initialize the object with all necessary attributes. Provide the SQL code itself. Specify the data store for the SQL code, so it can be appropriately interpreted. (Optional) Provide a name to use for the database for any objects in the query that are not qualified. (Optional) Provide a name to use for the schema for any objects in the query that are not qualified. Call the parse_query() method to actually parse the query. POST /api/sql/query/parse 1 2 3 4 5 6 7 8 9 10 { \"sql\" : \"INSERT INTO orders (order_name, customer_id, product_id) VALUES(SELECT 'test_order', id, 21 FROM customers)\" , // (1) \"source\" : \"snowflake\" , // (2) \"defaultDatabase\" : \"ORDERS\" , // (3) \"defaultSchema\" : \"PRODUCTION\" , // (4) \"linkOrphanColumnToFirstTable\" : false , \"showJoin\" : true , \"ignoreRecordSet\" : true , \"ignoreCoordinate\" : true } Provide the SQL code itself. Specify the data store for the SQL code, so it can be appropriately interpreted. (Optional) Provide a name to use for the database for any objects in the query that are not qualified. (Optional) Provide a name to use for the schema for any objects in the query that are not qualified. How to interpret the response Â¶ The parser interprets the query and provides back details about column-level processing. Database objects Â¶ Database objects include tables, views, result sets of queries, and processes. For objects like tables and views, the objects list includes the actual columns used. For result sets, the objects list may also include system-generated columns that represent changes in the number of records returned or an aggregate value Every object is assigned a numeric id value, that can be used to cross-reference the flow details in the relationships (below). Relationships Â¶ Relationships returned by the parser indicate the flow of data between the database objects. Each relationship is composed of: a target , indicating the output column of the flow one or more sources , indicating the input columns used to produce that output a type , which can be one of: fdd indicates a flow of data direct from source column to target column fdr indicates a flow where the source column impacts the target, either: the number of records in the target (for example, a column used in a where clause) the resulting value of the target (for example, an aggregate function) Numeric id , parentId and processId identifiers refer to the same numeric id fields used in the objects (above). So these can be used to lookup the additional object detail. Java Python Raw REST API Interpret the parsed response 8 9 List < ParsedQuery . DatabaseObject > objects = parsedQuery . getObjects (); // (1) List < ParsedQuery . Relationship > relationships = parsedQuery . getRelationships (); // (2) You can retrieve the list of objects from the response using getObjects() . You can retrieve the list of relationships (flows) between the objects using getRelationships() . Interpret the parsed response 14 15 objects = response . dbobjs # (1) relationships = response . relationships # (2) You can retrieve the list of objects from the response using the dbobjs property. You can retrieve the list of relationships (flows) between the objects using the relationships property. POST /api/sql/query/parse 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 { \"dbobjs\" : [ // (1) { \"columns\" : [ { \"name\" : \"id\" , \"id\" : \"13\" } ], \"displayName\" : \"ORDERS.PRODUCTION.customers\" , \"id\" : \"12\" , \"name\" : \"customers\" , \"type\" : \"table\" , \"database\" : \"ORDERS\" , \"schema\" : \"PRODUCTION\" }, { \"columns\" : [ { \"name\" : \"order_name\" , \"id\" : \"5\" }, { \"name\" : \"customer_id\" , \"id\" : \"6\" }, { \"name\" : \"product_id\" , \"id\" : \"7\" } ], \"displayName\" : \"ORDERS.PRODUCTION.orders\" , \"id\" : \"4\" , \"name\" : \"orders\" , \"type\" : \"table\" , \"database\" : \"ORDERS\" , \"schema\" : \"PRODUCTION\" }, { \"id\" : \"8\" , \"name\" : \"Query Insert-1\" , \"procedureName\" : \"batchQueries\" , \"queryHashId\" : \"0acc71f4f1b91a589c4e3fc652135c64\" , \"type\" : \"process\" , \"database\" : \"ORDERS\" , \"schema\" : \"PRODUCTION\" } ], \"relationships\" : [ // (2) { \"id\" : \"5\" , \"type\" : \"fdd\" , \"effectType\" : \"insert\" , \"target\" : { \"id\" : \"5\" , \"column\" : \"order_name\" , \"parentId\" : \"4\" , \"parentName\" : \"orders\" }, \"sources\" : [ { \"id\" : \"13\" , \"column\" : \"id\" , \"parentId\" : \"12\" , \"parentName\" : \"customers\" } ], \"processId\" : \"8\" , \"processType\" : \"sstinsert\" } ] } The objects are returned in the dbobjs list. The relationships are returned in the relationships list. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/lineage/manage/",
    "content": "/api/meta/entity/bulk (DELETE) /api/meta/entity/bulk (POST) Manage lineage Â¶ Create lineage between assets Â¶ Directly Â¶ 7.0.0 4.0.0 To create lineage between assets, you need to create a Process entity. Input and output assets must already exist Note that the assets you reference as the inputs and outputs of the process must already exist, before creating the process. Java Python Kotlin Raw REST API Create lineage between assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 LineageProcess process = LineageProcess . creator ( // (1) \"Source 1, Source 2, Source 3 -> Target 1, Target 2\" , // (2) \"default/snowflake/1657025257\" , // (3) \"dag_123\" , // (4) List . of ( // (5) Table . refByGuid ( \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ), Table . refByGuid ( \"d002dead-1655-4d75-abd6-ad889fa04bd4\" ), Table . refByQualifiedName ( \"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS\" )), List . of ( // (6) Table . refByGuid ( \"86d9a061-7753-4884-b988-a02d3954bc24\" ), Table . refByQualifiedName ( \"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS\" )), null ) // (7) . sql ( \"select * from somewhere;\" ) // (8) . sourceURL ( \"https://your.orchestrator/unique/id/123\" ) // (9) . build (); AssetMutationResponse response = process . save ( client ); // (10) assert response . getCreatedAssets (). size () == 1 // (11) assert response . getUpdatedAssets (). size () == 5 // (12) Use the creator() method to initialize the object with all necessary attributes for creating it . Provide a name for how the process will be shown in the UI. Provide the qualifiedName of the connection that ran the process. Tips for the connection The process itself must be created within a connection for both access control and icon labelling. Use a connection qualifiedName that indicates the system that ran the process: You could use the same connection qualifiedName as the source system, if it was the source system \"pushing\" data to the target(s). You could use the same connection qualifiedName as the target system, if it was the target system \"pulling\" data from the source(s). You could use a different connection qualifiedName from either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator). (Optional) Provide the unique ID of the process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also send null and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the process. Use your own ID if you can While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra processes in lineage as this process itself changes over time. By using your own ID for the process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated. Provide the list of inputs to the process. Note that each of these is only a Reference to an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either: its GUID (for the static <Type>.refByGuid() method) its qualifiedName (for the static <Type>.refByQualifiedName() method) Provide the list of outputs to the process. Note that each of these is again only a Reference to an asset. (Optional) Provide the parent LineageProcess in which this process ran (for example, if this process is a subprocess of some higher-level process). If this is a top-level process, you can also send null for this parameter (as in this example). (Optional) You can also add other properties to the lineage process, such as SQL code that runs within the process. (Optional) You can also provide a link to the process, which will provide a button to click to go to that link from the Atlan UI when viewing the process in Atlan. Call the save() method to actually create the process. Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single lineage process asset that was created. The response will also include the 5 data assets (3 inputs, 2 outputs) that were updated. Create lineage between assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Process , Table client = AtlanClient () process = Process . creator ( # (1) name = \"Source 1, Source 2, Source 3 -> Target 1, Target 2\" , # (2) connection_qualified_name = \"default/snowflake/1657025257\" , # (3) process_id = \"dag_123\" , # (4) inputs = [ # (5) Table . ref_by_guid ( guid = \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ), Table . ref_by_guid ( guid = \"d002dead-1655-4d75-abd6-ad889fa04bd4\" ), Table . ref_by_qualified_name ( qualified_name = \"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS\" ), ], outputs = [ # (6) Table . ref_by_guid ( guid = \"86d9a061-7753-4884-b988-a02d3954bc24\" ), Table . ref_by_qualified_name ( qualified_name = \"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS\" ), ], ) # (7) process . sql = \"select * from somewhere;\" # (8) process . source_url = \"https://your.orchestrator/unique/id/123\" # (9) response = client . asset . save ( process ) # (10) assert ( processes := response . assets_created ( Process )) # (11) assert len ( processes ) == 1 # (12) assert ( tables := response . assets_updated ( Table )) # (13) assert len ( tables ) == 2 # (14) Use the create() method to initialize the object with all necessary attributes for creating it . Provide a name for how the process will be shown in the UI. Provide the qualified_name of the connection that ran the process. Tips for the connection The process itself must be created within a connection for both access control and icon labelling. Use a connection qualified_name that indicates the system that ran the process: You could use the same connection qualified_name as the source system, if it was the source system \"pushing\" data to the target(s). You could use the same connection qualified_name as the target system, if it was the target system \"pulling\" data from the source(s). You could use a different connection qualified_name from either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator). (Optional) Provide the unique ID of the process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also leave it out and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the process. Use your own ID if you can While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra processes in lineage as this process itself changes over time. By using your own ID for the process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated. Provide the list of inputs to the process. Note that each of these is only a Reference to an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either: its GUID (for the ref_by_guid() method) its qualifiedName (for the ref_by_qualified_name() method) Provide the list of outputs to the process. Note that each of these is again only a Reference to an asset. (Optional) Provide the parent Process in which this process ran (for example, if this process is a subprocess of some higher-level process). If this is a top-level process, you can also send None for this parameter (as in this example). (Optional) You can also add other properties to the lineage process, such as SQL code that runs within the process. (Optional) You can also provide a link to the process, which will provide a button to click to go to that link from the Atlan UI when viewing the process in Atlan. Call the save() method to actually create the process. Check that a Process was created. Check that only 1 Process was created. Check that tables were updated. Check that 5 tables (3 inputs, 2 outputs) were updated. Create lineage between assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 val process = LineageProcess . creator ( // (1) \"Source 1, Source 2, Source 3 -> Target 1, Target 2\" , // (2) \"default/snowflake/1657025257\" , // (3) \"dag_123\" , // (4) listOf < ICatalog > ( // (5) Table . refByGuid ( \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ), Table . refByGuid ( \"d002dead-1655-4d75-abd6-ad889fa04bd4\" ), Table . refByQualifiedName ( \"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS\" )), listOf < ICatalog > ( // (6) Table . refByGuid ( \"86d9a061-7753-4884-b988-a02d3954bc24\" ), Table . refByQualifiedName ( \"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS\" )), null ) // (7) . sql ( \"select * from somewhere;\" ) // (8) . sourceURL ( \"https://your.orchestrator/unique/id/123\" ) // (9) . build () val response = process . save ( client ) // (10) assert ( response . createdAssets . size == 1 ) // (11) assert ( response . updatedAssets . size == 5 ) // (12) Use the creator() method to initialize the object with all necessary attributes for creating it . Provide a name for how the process will be shown in the UI. Provide the qualifiedName of the connection that ran the process. Tips for the connection The process itself must be created within a connection for both access control and icon labelling. Use a connection qualifiedName that indicates the system that ran the process: You could use the same connection qualifiedName as the source system, if it was the source system \"pushing\" data to the target(s). You could use the same connection qualifiedName as the target system, if it was the target system \"pulling\" data from the source(s). You could use a different connection qualifiedName from either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator). (Optional) Provide the unique ID of the process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also send null and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the process. Use your own ID if you can While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra processes in lineage as this process itself changes over time. By using your own ID for the process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated. Provide the list of inputs to the process. Note that each of these is only a Reference to an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either: its GUID (for the static <Type>.refByGuid() method) its qualifiedName (for the static <Type>.refByQualifiedName() method) Provide the list of outputs to the process. Note that each of these is again only a Reference to an asset. (Optional) Provide the parent LineageProcess in which this process ran (for example, if this process is a subprocess of some higher-level process). If this is a top-level process, you can also send null for this parameter (as in this example). (Optional) You can also add other properties to the lineage process, such as SQL code that runs within the process. (Optional) You can also provide a link to the process, which will provide a button to click to go to that link from the Atlan UI when viewing the process in Atlan. Call the save() method to actually create the process. Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single lineage process asset that was created. The response will also include the 5 data assets (3 inputs, 2 outputs) that were updated. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"entities\" : [ // (1) { \"typeName\" : \"Process\" , // (2) \"attributes\" : { \"name\" : \"Source 1, Source 2, Source 3 -> Target 1, Target 2\" , // (3) \"qualifiedName\" : \"default/snowflake/1657025257/dag_123\" , // (4) \"inputs\" : [ // (5) { \"typeName\" : \"Table\" , \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" }, { \"typeName\" : \"Table\" , \"guid\" : \"d002dead-1655-4d75-abd6-ad889fa04bd4\" }, { \"typeName\" : \"Table\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS\" } } ], \"outputs\" : [ // (6) { \"typeName\" : \"Table\" , \"guid\" : \"86d9a061-7753-4884-b988-a02d3954bc24\" }, { \"typeName\" : \"Table\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS\" } } ] } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for a Process asset (case-sensitive). You must provide a name of the integration process. You must provide a unique qualifiedName for the integration process (case-sensitive). You must list all of the input assets to the process. These can be referenced by GUID or by qualifiedName . You must list all of the output assets from the process. These can also be referenced by either GUID or qualifiedName . Using OpenLineage Â¶ Creating connection for OpenLineage Â¶ 6.0.0 You must first configure OpenLineage before creating lineage between assets. You can either configure a Spark Assets connection in Atlan before sending any OpenLineage events. (You can skip the Configure the integration in Apache Spark section), or you can follow the steps below to create the Spark connection via SDKs. Java Python Kotlin Raw REST API Coming soon Create Spark connection for using OpenLineage 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import AtlanConnectorType client = AtlanClient () admin_role_guid = client . role_cache . get_id_for_name ( \"$admin\" ) #(1) spark_connection = client . open_lineage . create_connection ( #(2) name = \"open_lineage_connection\" , connector_type = AtlanConnectorType . SPARK , admin_roles = [ admin_role_guid ], admin_users = [ \"jsmith\" ], admin_groups = [ \"group2\" ], ) Retrieve the GUID for the admin role, to use later for defining the roles that can administer the connection. To create OpenLineage connection using the open_lineage.create_connection() method. Below params are required: name : Provide a human-readable name for your connections. connector_type : Set the type of connection. Defaults to AtlanConnectorType.SPARK . (Optional) admin_roles : List the workspace roles that should be able to administer the connection (if any, defaults to None ). All users with that workspace role (current and future) will be administrators of the connection. Note that the values here need to be the GUID(s) of the workspace role(s). At least one of admin_roles, admin_groups, or admin_users must be provided. (Optional) admin_users : List the user names that can administer this connection (if any, defaults to None ). Note that the values here are the username(s) of the user(s). At least one of admin_roles, admin_groups, or admin_users must be provided. (Optional) admin_groups : List the group names that can administer this connection (if any, defaults to None ). All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s). At least one of admin_roles, admin_groups, or admin_users must be provided. Warning Note: At least one of the optional parameters admin_roles , admin_users , or admin_groups must be provided to successfully create the connection. Coming soon POST /api/service/credentials?testCredential=true 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"authType\" : \"atlan_api_key\" , // (1) \"name\" : \"default-spark-1716979138-0\" , //(2) \"connector\" : \"spark\" , // (3) \"connectorConfigName\" : \"atlan-connectors-spark\" , // (4) \"connectorType\" : \"event\" , // (5) \"extra\" : { \"events.enable-partial-assets\" : true , \"events.enabled\" : true , \"events.topic\" : \"openlineage_spark\" , // (6) \"events.urlPath\" : \"/events/openlineage/spark/api/v1/lineage\" // (7) } } The authType must be exactly atlan_api_key . Human-readable name for your credential which should follow the pattern: default-spark-<epoch>-0 , where <epoch> is the time in milliseconds at which the credential is being created. The connector must be exactly spark . The connectorConfigName must be exactly atlan-connectors-spark . The connectorType must be exactly event . The events.topic must be exactly openlineage_spark . The events.urlPath must be exactly /events/openlineage/spark/api/v1/lineages . POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"entities\" : [ { \"typeName\" : \"Connection\" , // (1) \"attributes\" : { \"name\" : \"open_lineage_connection\" , // (2) \"connectorName\" : \"spark\" , // (3) \"qualifiedName\" : \"default/spark/123456789\" , // (4) \"category\" : \"connector\" , // (5) \"defaultCredentialGuid\" : \"8b579147-6054-4a4c-8137-463cd349b393\" , // (6) \"adminRoles\" : [ // (7) \"e7ae0295-c60a-469a-bd2c-fb903943aa02\" ], \"adminGroups\" : [ // (8) \"group2\" ], \"adminUsers\" : [ // (9) \"jsmith\" ] } } ] } The typeName must be exactly Connection . Human-readable name for your connection, such as production or development . The connectorName must be exactly spark . Determines the icon This determines the icon that Atlan will use for all the assets in the connection. If you use a value that is not a known value, you will have a default gear icon instead. The qualifiedName should follow the pattern: default/spark/<epoch> , where <epoch> is the time in milliseconds at which the connection is being created. The category must be exactly connector . The defaultCredentialGuid should be obtained from the id in the response of the previous request. List any workspace roles that can administer this connection. All users with that workspace role (current and future) will be administrators of the connection. Note that the values here need to be the GUID(s) of the workspace role(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. List any groups that can administer this connection. All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. List any users that can administer this connection. Note that the values here are the username(s) of the user(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. Creating lineage between assets using OpenLineage Â¶ 2.5.1 4.0.0 To create lineage between assets through OpenLineage , you need to send at least two events: one indicating the start of a job run and the other indicating that job run is finished. Java Python Kotlin Raw REST API Start lineage between assets via OpenLineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 String snowflake = \"snowflake://abc123.snowflakecomputing.com\" ; // (1) OpenLineageJob olj = OpenLineageJob . creator ( // (2) \"ol-spark\" , \"dag_123\" , \"https://your.orchestrator/unique/id/123\" ). build (); OpenLineageRun olr = OpenLineageRun . creator ( olj ). build (); // (3) OpenLineageInputDataset inputDataset = olj . createInput ( snowflake , \"OPS.DEFAULT.RUN_STATS\" ) . build (); // (4) OpenLineageOutputDataset outputDataset = olj . createOutput ( snowflake , \"OPS.DEFAULT.FULL_STATS\" ) . build (); // (5) OpenLineageEvent start = OpenLineageEvent . creator ( // (6) olr , OpenLineage . RunEvent . EventType . START ) . input ( inputDataset ) // (7) . input ( olj . createInput ( snowflake , \"SOME.OTHER.TBL\" ). build ()) . input ( olj . createInput ( snowflake , \"AN.OTHER.TBL\" ). build ()) . output ( outputDataset ) // (8) . output ( olj . createOutput ( snowflake , \"AN.OTHER.VIEW\" ). build ()) . build (); start . emit ( client ); // (9) Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . Lineage is tracked through jobs. Each job must have: the name of a connection (that already exists in Atlan), a unique job name (used to idempotently update the same job with multiple runs), and a unique URI indicating the code or system responsible for producing this lineage. A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked through OpenLineageRun objects. You can define any number of inputs (sources) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . You can define any number of outputs (targets) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . Each run of a job must consist of at least two events â€” a START event indicating when the job ran began, and some terminal state indicating when the job run finished. You can chain any number of input s to the event to indicate the source datasets for the lineage. You can chain any number of output s to the event to indicate the target datasets for the lineage. Use the emit() method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successful emit() will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Complete lineage between assets via OpenLineage 1 2 3 4 5 OpenLineageEvent complete = OpenLineageEvent . creator ( // (1) olr , OpenLineage . RunEvent . EventType . COMPLETE ). build (); complete . emit ( client ); // (2) Since each run of a job must consist of at least two events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with a COMPLETE or had some error with a FAIL .) Once again, use the emit() method to actually send the event to Atlan to be processed (asynchronously). Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Start lineage between assets via OpenLineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import OpenLineageEventType from pyatlan.model.open_lineage import OpenLineageEvent , OpenLineageJob , OpenLineageRun client = AtlanClient () snowflake = \"snowflake://abc123.snowflakecomputing.com\" # (1) job = OpenLineageJob . creator ( # (2) connection_name = \"ol-spark\" , job_name = \"dag_123\" , producer = \"https://your.orchestrator/unique/id/123\" ) run = OpenLineageRun . creator ( job = job ) # (3) input_dataset = job . create_input ( namespace = snowflake , asset_name = \"OPS.DEFAULT.RUN_STATS\" ) # (4) output_dataset = job . create_output ( namespace = snowflake , asset_name = \"OPS.DEFAULT.FULL_STATS\" ) # (5) start = OpenLineageEvent . creator ( run = run , event_type = OpenLineageEventType . START ) # (6) start . inputs = [ input_dataset , job . create_input ( namespace = snowflake , asset_name = \"SOME.OTHER.TBL\" ), job . create_input ( namespace = snowflake , asset_name = \"AN.OTHER.TBL\" ), ] # (7) start . outputs = [ output_dataset , job . create_output ( namespace = snowflake , asset_name = \"AN.OTHER.VIEW\" ) ] # (8) start . emit ( client = client ) # (9) Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . Lineage is tracked through jobs. Each job must have: the name of a connection (that already exists in Atlan), a unique job name (used to idempotently update the same job with multiple runs), and a unique URI indicating the code or system responsible for producing this lineage. A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked through OpenLineageRun objects. You can define any number of inputs (sources) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . You can define any number of outputs (targets) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . Each run of a job must consist of at least two events â€” a START event indicating when the job ran began, and some terminal state indicating when the job run finished. You can chain any number of input s to the event to indicate the source datasets for the lineage. You can chain any number of output s to the event to indicate the target datasets for the lineage. Use the emit() method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successful emit() will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Complete lineage between assets via OpenLineage 1 2 3 4 5 complete = OpenLineageEvent . creator ( run = run , event_type = OpenLineageEventType . COMPLETE ) # (1) complete . emit ( client = client ) # (2) Since each run of a job must consist of at least two events,\ndo not forget to send the terminal state indicating when the job\nhas finished (and whether it was successful with a COMPLETE or had some error with a FAIL .) Once again, use the emit() method to actually send the\nevent to Atlan to be processed (asynchronously). Start lineage between assets via OpenLineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 val snowflake = \"snowflake://abc123.snowflakecomputing.com\" // (1) val olj = OpenLineageJob . creator ( // (2) \"ol-spark\" , \"dag_123\" , \"https://your.orchestrator/unique/id/123\" ). build () val olr = OpenLineageRun . creator ( olj ). build () // (3) val inputDataset = olj . createInput ( snowflake , \"OPS.DEFAULT.RUN_STATS\" ) . build () // (4) val outputDataset = olj . createOutput ( snowflake , \"OPS.DEFAULT.FULL_STATS\" ) . build () // (5) val start = OpenLineageEvent . creator ( // (6) olr , OpenLineage . RunEvent . EventType . START ) . input ( inputDataset ) // (7) . input ( olj . createInput ( snowflake , \"SOME.OTHER.TBL\" ). build ()) . input ( olj . createInput ( snowflake , \"AN.OTHER.TBL\" ). build ()) . output ( outputDataset ) // (8) . output ( olj . createOutput ( snowflake , \"AN.OTHER.VIEW\" ). build ()) . build () start . emit ( client ) // (9) Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . Lineage is tracked through jobs. Each job must have: the name of a connection (that already exists in Atlan), a unique job name (used to idempotently update the same job with multiple runs), and a unique URI indicating the code or system responsible for producing this lineage. A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked through OpenLineageRun objects. You can define any number of inputs (sources) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . You can define any number of outputs (targets) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . Each run of a job must consist of at least two events â€” a START event indicating when the job ran began, and some terminal state indicating when the job run finished. You can chain any number of input s to the event to indicate the source datasets for the lineage. You can chain any number of output s to the event to indicate the target datasets for the lineage. Use the emit() method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successful emit() will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Complete lineage between assets via OpenLineage 1 2 3 4 5 val complete = OpenLineageEvent . creator ( // (1) olr , OpenLineage . RunEvent . EventType . COMPLETE ). build () complete . emit ( client ) // (2) Since each run of a job must consist of at least two events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with a COMPLETE or had some error with a FAIL .) Once again, use the emit() method to actually send the event to Atlan to be processed (asynchronously). Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /events/openlineage/spark/api/v1/lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 { \"eventTime\" : \"2024-07-01T08:23:37.491542Z\" , // (1) \"producer\" : \"https://your.orchestrator/unique/id/123\" , // (2) \"schemaURL\" : \"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\" , \"eventType\" : \"START\" , // (3) \"job\" : { // (4) \"namespace\" : \"ol-spark\" , \"name\" : \"dag_123\" , \"facets\" : {} }, \"run\" : { // (5) \"runId\" : \"eefd52c3-5871-4f0e-8ff5-237e9a6efb53\" , \"facets\" : {} }, \"inputs\" : [ // (6) { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"OPS.DEFAULT.RUN_STATS\" , \"facets\" : {} }, { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"SOME.OTHER.TBL\" , \"facets\" : {} }, { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"AN.OTHER.TBL\" , \"facets\" : {} } ], \"outputs\" : [ // (7) { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"OPS.DEFAULT.FULL_STATS\" , \"facets\" : {} }, { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"AN.OTHER.VIEW\" , \"facets\" : {} } ] } Each event for a job run must have a time at which the event occurred. Each event must have a URI indicating the code or system responsible for producing this lineage. Each run of a job must consist of at least two events â€” a START event indicating when the job ran began, and some terminal state indicating when the job run finished. Lineage is tracked through jobs. Each job must have: the name of a connection (that already exists in Atlan) as its namespace , a unique job name (used to idempotently update the same job with multiple runs) A job must be run at least once for any lineage to exist, and each event for the same run of a job must be associated with the same runId . You can define any number of inputs (sources) for lineage. Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . You can define any number of outputs (targets) for lineage. Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . POST /events/openlineage/spark/api/v1/lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"eventTime\" : \"2024-07-01T08:23:38.360567Z\" , \"producer\" : \"https://your.orchestrator/unique/id/123\" , \"schemaURL\" : \"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\" , \"eventType\" : \"COMPLETE\" , // (1) \"run\" : { \"runId\" : \"eefd52c3-5871-4f0e-8ff5-237e9a6efb53\" , \"facets\" : {} }, \"job\" : { \"namespace\" : \"ol-spark\" , \"name\" : \"dag_123\" , \"facets\" : {} } } Since each run of a job must consist of at least two events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with a COMPLETE or had some error with a FAIL .) Create lineage between columns Â¶ Directly Â¶ 2.0.0 4.0.0 To create lineage between relational asset columns ,\nit is necessary to create a ColumnProcess entity. Lineage with relational columns Before creating the ColumnProcess, verify lineage already exists between the associated relational assets , and ensure that the\ncolumns referenced as inputs and outputs already exist. Java Python Kotlin Raw REST API Create lineage between columns 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ColumnProcess columnProcess = ColumnProcess . creator ( // (1) \"Source 1, Source 2, Source 3 -> Target 1, Target 2\" , // (2) \"default/snowflake/1657025257\" , // (3) \"dag_123\" , // (4) List . of ( // (5) Column . refByGuid ( \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ), Column . refByGuid ( \"d002dead-1655-4d75-abd6-ad889fa04bd4\" ), Column . refByQualifiedName ( \"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS/COLUMN\" )), List . of ( // (6) Column . refByGuid ( \"86d9a061-7753-4884-b988-a02d3954bc24\" ), Column . refByQualifiedName ( \"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS/COLUMN\" )), Process . refByGuid ( \"76d9a061-7753-9884-b988-a02d3954bc25\" )) // (7) . sql ( \"select * from somewhere;\" ) // (8) . sourceURL ( \"https://your.orchestrator/unique/id/123\" ) // (9) . build (); AssetMutationResponse response = columnProcess . save ( client ); // (10) assert response . getCreatedAssets (). size () == 1 // (11) assert response . getUpdatedAssets (). size () == 5 // (12) Use the creator() method to initialize the object with all necessary attributes for creating it . Provide a name for how the column process will be shown in the UI. Provide the qualifiedName of the connection that ran the column process. Tips for the connection The column process itself must be created within a connection for both access control and icon labelling. Use a connection qualifiedName that indicates the system that ran the column process: You could use the same connection qualifiedName as the source system, if it was the source system \"pushing\" data to the target(s). You could use the same connection qualifiedName as the target system, if it was the target system \"pulling\" data from the source(s). You could use a different connection qualifiedName from either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator). (Optional) Provide the unique ID of the column process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also send null and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the column process. Use your own ID if you can While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra column processes in lineage as this process itself changes over time. By using your own ID for the column process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated. Provide the list of inputs to the column process. Note that each of these is only a Reference to an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either: its GUID (for the static <Type>.refByGuid() method) its qualifiedName (for the static <Type>.refByQualifiedName() method) Provide the list of outputs to the column process. Note that each of these is again only a Reference to an asset. Provide the parent LineageProcess in which this process ran since this process is a subprocess of some higher-level process. (Optional) You can also add other properties to the column process, such as SQL code that runs within the column process. (Optional) You can also provide a link to the column process, which will provide a button to click to go to that link from the Atlan UI when viewing the column process in Atlan. Call the save() method to actually create the column process. Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single column process asset that was created. The response will also include the 5 column assets (3 inputs, 2 outputs) that were updated. Create lineage between columns 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Process , ColumnProcess , Column client = AtlanClient () column_process = ColumnProcess . creator ( # (1) name = \"Source 1, Source 2, Source 3 -> Target 1, Target 2\" , # (2) connection_qualified_name = \"default/snowflake/1657025257\" , # (3) process_id = \"dag_123\" , # (4) inputs = [ # (5) Column . ref_by_guid ( guid = \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ), Column . ref_by_guid ( guid = \"d002dead-1655-4d75-abd6-ad889fa04bd4\" ), Column . ref_by_qualified_name ( qualified_name = \"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS/COLUMN\" ), ], outputs = [ # (6) Column . ref_by_guid ( guid = \"86d9a061-7753-4884-b988-a02d3954bc24\" ), Column . ref_by_qualified_name ( qualified_name = \"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS/COLUMN\" ), ], parent = Process . ref_by_guid ( \"76d9a061-7753-9884-b988-a02d3954bc25\" ), ) # (7) column_process . sql = \"select * from somewhere;\" # (8) column_process . source_url = \"https://your.orchestrator/unique/id/123\" # (9) response = client . asset . save ( column_process ) # (10) assert ( column_processes := response . assets_created ( ColumnProcess )) # (11) assert len ( column_processes ) == 1 # (12) assert ( columns := response . assets_updated ( Column )) # (13) assert len ( columns ) == 2 # (14) Use the create() method to initialize the object with all necessary attributes for creating it . Provide a name for how the column process will be shown in the UI. Provide the qualified_name of the connection that ran the column process. Tips for the connection The column process itself must be created within a connection for both access control and icon labelling. Use a connection qualified_name that indicates the system that ran the column process: You could use the same connection qualified_name as the source system, if it was the source system \"pushing\" data to the target(s). You could use the same connection qualified_name as the target system, if it was the target system \"pulling\" data from the source(s). You could use a different connection qualified_name from either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator). (Optional) Provide the unique ID of the column process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also leave it out and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the column process. Use your own ID if you can While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra column processes in lineage as this column process itself changes over time. By using your own ID for the column process, any changes that occur in that column process over time (even if the inputs or outputs change) the same single column process in Atlan will be updated. Provide the list of inputs to the column process. Note that each of these is only a Reference to an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either: its GUID (for the ref_by_guid() method) its qualifiedName (for the ref_by_qualified_name() method) Provide the list of outputs to the column process. Note that each of these is again only a Reference to an asset. Provide the parent Process in which this process ran since this process is a subprocess of some  higher-level process. (Optional) You can also add other properties to the column process, such as SQL code that runs within the column process. (Optional) You can also provide a link to the column process, which will provide a button to click to go to that link from the Atlan UI when viewing the column process in Atlan. Call the save() method to actually create the column process. Check that a ColumnProcess was created. Check that only 1 ColumnProcess was created. Check that tables were updated. Check that 5 tables (3 inputs, 2 outputs) were updated. Create lineage between columns 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 val columnProcess = ColumnProcess . creator ( // (1) \"Source 1, Source 2, Source 3 -> Target 1, Target 2\" , // (2) \"default/snowflake/1657025257\" , // (3) \"dag_123\" , // (4) listOf < ICatalog > ( // (5) Column . refByGuid ( \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ), Column . refByGuid ( \"d002dead-1655-4d75-abd6-ad889fa04bd4\" ), Column . refByQualifiedName ( \"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS/COLUMN\" )), listOf < ICatalog > ( // (6) Column . refByGuid ( \"86d9a061-7753-4884-b988-a02d3954bc24\" ), Column . refByQualifiedName ( \"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS/COLUMN\" )), Process . refByGuid ( \"76d9a061-7753-9884-b988-a02d3954bc25\" )) // (7) . sql ( \"select * from somewhere;\" ) // (8) . sourceURL ( \"https://your.orchestrator/unique/id/123\" ) // (9) . build () val response = columnProcess . save ( client ) // (10) assert ( response . createdAssets . size == 1 ) // (11) assert ( response . updatedAssets . size == 5 ) // (12) Use the creator() method to initialize the object with all necessary attributes for creating it . Provide a name for how the column process will be shown in the UI. Provide the qualifiedName of the connection that ran the column process. Tips for the connection The column process itself must be created within a connection for both access control and icon labelling. Use a connection qualifiedName that indicates the system that ran the column process: You could use the same connection qualifiedName as the source system, if it was the source system \"pushing\" data to the target(s). You could use the same connection qualifiedName as the target system, if it was the target system \"pulling\" data from the source(s). You could use a different connection qualifiedName from either source or target, if there is a system in-between doing the processing (for example an ETL engine or orchestrator). (Optional) Provide the unique ID of the column process within that connection. This could be the unique DAG ID for an orchestrator, for example. Since it is optional, you can also send null and the SDK will generate a unique ID for you based on the unique combination of inputs and outputs for the column process. Use your own ID if you can While the SDK can generate this ID for you, since it is based on the unique combination of inputs and outputs the ID can change if those inputs or outputs change. This could result in extra column processes in lineage as this process itself changes over time. By using your own ID for the column process, any changes that occur in that process over time (even if the inputs or outputs change) the same single process in Atlan will be updated. Provide the list of inputs to the column process. Note that each of these is only a Reference to an asset, not a full asset object. For a reference you only need (in addition to the type of asset) either: its GUID (for the static <Type>.refByGuid() method) its qualifiedName (for the static <Type>.refByQualifiedName() method) Provide the list of outputs to the column process. Note that each of these is again only a Reference to an asset. Provide the parent LineageProcess in which this process ran since this process is a subprocess of some higher-level process. (Optional) You can also add other properties to the column process, such as SQL code that runs within the column process. (Optional) You can also provide a link to the column process, which will provide a button to click to go to that link from the Atlan UI when viewing the column process in Atlan. Call the save() method to actually create the column process. Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single column process asset that was created. The response will also include the 5 column assets (3 inputs, 2 outputs) that were updated. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 { \"entities\" : [ // (1) { \"typeName\" : \"ColumnProcess\" , // (2) \"attributes\" : { \"name\" : \"Source 1, Source 2, Source 3 -> Target 1, Target 2\" , // (3) \"qualifiedName\" : \"default/snowflake/1657025257/dag_123\" , // (4) \"inputs\" : [ // (5) { \"typeName\" : \"Column\" , \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" }, { \"typeName\" : \"Column\" , \"guid\" : \"d002dead-1655-4d75-abd6-ad889fa04bd4\" }, { \"typeName\" : \"Column\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657025257/OPS/DEFAULT/RUN_STATS\" } } ], \"outputs\" : [ // (6) { \"typeName\" : \"Column\" , \"guid\" : \"86d9a061-7753-4884-b988-a02d3954bc24\" }, { \"typeName\" : \"Column\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657025257/OPS/DEFAULT/FULL_STATS\" } } ], \"process\" : { // (7) \"guid\" : \"76d9a061-7753-9884-b988-a02d3954bc25\" , \"typeName\" : \"Process\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/snowflake/1657025257/parent_123\" } } } } ] } All assets must be wrapped in an entities array. You must provide the exact type name for a ColumnProcess asset (case-sensitive). You must provide a name of the integration column process. You must provide a unique qualifiedName for the integration column process (case-sensitive). You must list all of the input assets to the column process. These can be referenced by GUID or by qualifiedName . You must list all of the output assets from the column process. These can also be referenced by either GUID or qualifiedName . You must provide the parent LineageProcess in which this process ran since this process is a subprocess of some higher-level process. Using OpenLineage Â¶ 2.5.1 4.0.0 To create column-lineage between assets through OpenLineage , you need only extend the details of the outputs you send in your OpenLineage events. You must first configure OpenLineage You must first configure a Spark Assets connection in Atlan before sending any OpenLineage events. (You can skip the Configure the integration in Apache Spark section.) Java Python Kotlin Raw REST API Start column-level lineage between assets via OpenLineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 String snowflake = \"snowflake://abc123.snowflakecomputing.com\" ; // (1) OpenLineageJob olj = OpenLineageJob . creator ( // (2) \"ol-spark\" , \"dag_123\" , \"https://your.orchestrator/unique/id/123\" ). build (); OpenLineageRun olr = OpenLineageRun . creator ( olj ). build (); // (3) OpenLineageInputDataset inputDataset = olj . createInput ( snowflake , \"OPS.DEFAULT.RUN_STATS\" ) . build (); // (4) OpenLineageOutputDataset outputDataset = olj . createOutput ( snowflake , \"OPS.DEFAULT.FULL_STATS\" ) // (5) . toField ( // (6) \"COLUMN\" , // (7) listOf ( // (8) inputDataset . fromField ( \"COLUMN\" ). build (), inputDataset . fromField ( \"ONE\" ). build (), inputDataset . fromField ( \"TWO\" ). build (), ), ) . toField ( \"ANOTHER\" , listOf ( inputDataset . fromField ( \"THREE\" ). build (), ), ) . build (); OpenLineageEvent start = OpenLineageEvent . creator ( // (9) olr , OpenLineage . RunEvent . EventType . START ) . input ( inputDataset ) // (10) . input ( olj . createInput ( snowflake , \"SOME.OTHER.TBL\" ). build ()) . input ( olj . createInput ( snowflake , \"AN.OTHER.TBL\" ). build ()) . output ( outputDataset ) // (11) . output ( olj . createOutput ( snowflake , \"AN.OTHER.VIEW\" ). build ()) . build (); start . emit ( client ); // (12) Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . Lineage is tracked through jobs. Each job must have: the name of a connection (that already exists in Atlan), a unique job name (used to idempotently update the same job with multiple runs), and a unique URI indicating the code or system responsible for producing this lineage. A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked through OpenLineageRun objects. You can define any number of inputs (sources) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . You can define any number of outputs (targets) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . For column-level lineage, you specify the mapping only on the target (outputs) end of the lineage, by chaining a toField for each output column. Each key for such a toField() chain is the name of a field (column) in the output dataset. You can then provide a list that defines all input (source) fields that map to this output field in column-level lineage. Create input fields from input datasets You can quickly create such a input (source) field from an input dataset using the fromField() method and the name of the column in that input dataset. Each run of a job must consist of at least two events â€” a START event indicating when the job ran began, and some terminal state indicating when the job run finished. You can chain any number of input s to the event to indicate the source datasets for the lineage. You can chain any number of output s to the event to indicate the target datasets for the lineage. Use the emit() method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successful emit() will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Complete lineage between assets via OpenLineage 1 2 3 4 5 OpenLineageEvent complete = OpenLineageEvent . creator ( // (1) olr , OpenLineage . RunEvent . EventType . COMPLETE ). build (); complete . emit ( client ); // (2) Since each run of a job must consist of at least two events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with a COMPLETE or had some error with a FAIL .) Once again, use the emit() method to actually send the event to Atlan to be processed (asynchronously). Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Start column-level lineage between assets via OpenLineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import OpenLineageEventType from pyatlan.model.open_lineage import OpenLineageEvent , OpenLineageJob , OpenLineageRun client = AtlanClient () snowflake = \"snowflake://abc123.snowflakecomputing.com\" # (1) job = OpenLineageJob . creator ( # (2) connection_name = \"ol-spark\" , job_name = \"dag_123\" , producer = \"https://your.orchestrator/unique/id/123\" ) run = OpenLineageRun . creator ( job = job ) # (3) input_dataset = job . create_input ( namespace = snowflake , asset_name = \"OPS.DEFAULT.RUN_STATS\" ) # (4) output_dataset = job . create_output ( namespace = snowflake , asset_name = \"OPS.DEFAULT.FULL_STATS\" ) # (5) output_dataset . to_fields = [ # (6) { # (7) \"COLUMN\" : [ input_dataset . from_field ( field_name = \"COLUMN\" ), input_dataset . from_field ( field_name = \"ONE\" ), input_dataset . from_field ( field_name = \"TWO\" ), ] # (8) }, { \"ANOTHER\" : [ input_dataset . from_field ( field_name = \"THREE\" ), ] }, ] start = OpenLineageEvent . creator ( run = run , event_type = OpenLineageEventType . START ) # (9) start . inputs = [ input_dataset , job . create_input ( namespace = snowflake , asset_name = \"SOME.OTHER.TBL\" ), job . create_input ( namespace = snowflake , asset_name = \"AN.OTHER.TBL\" ), ] # (10) start . outputs = [ output_dataset , job . create_output ( namespace = snowflake , asset_name = \"AN.OTHER.VIEW\" ) ] # (11) start . emit () # (12) Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . Lineage is tracked through jobs. Each job must have: the name of a connection (that already exists in Atlan), a unique job name (used to idempotently update the same job with multiple runs), and a unique URI indicating the code or system responsible for producing this lineage. A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked through OpenLineageRun objects. You can define any number of inputs (sources) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . You can define any number of outputs (targets) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . For column-level lineage, you specify the mapping only on the target (outputs) end of the lineage to the to_fields attribute. Each key is the name of a field (column) in the output dataset. You can then provide a list that defines all input (source)\nfields that map to this output field in column-level lineage. Create input fields from input datasets You can quickly create such a input (source) field from an input dataset using the from_Field() method and the name of the column in that input dataset. Each run of a job must consist of at least two events â€” a START event indicating when the job ran began, and some terminal state indicating when the job run finished. You can chain any number of input s to the event to indicate the source datasets for the lineage. You can chain any number of output s to the event to indicate the target datasets for the lineage. Use the emit() method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successful emit() will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Complete lineage between assets via OpenLineage 1 2 3 4 5 complete = OpenLineageEvent . creator ( run = run , event_type = OpenLineageEventType . COMPLETE ) # (1) complete . emit () # (2) Since each run of a job must consist of at least two events,\ndo not forget to send the terminal state indicating when the job\nhas finished (and whether it was successful with a COMPLETE or had some error with a FAIL .) Once again, use the emit() method to actually send the\nevent to Atlan to be processed (asynchronously). Start column-level lineage between assets via OpenLineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 val snowflake = \"snowflake://abc123.snowflakecomputing.com\" // (1) val olj = OpenLineageJob . creator ( // (2) \"ol-spark\" , \"dag_123\" , \"https://your.orchestrator/unique/id/123\" ). build () val olr = OpenLineageRun . creator ( olj ). build () // (3) val inputDataset = olj . createInput ( snowflake , \"OPS.DEFAULT.RUN_STATS\" ) . build () // (4) val outputDataset = olj . createOutput ( snowflake , \"OPS.DEFAULT.FULL_STATS\" ) // (5) . toField ( // (6) \"COLUMN\" , // (7) listOf ( // (8) inputDataset . fromField ( \"COLUMN\" ). build (), inputDataset . fromField ( \"ONE\" ). build (), inputDataset . fromField ( \"TWO\" ). build (), ), ) . toField ( \"ANOTHER\" , listOf ( inputDataset . fromField ( \"THREE\" ). build (), ), ) . build () val start = OpenLineageEvent . creator ( // (9) olr , OpenLineage . RunEvent . EventType . START ) . input ( inputDataset ) // (10) . input ( olj . createInput ( snowflake , \"SOME.OTHER.TBL\" ). build ()) . input ( olj . createInput ( snowflake , \"AN.OTHER.TBL\" ). build ()) . output ( outputDataset ) // (11) . output ( olj . createOutput ( snowflake , \"AN.OTHER.VIEW\" ). build ()) . build () start . emit ( client ) // (12) Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . Lineage is tracked through jobs. Each job must have: the name of a connection (that already exists in Atlan), a unique job name (used to idempotently update the same job with multiple runs), and a unique URI indicating the code or system responsible for producing this lineage. A job must be run at least once for any lineage to exist, and these separate runs of the same job are tracked through OpenLineageRun objects. You can define any number of inputs (sources) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . You can define any number of outputs (targets) for lineage. The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . For column-level lineage, you specify the mapping only on the target (outputs) end of the lineage, by chaining a toField for each output column. Each key for such a toField() chain is the name of a field (column) in the output dataset. You can then provide a list that defines all input (source) fields that map to this output field in column-level lineage. Create input fields from input datasets You can quickly create such a input (source) field from an input dataset using the fromField() method and the name of the column in that input dataset. Each run of a job must consist of at least two events â€” a START event indicating when the job ran began, and some terminal state indicating when the job run finished. You can chain any number of input s to the event to indicate the source datasets for the lineage. You can chain any number of output s to the event to indicate the target datasets for the lineage. Use the emit() method to actually send the event to Atlan to be processed. The processing itself occurs asynchronously, so a successful emit() will only indicate that the event has been successfully sent to Atlan, not that it has (yet) been processed. Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Complete lineage between assets via OpenLineage 1 2 3 4 5 val complete = OpenLineageEvent . creator ( // (1) olr , OpenLineage . RunEvent . EventType . COMPLETE ). build () complete . emit ( client ) // (2) Since each run of a job must consist of at least two events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with a COMPLETE or had some error with a FAIL .) Once again, use the emit() method to actually send the event to Atlan to be processed (asynchronously). Because this operation will directly persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /events/openlineage/spark/api/v1/lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 { \"eventTime\" : \"2024-07-01T08:23:37.491542Z\" , // (1) \"producer\" : \"https://your.orchestrator/unique/id/123\" , // (2) \"schemaURL\" : \"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\" , \"eventType\" : \"START\" , // (3) \"job\" : { // (4) \"namespace\" : \"ol-spark\" , \"name\" : \"dag_123\" , \"facets\" : {} }, \"run\" : { // (5) \"runId\" : \"eefd52c3-5871-4f0e-8ff5-237e9a6efb53\" , \"facets\" : {} }, \"inputs\" : [ // (6) { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"OPS.DEFAULT.RUN_STATS\" , \"facets\" : {} }, { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"SOME.OTHER.TBL\" , \"facets\" : {} }, { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"AN.OTHER.TBL\" , \"facets\" : {} } ], \"outputs\" : [ // (7) { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"OPS.DEFAULT.FULL_STATS\" , \"facets\" : { \"columnLineage\" : { // (8) \"_producer\" : \"https://your.orchestrator/unique/id/123\" , \"_schemaURL\" : \"https://openlineage.io/spec/facets/1-1-0/ColumnLineageDatasetFacet.json#/$defs/ColumnLineageDatasetFacet\" , \"fields\" : { \"COLUMN\" : { // (9) \"inputFields\" : [ // (10) { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"OPS.DEFAULT.RUN_STATS\" , \"field\" : \"COLUMN\" }, { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"OPS.DEFAULT.RUN_STATS\" , \"field\" : \"ONE\" }, { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"OPS.DEFAULT.RUN_STATS\" , \"field\" : \"TWO\" } ] }, \"ANOTHER\" : { \"inputFields\" : [ { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"OPS.DEFAULT.RUN_STATS\" , \"field\" : \"THREE\" } ] } } } } }, { \"namespace\" : \"snowflake://abc123.snowflakecomputing.com\" , \"name\" : \"AN.OTHER.VIEW\" , \"facets\" : {} } ] } Each event for a job run must have a time at which the event occurred. Each event must have a URI indicating the code or system responsible for producing this lineage. Each run of a job must consist of at least two events â€” a START event indicating when the job ran began, and some terminal state indicating when the job run finished. Lineage is tracked through jobs. Each job must have: the name of a connection (that already exists in Atlan) as its namespace , a unique job name (used to idempotently update the same job with multiple runs) A job must be run at least once for any lineage to exist, and each event for the same run of a job must be associated with the same runId . You can define any number of inputs (sources) for lineage. Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . You can define any number of outputs (targets) for lineage. Datasets used in data lineage need a namespace that follows the source-specific naming standards of OpenLineage . The name of a dataset should use a . -qualified form. For example, a table should be DATABASE_NAME.SCHEMA_NAME.TABLE_NAME . For column-level lineage, you specify the mapping only on the target (outputs) end of the lineage, by including a columnLineage facet with an embedded fields object. Each key for the fields object is the name of a field (column) in the output dataset. You can then provide a list that defines all input (source) fields that map to this output field in column-level lineage. POST /events/openlineage/spark/api/v1/lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 { \"eventTime\" : \"2024-07-01T08:23:38.360567Z\" , \"producer\" : \"https://your.orchestrator/unique/id/123\" , \"schemaURL\" : \"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\" , \"eventType\" : \"COMPLETE\" , // (1) \"run\" : { \"runId\" : \"eefd52c3-5871-4f0e-8ff5-237e9a6efb53\" , \"facets\" : {} }, \"job\" : { \"namespace\" : \"ol-spark\" , \"name\" : \"dag_123\" , \"facets\" : {} } } Since each run of a job must consist of at least two events, do not forget to send the terminal state indicating when the job has finished (and whether it was successful with a COMPLETE or had some error with a FAIL .) Remove lineage between assets Â¶ 7.0.0 4.0.0 To remove lineage between assets, you need to delete the Process entity that links them: Only deletes the process indicated, no more Also be aware that this will only delete the process with the GUID specified. It will not remove any column processes that may also exist. To remove those column processes as well, you must identify the GUID of each column-level process and call the same purge method against each of those GUIDs. Java Python Kotlin Raw REST API Remove lineage between assets 1 2 3 4 5 6 7 AssetMutationResponse response = Asset . purge ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ); // (1) Asset deleted = response . getDeletedAssets (). get ( 0 ); // (2) LineageProcess process ; if ( deleted instanceof LineageProcess ) { process = ( LineageProcess ) deleted ; // (3) } Provide the GUID for the process to the static Asset.purge() method. Because this operation will directly remove the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single process that was purged. If you want to confirm the details, you'll need to type-check and then cast the generic Asset returned into a Process . Remove lineage between assets 1 2 3 4 5 6 7 8 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Process client = AtlanClient () response = client . asset . purge_by_guid ( # (1) guid = \"b4113341-251b-4adc-81fb-2420501c30e6\" ) # (2) assert ( processes := response . assets_deleted ( Process )) # (3) assert len ( processes ) == 1 # (4) Invoke the asset.purge_by_guid to delete the Process . Provide the GUID of the process to be purged. Check that a Process was purged. Check that only 1 Process was purged. Remove lineage between assets 1 2 3 4 val response : AssetMutationResponse = Asset . purge ( client , \"b4113341-251b-4adc-81fb-2420501c30e6\" ) // (1) val deleted = response . deletedAssets [ 0 ] // (2) val process = if ( deleted is LineageProcess ) deleted else null // (3) Provide the GUID for the process to the static Asset.purge() method. Because this operation will directly remove the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. The response will include that single process that was purged. If you want to confirm the details, you'll need to type-check and then cast the generic Asset returned into a Process . DELETE /api/meta/entity/bulk?guid=6fa1f0d0-5720-4041-8243-c2a5628b68bf&deleteType=PURGE 1 // (1) All of the details are in the request URL, there is no payload for a deletion. The GUID for the process itself (not any of its inputs or outputs) is what is listed in the URL. More information This will irreversibly delete the process, and therefore the lineage it represented. The input and output assets themselves will also be updated, to no longer be linked to the (now non-existent) process. However, the input and output assets themselves will continue to exist in Atlan. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/profiling-and-popularity/",
    "content": "Profiling and popularity Â¶ Atlan crawls additional context from some sources, to give you information on the values that exist within the data and how the data is used. Profiling Â¶ Profiling gives additional context to columns in relational stores. From profiling, you can see various summarized information such as: numerical statistics (min, max, mean, median, standard deviation, sum, variance) for numeric columns minimum, maximum, and average lengths for string columns distinct value counts and percentages missing value counts and percentages Profiling is only available on columns You will only be able to populate this summary information on columns, not on other assets in Atlan. Popularity Â¶ Popularity gives additional context on how assets are being used. From popularity, you can see information such as: which users are querying assets the costs associated with those queries how long the queries run the queries themselves This information can be populated on all assets in Atlan. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/profiling-and-popularity/popularity/",
    "content": "/api/meta/entity/bulk (POST) /api/meta/entity/uniqueAttribute/type/{typeName}?attr:qualifiedName={qualifiedName} (GET) Manage popularity Â¶ Popularity gives additional context to the usage of assets. From popularity, you can see various information such as: users who have queried an asset, including: those who have most recently queried the asset those who have most frequently queried the asset details about queries against the asset, including: the most frequently run queries the slowest-running queries the most expensive queries Popularity is only populated on certain assets by Atlan Out-of-the-box, only specific crawlers currently populate popularity information on assets. However, the attributes involved are actually available behind-the-scenes on all assets. Make details visible in the UI Â¶ Usage details not visible by default By default, only the crawlers that support usage details out-of-the-box will show the usage details in the Atlan UI. To make usage details visible for other assets you must enable this at a connection level. During connection creation Â¶ 7.0.0 4.0.0 To create a popularity-enabled connection: Java Python Kotlin Raw REST API Create a popularity-enabled connection 1 2 3 4 5 6 7 8 9 10 String adminRoleGuid = client . getRoleCache (). getIdForName ( \"$admin\" ); // (1) Connection connection = Connection . creator ( // (2) \"development\" , // (3) AtlanConnectorType . HIVE , // (4) List . of ( adminRoleGuid ), // (5) List . of ( \"group2\" ), // (6) List . of ( \"jsmith\" )) // (7) . hasPopularityInsights ( true ) // (8) . build (); AssetMutationResponse response = connection . save ( client ); // (9) Retrieve the GUID for the admin role, to use later for defining the roles that can administer the connection. Build up the minimum request to create a connection. Provide a human-readable name for your connection. Set the type of connection. Determines the icon This determines the icon that Atlan will use for all the assets in the connection. List the workspace roles that should be able to administer the connection (or null if none). All users with that workspace role (current and future) will be administrators of the connection. Note that the values here need to be the GUID(s) of the workspace role(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. List the group names that can administer this connection (or null if none). All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. List the user names that can administer this connection (or null if none). Note that the values here are the username(s) of the user(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. Set the hasPopularityInsights property to true â€” this is the key piece to ensuring usage details will be visible in the UI for assets in this connection. Actually call Atlan to create the connection. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a relational connection 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Connection from pyatlan.model.enums import AtlanConnectorType client = AtlanClient () admin_role_guid = client . role_cache . get_id_for_name ( \"$admin\" ) # (1) connection = Connection . creator ( # (2) client = client , # (3) name = \"development\" , # (4) connector_type = AtlanConnectorType . HIVE , # (5) admin_roles = [ admin_role_guid ], # (6) admin_groups = [ \"group2\" ], # (7) admin_users = [ \"jsmith\" ], # (8) ) connection . has_popularity_insights = True # (9) response = client . asset . save ( connection ) # (10) Retrieve the GUID for the admin role, to use later for defining the roles that can administer the connection. Build up the minimum request to create a connection. You must provide a client instance. Provide a human-readable name for your connection. Set the type of connection. Determines the icon This determines the icon that Atlan will use for all the assets in the connection. List the workspace roles that should be able to administer the connection (if any, defaults to None ). All users with that workspace role (current and future) will be administrators of the connection. Note that the values here need to be the GUID(s) of the workspace role(s). At least one of admin_roles , admin_groups , or admin_users must be provided. List the group names that can administer this connection (if any, defaults to None ). All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s). At least one of admin_roles , admin_groups , or admin_users must be provided. List the user names that can administer this connection (if any, defaults to None ). Note that the values here are the username(s) of the user(s). At least one of admin_roles , admin_groups , or admin_users must be provided. Set the has_popularity_insights property to True â€” this is the key piece to ensuring usage details will be visible in the UI for assets in this connection. Actually call Atlan to create the connection. Create a popularity-enabled connection 1 2 3 4 5 6 7 8 9 10 val adminRoleGuid = client . roleCache . getIdForName ( \"\\ $ admin \" ) // (1) val connection = Connection . creator ( // (2) \"development\" , // (3) AtlanConnectorType . HIVE , // (4) java . util . List . of ( adminRoleGuid ), // (5) listOf ( \"group2\" ), // (6) listOf ( \"jsmith\" )) // (7) . hasPopularityInsights ( true ) // (8) . build () val response = connection . save ( client ) // (9) Retrieve the GUID for the admin role, to use later for defining the roles that can administer the connection. Build up the minimum request to create a connection. Provide a human-readable name for your connection. Set the type of connection. Determines the icon This determines the icon that Atlan will use for all the assets in the connection. List the workspace roles that should be able to administer the connection (or null if none). All users with that workspace role (current and future) will be administrators of the connection. Note that the values here need to be the GUID(s) of the workspace role(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. List the group names that can administer this connection (or null if none). All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. List the user names that can administer this connection (or null if none). Note that the values here are the username(s) of the user(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. Set the hasPopularityInsights property to true â€” this is the key piece to ensuring usage details will be visible in the UI for assets in this connection. Actually call Atlan to create the connection. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 { \"entities\" : [ { \"typeName\" : \"Connection\" , // (1) \"attributes\" : { \"name\" : \"development\" , // (2) \"connectorName\" : \"hive\" , // (3) \"qualifiedName\" : \"default/hive/123456789\" , // (4) \"category\" : \"warehouse\" , // (5) \"adminRoles\" : [ // (6) \"e7ae0295-c60a-469a-bd2c-fb903943aa02\" ], \"adminGroups\" : [ // (7) \"group2\" ], \"adminUsers\" : [ // (8) \"jsmith\" ], \"hasPopularityInsights\" : true // (9) } } ] } The typeName must be exactly Connection . Human-readable name for your connection. The connectorName should be a known value, such as hive . Determines the icon This determines the icon that Atlan will use for all the assets in the connection. If you use a value that is not a known value, you will have a default gear icon instead. The qualifiedName should follow the pattern: default/<connectorName>/<epoch> , where <epoch> is the time in milliseconds at which the connection is being created, and <connectorName> exactly matches the value used for connectorName (above). The category should also be a known value, that defines the kind of relational store. This could for example be warehouse or rdbms . List any workspace roles that can administer this connection. All users with that workspace role (current and future) will be administrators of the connection. Note that the values here need to be the GUID(s) of the workspace role(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. List any groups that can administer this connection. All users within that group (current and future) will be administrators of the connection. Note that the values here are the name(s) of the group(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. List any users that can administer this connection. Note that the values here are the username(s) of the user(s). At least one of adminRoles , adminGroups , or adminUsers must be provided. Set the hasPopularityInsights property to true â€” this is the key piece to ensuring usage details will be visible in the UI for assets in this connection. Update an existing connection Â¶ 2.0.0 4.0.0 To update an existing connection: Java Python Kotlin Raw REST API Update connection to make popularity visible 1 2 3 4 5 Connection connection = Connection . get ( \"default/hive/1657025257\" ); // (1) AssetMutationResponse updated = connection . trimToRequired () // (2) . hasPopularityInsights ( true ) // (3) . build () // (4) . save ( client ); // (5) Build an object referencing the existing connection. In this example we will retrieve it first, but you could also search for it or use the updater() method to construct an update. If starting from an existing object, remember to trimToRequired() to get a builder with the minimal required information for an update. Set the hasPopularityInsights property to true . Build the object, so it is ready to be persisted. Persist the object by saving it. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update connection to make popularity visible 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Connection client = AtlanClient () connection = client . asset . get_by_qualified_name ( # (1) qualified_name = \"default/hive/1657025257\" , asset_type = Connection ) . trim_to_required () # (2) connection . has_popularity_insights = True # (3) updated = client . asset . save ( connection ) # (4) Build an object referencing the existing connection. In this example we will retrieve it first, but you could also search for it or use the updater() method to construct an update. If starting from an existing object, remember to trim_to_required() to get an asset with the minimal required information for an update. Set the has_popularity_insights property to True . Persist the object by saving it. Update connection to make popularity visible 1 2 3 4 5 val connection = Connection . get ( \"default/hive/1657025257\" ) // (1) val updated = connection . trimToRequired () // (2) . hasPopularityInsights ( true ) // (3) . build () // (4) . save ( client ) // (5) Build an object referencing the existing connection. In this example we will retrieve it first, but you could also search for it or use the updater() method to construct an update. If starting from an existing object, remember to trimToRequired() to get a builder with the minimal required information for an update. Set the hasPopularityInsights property to true . Build the object, so it is ready to be persisted. Persist the object by saving it. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ // (1) { \"typeName\" : \"Connection\" , // (2) \"attributes\" : { // (3) \"qualifiedName\" : \"default/hive/1657025257\" , // (4) \"name\" : \"development\" , // (5) \"hasPopularityInsights\" : true // (6) } } ] } The connection must be wrapped in an entities array. You must use a typeName of Connection to update the connection. You must then wrap additional details in an attributes object. You must provide the exact qualifiedName of the connection (case-sensitive). You must provide the exact name of the connection (case-sensitive). Set the hasPopularityInsights property to true . Retrieve usage details Â¶ 1.4.0 4.0.0 Since popularity details are only available on certain assets, you will need to retrieve one of those assets to see the usage details: Java Python Kotlin Raw REST API Retrieve popularity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Table table = Table . get ( client , // (1) \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS\" ); // (2) table . getPopularityScore (); // (3) table . getSourceReadCount (); table . getSourceReadQueryCost (); table . getSourceReadUserCount (); table . getSourceTotalCost (); table . getSourceQueryComputeCosts (); // (4) table . getSourceReadRecentUsers (); table . getSourceReadTopUsers (); table . getSourceQueryComputeCostRecords (); // (5) table . getSourceReadRecentUserRecords (); table . getSourceReadTopUserRecords (); table . getSourceReadExpensiveQueryRecords (); // (6) table . getSourceReadPopularQueryRecords (); table . getSourceReadSlowQueryRecords (); Use the get() method to retrieve all details about a specific asset. Because this operation will retrieve the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Provide the full, case-sensitive qualifiedName of the asset. You can retrieve single quantified metrics for various aspects, such as: An overall popularity score for the asset (higher is better), using getPopularityScore() . The total number of all read operations on the asset, using getSourceReadCount() . The total cost of all read operations on the asset, using getSourceReadQueryCost() . The total number of unique users that read data from the asset, using getSourceReadUserCount() . The total cost of all operations on the asset, using getSourceTotalCost() . You can retrieve lists of various aspects, such as: The top query running engines (warehouses), using getSourceQueryComputeCosts() . The users who most recently queried the asset, using getSourceReadRecentUsers() . The users who most frequently query the asset, using getSourceReadTopUsers() . You can also list more detailed information about individual metrics, such as: The top query running engines (warehouses) and their related cost, using getSourceQueryComputeCostRecords() . The users who most recently queried the asset, the number of times they've queried it and when they last queried it, using getSourceReadRecentUserRecords() . The users who most frequently query the asset, the number of times they've queried it and when they last queried it, using getSourceReadTopUserRecords() . You can also list details about specific queries, such as: The list of most expensive queries against this asset, using getSourceReadExpensiveQueryRecords() . The list of the most frequently run queries against this asset, using getSourceReadPopularQueryRecords() . The list of the slowest-running queries against this asset, using getSourceReadSlowQueryRecords() . Retrieve popularity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table client = AtlanClient () table = client . asset . get_by_qualified_name ( # (1) qualified_name = \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS\" , # (2) asset_type = Table ) table . popularity_score # (3) table . source_read_count table . source_read_query_cost table . source_read_user_count table . source_total_cost table . source_query_compute_cost_list # (4) table . source_read_recent_user_list table . source_read_top_user_list table . source_query_compute_cost_record_list # (5) table . source_read_recent_user_record_list table . source_read_top_user_record_list table . source_read_expensive_query_record_list # (6) table . source_read_popular_query_record_list table . source_read_slow_query_record_list Use the asset.get_by_qualified_name() method to retrieve all details about a specific asset. Provide the full, case-sensitive qualified_name of the asset, and the type of the asset. You can retrieve single quantified metrics for various aspects, such as: An overall popularity score for the asset (higher is better), using popularity_score . The total number of all read operations on the asset, using source_read_count . The total cost of all read operations on the asset, using source_read_query_cost . The total number of unique users that read data from the asset, using source_read_user_count . The total cost of all operations on the asset, using source_total_cost . You can retrieve lists of various aspects, such as: The top query running engines (warehouses), using source_query_compute_cost_list . The users who most recently queried the asset, using source_read_recent_user_list . The users who most frequently query the asset, using source_read_top_user_list . You can also list more detailed information about individual metrics, such as: The top query running engines (warehouses) and their related cost, using source_query_compute_cost_record_list . The users who most recently queried the asset, the number of times they've queried it and when they last queried it, using source_read_recent_user_record_list . The users who most frequently query the asset, the number of times they've queried it and when they last queried it, using source_read_top_user_record_list . You can also list details about specific queries, such as: The list of most expensive queries against this asset, using source_read_expensive_query_record_list . The list of the most frequently run queries against this asset, using source_read_popular_query_record_list . The list of the slowest-running queries against this asset, using source_read_slow_query_record_list . Retrieve popularity 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 val table = Table . get ( client , // (1) \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS\" ) // (2) table . popularityScore // (3) table . sourceReadCount table . sourceReadQueryCost table . sourceReadUserCount table . sourceTotalCost table . sourceQueryComputeCosts // (4) table . sourceReadRecentUsers table . sourceReadTopUsers table . sourceQueryComputeCostRecords // (5) table . sourceReadRecentUserRecords table . sourceReadTopUserRecords table . sourceReadExpensiveQueryRecords // (6) table . sourceReadPopularQueryRecords table . sourceReadSlowQueryRecords Use the get() method to retrieve all details about a specific asset. Because this operation will retrieve the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Provide the full, case-sensitive qualifiedName of the asset. You can retrieve single quantified metrics for various aspects, such as: An overall popularity score for the asset (higher is better), using .popularityScore . The total number of all read operations on the asset, using .sourceReadCount . The total cost of all read operations on the asset, using .sourceReadQueryCost . The total number of unique users that read data from the asset, using .sourceReadUserCount . The total cost of all operations on the asset, using .sourceTotalCost . You can retrieve lists of various aspects, such as: The top query running engines (warehouses), using .sourceQueryComputeCosts . The users who most recently queried the asset, using .sourceReadRecentUsers . The users who most frequently query the asset, using .sourceReadTopUsers . You can also list more detailed information about individual metrics, such as: The top query running engines (warehouses) and their related cost, using .sourceQueryComputeCostRecords . The users who most recently queried the asset, the number of times they've queried it and when they last queried it, using .sourceReadRecentUserRecords . The users who most frequently query the asset, the number of times they've queried it and when they last queried it, using .sourceReadTopUserRecords . You can also list details about specific queries, such as: The list of most expensive queries against this asset, using .sourceReadExpensiveQueryRecords . The list of the most frequently run queries against this asset, using .sourceReadPopularQueryRecords . The list of the slowest-running queries against this asset, using .sourceReadSlowQueryRecords . GET /api/meta/entity/uniqueAttribute/type/Table?attr:qualifiedName=default%2Fhive%2F1657025257%2FOPS%2FDEFAULT%2FRUN_STATS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 { \"entity\" : { // (1) \"typeName\" : \"Table\" , // (2) \"attributes\" : { // (3) \"name\" : \"RUN_STATS\" , \"qualifiedName\" : \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS\" , \"popularityScore\" : 4.8121843 , // (4) \"sourceReadCount\" : 122 , \"sourceReadQueryCost\" : 0.001278 , \"sourceReadUserCount\" : 1 , \"sourceTotalCost\" : 0.001278 , \"sourceLastReadAt\" : 1689508975245 , \"sourceCostUnit\" : \"Credits\" , \"sourceQueryComputeCostList\" : [ \"TRANSFORMING\" ], \"sourceReadTopUserList\" : [ \"ATLANADMIN\" ], \"sourceQueryComputeCostRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordTotalUserCount\" : 0 , \"recordUser\" : null , \"recordQueryCount\" : 0 , \"recordWarehouse\" : \"TRANSFORMING\" , \"recordComputeCost\" : 0.001278 , \"recordLastTimestamp\" : 0 , \"recordQuery\" : null , \"recordQueryDuration\" : 0 , \"recordMaxComputeCost\" : 0 , \"recordComputeCostUnit\" : \"Credits\" } } ], \"sourceReadRecentUserRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordTotalUserCount\" : 0 , \"recordUser\" : \"ATLANADMIN\" , \"recordQueryCount\" : 122 , \"recordWarehouse\" : null , \"recordComputeCost\" : 0 , \"recordLastTimestamp\" : 1689508975245 , \"recordQuery\" : null , \"recordQueryDuration\" : 0 , \"recordMaxComputeCost\" : 0 , \"recordComputeCostUnit\" : null } } ], \"sourceReadTopUserRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordTotalUserCount\" : 0 , \"recordUser\" : \"ATLANADMIN\" , \"recordQueryCount\" : 122 , \"recordWarehouse\" : null , \"recordComputeCost\" : 0 , \"recordLastTimestamp\" : 1689508975245 , \"recordQuery\" : null , \"recordQueryDuration\" : 0 , \"recordMaxComputeCost\" : 0 , \"recordComputeCostUnit\" : null } } ], \"sourceReadExpensiveQueryRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordTotalUserCount\" : 1 , \"recordUser\" : null , \"recordQueryCount\" : 36 , \"recordWarehouse\" : null , \"recordComputeCost\" : 0.000407 , \"recordLastTimestamp\" : 0 , \"recordQuery\" : \"SELECT * from RUN_STATS\" , \"recordQueryDuration\" : 0 , \"recordMaxComputeCost\" : 4.8e-05 , \"recordComputeCostUnit\" : \"Credits\" } } ], \"sourceReadPopularQueryRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordTotalUserCount\" : 1 , \"recordUser\" : null , \"recordQueryCount\" : 36 , \"recordWarehouse\" : null , \"recordComputeCost\" : 0.000309 , \"recordLastTimestamp\" : 0 , \"recordQuery\" : \"SELECT * from RUN_STATS\" , \"recordQueryDuration\" : 0 , \"recordMaxComputeCost\" : 0 , \"recordComputeCostUnit\" : \"Credits\" } } ], \"sourceReadSlowQueryRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordTotalUserCount\" : 0 , \"recordUser\" : \"ATLANADMIN\" , \"recordQueryCount\" : 0 , \"recordWarehouse\" : null , \"recordComputeCost\" : 0 , \"recordLastTimestamp\" : 1688584846605 , \"recordQuery\" : \"SELECT * from RUN_STATS\" , \"recordQueryDuration\" : 336 , \"recordMaxComputeCost\" : 0 , \"recordComputeCostUnit\" : null } } ] } } } All asset details will come back wrapped in a top-level entity object in the payload. The typeName will indicate what kind of asset is returned. The detailed usage information will be embedded in the attributes object within the outer entity object. With the exception of the popularityScore , all other usage details have names that all start with source... Add your own usage details Â¶ 2.0.0 4.0.0 In cases where Atlan does not popularity details from the source, you may want to add your own. You can do this by either adding the usage details when creating the asset (programmatically) or by updating the attributes on an existing asset: Java Python Kotlin Raw REST API Add or update usage details 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 Table table = Table . updater ( // (1) \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS\" , \"RUN_STATS\" ) . popularityScore ( 1.2345 ) // (2) . sourceReadCount ( 123L ) . sourceReadQueryCost ( 5.4321 ) . sourceReadUserCount ( 5L ) . sourceTotalCost ( 5.4321 ) . sourceQueryComputeCosts ( List . of ( \"A\" , \"B\" )) // (3) . sourceReadRecentUsers ( List . of ( \"c\" , \"d\" , \"e\" )) . sourceReadRecentUser ( \"f\" ) . sourceReadRecentUser ( \"g\" ) . sourceReadTopUsers ( List . of ( \"g\" , \"f\" , \"e\" )) . sourceReadTopUser ( \"d\" ) . sourceReadTopUser ( \"c\" ) . sourceQueryComputeCostRecord ( // (4) PopularityInsights . builder () . recordComputeCost ( 4.4321 ) . recordComputeCostUnit ( SourceCostUnitType . CREDITS ) . recordWarehouse ( \"A\" ) . build ()) . sourceQueryComputeCostRecord ( PopularityInsights . builder () . recordComputeCost ( 1.0 ) . recordComputeCostUnit ( SourceCostUnitType . CREDITS ) . recordWarehouse ( \"B\" ) . build ()) . sourceReadRecentUserRecord ( PopularityInsights . builder () . recordUser ( \"c\" ) . recordLastTimestamp ( 1234567899000L ) . build ()) . sourceReadRecentUserRecord ( PopularityInsights . builder () . recordUser ( \"d\" ) . recordLastTimestamp ( 1234567898000L ) . build ()) . sourceReadRecentUserRecord ( PopularityInsights . builder () . recordUser ( \"e\" ) . recordLastTimestamp ( 1234567897000L ) . recordQueryCount ( 3L ) . build ()) . sourceReadTopUserRecord ( PopularityInsights . builder () . recordUser ( \"g\" ) . recordQueryCount ( 100L ) . recordLastTimestamp ( 1234567895000L ) . build ()) . sourceReadTopUserRecord ( PopularityInsights . builder () . recordUser ( \"f\" ) . recordQueryCount ( 20L ) . recordLastTimestamp ( 1234567896000L ) . build ()) . sourceReadTopUserRecord ( PopularityInsights . builder () . recordUser ( \"e\" ) . recordQueryCount ( 3L ) . recordLastTimestamp ( 1234567897000L ) . build ()) . sourceReadExpensiveQueryRecord ( PopularityInsights . builder () . recordComputeCost ( 5.4321 ) . recordMaxComputeCost ( 5.4321 ) . recordComputeCostUnit ( SourceCostUnitType . CREDITS ) . recordQuery ( \"SELECT * from RUN_STATS\" ) . recordQueryCount ( 123L ) . recordTotalUserCount ( 5L ) . build ()) . sourceReadPopularQueryRecord ( PopularityInsights . builder () . recordComputeCost ( 5.4321 ) . recordComputeCostUnit ( SourceCostUnitType . CREDITS ) . recordQueryCount ( 123L ) . recordQuery ( \"SELECT * from RUN_STATS\" ) . recordTotalUserCount ( 5L ) . build ()) . sourceReadSlowQueryRecord ( PopularityInsights . builder () . recordUser ( \"g\" ) . recordLastTimestamp ( 124567895000L ) . recordQueryDuration ( 321L ) . recordQuery ( \"SELECT * from RUN_STATS\" ) . build ()) . build (); AssetMutationResponse response = table . save ( client ); // (5) Use the updater() method to update an existing asset, providing the required details for that particular asset type (for more details, see Updating an asset ). For single quantified metrics, you can directly set the metric. For lists, you can directly set the lists or individually add elements (or a combination of the two). For the more detailed records, you need to build the PopularityInsights object with its embedded details. As with the lists you can associate these detailed records with the asset many-at-a-time or one-by-one. Finally, you must save the object you've built up to persist this information in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add or update usage details 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Table from pyatlan.model.enums import SourceCostUnitType from pyatlan.model.structs import PopularityInsights client = AtlanClient () table = Table . updater ( # (1) qualified_name = \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS\" , name = \"RUN_STATS\" ) table . popularity_score = 1.2345 # (2) table . source_read_count = 123 table . source_read_query_cost = 5.4321 table . source_read_user_count = 5 table . source_total_cost = 5.4321 table . source_query_compute_cost_list = [ \"A\" , \"B\" ] # (3) table . source_read_recent_user_list = [ \"c\" , \"d\" , \"e\" , \"f\" , \"g\" ] table . source_read_top_user_list = [ \"g\" , \"f\" , \"e\" , \"d\" , \"c\" ] table . source_query_compute_cost_record_list = [ # (4) PopularityInsights ( record_compute_cost = 4.4321 , record_compute_cost_unit = SourceCostUnitType . CREDITS , record_warehouse = \"A\" ), PopularityInsights ( record_compute_cost = 1.0 , record_compute_cost_unit = SourceCostUnitType . CREDITS , record_warehouse = \"B\" ) ] table . source_read_recent_user_record_list = [ PopularityInsights ( record_user = \"c\" , record_last_timestamp = 1234567899000 ), PopularityInsights ( record_user = \"d\" , record_last_timestamp = 1234567898000 ), PopularityInsights ( record_user = \"e\" , record_last_timestamp = 1234567897000 , record_query_count = 3 ) ] table . source_read_top_user_record_list = [ PopularityInsights ( record_user = \"g\" , record_query_count = 100 , record_last_timestamp = 1234567895000 ), PopularityInsights ( record_user = \"f\" , record_query_count = 20 , record_last_timestamp = 1234567896000 ), PopularityInsights ( record_user = \"e\" , record_query_count = 3 , record_last_timestamp = 1234567897000 ) ] table . source_read_expensive_query_record_list = [ PopularityInsights ( record_compute_cost = 5.4321 , record_max_compute_cost = 5.4321 , record_compute_cost_unit = SourceCostUnitType . CREDITS , record_query = \"SELECT * from RUN_STATS\" , record_query_count = 123 , record_total_user_count = 5 ) ] table . source_read_popular_query_record_list = [ PopularityInsights ( record_compute_cost = 5.4321 , record_compute_cost_unit = SourceCostUnitType . CREDITS , record_query_count = 123 , record_query = \"SELECT * from RUN_STATS\" , record_total_user_count = 5 ) ] table . source_read_slow_query_record_list = [ PopularityInsights ( record_user = \"g\" , record_last_timestamp = 1234567895000 , record_query_duration = 321 , record_query = \"SELECT * from RUN_STATS\" ) ] response = client . asset . save ( table ) # (5) Use the updater() method to update an existing asset, providing the required details for that particular asset type (for more details, see Updating an asset ). For single quantified metrics, you can directly set the metric. For lists, you can directly set the lists. For the more detailed records, you need to build each PopularityInsights object with its embedded details. Finally, you must save the object you've built up to persist this information in Atlan. Add or update usage details 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 val table = Table . updater ( // (1) \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS\" , \"RUN_STATS\" ) . popularityScore ( 1.2345 ) // (2) . sourceReadCount ( 123L ) . sourceReadQueryCost ( 5.4321 ) . sourceReadUserCount ( 5L ) . sourceTotalCost ( 5.4321 ) . sourceQueryComputeCosts ( List . of ( \"A\" , \"B\" )) // (3) . sourceReadRecentUsers ( List . of ( \"c\" , \"d\" , \"e\" )) . sourceReadRecentUser ( \"f\" ) . sourceReadRecentUser ( \"g\" ) . sourceReadTopUsers ( List . of ( \"g\" , \"f\" , \"e\" )) . sourceReadTopUser ( \"d\" ) . sourceReadTopUser ( \"c\" ) . sourceQueryComputeCostRecord ( // (4) PopularityInsights . builder () . recordComputeCost ( 4.4321 ) . recordComputeCostUnit ( SourceCostUnitType . CREDITS ) . recordWarehouse ( \"A\" ) . build ()) . sourceQueryComputeCostRecord ( PopularityInsights . builder () . recordComputeCost ( 1.0 ) . recordComputeCostUnit ( SourceCostUnitType . CREDITS ) . recordWarehouse ( \"B\" ) . build ()) . sourceReadRecentUserRecord ( PopularityInsights . builder () . recordUser ( \"c\" ) . recordLastTimestamp ( 1234567899000L ) . build ()) . sourceReadRecentUserRecord ( PopularityInsights . builder () . recordUser ( \"d\" ) . recordLastTimestamp ( 1234567898000L ) . build ()) . sourceReadRecentUserRecord ( PopularityInsights . builder () . recordUser ( \"e\" ) . recordLastTimestamp ( 1234567897000L ) . recordQueryCount ( 3L ) . build ()) . sourceReadTopUserRecord ( PopularityInsights . builder () . recordUser ( \"g\" ) . recordQueryCount ( 100L ) . recordLastTimestamp ( 1234567895000L ) . build ()) . sourceReadTopUserRecord ( PopularityInsights . builder () . recordUser ( \"f\" ) . recordQueryCount ( 20L ) . recordLastTimestamp ( 1234567896000L ) . build ()) . sourceReadTopUserRecord ( PopularityInsights . builder () . recordUser ( \"e\" ) . recordQueryCount ( 3L ) . recordLastTimestamp ( 1234567897000L ) . build ()) . sourceReadExpensiveQueryRecord ( PopularityInsights . builder () . recordComputeCost ( 5.4321 ) . recordMaxComputeCost ( 5.4321 ) . recordComputeCostUnit ( SourceCostUnitType . CREDITS ) . recordQuery ( \"SELECT * from RUN_STATS\" ) . recordQueryCount ( 123L ) . recordTotalUserCount ( 5L ) . build ()) . sourceReadPopularQueryRecord ( PopularityInsights . builder () . recordComputeCost ( 5.4321 ) . recordComputeCostUnit ( SourceCostUnitType . CREDITS ) . recordQueryCount ( 123L ) . recordQuery ( \"SELECT * from RUN_STATS\" ) . recordTotalUserCount ( 5L ) . build ()) . sourceReadSlowQueryRecord ( PopularityInsights . builder () . recordUser ( \"g\" ) . recordLastTimestamp ( 124567895000L ) . recordQueryDuration ( 321L ) . recordQuery ( \"SELECT * from RUN_STATS\" ) . build ()) . build () val response = table . save ( client ) // (5) Use the updater() method to update an existing asset, providing the required details for that particular asset type (for more details, see Updating an asset ). For single quantified metrics, you can directly set the metric. For lists, you can directly set the lists or individually add elements (or a combination of the two). For the more detailed records, you need to build the PopularityInsights object with its embedded details. As with the lists you can associate these detailed records with the asset many-at-a-time or one-by-one. Finally, you must save the object you've built up to persist this information in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 { \"entities\" : [{ // (1) \"typeName\" : \"Table\" , // (2) \"attributes\" : { \"name\" : \"RUN_STATS\" , // (3) \"qualifiedName\" : \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS\" , // (4) \"popularityScore\" : 1.2345 , // (5) \"sourceReadCount\" : 123 , \"sourceReadQueryCost\" : 5.4321 , \"sourceReadUserCount\" : 5 , \"sourceTotalCost\" : 5.4321 , \"sourceLastReadAt\" : 1689508975245 , \"sourceCostUnit\" : \"Credits\" , \"sourceQueryComputeCostList\" : [ // (6) \"A\" , \"B\" ], \"sourceReadTopUserList\" : [ \"c\" , \"d\" , \"e\" , \"f\" , \"g\" ], \"sourceReadRecentUserList\" : [ \"g\" , \"f\" , \"e\" , \"d\" , \"c\" ], \"sourceQueryComputeCostRecordList\" : [ // (7) { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordWarehouse\" : \"A\" , \"recordComputeCost\" : 4.4321 , \"recordComputeCostUnit\" : \"Credits\" } }, { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordWarehouse\" : \"B\" , \"recordComputeCost\" : 1.0 , \"recordComputeCostUnit\" : \"Credits\" } } ], \"sourceReadRecentUserRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordUser\" : \"c\" , \"recordLastTimestamp\" : 1234567899000 } }, { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordUser\" : \"d\" , \"recordLastTimestamp\" : 1234567898000 } }, { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordUser\" : \"e\" , \"recordLastTimestamp\" : 1234567897000 , \"recordQueryCount\" : 3 } } ], \"sourceReadTopUserRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordUser\" : \"g\" , \"recordQueryCount\" : 100 , \"recordLastTimestamp\" : 1234567895000 } }, { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordUser\" : \"f\" , \"recordQueryCount\" : 20 , \"recordLastTimestamp\" : 1234567896000 } }, { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordUser\" : \"e\" , \"recordQueryCount\" : 3 , \"recordLastTimestamp\" : 1234567897000 } } ], \"sourceReadExpensiveQueryRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordTotalUserCount\" : 5 , \"recordQueryCount\" : 123 , \"recordComputeCost\" : 5.4321 , \"recordQuery\" : \"SELECT * from RUN_STATS\" , \"recordMaxComputeCost\" : 5.4321 , \"recordComputeCostUnit\" : \"Credits\" } } ], \"sourceReadPopularQueryRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordTotalUserCount\" : 5 , \"recordQueryCount\" : 123 , \"recordComputeCost\" : 5.4321 , \"recordQuery\" : \"SELECT * from RUN_STATS\" , \"recordComputeCostUnit\" : \"Credits\" } } ], \"sourceReadSlowQueryRecordList\" : [ { \"typeName\" : \"PopularityInsights\" , \"attributes\" : { \"recordUser\" : \"g\" , \"recordLastTimestamp\" : 124567895000 , \"recordQuery\" : \"SELECT * from RUN_STATS\" , \"recordQueryDuration\" : 321 } } ] } }] } All assets must be wrapped in an entities array. The typeName must match the appropriate type for the asset being updated. You must provide the exact name of the asset (case-sensitive). You must provide the exact qualifiedName of the asset (case-sensitive). For single quantified metrics, you can directly set the metric. For lists, you can directly set the lists. For the more detailed records, you need to build each PopularityInsights object with its embedded details. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/custom-metadata/",
    "content": "Custom metadata structures overview Â¶ Custom metadata in Atlan is structurally composed of two levels: The custom metadata definition (sometimes referred to as set ) itself, defined as a CustomMetadataDef The attributes (or properties) within that custom metadata set, each defined as an AttributeDef erDiagram\n    CustomMetadataDef ||--o{ AttributeDef : contains Through Atlan's APIs, you can create your own custom metadata sets and attributes programmatically. Once the structures exist, you can then also change custom metadata values on assets themselves. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/profiling-and-popularity/profiling/",
    "content": "/api/meta/entity/bulk (POST) /api/meta/entity/uniqueAttribute/type/{typeName}?attr:qualifiedName={qualifiedName} (GET) Manage column profiling information Â¶ Profiling gives additional context to columns in relational stores. From profiling, you can see various summarized information such as: numerical statistics (min, max, mean, median, standard deviation, sum, variance) for numeric columns minimum, maximum, and average lengths for string columns distinct value counts and percentages missing value counts and percentages Profiling is only available on columns You will only be able to populate this summary information on columns, not on other assets in Atlan. Retrieve profiles Â¶ 1.4.0 4.0.0 Since profiles are only available on columns, you will need to retrieve column assets to see the profiles: Java Python Kotlin Raw REST API Retrieve profiles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Column column = Column . get ( client , // (1) \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS/STATUS\" , true ); // (2) column . getColumnDistinctValuesCount (); // (3) column . getColumnUniqueValuesCount (); column . getColumnUniquenessPercentage (); column . getColumnDuplicateValuesCount (); column . getColumnMissingValuesCount (); column . getColumnMissingValuesPercentage (); column . getColumnMax (); // (4) column . getColumnMin (); column . getColumnMean (); column . getColumnMedian (); column . getColumnStandardDeviation (); column . getColumnVariance (); column . getColumnSum (); column . getColumnMinimumStringLength (); // (5) column . getColumnMaximumStringLength (); column . getColumnAverageLength (); Use the get() method to retrieve all details about a specific column. Because this operation will retrieve the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Provide the full, case-sensitive qualifiedName of the column. Some profile information is common, regardless of the data type of the column. Some profile information is specific to numeric columns. Some profile information is specific to string columns. Retrieve profiles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Column client = AtlanClient () column = client . asset . get_by_qualified_name ( # (1) qualified_name = \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS/STATUS\" , # (2) asset_type = Column ) column . column_distinct_values_count # (3) column . column_unique_values_count column . column_uniqueness_percentage column . column_duplicate_values_count column . column_missing_values_count column . column_missing_values_percentage column . column_max # (4) column . column_min column . column_mean column . column_median column . column_standard_deviation column . column_variance column . column_sum column . column_minimum_string_length # (5) column . column_maximum_string_length column . column_average_length Use the get_by_qualified_name() method to retrieve all details about a specific column. Provide the full, case-sensitive qualifiedName of the column. Some profile information is common, regardless of the data type of the column. Some profile information is specific to numeric columns. Some profile information is specific to string columns. Retrieve profiles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 val column = Column . get ( client , // (1) \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS/STATUS\" ) // (2) column . columnDistinctValuesCount // (3) column . columnUniqueValuesCount column . columnUniquenessPercentage column . columnDuplicateValuesCount column . columnMissingValuesCount column . columnMissingValuesPercentage column . columnMax // (4) column . columnMin column . columnMean column . columnMedian column . columnStandardDeviation column . columnVariance column . columnSum column . columnMinimumStringLength // (5) column . columnMaximumStringLength column . columnAverageLength Use the get() method to retrieve all details about a specific column. Because this operation will retrieve the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Provide the full, case-sensitive qualifiedName of the column. Some profile information is common, regardless of the data type of the column. Some profile information is specific to numeric columns. Some profile information is specific to string columns. GET /api/meta/entity/uniqueAttribute/type/Column?attr:qualifiedName=default%2Fhive%2F1657025257%2FOPS%2FDEFAULT%2FRUN_STATS%2FSTATUS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"entity\" : { // (1) \"typeName\" : \"Column\" , // (2) \"attributes\" : { // (3) \"name\" : \"STATUS\" , \"qualifiedName\" : \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS/STATUS\" , \"columnDistinctValuesCount\" : 123 , // (4) \"columnUniqueValuesCount\" : 123 , \"columnUniquenessPercentage\" : 50.0 , \"columnDuplicateValuesCount\" : 123 , \"columnMissingValuesCount\" : 123 , \"columnMissingValuesPercentage\" : 50.0 , \"columnMax\" : 321.0 , \"columnMin\" : 1.0 , \"columnMean\" : 123.0 , \"columnMedian\" : 123.0 , \"columnStandardDeviation\" : 3.0 , \"columnVariance\" : 1.0 , \"columnSum\" : 654321.0 , \"columnMinimumStringLength\" : 0 , \"columnMaximumStringLength\" : 123 , \"columnAverageLength\" : 123.0 } } } All column details will come back wrapped in a top-level entity object in the payload. The typeName will always be Column . The detailed profiling information will be embedded in the attributes object within the outer entity object. The column profiling details have names that all start with column... Add your own profiles Â¶ 2.0.0 4.0.0 In cases where Atlan does not profile the source, you may want to add your own profiles. You can do this by either adding the profile when creating the column (programmatically) or by updating the attributes on an existing column: Java Python Kotlin Raw REST API Add or update profiles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Column column = Column . updater ( // (1) \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS/STATUS\" , // (2) \"STATUS\" ) // (3) . columnDistinctValuesCount ( 123 ) // (4) . columnUniqueValuesCount ( 123 ) . columnUniquenessPercentage ( 50.0 ) . columnDuplicateValuesCount ( 123 ) . columnMissingValuesCount ( 123 ) . columnMissingValuesPercentage ( 50.0 ) . columnMax ( 321.0 ) // (5) . columnMin ( 1.0 ) . columnMean ( 123.0 ) . columnMedian ( 123.0 ) . columnStandardDeviation ( 3.0 ) . columnVariance ( 1.0 ) . columnSum ( 654321.0 ) . columnMinimumStringLength ( 0 ) // (6) . columnMaximumStringLength ( 123 ) . columnAverageLength ( 123.0 ) . build (); // (7) AssetMutationResponse response = column . save ( client ); // (8) Use the updater() method to update an existing column asset (for more details, see Updating an asset ). Provide the full, case-sensitive qualifiedName of the column. Provide the case-sensitive name of the column. Some profile information is common, regardless of the data type of the column. All are optional, so fill in only the pieces you want or for which you have the information. Some profile information is specific to numeric columns. All are optional, so fill in only the pieces you want or for which you have the information. Some profile information is specific to string columns. All are optional, so fill in only the pieces you want or for which you have the information. Use the build() method to construct the column object to be updated in Atlan. Then call the save() method against this built-up object to actually apply the update to Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve profiles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Column client = AtlanClient () column = Column . updater ( # (1) qualified_name = \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS/STATUS\" , # (2) name = \"STATUS\" # (3) ) column . column_distinct_values_count = 123 # (4) column . column_unique_values_count = 123 column . column_uniqueness_percentage = 50.0 column . column_duplicate_values_count = 123 column . column_missing_values_count = 123 column . column_missing_values_percentage = 50.0 column . column_max = 321.0 # (5) column . column_min = 1.0 column . column_mean = 123.0 column . column_median = 123.0 column . column_standard_deviation = 3.0 column . column_variance = 1.0 column . column_sum = 654321.0 column . column_minimum_string_length = 0 # (6) column . column_maximum_string_length = 123 column . column_average_length = 123.0 response = client . asset . save ( column ) # (7) Use the updater() method to update an existing column asset (for more details, see Updating an asset ). Provide the full, case-sensitive qualified_name of the column. Provide the case-sensitive name of the column. Some profile information is common, regardless of the data type of the column. All are optional, so fill in only the pieces you want or for which you have the information. Some profile information is specific to numeric columns. All are optional, so fill in only the pieces you want or for which you have the information. Some profile information is specific to string columns. All are optional, so fill in only the pieces you want or for which you have the information. Then call the save() method with this built-up object to actually apply the update to Atlan. Add or update profiles 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 val column = Column . updater ( // (1) \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS/STATUS\" , // (2) \"STATUS\" ) // (3) . columnDistinctValuesCount ( 123 ) // (4) . columnUniqueValuesCount ( 123 ) . columnUniquenessPercentage ( 50.0 ) . columnDuplicateValuesCount ( 123 ) . columnMissingValuesCount ( 123 ) . columnMissingValuesPercentage ( 50.0 ) . columnMax ( 321.0 ) // (5) . columnMin ( 1.0 ) . columnMean ( 123.0 ) . columnMedian ( 123.0 ) . columnStandardDeviation ( 3.0 ) . columnVariance ( 1.0 ) . columnSum ( 654321.0 ) . columnMinimumStringLength ( 0 ) // (6) . columnMaximumStringLength ( 123 ) . columnAverageLength ( 123.0 ) . build () // (7) val response = column . save ( client ) // (8) Use the updater() method to update an existing column asset (for more details, see Updating an asset ). Provide the full, case-sensitive qualifiedName of the column. Provide the case-sensitive name of the column. Some profile information is common, regardless of the data type of the column. All are optional, so fill in only the pieces you want or for which you have the information. Some profile information is specific to numeric columns. All are optional, so fill in only the pieces you want or for which you have the information. Some profile information is specific to string columns. All are optional, so fill in only the pieces you want or for which you have the information. Use the build() method to construct the column object to be updated in Atlan. Then call the save() method against this built-up object to actually apply the update to Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"entities\" : [ // (1) { \"typeName\" : \"Column\" , // (2) \"attributes\" : { \"name\" : \"STATUS\" , // (3) \"qualifiedName\" : \"default/hive/1657025257/OPS/DEFAULT/RUN_STATS/STATUS\" , // (4) \"columnDistinctValuesCount\" : 123 , // (5) \"columnUniqueValuesCount\" : 123 , \"columnUniquenessPercentage\" : 50.0 , \"columnDuplicateValuesCount\" : 123 , \"columnMissingValuesCount\" : 123 , \"columnMissingValuesPercentage\" : 50.0 , \"columnMax\" : 321.0 , // (6) \"columnMin\" : 1.0 , \"columnMean\" : 123.0 , \"columnMedian\" : 123.0 , \"columnStandardDeviation\" : 3.0 , \"columnVariance\" : 1.0 , \"columnSum\" : 654321.0 , \"columnMinimumStringLength\" : 0 , // (7) \"columnMaximumStringLength\" : 123 , \"columnAverageLength\" : 123.0 } } ] } All columns must be wrapped in an entities array. The typeName must always be Column for profiling information. You must provide the exact name of the column (case-sensitive). You must provide the exact qualifiedName of the column (case-sensitive). Some profile information is common, regardless of the data type of the column. All are optional, so fill in only the pieces you want or for which you have the information. Some profile information is specific to numeric columns. All are optional, so fill in only the pieces you want or for which you have the information. Some profile information is specific to string columns. All are optional, so fill in only the pieces you want or for which you have the information. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/custom-metadata/create/",
    "content": "/api/meta/types/typedefs (POST) Create custom metadata Â¶ Like other objects in the SDK that you can create, custom metadata implements the builder pattern. This allows you to progressively build-up the structure you want to create. There are limits to the number of custom metadata properties you can create Atlan currently preserves details of custom metadata in its audit log. This allows Atlan to retain an audit trail of actions users took on custom metadata on each asset, even if the custom metadata definition itself is deleted. However, this also places an upper limit on the number of custom metadata properties you can (structurally) define in Atlan. Even if you delete the custom metadata definitions, any that you have previously defined will still take up \"space\" within this limit. More details By default this is ~1000 properties. Note that this limit applies only to the structural definition of the properties themselves, not the values captured for assets. If you see an error like the following, it means you have reached this limit: { \"errorCode\" : \"ATLAS-500-00-001\" , \"errorMessage\" : \"Unable to push entity audits to ES\" , \"errorCause\" : \"[{type=mapper_parsing_exception, reason=failed to parse, caused_by={type=illegal_argument_exception, reason=Limit of total fields [1000] has been exceeded while adding new fields [5]}}]\" } You will need to contact Atlan support to extend this threshold if you reach it. Build minimal object needed Â¶ 7.0.0 4.0.0 For example, to create a custom metadata structure to capture RACI assignments: Java Python Kotlin Raw REST API Build custom metadata definition for creation 1 2 3 4 5 6 7 8 9 10 11 12 13 CustomMetadataDef customMetadataDef = CustomMetadataDef . creator ( \"RACI\" ) // (1) . attributeDef ( // (2) AttributeDef . of ( client , \"Responsible\" , // (3) AtlanCustomAttributePrimitiveType . USERS , // (4) null , // (5) false )) // (6) . attributeDef ( AttributeDef . of ( client , \"Accountable\" , AtlanCustomAttributePrimitiveType . USERS , false )) . attributeDef ( AttributeDef . of ( client , \"Consulted\" , AtlanCustomAttributePrimitiveType . GROUPS , true )) . attributeDef ( AttributeDef . of ( client , \"Informed\" , AtlanCustomAttributePrimitiveType . GROUPS , true )) . options ( CustomMetadataOptions . withImage ( \"https://example.com/logo.png\" , true )) // (7) . options ( CustomMetadataOptions . withEmoji ( \"ðŸ‘ª\" )) // (8) . options ( CustomMetadataOptions . withIcon ( AtlanIcon . ROCKET_LAUNCH , AtlanTagColor . RED )) // (9) . build (); // (10) When creating the custom metadata structure, you must provide a name ( RACI in this example). You can then add as many attributes to that structure as you want. Each attribute must have a name. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Each attribute must have a type. If the type is AtlanCustomAttributePrimitiveType.OPTIONS then you must also specify the enumeration that defines the valid values for this attribute (in this example the type is not an enumeration, so this is null and could even be left out as in the subsequent lines). You must also specify whether the attribute allows multiple values to be captured on it ( true ) or only a single value ( false ). You can also provide a custom logo for the custom metadata by providing a URL to an image. The second argument controls whether this custom metadata is editable via the UI â€” for false it is editable via the UI, while for true the custom metadata will only be editable via APIs (including via SDK). Or you can use an emoji as the custom icon for the custom metadata. Or you can use a built-in icon for the custom metadata. The second argument controls the color to use for the icon. As with all other builder patterns, you must build() the object you've defined. Build custom metadata definition for creation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from pyatlan.model.typedef import AttributeDef , CustomMetadataDef from pyatlan.model.enums import AtlanCustomAttributePrimitiveType from pyatlan.client.atlan import AtlanClient cm_def = CustomMetadataDef . create ( display_name = \"RACI\" ) # (1) cm_def . attribute_defs = [ # (2) AttributeDef . create ( client = client , # (3) display_name = \"Responsible\" , # (4) attribute_type = AtlanCustomAttributePrimitiveType . USERS , # (5) options_name = None , # (6) multi_valued = False , # (7) ), AttributeDef . create ( client = client , display_name = \"Accountable\" , attribute_type = AtlanCustomAttributePrimitiveType . USERS , ), AttributeDef . create ( client = client , display_name = \"Consulted\" , attribute_type = AtlanCustomAttributePrimitiveType . GROUPS , multi_valued = True , ), AttributeDef . create ( client = client , display_name = \"Informed\" , attribute_type = AtlanCustomAttributePrimitiveType . GROUPS , multi_valued = True , ), ] cm_def . options = CustomMetadataDef . Options . with_logo_from_url ( # (8) url = \"https://example.com/logo.png\" , locked = True ) cm_def . options = CustomMetadataDef . Options . with_logo_as_emoji ( # (9) emoji = \"ðŸ‘ª\" , locked = False ) When creating the custom metadata structure, you must provide a name ( RACI in this example). You can then add as many attributes to that structure as you want. You must provide a client instance. Each attribute must have a name. Each attribute must have a type. If the type is AtlanCustomAttributePrimitiveType.OPTIONS then you must also specify the enumeration that defines the valid values for this attribute (in this example none are enumerations, so this is the default value for the argument: None ). You can also specify whether the attribute allows multiple values to be captured on it ( True ) or only a single value ( False ) (the default). You can also provide a custom logo for the custom metadata by providing a URL to an image. The second argument controls whether this custom metadata is editable via the UI â€” for locked=False it is editable via the UI, while for locked=True the custom metadata will only be editable via APIs (including via SDK). Or you can use an emoji as the custom icon for the custom metadata. Build custom metadata definition for creation 1 2 3 4 5 6 7 8 9 10 11 12 13 val customMetadataDef = CustomMetadataDef . creator ( \"RACI\" ) // (1) . attributeDef ( // (2) AttributeDef . of ( client , \"Responsible\" , // (3) AtlanCustomAttributePrimitiveType . USERS , // (4) null , // (5) false )) // (6) . attributeDef ( AttributeDef . of ( \"Accountable\" , AtlanCustomAttributePrimitiveType . USERS , false )) . attributeDef ( AttributeDef . of ( \"Consulted\" , AtlanCustomAttributePrimitiveType . GROUPS , true )) . attributeDef ( AttributeDef . of ( \"Informed\" , AtlanCustomAttributePrimitiveType . GROUPS , true )) . options ( CustomMetadataOptions . withImage ( \"https://example.com/logo.png\" , true )) // (7) . options ( CustomMetadataOptions . withEmoji ( \"ðŸ‘ª\" )) // (8) . options ( CustomMetadataOptions . withIcon ( AtlanIcon . ROCKET_LAUNCH , AtlanTagColor . RED )) // (9) . build () // (10) When creating the custom metadata structure, you must provide a name ( RACI in this example). You can then add as many attributes to that structure as you want. Each attribute must have a name. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Each attribute must have a type. If the type is AtlanCustomAttributePrimitiveType.OPTIONS then you must also specify the enumeration that defines the valid values for this attribute (in this example the type is not an enumeration, so this is null and could even be left out as in the subsequent lines). You must also specify whether the attribute allows multiple values to be captured on it ( true ) or only a single value ( false ). You can also provide a custom logo for the custom metadata by providing a URL to an image. The second argument controls whether this custom metadata is editable via the UI â€” for false it is editable via the UI, while for true the custom metadata will only be editable via APIs (including via SDK). Or you can use an emoji as the custom icon for the custom metadata. Or you can use a built-in icon for the custom metadata. The second argument controls the color to use for the icon. As with all other builder patterns, you must build() the object you've defined. POST /api/meta/types/typedefs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 { \"businessMetadataDefs\" : [ // (1) { \"category\" : \"BUSINESS_METADATA\" , // (2) \"name\" : \"RACI\" , // (3) \"attributeDefs\" : [ // (4) { \"name\" : \"\" , // (5) \"displayName\" : \"Responsible\" , // (6) \"description\" : \"\" , \"typeName\" : \"string\" , \"isOptional\" : true , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"isUnique\" : false , \"isIndexable\" : true , \"includeInNotification\" : false , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"S3Object\\\",\\\"LookerLook\\\",\\\"TableauSite\\\",\\\"SnowflakeStream\\\",\\\"ModeCollection\\\",\\\"LookerDashboard\\\",\\\"PowerBIWorkspace\\\",\\\"Collection\\\",\\\"AtlasGlossaryCategory\\\",\\\"TableauFlow\\\",\\\"LookerView\\\",\\\"TableauProject\\\",\\\"LookerExplore\\\",\\\"ModeReport\\\",\\\"PowerBIColumn\\\",\\\"Query\\\",\\\"ColumnProcess\\\",\\\"SalesforceDashboard\\\",\\\"SalesforceObject\\\",\\\"BIProcess\\\",\\\"DbtModelColumn\\\",\\\"S3Bucket\\\",\\\"SigmaDataElement\\\",\\\"DataStudioAsset\\\",\\\"DbtProcess\\\",\\\"DbtModel\\\",\\\"PowerBIDataset\\\",\\\"Column\\\",\\\"DbtMetric\\\",\\\"TableauDashboard\\\",\\\"SigmaDataset\\\",\\\"LookerQuery\\\",\\\"APISpec\\\",\\\"MetabaseDashboard\\\",\\\"Process\\\",\\\"PowerBIDashboard\\\",\\\"APIPath\\\",\\\"ModeChart\\\",\\\"PowerBIDataflow\\\",\\\"SalesforceField\\\",\\\"GCSObject\\\",\\\"SalesforceReport\\\",\\\"View\\\",\\\"Folder\\\",\\\"TableauMetric\\\",\\\"MaterialisedView\\\",\\\"PresetDashboard\\\",\\\"PowerBIDatasource\\\",\\\"ModeWorkspace\\\",\\\"SigmaPage\\\",\\\"LookerField\\\",\\\"SigmaWorkbook\\\",\\\"PowerBIMeasure\\\",\\\"TableauWorkbook\\\",\\\"LookerModel\\\",\\\"MetabaseCollection\\\",\\\"ModeQuery\\\",\\\"GCSBucket\\\",\\\"LookerTile\\\",\\\"Table\\\",\\\"PowerBITile\\\",\\\"PowerBIPage\\\",\\\"SalesforceOrganization\\\",\\\"PresetWorkspace\\\",\\\"TableauDatasource\\\",\\\"PresetDataset\\\",\\\"TableauCalculatedField\\\",\\\"LookerFolder\\\",\\\"TableauWorksheet\\\",\\\"MetabaseQuestion\\\",\\\"AtlasGlossary\\\",\\\"PresetChart\\\",\\\"PowerBITable\\\",\\\"LookerProject\\\",\\\"SnowflakePipe\\\",\\\"PowerBIReport\\\",\\\"SigmaDatasetColumn\\\",\\\"TableauDatasourceField\\\",\\\"TablePartition\\\",\\\"AtlasGlossaryTerm\\\",\\\"SigmaDataElementField\\\",\\\"Schema\\\",\\\"Database\\\",\\\"DbtColumnProcess\\\"]\" , \"allowSearch\" : false , \"maxStrLength\" : \"100000000\" , \"allowFiltering\" : true , \"multiValueSelect\" : true , // (7) \"showInOverview\" : false , \"primitiveType\" : \"users\" , \"isEnum\" : false , \"customType\" : \"users\" // (8) }, \"isNew\" : true // (9) }, { \"name\" : \"\" , \"displayName\" : \"Accountable\" , \"description\" : \"\" , \"typeName\" : \"string\" , \"isOptional\" : true , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"isUnique\" : false , \"isIndexable\" : true , \"includeInNotification\" : false , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"S3Object\\\",\\\"LookerLook\\\",\\\"TableauSite\\\",\\\"SnowflakeStream\\\",\\\"ModeCollection\\\",\\\"LookerDashboard\\\",\\\"PowerBIWorkspace\\\",\\\"Collection\\\",\\\"AtlasGlossaryCategory\\\",\\\"TableauFlow\\\",\\\"LookerView\\\",\\\"TableauProject\\\",\\\"LookerExplore\\\",\\\"ModeReport\\\",\\\"PowerBIColumn\\\",\\\"Query\\\",\\\"ColumnProcess\\\",\\\"SalesforceDashboard\\\",\\\"SalesforceObject\\\",\\\"BIProcess\\\",\\\"DbtModelColumn\\\",\\\"S3Bucket\\\",\\\"SigmaDataElement\\\",\\\"DataStudioAsset\\\",\\\"DbtProcess\\\",\\\"DbtModel\\\",\\\"PowerBIDataset\\\",\\\"Column\\\",\\\"DbtMetric\\\",\\\"TableauDashboard\\\",\\\"SigmaDataset\\\",\\\"LookerQuery\\\",\\\"APISpec\\\",\\\"MetabaseDashboard\\\",\\\"Process\\\",\\\"PowerBIDashboard\\\",\\\"APIPath\\\",\\\"ModeChart\\\",\\\"PowerBIDataflow\\\",\\\"SalesforceField\\\",\\\"GCSObject\\\",\\\"SalesforceReport\\\",\\\"View\\\",\\\"Folder\\\",\\\"TableauMetric\\\",\\\"MaterialisedView\\\",\\\"PresetDashboard\\\",\\\"PowerBIDatasource\\\",\\\"ModeWorkspace\\\",\\\"SigmaPage\\\",\\\"LookerField\\\",\\\"SigmaWorkbook\\\",\\\"PowerBIMeasure\\\",\\\"TableauWorkbook\\\",\\\"LookerModel\\\",\\\"MetabaseCollection\\\",\\\"ModeQuery\\\",\\\"GCSBucket\\\",\\\"LookerTile\\\",\\\"Table\\\",\\\"PowerBITile\\\",\\\"PowerBIPage\\\",\\\"SalesforceOrganization\\\",\\\"PresetWorkspace\\\",\\\"TableauDatasource\\\",\\\"PresetDataset\\\",\\\"TableauCalculatedField\\\",\\\"LookerFolder\\\",\\\"TableauWorksheet\\\",\\\"MetabaseQuestion\\\",\\\"AtlasGlossary\\\",\\\"PresetChart\\\",\\\"PowerBITable\\\",\\\"LookerProject\\\",\\\"SnowflakePipe\\\",\\\"PowerBIReport\\\",\\\"SigmaDatasetColumn\\\",\\\"TableauDatasourceField\\\",\\\"TablePartition\\\",\\\"AtlasGlossaryTerm\\\",\\\"SigmaDataElementField\\\",\\\"Schema\\\",\\\"Database\\\",\\\"DbtColumnProcess\\\"]\" , \"allowSearch\" : false , \"maxStrLength\" : \"100000000\" , \"allowFiltering\" : true , \"multiValueSelect\" : false , \"showInOverview\" : false , \"primitiveType\" : \"users\" , \"isEnum\" : false , \"customType\" : \"users\" }, \"isNew\" : true }, { \"name\" : \"\" , \"displayName\" : \"Consulted\" , \"description\" : \"\" , \"typeName\" : \"array<string>\" , \"isOptional\" : true , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"isUnique\" : false , \"isIndexable\" : true , \"includeInNotification\" : false , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"S3Object\\\",\\\"LookerLook\\\",\\\"TableauSite\\\",\\\"SnowflakeStream\\\",\\\"ModeCollection\\\",\\\"LookerDashboard\\\",\\\"PowerBIWorkspace\\\",\\\"Collection\\\",\\\"AtlasGlossaryCategory\\\",\\\"TableauFlow\\\",\\\"LookerView\\\",\\\"TableauProject\\\",\\\"LookerExplore\\\",\\\"ModeReport\\\",\\\"PowerBIColumn\\\",\\\"Query\\\",\\\"ColumnProcess\\\",\\\"SalesforceDashboard\\\",\\\"SalesforceObject\\\",\\\"BIProcess\\\",\\\"DbtModelColumn\\\",\\\"S3Bucket\\\",\\\"SigmaDataElement\\\",\\\"DataStudioAsset\\\",\\\"DbtProcess\\\",\\\"DbtModel\\\",\\\"PowerBIDataset\\\",\\\"Column\\\",\\\"DbtMetric\\\",\\\"TableauDashboard\\\",\\\"SigmaDataset\\\",\\\"LookerQuery\\\",\\\"APISpec\\\",\\\"MetabaseDashboard\\\",\\\"Process\\\",\\\"PowerBIDashboard\\\",\\\"APIPath\\\",\\\"ModeChart\\\",\\\"PowerBIDataflow\\\",\\\"SalesforceField\\\",\\\"GCSObject\\\",\\\"SalesforceReport\\\",\\\"View\\\",\\\"Folder\\\",\\\"TableauMetric\\\",\\\"MaterialisedView\\\",\\\"PresetDashboard\\\",\\\"PowerBIDatasource\\\",\\\"ModeWorkspace\\\",\\\"SigmaPage\\\",\\\"LookerField\\\",\\\"SigmaWorkbook\\\",\\\"PowerBIMeasure\\\",\\\"TableauWorkbook\\\",\\\"LookerModel\\\",\\\"MetabaseCollection\\\",\\\"ModeQuery\\\",\\\"GCSBucket\\\",\\\"LookerTile\\\",\\\"Table\\\",\\\"PowerBITile\\\",\\\"PowerBIPage\\\",\\\"SalesforceOrganization\\\",\\\"PresetWorkspace\\\",\\\"TableauDatasource\\\",\\\"PresetDataset\\\",\\\"TableauCalculatedField\\\",\\\"LookerFolder\\\",\\\"TableauWorksheet\\\",\\\"MetabaseQuestion\\\",\\\"AtlasGlossary\\\",\\\"PresetChart\\\",\\\"PowerBITable\\\",\\\"LookerProject\\\",\\\"SnowflakePipe\\\",\\\"PowerBIReport\\\",\\\"SigmaDatasetColumn\\\",\\\"TableauDatasourceField\\\",\\\"TablePartition\\\",\\\"AtlasGlossaryTerm\\\",\\\"SigmaDataElementField\\\",\\\"Schema\\\",\\\"Database\\\",\\\"DbtColumnProcess\\\"]\" , \"allowSearch\" : false , \"maxStrLength\" : \"100000000\" , \"allowFiltering\" : true , \"multiValueSelect\" : true , \"showInOverview\" : false , \"primitiveType\" : \"groups\" , \"isEnum\" : false , \"customType\" : \"groups\" }, \"isNew\" : true }, { \"name\" : \"\" , \"displayName\" : \"Informed\" , \"description\" : \"\" , \"typeName\" : \"array<string>\" , \"isOptional\" : true , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"isUnique\" : false , \"isIndexable\" : true , \"includeInNotification\" : false , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"S3Object\\\",\\\"LookerLook\\\",\\\"TableauSite\\\",\\\"SnowflakeStream\\\",\\\"ModeCollection\\\",\\\"LookerDashboard\\\",\\\"PowerBIWorkspace\\\",\\\"Collection\\\",\\\"AtlasGlossaryCategory\\\",\\\"TableauFlow\\\",\\\"LookerView\\\",\\\"TableauProject\\\",\\\"LookerExplore\\\",\\\"ModeReport\\\",\\\"PowerBIColumn\\\",\\\"Query\\\",\\\"ColumnProcess\\\",\\\"SalesforceDashboard\\\",\\\"SalesforceObject\\\",\\\"BIProcess\\\",\\\"DbtModelColumn\\\",\\\"S3Bucket\\\",\\\"SigmaDataElement\\\",\\\"DataStudioAsset\\\",\\\"DbtProcess\\\",\\\"DbtModel\\\",\\\"PowerBIDataset\\\",\\\"Column\\\",\\\"DbtMetric\\\",\\\"TableauDashboard\\\",\\\"SigmaDataset\\\",\\\"LookerQuery\\\",\\\"APISpec\\\",\\\"MetabaseDashboard\\\",\\\"Process\\\",\\\"PowerBIDashboard\\\",\\\"APIPath\\\",\\\"ModeChart\\\",\\\"PowerBIDataflow\\\",\\\"SalesforceField\\\",\\\"GCSObject\\\",\\\"SalesforceReport\\\",\\\"View\\\",\\\"Folder\\\",\\\"TableauMetric\\\",\\\"MaterialisedView\\\",\\\"PresetDashboard\\\",\\\"PowerBIDatasource\\\",\\\"ModeWorkspace\\\",\\\"SigmaPage\\\",\\\"LookerField\\\",\\\"SigmaWorkbook\\\",\\\"PowerBIMeasure\\\",\\\"TableauWorkbook\\\",\\\"LookerModel\\\",\\\"MetabaseCollection\\\",\\\"ModeQuery\\\",\\\"GCSBucket\\\",\\\"LookerTile\\\",\\\"Table\\\",\\\"PowerBITile\\\",\\\"PowerBIPage\\\",\\\"SalesforceOrganization\\\",\\\"PresetWorkspace\\\",\\\"TableauDatasource\\\",\\\"PresetDataset\\\",\\\"TableauCalculatedField\\\",\\\"LookerFolder\\\",\\\"TableauWorksheet\\\",\\\"MetabaseQuestion\\\",\\\"AtlasGlossary\\\",\\\"PresetChart\\\",\\\"PowerBITable\\\",\\\"LookerProject\\\",\\\"SnowflakePipe\\\",\\\"PowerBIReport\\\",\\\"SigmaDatasetColumn\\\",\\\"TableauDatasourceField\\\",\\\"TablePartition\\\",\\\"AtlasGlossaryTerm\\\",\\\"SigmaDataElementField\\\",\\\"Schema\\\",\\\"Database\\\",\\\"DbtColumnProcess\\\"]\" , \"allowSearch\" : false , \"maxStrLength\" : \"100000000\" , \"allowFiltering\" : true , \"multiValueSelect\" : true , \"showInOverview\" : false , \"primitiveType\" : \"groups\" , \"isEnum\" : false , \"customType\" : \"groups\" }, \"isNew\" : true } ], \"displayName\" : \"RACI\" , // (10) \"options\" : { // (11) \"logoType\" : \"emoji\" , \"emoji\" : \"ðŸ‘ª\" } } ] } All custom metadata structural definitions must be specified within the businessMetadataDefs array. Each structural definition must be defined with a category set to BUSINESS_METADATA . Whatever name you provide for the structural definition will be replaced by a hashed-string generated name by the back-end. Within the structural definition, you need to define each attribute inside the attributeDefs array. You should leave the name of the attribute as an empty string. This will be generated by the back-end. Instead, provide the name for the attribute (as it should appear in the UI) to the displayName . There are various properties of each attribute that define how the attribute is validated and will behave in the UI. You can use the multiValueSelect to specify whether to allow multiple values for this attribute on a single asset (in this example multiple values will be allowed). One of the properties that must be specified is the type of the custom attribute. In this example, we use an Atlan-specific custom type to capture users. For each attribute that is to be newly created and added to the structure, set the isNew to true . Specify the name of the custom metadata structure, as it should appear in the UI, to the displayName . Finally, set options on the custom metadata structure to control its appearance â€” specifically the icon that should be used in the UI. Create the custom metadata from the object Â¶ 1.3.3 4.0.0 Now that the object is built, this customMetadataDef object will have the required information for Atlan to create it.\nYou can then actually create the custom metadata definition in Atlan by calling the create() method on the object itself: Java Python Kotlin Raw REST API Create the custom metadata definition 14 CustomMetadataDef response = customMetadataDef . create ( client ); // (1) The create() operation will actually create the custom metadata definition within Atlan, including all the attributes that were defined as part of it. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create the custom metadata definition 33 34 35 36 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . typedef . create ( cm_def ) # (1) The typedef.create() operation will actually create the custom metadata definition within Atlan, including all the attributes that were defined as part of it. Create the custom metadata definition 14 val response = customMetadataDef . create ( client ) // (1) The create() operation will actually create the custom metadata definition within Atlan, including all the attributes that were defined as part of it. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Creation implicit in step above The actual creation of the custom metadata structure is implicit in the example above. Now that the custom metadata structure has been created, you can set its values per asset . Limit applicability of an attribute Â¶ 4.0.0 You can also limit the assets the custom metadata applies to in Atlan. Anywhere you create an attribute definition, you can: Java Python Kotlin Raw REST API Limit applicability of an attribute 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 AttributeDef responsible = AttributeDef . of ( client , \"Responsible\" , // (1) AtlanCustomAttributePrimitiveType . USERS , false ); responsible = responsible . toBuilder () // (2) . options ( responsible . getOptions (). toBuilder () // (3) . clearApplicableConnections () // (4) . clearApplicableAssetTypes () . clearApplicableGlossaries () . clearApplicableGlossaryTypes () . applicableConnection ( \"default/snowflake/1234567890\" ) // (5) . applicableAssetType ( Database . TYPE_NAME ) // (6) . applicableGlossary ( Glossary . findByName ( \"Example\" ). getQualifiedName ()) // (7) . applicableGlossaryType ( GlossaryTerm . TYPE_NAME ) // (8) . build ()) // (9) . build (); // (10) We still recommend creating the attribute using the Attribute.of() factory method. This ensures all required settings are configured based on the type of the attribute. You can then clone the attribute into a mutable form using toBuilder() . Set the options on this clone to change its applicability. You can use the toBuilder() on the options themselves to get a mutable clone of the options that have already been setup by the AttributeDef.of() factory method. By default, the AttributeDef.of() method will ensure a custom metadata attribute applies to all assets. To limit its applicability, you need to remove these \"grants\" by clearing out: Connections the custom metadata attribute applies to (by default, all assets in all connections that existed when the attribute was created will be capable of using this custom metadata attribute). Asset types the custom metadata attribute applies to (by default, all asset types will be capable of using this custom metadata attribute). Glossaries the custom metadata attribute applies to (by default, all objects in a glossary that existed when the attribute was created will be capable of using this custom metadata attribute). Glossary asset types the custom metadata applies to (by default, glossaries, terms and categories will be capable of using this custom metadata attribute). You can chain any number of applicableConnection() calls to specify the qualiedName s of connections. The custom metadata attribute will only be available to assets within these connections. To use all connections To select all connections, instead chain .applicableConnections(Connection.getAllQualifiedNames()) . You can chain any number of applicableAssetType() calls to specify the types of assets for the custom metadata attribute. The custom metadata attribute will only be available to assets of these types, within the connections specified in the line above. To use all asset types To select all asset types, instead chain .applicableAssetTypes(AttributeDefOptions.ALL_ASSET_TYPES) . You can chain any number of applicableGlossary() calls to specify the qualifiedName s of glossaries. The custom metadata attribute will only be available to assets within these glossaries. To use all glossaries To select all glossaries, instead chain .applicableGlossaries(Glossary.getAllQualifiedNames()) . You can chain any number of applicableGlossaryType() calls to specify the types of glossary assets for the custom metadata attribute. The custom metadata attribute will only be available to glossary assets of these types, within the glossaries specified in the line above. To use all glossary asset types To select all glossary asset types, instead use .applicableGlossaryTypes(AttributeDefOptions.ALL_GLOSSARY_TYPES) . You then need to build all of these options. And finally you need to build the changes back into the attribute definition. You can then use the attribute definition ( responsible in this example) as you would any other attribute definition, for example passing it to the chained .attributeDef() as part of CustomMetadataDef.creator() shown earlier. Coming soon Limit applicability of an attribute 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 var responsible = AttributeDef . of ( client , \"Responsible\" , // (1) AtlanCustomAttributePrimitiveType . USERS , false ) responsible = responsible . toBuilder () // (2) . options ( responsible . options . toBuilder () // (3) . clearApplicableConnections () // (4) . clearApplicableAssetTypes () . clearApplicableGlossaries () . clearApplicableGlossaryTypes () . applicableConnection ( \"default/snowflake/1234567890\" ) // (5) . applicableAssetType ( Database . TYPE_NAME ) // (6) . applicableGlossary ( Glossary . findByName ( \"Example\" ). qualifiedName ) // (7) . applicableGlossaryType ( GlossaryTerm . TYPE_NAME ) // (8) . build ()) // (9) . build () // (10) We still recommend creating the attribute using the Attribute.of() factory method. This ensures all required settings are configured based on the type of the attribute. You can then clone the attribute into a mutable form using toBuilder() . Set the options on this clone to change its applicability. You can use the toBuilder() on the options themselves to get a mutable clone of the options that have already been setup by the AttributeDef.of() factory method. By default, the AttributeDef.of() method will ensure a custom metadata attribute applies to all assets. To limit its applicability, you need to remove these \"grants\" by clearing out: Connections the custom metadata attribute applies to (by default, all assets in all connections that existed when the attribute was created will be capable of using this custom metadata attribute). Asset types the custom metadata attribute applies to (by default, all asset types will be capable of using this custom metadata attribute). Glossaries the custom metadata attribute applies to (by default, all objects in a glossary that existed when the attribute was created will be capable of using this custom metadata attribute). Glossary asset types the custom metadata applies to (by default, glossaries, terms and categories will be capable of using this custom metadata attribute). You can chain any number of applicableConnection() calls to specify the qualiedName s of connections. The custom metadata attribute will only be available to assets within these connections. To use all connections To select all connections, instead chain .applicableConnections(Connection.getAllQualifiedNames()) . You can chain any number of applicableAssetType() calls to specify the types of assets for the custom metadata attribute. The custom metadata attribute will only be available to assets of these types, within the connections specified in the line above. To use all asset types To select all asset types, instead chain .applicableAssetTypes(AttributeDefOptions.ALL_ASSET_TYPES) . You can chain any number of applicableGlossary() calls to specify the qualifiedName s of glossaries. The custom metadata attribute will only be available to assets within these glossaries. To use all glossaries To select all glossaries, instead chain .applicableGlossaries(Glossary.getAllQualifiedNames()) . You can chain any number of applicableGlossaryType() calls to specify the types of glossary assets for the custom metadata attribute. The custom metadata attribute will only be available to glossary assets of these types, within the glossaries specified in the line above. To use all glossary asset types To select all glossary asset types, instead use .applicableGlossaryTypes(AttributeDefOptions.ALL_GLOSSARY_TYPES) . You then need to build all of these options. And finally you need to build the changes back into the attribute definition. You can then use the attribute definition ( responsible in this example) as you would any other attribute definition, for example passing it to the chained .attributeDef() as part of CustomMetadataDef.creator() shown earlier. Coming soon Applicability is combined across connections and glossaries If you specify both connections (and asset types) and glossaries (and glossary types), then the custom metadata attribute will be available across both data assets in those connections and glossary objects in those glossaries. In other words, these options are not mutually exclusive, but are combined. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/custom-metadata/badge/",
    "content": "/api/meta/types/typedefs (POST) Manage custom metadata badges Â¶ You can use badges in Atlan to provide quick indicators of key signals from custom metadata. They appear in the Overview of the asset, rather than nested within the custom metadata tab. Badges are a kind of asset Badges are actually modeled as just another kind of asset in Atlan. This means all the standard CRUD operations apply to badges the same as any other asset. Create a badge Â¶ 7.0.0 4.0.0 For example, to create a badge for custom metadata capturing a count of data quality checks that have run: Java Python Kotlin Raw REST API Build a badge 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Badge badge = Badge . creator ( client , // (1) \"DQ Count\" , // (2) \"Data Quality\" , // (3) \"Count\" ) // (4) . userDescription ( \"How many data quality checks ran against this asset.\" ) // (5) . badgeCondition ( // (6) BadgeCondition . of ( BadgeComparisonOperator . GTE , // (7) 5 , // (8) BadgeConditionColor . GREEN )) // (9) . badgeCondition ( BadgeCondition . of ( BadgeComparisonOperator . LT , 5 , BadgeConditionColor . YELLOW )) . badgeCondition ( BadgeCondition . of ( BadgeComparisonOperator . LTE , 2 , BadgeConditionColor . RED )) . build (); // (10) AssetMutationResponse response = badge . save ( client ); // (11) Like with any other asset, use the creator() method to ensure you provide the minimal information required to create a badge. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You must provide a name for the badge. You must specify the name of the custom metadata set the badge will summarize. You must provide the name of the custom metadata property within that set the badge will represent. You can optionally provide other details about the badge, like with any other asset. In this example we provide a description for the badge. You can then specify any number of conditions to represent in the badge. Each condition is comprised of an operator (standard mathematical comparisons), ...a value against which to compare the asset's value for the property, ...and the color to apply to the badge if the asset's value for the property matches the value (as compared through the operator). This can be one of the predefined colors, or any RGB-based hex value for a custom color. As with all other builder patterns, you must build() the object you've defined. The save() operation will actually create the badge within Atlan, including its conditions. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Build a badge 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import Badge from pyatlan.model.enums import BadgeConditionColor , BadgeComparisonOperator from pyatlan.model.structs import BadgeCondition client = AtlanClient () badge = Badge . creator ( # (1) client = client , # (2) name = \"DQ Count\" , # (3) cm_name = \"Data Quality\" , # (4) cm_attribute = \"count\" , # (5) badge_conditions = [ # (6) BadgeCondition . create ( # (7) badge_condition_operator = BadgeComparisonOperator . GTE , # (8) badge_condition_value = \"5\" , # (9) badge_condition_colorhex = BadgeConditionColor . GREEN , # (10) ), BadgeCondition . create ( badge_condition_operator = BadgeComparisonOperator . LT , badge_condition_value = \"5\" , badge_condition_colorhex = BadgeConditionColor . YELLOW , ), BadgeCondition . create ( badge_condition_operator = BadgeComparisonOperator . LTE , badge_condition_value = \"2\" , badge_condition_colorhex = BadgeConditionColor . RED , ), ], ) badge . user_description = \"How many data quality checks ran against this asset.\" # (11) response = client . asset . save ( badge ) # (12) assert ( assets := response . assets_created ( asset_type = Badge ) # (13) Like with any other asset, use the create() method to ensure you provide the minimal information required to create a badge. You must provide a client instance. You must provide a name for the badge. You must specify the name of the custom metadata set the badge will summarize. You must provide the name of the custom metadata property within that set the badge will represent. Property is renamed The property names used have been converted to the standard python form, i.e. lowercase with spaces replaced with an underscore. You can then specify any number of conditions to represent in the badge. Use the 'create()' method to provide the information needed to create the BadgeCondition . Each condition is comprised of an operator (standard mathematical comparisons), ...a value against which to compare the asset's value for the property, ...and the color to apply to the badge if the asset's value for the property matches the value (as compared through the operator). This can be one of the predefined colors, or any RGB-based hex value for a custom color. You can optionally provide other details about the badge, like with any other asset. In this example we provide a description for the badge. The save() operation will actually create the badge within Atlan, including its conditions. Check that the Badge was created. Build a badge 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 val badge = Badge . creator ( client , // (1) \"DQ Count\" , // (2) \"Data Quality\" , // (3) \"Count\" ) // (4) . userDescription ( \"How many data quality checks ran against this asset.\" ) // (5) . badgeCondition ( // (6) BadgeCondition . of ( BadgeComparisonOperator . GTE , // (7) 5 , // (8) BadgeConditionColor . GREEN )) // (9) . badgeCondition ( BadgeCondition . of ( BadgeComparisonOperator . LT , 5 , BadgeConditionColor . YELLOW )) . badgeCondition ( BadgeCondition . of ( BadgeComparisonOperator . LTE , 2 , BadgeConditionColor . RED )) . build () // (10) val response = badge . save ( client ) // (11) Like with any other asset, use the creator() method to ensure you provide the minimal information required to create a badge. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You must provide a name for the badge. You must specify the name of the custom metadata set the badge will summarize. You must provide the name of the custom metadata property within that set the badge will represent. You can optionally provide other details about the badge, like with any other asset. In this example we provide a description for the badge. You can then specify any number of conditions to represent in the badge. Each condition is comprised of an operator (standard mathematical comparisons), ...a value against which to compare the asset's value for the property, ...and the color to apply to the badge if the asset's value for the property matches the value (as compared through the operator). This can be one of the predefined colors, or any RGB-based hex value for a custom color. As with all other builder patterns, you must build() the object you've defined. The save() operation will actually create the badge within Atlan, including its conditions. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 { \"entities\" : [ // (1) { \"typeName\" : \"Badge\" , // (2) \"attributes\" : { \"name\" : \"Count\" , // (3) \"badgeMetadataAttribute\" : \"gA1HGY8JClmG8wXxC9i4EX.NYcrFDTHGVpBKHv0Ihk8xC\" , // (4) \"qualifiedName\" : \"badges/global/gA1HGY8JClmG8wXxC9i4EX.NYcrFDTHGVpBKHv0Ihk8xC\" , // (5) \"userDescription\" : \"How many data quality checks ran against this asset.\" , // (6) \"badgeConditions\" : [ // (7) { \"badgeConditionOperator\" : \"gte\" , // (8) \"badgeConditionValue\" : \"5\" , // (9) \"badgeConditionColorhex\" : \"#047960\" // (10) }, { \"badgeConditionOperator\" : \"lt\" , \"badgeConditionValue\" : \"5\" , \"badgeConditionColorhex\" : \"#F7B43D\" }, { \"badgeConditionOperator\" : \"lte\" , \"badgeConditionValue\" : \"2\" , \"badgeConditionColorhex\" : \"#BF1B1B\" } ] } } ] } Like with any other asset, define the badge within an entities array. You must use the exact value Badge as the typeName for a badge. You must provide a name for the badge. You must specify the full name of the custom metadata property (its set and property name). These must also use Atlan's internal hashed-string representation . You must provide a qualifiedName for the badge that uses the format: badges/global/<custom-metadata-property> , where ` is the full name of the custom metadata property (using Atlan's internal hashed-string representation ). You can optionally provide other details about the badge, like with any other asset. In this example we provide a description for the badge. You can then specify any number of conditions to represent in the badge. Each condition is comprised of an operator (standard mathematical comparisons), ...a value against which to compare the asset's value for the property, Must be a string in the JSON The value you provide must always be a string in JSON. For actual string values (for text fields and options fields), you must further wrap the string itself in double-quotes. ...and the color to apply to the badge if the asset's value for the property matches the value (as compared through the operator). This should be an RGB-based hex value. (The colors given in this example are the standard green, amber, red used in the UI.) Now that the badge has been created, any assets with a value set for the custom metadata will show the badge on its overview tab. Delete a badge Â¶ 1.4.0 4.0.0 You can delete a badge at any time using: Java Python Kotlin Raw REST API Delete a badge 1 AssetDeletionResponse response = Badge . purge ( client , \"1c932bbb-fbe6-4bbc-9d0d-3df2f1fa4f81\" ); // (1) The purge() operation will permanently delete the badge, given the GUID of the badge. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a badge 1 response = client . asset . purge_by_guid ( \"1c932bbb-fbe6-4bbc-9d0d-3df2f1fa4f81\" ) # (1) The asset.purge_by_guid() operation will permanently delete the badge, given the GUID of the badge. Delete a badge 1 val response = Badge . purge ( client , \"1c932bbb-fbe6-4bbc-9d0d-3df2f1fa4f81\" ) // (1) The purge() operation will permanently delete the badge, given the GUID of the badge. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. DELETE /api/meta/entity/bulk?guid=1c932bbb-fbe6-4bbc-9d0d-3df2f1fa4f81&deleteType=PURGE 1 // (1) All information needed to permanently delete the badge is provided in the URL (in particular the GUID of the badge). Now that the badge has been deleted, no assets will show it on their overview tab. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/custom-metadata/delete/",
    "content": "/api/meta/types/typedef/name/{name} (DELETE) Delete custom metadata Â¶ There are limits to the number of custom metadata properties you can create Atlan currently preserves details of custom metadata in its audit log. This allows Atlan to retain an audit trail of actions users took on custom metadata on each asset, even if the custom metadata definition itself is deleted. However, this also places an upper limit on the number of custom metadata properties you can create in Atlan. Even if you delete the custom metadata definitions, any that you have previously defined will still take up \"space\" within this limit. More details By default this is ~1000 properties. If you see an error like the following, it means you have reached this limit: { \"errorCode\" : \"ATLAS-500-00-001\" , \"errorMessage\" : \"Unable to push entity audits to ES\" , \"errorCause\" : \"[{type=mapper_parsing_exception, reason=failed to parse, caused_by={type=illegal_argument_exception, reason=Limit of total fields [1000] has been exceeded while adding new fields [5]}}]\" } You will need to contact Atlan support to extend this threshold if you reach it. 1.3.3 4.0.0 To delete a custom metadata structure: Java Python Kotlin Raw REST API Delete custom metadata structure 1 CustomMetadataDef . purge ( client , \"RACI\" ); // (1) You only need to call the CustomMetadataDef.purge() method with the human-readable name of the custom metadata structure, and it will be deleted. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete custom metadata structure 1 2 3 4 5 from pyatlan.model.typedef import CustomMetadataDef from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . typedef . purge ( \"RACI\" , CustomMetadataDef ) # (1) You only need to call the typedef.purge() method with the human-readable name of the custom metadata structure, and it will be deleted. Delete custom metadata structure 1 CustomMetadataDef . purge ( client , \"RACI\" ) // (1) You only need to call the CustomMetadataDef.purge() method with the human-readable name of the custom metadata structure, and it will be deleted. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. DELETE /api/meta/types/typedef/name/MNJ8mpLsIOaP4OQnLNhRta 1 Use the hashed-string representation When deleting the custom metadata structure using the raw API, you must use the hashed-string representation of its name in the API call. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/common-examples/lineage/traverse/",
    "content": "/api/meta/lineage/getlineage (POST) /api/meta/lineage/list (POST) Traverse lineage Â¶ Retrieve lineage Â¶ 2.5.3 3.1.1 To fetch lineage, you need to request lineage from Atlan from a particular starting point: Java Python Kotlin Raw REST API Retrieve lineage 1 2 3 4 5 6 7 8 9 10 11 FluentLineage . builder ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) // (1) . depth ( 1000000 ) // (2) . direction ( AtlanLineageDirection . DOWNSTREAM ) // (3) . pageSize ( 10 ) // (4) . includeOnResults ( Asset . NAME ) // (5) . immediateNeighbors ( true ) // (6) . stream () // (7) . forEach ( result -> { // (8) // Do something with the result }); Build a request for lineage with the starting point for your lineage retrieval (the GUID of an asset). If you already have an asset, you can also instead run requestLineage() on the asset to directly build the same request. You can specify how far you want lineage to be fetched using depth() . A depth of 1 will only fetch immediate upstream and downstream assets, while 2 will also fetch the immediate upstream and downstream assets of those assets, and so on. The default value of 1000000 will fetch upstream and downstream assets up to 1,000,000 hops away (basically all lineage). You can fetch only upstream assets or only downstream assets. In the list API, you cannot access both directions at the same time. You can specify how many results to include per page of results (defaults to 10). You can also specify any extra attributes you want to include in each asset in the resulting list. To include details about which asset is upstream and downstream of which other asset, set immediateNeighbors to true . (Without this, all downstream assets will be listed in breadth-first order, but you will not know specifically which asset is downstream of which other asset.) You can then directly stream the results from the request. These will be lazily-fetched and paged automatically. A normal Java Stream is created, so you can apply any stream-based operations to it (filtering, mapping, collecting, or doing something for each result as in this example). Retrieve lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import LineageDirection from pyatlan.model.assets import Asset from pyatlan.model.lineage import FluentLineage client = AtlanClient () request = FluentLineage ( # (1) starting_guid = \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , # (2) depth = 1000000 , # (3) direction = LineageDirection . DOWNSTREAM , # (4) size = 10 , # (5) includes_on_results = Asset . NAME , # (6) immediate_neighbors = True , # (7) ) . request response = client . asset . get_lineage_list ( request ) # (8) for asset in response : # (9) ... Build a request for lineage by specifying the parameters on the constructor. The starting point for lineage must be the GUID of an asset. You can specify how far you want lineage to be fetched using depth . A depth of 1 will only fetch immediate upstream and downstream assets, while 2 will also fetch the immediate upstream and downstream assets of those assets, and so on. The default value of 1000000 will fetch upstream and downstream assets up to 1,000,000 hops away (basically all lineage). You can fetch only upstream assets or only downstream assets. In the list API, you cannot access both directions at the same time. You can specify how many results to include per page of results (defaults to 10). You can also specify any extra attributes you want to include in each asset in the resulting list. The immediate_neighbors parameter, when set to True , includes direct upstream and downstream connections for each asset, enabling detailed lineage traversal. Call the asset.get_lineage_list() method using the request to actually retrieve the lineage details from Atlan. Iterate through the results Retrieve lineage 1 2 3 4 5 6 7 8 9 10 11 FluentLineage . builder ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) // (1) . depth ( 1000000 ) // (2) . direction ( AtlanLineageDirection . DOWNSTREAM ) // (3) . pageSize ( 10 ) // (4) . includeOnResults ( Asset . NAME ) // (5) . immediateNeighbors ( true ) // (6) . stream () // (7) . forEach { // (8) // Do something with the result } Build a request for lineage with the starting point for your lineage retrieval (the GUID of an asset). If you already have an asset, you can also instead run requestLineage() on the asset to directly build the same request. You can specify how far you want lineage to be fetched using depth() . A depth of 1 will only fetch immediate upstream and downstream assets, while 2 will also fetch the immediate upstream and downstream assets of those assets, and so on. The default value of 1000000 will fetch upstream and downstream assets up to 1,000,000 hops away (basically all lineage). You can fetch only upstream assets or only downstream assets. In the list API, you cannot access both directions at the same time. You can specify how many results to include per page of results (defaults to 10). You can also specify any extra attributes you want to include in each asset in the resulting list. To include details about which asset is upstream and downstream of which other asset, set immediateNeighbors to true . (Without this, all downstream assets will be listed in breadth-first order, but you will not know specifically which asset is downstream of which other asset.) You can then directly stream the results from the request. These will be lazily-fetched and paged automatically. A normal Java Stream is created, so you can apply any stream-based operations to it (filtering, mapping, collecting, or doing something for each result as in this example). POST /api/meta/lineage/list 1 2 3 4 5 6 7 8 9 10 11 12 { \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , // (1) \"depth\" : 1000000 , // (2) \"direction\" : \"OUTPUT\" , // (3) \"from\" : 0 , // (4) \"size\" : 10 , // (5) \"attributes\" : [ // (6) \"name\" ], \"excludeMeanings\" : true , \"excludeClassifications\" : true } The starting point for lineage must be the GUID of an asset. You can specify how far you want lineage to be fetched using depth() . A depth of 1 will only fetch immediate upstream and downstream assets, while 2 will also fetch the immediate upstream and downstream assets of those assets, and so on. A value of 1000000 will fetch upstream and downstream assets up to 1,000,000 hops away (basically all lineage). You can fetch only upstream assets ( INPUT ) or only downstream assets ( OUTPUT ). In the list API, you cannot access both directions at the same time. You can specify the starting point for a page of results (you must provide a value: 0 will start at the first result). You can specify how many results to include per page of results (you must provide a value: we suggest starting at 10 ). You can also specify any extra attributes you want to include in each asset in the resulting list. Traverse lineage Â¶ The new lineage list API returns results in breadth-first order. So you can traverse the lineage by progressing through the result list in the order they are returned, even across multiple pages of results. Downstream assets Â¶ 2.5.3 3.1.1 To traverse downstream assets in lineage: Java Python Kotlin Raw REST API Traverse downstream lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 FluentLineage . builder ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) // (1) . direction ( AtlanLineageDirection . DOWNSTREAM ) // (2) . immediateNeighbors ( true ) // (3) . stream () // (4) . filter ( a -> ! ( a instanceof ILineageProcess )) // (5) . limit ( 100 ) // (6) . forEach ( result -> { // (7) // Do something with each result for ( LineageRef ref : result . getImmediateDownstream ()) { // (8) String downstreamGuid = ref . getGuid (); // (9) } }); Specify the GUID of an asset for the starting point. (Or from an asset itself, use requestLineage() to start the same builder.) Request the DOWNSTREAM direction. If you want to understand specifically which assets are downstream from which other assets, set immediateNeighbors to true . You can then stream the results from the request. The pages will be lazily-fetched in the background, as-needed. With streams, you can apply additional filters over the results (in this example any processes in the results will be ignored). With streams, you can also limit the total number of results you want to process â€” independently from page size of retrievals. With lazy-fetching of the results, this will ensure you only retrieve the number of pages required to complete the stream. Of course, you still need to actually do something with those remaining results. If immediateNeighbors is true , each asset will have a list of downstream lineage references populated in .getImmediateDownstream() . You can, for example, retrieve the GUID of each of these downstream references to see the assets that are immediately downstream from the asset you are iterating through in the lineage results. Traverse downstream lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import LineageDirection from pyatlan.model.lineage import FluentLineage client = AtlanClient () request = FluentLineage ( starting_guid = \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , # (1) direction = LineageDirection . DOWNSTREAM , # (2) immediate_neighbors = True , # (3) ) . request response = client . asset . get_lineage_list ( request ) # (4) for asset in response : # (5) ... # (6) for ref in asset . immediate_downstream : # (7) downstream_guid = ref . guid # (8) Specify the GUID of an asset for the starting point. Request the DOWNSTREAM direction. If you want to understand specifically which assets are downstream from which other assets, set immediate_neighbors to True . Call the get_lineage_list method using the request to get the results You can then iterate through all of the results. The pages will be lazily-fetched in the background as-needed, and each result looped through. Do something with the result. If immediate_neighbors is True , each asset will have a list of downstream lineage references populated in .immediate_downstream . You can, for example, retrieve the GUID of each of these downstream references to see the assets that are immediately downstream from the asset you are iterating through in the lineage results. Traverse downstream lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 FluentLineage . builder ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) // (1) . direction ( AtlanLineageDirection . DOWNSTREAM ) // (2) . immediateNeighbors ( true ) // (3) . stream () // (4) . filter { it !is ILineageProcess } // (5) . limit ( 100 ) // (6) . forEach { result -> // (7) // Do something with each result result . immediateDownstream . forEach { ref -> // (8) val downstreamGuid = ref . guid // (9) } } Specify the GUID of an asset for the starting point. (Or from an asset itself, use requestLineage() to start the same builder.) Request the DOWNSTREAM direction. If you want to understand specifically which assets are downstream from which other assets, set immediateNeighbors to true . You can then stream the results from the request. The pages will be lazily-fetched in the background, as-needed. With streams, you can apply additional filters over the results (in this example any processes in the results will be ignored). With streams, you can also limit the total number of results you want to process â€” independently from page size of retrievals. With lazy-fetching of the results, this will ensure you only retrieve the number of pages required to complete the stream. Of course, you still need to actually do something with those remaining results. If immediateNeighbors is true , each asset will have a list of downstream lineage references populated in .getImmediateDownstream() . You can, for example, retrieve the GUID of each of these downstream references to see the assets that are immediately downstream from the asset you are iterating through in the lineage results. POST /api/meta/lineage/list 1 2 3 4 5 6 7 8 9 { \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , // (1) \"depth\" : 1000000 , \"direction\" : \"OUTPUT\" , // (2) \"from\" : 0 , \"size\" : 10 , \"excludeMeanings\" : true , \"excludeClassifications\" : true } Specify the GUID of an asset for the starting point. Request the OUTPUT (downstream) direction. Upstream assets Â¶ 2.5.3 3.1.1 To traverse upstream assets in lineage: Java Python Kotlin Raw REST API Traverse upstream lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 FluentLineage . builder ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) // (1) . direction ( AtlanLineageDirection . UPSTREAM ) // (2) . immediateNeighbors ( true ) // (3) . stream () // (4) . filter ( a -> ! ( a instanceof ILineageProcess )) // (5) . limit ( 100 ) // (6) . forEach ( result -> { // (7) // Do something with each result for ( LineageRef ref : result . getImmediateUpstream ()) { // (8) String upstreamGuid = ref . getGuid (); // (9) } }); Specify the GUID of an asset for the starting point. (Or from an asset itself, use requestLineage() to start the same builder.) Request the UPSTREAM direction. If you want to understand specifically which assets are upstream from which other assets, set immediateNeighbors to true . You can then stream the results from the request. The pages will be lazily-fetched in the background, as-needed. With streams, you can apply additional filters over the results (in this example any processes in the results will be ignored). With streams, you can also limit the total number of results you want to process â€” independently from page size of retrievals. With lazy-fetching of the results, this will ensure you only retrieve the number of pages required to complete the stream. Of course, you still need to actually do something with those remaining results. If immediateNeighbors is true , each asset will have a list of upstream lineage references populated in .getImmediateUpstream() . You can, for example, retrieve the GUID of each of these upstream references to see the assets that are immediately upstream from the asset you are iterating through in the lineage results. Traverse upstream lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import LineageDirection from pyatlan.model.lineage import FluentLineage client = AtlanClient () request = ( FluentLineage ( starting_guid = \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) # (1) . direction ( LineageDirection . UPSTREAM ) # (2) . immediate_neighbors ( True ) # (3) . request ) response = client . asset . get_lineage_list ( request ) # (4) for asset in response : # (5) ... # (6) for ref in asset . immediate_upstream : # (7) upstream_guid = ref . guid # (8) Specify the GUID of an asset for the starting point. Request the UPSTREAM direction. If you want to understand specifically which assets are upstream from which other assets, set immediate_neighbors to True . Call the get_lineage_list method using the request to get the results You can then iterate through all of the results. The pages will be lazily-fetched in the background as-needed, and each result looped through. Do something with the result. If immediate_neighbors is True , each asset will have a list of upstream lineage references populated in .immediate_upstream . You can, for example, retrieve the GUID of each of these upstream references to see the assets that are immediately upstream from the asset you are iterating through in the lineage results. Traverse upstream lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 FluentLineage . builder ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) // (1) . direction ( AtlanLineageDirection . UPSTREAM ) // (2) . immediateNeighbors ( true ) // (3) . stream () // (4) . filter { it !is ILineageProcess } // (5) . limit ( 100 ) // (6) . forEach { // Do something with each result result . immediateUpstream . forEach { ref -> // (8) val upstreamGuid = ref . guid // (9) } } Specify the GUID of an asset for the starting point. (Or from an asset itself, use requestLineage() to start the same builder.) Request the UPSTREAM direction. If you want to understand specifically which assets are upstream from which other assets, set immediateNeighbors to true . You can then stream the results from the request. The pages will be lazily-fetched in the background, as-needed. With streams, you can apply additional filters over the results (in this example any processes in the results will be ignored). With streams, you can also limit the total number of results you want to process â€” independently from page size of retrievals. With lazy-fetching of the results, this will ensure you only retrieve the number of pages required to complete the stream. Of course, you still need to actually do something with those remaining results. If immediateNeighbors is true , each asset will have a list of upstream lineage references populated in .getImmediateUpstream() . You can, for example, retrieve the GUID of each of these upstream references to see the assets that are immediately upstream from the asset you are iterating through in the lineage results. POST /api/meta/lineage/list 1 2 3 4 5 6 7 8 9 { \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , // (1) \"depth\" : 1000000 , \"direction\" : \"INPUT\" , // (2) \"from\" : 0 , \"size\" : 10 , \"excludeMeanings\" : true , \"excludeClassifications\" : true } Specify the GUID of an asset for the starting point. Request the INPUT (upstream) direction. Filter lineage Â¶ You can also filter the information fetched through lineage. This can help improve performance of your code by limiting the results it will fetch to only those you require. Retrieve only active assets In most cases for lineage you only care about active assets. By filtering to only active assets, you can improve the performance of lineage retrieval by as much as 10x. (The new FluentLineage interface will do this automatically, unless you explicitly request the inclusion of archived assets.) Not possible to filter by custom metadata You currently cannot filter lineage based on the values of custom metadata. Limit assets in response Â¶ 1.9.5 4.0.0 You can limit the assets you will see in the response through entity filters. These restrict what assets will be included in the results, but still traverse all of the lineage: Java Python Kotlin Raw REST API Limit assets in response 1 2 3 4 5 6 List < Asset > verifiedAssets = Asset . lineage ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) // (1) . direction ( AtlanLineageDirection . UPSTREAM ) . includeInResults ( Asset . CERTIFICATE_STATUS . inLineage . eq ( CertificateStatus . VERIFIED )) // (2) . includesCondition ( FilterList . Condition . AND ) // (3) . stream () // (4) . collect ( Collectors . toList ()); // (5) Build the request as you would above, or request it directly from an asset. Because this operation will directly request lineage for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add one or more includeInResults to the request before sending it to Atlan. Each of these defines criteria for which assets should be filtered for inclusion in the results, in this example only assets with a verified certificate will be included. The criterion itself is composed of: The field by which you want to filter ( Asset.CERTIFICATE_STATUS in this example). A fixed member within that field that builds lineage filters, called .inLineage . The operator you want to use to compare values for that field in order to determine whether or not an asset matches ( .eq() in this example). The value you want to compare against using that operator ( CertificateStatus.VERIFIED in this example). Optionally, you can use includesCondition in your lineage request to specify whether the includeInResults criteria\nshould be combined with AND (default) or if any matching is sufficient ( OR ). When you then stream the results, only those assets that match the filter criteria will be included in the response. You can then collect them (standard stream operation) to give a complete list, across pages, of those assets that match the criteria. Limit assets in response 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import CertificateStatus , LineageDirection from pyatlan.model.assets import Asset from pyatlan.model.lineage import FilterList client = AtlanClient () request = ( Asset . lineage ( guid = \"f23dfa3b-3a8a-417a-b2fb-17fdfca9d442\" ) # (1) . direction ( LineageDirection . UPSTREAM ) . include_in_results ( Asset . CERTIFICATE_STATUS . in_lineage . eq ( CertificateStatus . VERIFIED ) # (2) ) . includes_condition ( FilterList . Condition . AND ) . request # (3) ) response = client . asset . get_lineage_list ( request ) # (4) verified_assets : list [ Asset ] = [] for asset in response : # (5) verified_assets . append ( asset ) Build the request as you would above, or request it directly from an asset. Add one or more include_in_results to the request before sending it to Atlan. Each of these defines criteria for which assets should be filtered for inclusion in the results, in this example only assets with a verified certificate will be included. The criterion itself is composed of: The field by which you want to filter ( Asset.CERTIFICATE_STATUS in this example). A fixed member within that field that builds lineage filters, called .in_lineage . The operator you want to use to compare values for that field in order to determine whether or not an asset matches ( .eq() in this example). The value you want to compare against using that operator ( CertificateStatus.VERIFIED in this example). Optionally, you can use .includes_condition in your lineage request to specify whether the includes_in_results criteria\nshould be combined with AND (default) or if any matching is sufficient ( OR ). Use the request to get the response that can be used to iterate through the assets. Iterate through the assets and use them as you will. Limit assets in response 1 2 3 4 5 6 val verifiedAssets = Asset . lineage ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) // (1) . direction ( AtlanLineageDirection . UPSTREAM ) . includeInResults ( Asset . CERTIFICATE_STATUS . inLineage . eq ( CertificateStatus . VERIFIED )) // (2) . includesCondition ( FilterList . Condition . AND ) // (3) . stream () // (4) . toList () // (5) Build the request as you would above, or request it directly from an asset. Because this operation will directly request lineage for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add one or more includeInResults to the request before sending it to Atlan. Each of these defines criteria for which assets should be filtered for inclusion in the results, in this example only assets with a verified certificate will be included. The criterion itself is composed of: The field by which you want to filter ( Asset.CERTIFICATE_STATUS in this example). A fixed member within that field that builds lineage filters, called .inLineage . The operator you want to use to compare values for that field in order to determine whether or not an asset matches ( .eq() in this example). The value you want to compare against using that operator ( CertificateStatus.VERIFIED in this example). Optionally, you can use includesCondition in your lineage request to specify whether the includeInResults criteria\nshould be combined with AND (default) or if any matching is sufficient ( OR ). When you then stream the results, only those assets that match the filter criteria will be included in the response. You can then collect them (standard stream operation) to give a complete list, across pages, of those assets that match the criteria. POST /api/meta/lineage/list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , \"depth\" : 1000000 , \"direction\" : \"INPUT\" , \"entityFilters\" : { // (1) \"condition\" : \"AND\" , \"criterion\" : [ // (2) { \"attributeName\" : \"certificateStatus\" , \"operator\" : \"contains\" , \"attributeValue\" : \"VERIFIED\" } ] }, \"from\" : 0 , \"size\" : 10 , \"excludeMeanings\" : true , \"excludeClassifications\" : true } Build the request as you would above, but add entityFilters to the request before sending it to Atlan. You can specify any number of criteria to include in your filters, and whether they should all apply (condition of AND ) or only any one of them need apply (condition of OR ) for a resulting asset to be included. Each filter criterion is a combination of: The name of the field in Atlan to use for filtering the results The value of that field that should be compared against for filtering An operator that defines how that comparison should be done to be considered a match Limit lineage traversal Â¶ 1.9.5 4.0.0 You can also limit how much of the lineage is traversed. You can do this both at an asset-level and a relationship-level: Java Python Kotlin Raw REST API Limit lineage traversal 1 2 3 4 5 6 7 8 List < Asset > activeAssets = Asset . lineage ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) . direction ( AtlanLineageDirection . DOWNSTREAM ) . whereAsset ( FluentLineage . ACTIVE ) // (1) . assetsCondition ( FilterList . Condition . AND ) // (2) . whereRelationship ( FluentLineage . ACTIVE ) . relationshipsCondition ( FilterList . Condition . AND ) // (3) . stream () // (4) . collect ( Collectors . toList ()); // (5) Provide your conditions to the whereAsset and whereRelationship of the request. This will ensure that once an asset (or relationship) is found in lineage traversal that does not match the conditions, further lineage traversal beyond that asset (or relationship) will not be done. In this example, that means that once we hit an archived or soft-deleted asset (or relationship) in the lineage, we will not look for any further downstream lineage from that archived or soft-deleted asset (or relationship). (In other words, we will limit the lineage results to only active assets by short-circuiting traversal when we hit an archived or soft-deleted asset or relationship.) FluentLineage.ACTIVE constant Note that the FluentLineage.ACTIVE example here is a predefined filter constant. If you look at its code, it is equivalent to writing any other lineage filter: Asset . STATUS . inLineage . eq ( AtlanStatus . ACTIVE ) When you request lineage directly on an asset, as in the example above, by default only active assets and relationships are included. (In other words, the filters by FluentLineage.ACTIVE are applied by default when using the Asset.lineage() request style.) Optionally, you can use assetsCondition in your lineage request to specify whether the whereAsset criteria\nshould be combined with AND (default) or if any matching is sufficient ( OR ). Optionally, you can use relationshipsCondition in your lineage request to specify whether the whereRelationship criteria\nshould be combined with AND (default) or if any matching is sufficient ( OR ). When you then fetch the results and iterate through them, not only are those assets that match the filter criteria the only ones included in the response, but the traversal is likely to run significantly faster as well by entirely skipping any further downstream traversal through the assets that do not match. You can continue to process the results from there as you would with any stream: filtering, mapping, running something for each result, or in this example collecting them into a list. Limit lineage traversal 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import LineageDirection from pyatlan.model.assets import Asset from pyatlan.model.lineage import FluentLineage from pyatlan.model.lineage import FilterList client = AtlanClient () request = ( Asset . lineage ( guid = \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) . direction ( LineageDirection . DOWNSTREAM ) . where_assets ( FluentLineage . ACTIVE ) # (1) . assets_condition ( FilterList . Condition . AND ) # (2) . where_relationships ( FluentLineage . ACTIVE ) . relationships_condition ( FilterList . Condition . AND ) # (3) . request ) response = client . asset . get_lineage_list ( request ) # (4) for asset in response : # (5) ... Provide your conditions to the where_assets and where_relationships of FluentLineage. This will ensure that once an asset (or relationship) is found in lineage traversal that does not match the conditions, further lineage traversal beyond that asset (or relationship) will not be done. In this example, that means that once we hit an archived or soft-deleted asset (or relationship) in the lineage, we will not look for any further downstream lineage from that archived or soft-deleted asset (or relationship). (In other words, we will limit the lineage results to only active assets by short-circuiting traversal when we hit an archived or soft-deleted asset or relationship.) FluentLineage.ACTIVE constant Note that the FluentLineage.ACTIVE example here is a predefined filter constant. If you look at its code, it is equivalent to writing any other lineage filter: Asset . STATUS . in_lineage . eq ( EntityStatus . ACTIVE ) When you request lineage directly on an asset, as in the example above, by default only active assets and relationships are included. (In other words, the filters by FluentLineage.ACTIVE are applied by default when using the Asset.lineage() request style.) Optionally, you can use assets_condition in your lineage request to specify whether the where_assets criteria\nshould be combined with AND (default) or if any matching is sufficient ( OR ). Optionally, you can use relationships_condition in your lineage request to specify whether the where_relationships criteria\nshould be combined with AND (default) or if any matching is sufficient ( OR ). Use the request to get the response that can be used to iterate through the assets. Iterate through the assets and use them as you will. Limit lineage traversal 1 2 3 4 5 6 7 8 val activeAssets = Asset . lineage ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) . direction ( AtlanLineageDirection . DOWNSTREAM ) . whereAsset ( FluentLineage . ACTIVE ) // (1) . assetsCondition ( FilterList . Condition . AND ) // (2) . whereRelationship ( FluentLineage . ACTIVE ) . relationshipsCondition ( FilterList . Condition . AND ) // (3) . stream () // (4) . collect ( Collectors . toList ()); // (5) Provide your conditions to the whereAsset and whereRelationship of the request. This will ensure that once an asset (or relationship) is found in lineage traversal that does not match the conditions, further lineage traversal beyond that asset (or relationship) will not be done. In this example, that means that once we hit an archived or soft-deleted asset (or relationship) in the lineage, we will not look for any further downstream lineage from that archived or soft-deleted asset (or relationship). (In other words, we will limit the lineage results to only active assets by short-circuiting traversal when we hit an archived or soft-deleted asset or relationship.) FluentLineage.ACTIVE constant Note that the FluentLineage.ACTIVE example here is a predefined filter constant. If you look at its code, it is equivalent to writing any other lineage filter: Asset . STATUS . inLineage . eq ( AtlanStatus . ACTIVE ) When you request lineage directly on an asset, as in the example above, by default only active assets and relationships are included. (In other words, the filters by FluentLineage.ACTIVE are applied by default when using the Asset.lineage() request style.) Optionally, you can use assetsCondition in your lineage request to specify whether the whereAsset criteria\nshould be combined with AND (default) or if any matching is sufficient ( OR ). Optionally, you can use relationshipsCondition in your lineage request to specify whether the whereRelationship criteria\nshould be combined with AND (default) or if any matching is sufficient ( OR ). When you then fetch the results and iterate through them, not only are those assets that match the filter criteria the only ones included in the response, but the traversal is likely to run significantly faster as well by entirely skipping any further downstream traversal through the assets that do not match. You can continue to process the results from there as you would with any stream: filtering, mapping, running something for each result, or in this example collecting them into a list. POST /api/meta/lineage/list 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , \"depth\" : 1000000 , \"direction\" : \"OUTPUT\" , \"entityFilters\" : { \"condition\" : \"AND\" , \"criterion\" : [ { \"attributeName\" : \"__state\" , \"operator\" : \"=\" , \"attributeValue\" : \"ACTIVE\" } ] }, \"entityTraversalFilters\" : { // (1) \"condition\" : \"AND\" , \"criterion\" : [ { \"attributeName\" : \"__state\" , \"operator\" : \"=\" , \"attributeValue\" : \"ACTIVE\" } ] }, \"from\" : 0 , \"size\" : 10 , \"excludeMeanings\" : true , \"excludeClassifications\" : true } Provide your conditions to the entityTraversalFilters of the request. This will ensure that once an asset is found in lineage traversal that does not match the conditions, further lineage traversal beyond that asset will not be done. In this example, that means that once we hit an archived of soft-deleted asset in the lineage, we will not look for any further downstream lineage from that archived or soft-deleted asset. (In other words, we will limit the lineage results to only active assets by short-circuiting traversal when we hit an archived or soft-deleted asset.) Limit asset details Â¶ 1.4.0 4.0.0 You can also limit the details for each asset returned by lineage: Java Python Kotlin Raw REST API Limit asset details 1 2 3 4 5 6 7 8 9 10 LineageListRequest request = Asset . lineage ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) . direction ( AtlanLineageDirection . DOWNSTREAM ) . includeOnResults ( Asset . DESCRIPTION ) // (1) . toRequestBuilder () // (2) . excludeAtlanTags ( false ) // (3) . excludeMeanings ( false ) // (4) . build (); List < Asset > withTagsAndTerms = request . fetch ( client ) // (5) . stream () // (6) . collect ( Collectors . toList ()); Build the request as above, but chain as many includeOnResults as you like to specify the attributes you want to include on each asset in the lineage. You can also decide whether to include or exclude Atlan tags and assigned business terms, but to do this you must first conver the fluent lineage request into a LineageListRequest . You can do this by chaining toRequestBuilder() . You can then use excludeAtlanTags(false) to ensure that Atlan tags are included on each asset in lineage. You can also use excludeMeanings(false) to ensure that assigned business terms are included on each asset in lineage. You then need to call fetch() on the LineageListRequest to actually run the lineage request. Because this operation will directly request lineage for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then stream and further transform or collect the results from the request, directly. Limit asset details 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import LineageDirection from pyatlan.model.assets import Asset client = AtlanClient () request = ( Asset . lineage ( guid = \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) . direction ( LineageDirection . DOWNSTREAM ) . include_on_results ( Asset . DESCRIPTION ) # (1) . exclude_atlan_tags ( False ) # (2) . exclude_meanings ( False ) # (3) . request ) response = client . asset . get_lineage_list ( request ) # (4) for asset in response : # (5) ... Build the request as above, but chain as many include_on_results as you like to specify the attributes you want to include on each asset in the lineage. You can use exclude_atlan_tags(False) to ensure that Atlan tags are included on each asset in lineage. You can use exclude_meanings(False) to ensure that assigned business terms are included on each asset in lineage. You then need to call get_lineage_list() with the LineageListRequest to actually run the lineage request. You can then iterate through and further transform or collect the results from the request. Limit asset details 1 2 3 4 5 6 7 8 9 10 val request = Asset . lineage ( client , \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) . direction ( AtlanLineageDirection . DOWNSTREAM ) . includeOnResults ( Asset . DESCRIPTION ) // (1) . toRequestBuilder () // (2) . excludeAtlanTags ( false ) // (3) . excludeMeanings ( false ) // (4) . build () val withTagsAndTerms = request . fetch ( client ) // (5) . stream () // (6) . toList () Build the request as above, but chain as many includeOnResults as you like to specify the attributes you want to include on each asset in the lineage. You can also decide whether to include or exclude Atlan tags and assigned business terms, but to do this you must first conver the fluent lineage request into a LineageListRequest . You can do this by chaining toRequestBuilder() . You can then use excludeAtlanTags(false) to ensure that Atlan tags are included on each asset in lineage. You can also use excludeMeanings(false) to ensure that assigned business terms are included on each asset in lineage. You then need to call fetch() on the LineageListRequest to actually run the lineage request. Because this operation will directly request lineage for the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then stream and further transform or collect the results from the request, directly. POST /api/meta/lineage/list 1 2 3 4 5 6 7 8 9 10 11 12 { \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , \"depth\" : 1000000 , \"direction\" : \"OUTPUT\" , \"attributes\" : [ // (1) \"description\" ], \"from\" : 0 , \"size\" : 10 , \"excludeClassifications\" : false , // (2) \"excludeMeanings\" : false // (3) } Build the request as above, but add as many field names as you like to specify the attributes you want to include on each asset in the lineage. You can use \"excludeClassifications\": false to ensure that Atlan tags are included on each asset in lineage. You can use \"excludeMeanings\": false to ensure that assigned business terms are included on each asset in lineage. Original API Â¶ Deprecated and removed The original lineage API was previously deprecated, and now no longer exists in the latest releases of the SDKs. It is slower, does not support paging, and will not receive any enhancements. We would therefore strongly recommend using the newer API (described above); however, the original API is described here for completeness. Retrieve lineage (deprecated) To fetch lineage, you need to request lineage from Atlan from a particular starting point: Java Python Raw REST API Retrieve lineage 1 2 3 4 5 6 7 8 LineageRequest request = LineageRequest . builder () // (1) . guid ( \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" ) // (2) . depth ( 0 ) // (3) . direction ( AtlanLineageDirection . BOTH ) // (4) . hideProcess ( true ) // (5) . allowDeletedProcess ( false ) // (6) . build (); // (7) LineageResponse response = request . fetch (); // (8) Build a LineageRequest to specify the starting point for your lineage retrieval. The starting point for lineage must be the GUID of an asset. You can specify how far you want lineage to be fetched using depth() . A depth of 1 will only fetch immediate upstream and downstream assets, while 2 will also fetch the immediate upstream and downstream assets of those assets, and so on. The default value of 0 will fetch all upstream and downstream assets. If you expect extensive lineage, change the default! The default value of 0 can result in a long-running API call with a very large response payload. If you expect your lineage to be extensive, you may want to try smaller depths first. You can fetch only upstream assets, only downstream assets, or lineage in both directions. Decide whether to include processes in the response. Use true if you want to use the SDK's traversal helpers Currently the SDK's traversal logic only works when this is set to true . Unless you want to code your own traversal logic, set hideProcess to true . If allowDeletedProcess is set to true and hideProcess is set to false then deleted (archived) processes will also be included in the response. Build the request. Call the fetch() method to actually retrieve the lineage details from Atlan. Retrieve lineage 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import LineageDirection from pyatlan.model.lineage import LineageRequest client = AtlanClient () request = LineageRequest ( # (1) guid = \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , # (2) depth = 0 , # (3) direction = LineageDirection . BOTH , # (4) hide_process = True , # (5) allow_deleted_process = False , # (6) ) response = client . asset . get_lineage ( request ) # (7) Build a LineageRequest to specify the starting point for your lineage retrieval. The starting point for lineage must be the GUID of an asset. You can specify how far you want lineage to be fetched using depth . A depth of 1 will only fetch immediate upstream and downstream assets, while 2 will also fetch the immediate upstream and downstream assets of those assets, and so on. The default value of 0 will fetch all upstream and downstream assets. If you expect extensive lineage, change the default! The default value of 0 can result in a long-running API call with a very large response payload. If you expect your lineage to be extensive, you may want to try smaller depths first. You can fetch only upstream assets, only downstream assets, or lineage in both directions. Decide whether to include processes in the response. Use True if you want to use the SDK's traversal helpers Currently the SDK's traversal logic only works when this is set to True . Unless you want to code your own traversal logic, set hide_process to True . If allow_deleted_process is set to True and hide_process is set to False then deleted (archived) processes will also be included in the response. Call the asset.get_lineage() method to actually retrieve the lineage details from Atlan. POST /api/meta/lineage/getlineage 1 2 3 4 5 6 7 { \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , // (1) \"depth\" : 0 , // (2) \"direction\" : \"BOTH\" , // (3) \"hideProcess\" : true , // (4) \"allowDeletedProcess\" : false // (5) } The starting point for lineage must be the GUID of an asset. You can specify how far you want lineage to be fetched using depth . A depth of 1 will only fetch immediate upstream and downstream assets, while 2 will also fetch the immediate upstream and downstream assets of those assets, and so on. The default value of 0 will fetch all upstream and downstream assets. If you expect extensive lineage, change the default! The default value of 0 can result in a long-running API call with a very large response payload. If you expect your lineage to be extensive, you may want to try smaller depths first. You can fetch only upstream assets, only downstream assets, or lineage in both directions. Decide whether to include processes in the response. If allowDeletedProcess is set to true and hideProcess is set to false then deleted (archived) processes will also be included in the response. Traverse lineage (deprecated) To assist with traversal of the lineage, the SDK provides some helper methods. Downstream assets (deprecated) To retrieve assets immediately downstream from the originally-requested asset: Java Python Raw REST API Retrieve downstream assets 9 10 11 Set < String > downstreamGuids = response . getDownstreamAssetGuids (); // (1) List < Asset > downstreamAssets = response . getDownstreamAssets (); // (2) downstreamGuids = response . getDownstreamProcessGuids (); // (3) The getDownstreamAssetGuids() method will return the GUIDs of assets that are immediately downstream. The getDownstreamAssets() method will return the asset objects for the assets that are immediately downstream. The getDownstreamProcessGuids() method will return the GUIDs of the processes that run immediately downstream. Retrieve downstream assets 14 15 16 downstream_guids = response . get_downstream_asset_guids () # (1) downstream_assets = response . get_downstream_assets () # (2) downstream_process_guids = response . get_downstream_process_guids () # (3) The get_downstream_asset_guids() method will return the GUIDs of assets that are immediately downstream. The get_downstream_assets() method will return the asset objects for the assets that are immediately downstream. The get_downstream_process_guids() method will return the GUIDs of the processes that run immediately downstream. POST /api/meta/lineage/getlineage 1 2 3 4 5 { \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , // (1) \"depth\" : 1 , // (2) \"direction\" : \"OUTPUT\" // (3) } The starting point for lineage must be the GUID of an asset. A depth of 1 will only fetch immediate upstream and downstream assets. A direction of OUTPUT will fetch only downstream assets. Upstream assets (deprecated) To retrieve assets immediately upstream from the originally-requested asset: Java Python Raw REST API Retrieve upstream assets 9 10 11 Set < String > upstreamGuids = response . getUpstreamAssetGuids (); // (1) List < Asset > upstreamAssets = response . getUpstreamAssets (); // (2) upstreamGuids = response . getUpstreamProcessGuids (); // (3) The getUpstreamAssetGuids() method will return the GUIDs of assets that are immediately upstream. The getUpstreamAssets() method will return the asset objects for the assets that are immediately upstream. The getUpstreamProcessGuids() method will return the GUIDs of the processes that run immediately upstream. Retrieve upstream assets 14 15 16 upstream_guids = response . get_upstream_asset_guids () # (1) upstream_assets = response . get_upstream_assets () # (2) upstream_process_guids = response . get_upstream_process_guids () # (3) The get_upstream_asset_guids() method will return the GUIDs of assets that are immediately upstream. The get_upstream_assets() method will return the asset objects for the assets that are immediately upstream. The get_upstream_process_guids() method will return the GUIDs of the processes that run immediately upstream. POST /api/meta/lineage/getlineage 1 2 3 4 5 { \"guid\" : \"495b1516-aaaf-4390-8cfd-b11ade7a7799\" , // (1) \"depth\" : 1 , // (2) \"direction\" : \"INPUT\" // (3) } The starting point for lineage must be the GUID of an asset. A depth of 1 will only fetch immediate upstream and downstream assets. A direction of INPUT will fetch only upstream assets. Depth-first traversal (deprecated) You might want to traverse more than only the immediate upstream or downstream assets. To retrieve all assets that are downstream from the originally-requested asset, across multiple degrees of separation, using a depth-first search traversal: Java Python Raw REST API Retrieve all downstream assets 9 10 List < String > dfsDownstreamGuids = response . getAllDownstreamAssetGuidsDFS (); // (1) List < Asset > dfsDownstream = response . getAllDownstreamAssetsDFS (); // (2) The getAllDownstreamAssetGuidsDFS() method will return the GUIDs of all assets that are downstream. The first GUID will always be the GUID of the asset used as the starting point for lineage, so even if there is no downstream lineage this will still return a list with a single GUID. The traversal will be in depth-first order downstream. This means after the GUID for the starting point, the list will contain GUIDs of assets immediately downstream. These will be followed by the assets that are immediately downstream from those assets, and so on. (The deeper you get into the list, the further downstream you will be in lineage from the starting point.) The getAllDownstreamAssetsDFS() method will return the asset objects for all assets that are downstream. The first asset object will always be the object for the asset used as the starting point for lineage, so even if there is no downstream lineage this will still return a list with a single asset. The traversal will be in depth-first order downstream. This means after the asset for the starting point, the list will contain assets of assets immediately downstream. These will be followed by the assets that are immediately downstream from those assets, and so on. (The deeper you get into the list, the further downstream you will be in lineage from the starting point.) Retrieve all upstream assets 9 10 List < String > dfsDownstreamGuids = response . getAllUpstreamAssetGuidsDFS (); // (1) List < Asset > dfsUpstream = response . getAllUpstreamAssetsDFS (); // (2) The getAllUpstreamAssetGuidsDFS() method will return the GUIDs of all assets that are upstream. The first GUID will always be the GUID of the asset used as the starting point for lineage, so even if there is no upstream lineage this will still return a list with a single GUID. The traversal will be in depth-first order upstream. This means after the GUID for the starting point, the list will contain GUIDs of assets immediately upstream. These will be followed by the assets that are immediately upstream from those assets, and so on. (The deeper you get into the list, the further upstream you will be in lineage from the starting point.) The getAllUpstreamAssetsDFS() method will return the asset objects for all assets that are upstream. The first asset object will always be the object for the asset used as the starting point for lineage, so even if there is no upstream lineage this will still return a list with a single asset. The traversal will be in depth-first order upstream. This means after the asset for the starting point, the list will contain assets of assets immediately upstream. These will be followed by the assets that are immediately upstream from those assets, and so on. (The deeper you get into the list, the further upstream you will be in lineage from the starting point.) Retrieve all downstream assets 14 15 dfs_downstream_guids = response . get_all_downstream_asset_guids_dfs () # (1) dfs_downstream_assets = response . get_all_downstream_assets_dfs () # (2) The get_all_downstream_asset_guids_dfs() method will return the GUIDs of all assets that are downstream. The first GUID will always be the GUID of the asset used as the starting point for lineage, so even if there is no downstream lineage this will still return a list with a single GUID. The traversal will be in depth-first order downstream. This means after the GUID for the starting point, the list will contain GUIDs of assets immediately downstream. These will be followed by the assets that are immediately downstream from those assets, and so on. (The deeper you get into the list, the further downstream you will be in lineage from the starting point.) The get_all_downstream_assets_dfs() method will return the asset objects for all assets that are downstream. The first asset object will always be the object for the asset used as the starting point for lineage, so even if there is no downstream lineage this will still return a list with a single asset. The traversal will be in depth-first order downstream. This means after the asset for the starting point, the list will contain assets of assets immediately downstream. These will be followed by the assets that are immediately downstream from those assets, and so on. (The deeper you get into the list, the further downstream you will be in lineage from the starting point.) Retrieve all upstream assets 14 15 dfs_upstream_guids = response . get_all_upstream_asset_guids_dfs () # (1) dfs_upstream_assets = response . get_all_upstream_assets_dfs () # (2) The get_all_upstream_asset_guids_dfs() method will return the GUIDs of all assets that are upstream. The first GUID will always be the GUID of the asset used as the starting point for lineage, so even if there is no upstream lineage this will still return a list with a single GUID. The traversal will be in depth-first order upstream. This means after the GUID for the starting point, the list will contain GUIDs of assets immediately upstream. These will be followed by the assets that are immediately upstream from those assets, and so on. (The deeper you get into the list, the further upstream you will be in lineage from the starting point.) The get_all_upstream_assets_dfs() method will return the asset objects for all assets that are upstream. The first asset object will always be the object for the asset used as the starting point for lineage, so even if there is no upstream lineage this will still return a list with a single asset. The traversal will be in depth-first order upstream. This means after the asset for the starting point, the list will contain assets of assets immediately upstream. These will be followed by the assets that are immediately upstream from those assets, and so on. (The deeper you get into the list, the further upstream you will be in lineage from the starting point.) Multiple API calls You may either need to make multiple API calls using the approaches above, or retrieve all downstream lineage and in your code traverse the returned relationships. More details on what all means here Keep in mind that when we say \"all\" above we mean all assets that are found in the response . If you have modified your request parameters to limit the lineage (for example, through depth() or direction() ) then this will only traverse what is found in the response â€” not necessarily all lineage in Atlan. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/custom-metadata/enums/",
    "content": "/api/meta/types/typedef/name/{name} (GET) /api/meta/types/typedef/{name} (DELETE) /api/meta/types/typedefs (POST) /api/meta/types/typedefs (PUT) /api/meta/types/typedefs/?type=ENUM (GET) Manage options (enumerations) Â¶ Options (or enumerations ) in Atlan allow you to define a set of valid values\nfor custom metadata attributes. Like other objects in the SDK, enumerations implement\nthe builder pattern. This allows you to progressively build-up the list of values you want to create. Build minimal object needed Â¶ 1.3.3 1.0.0 For example, to create an enumeration to capture a data quality dimension: Java Python Kotlin Raw REST API Build enumeration for creation 1 2 3 4 EnumDef enumDef = EnumDef . creator ( \"DataQualityDimensions\" , // (1) List . of ( \"Accuracy\" , \"Completeness\" , \"Consistency\" , \"Timeliness\" , \"Validity\" , \"Uniqueness\" )) // 2 . build (); // (3) When creating the enumeration, you must provide a name ( DataQualityDimensions in this example). You can then add as many valid values as you want: always as a list of strings. As with all other builder patterns, you must build() the object you've defined. Build enumeration for creation 1 2 3 4 5 6 from pyatlan.model.typedef import EnumDef enum_def = EnumDef . create ( name = \"DataQualityDimensions\" , # (1) values = [ \"Accuracy\" , \"Completeness\" , \"Consistency\" , \"Timeliness\" , \"Validity\" , \"Uniqueness\" ] # (2) ) When creating the enumeration, you must provide a name ( DataQualityDimensions in this example). You can then add as many valid values as you want: always as a list of strings. Build enumeration for creation 1 2 3 4 val enumDef = EnumDef . creator ( \"DataQualityDimensions\" , // (1) listOf ( \"Accuracy\" , \"Completeness\" , \"Consistency\" , \"Timeliness\" , \"Validity\" , \"Uniqueness\" )) // 2 . build () // (3) When creating the enumeration, you must provide a name ( DataQualityDimensions in this example). You can then add as many valid values as you want: always as a list of strings. As with all other builder patterns, you must build() the object you've defined. POST /api/meta/types/typedefs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 { \"enumDefs\" : [ // (1) { \"category\" : \"ENUM\" , // (2) \"name\" : \"DataQualityDimensions\" , // (3) \"elementDefs\" : [ // (4) { \"value\" : \"Accuracy\" , \"ordinal\" : 0 }, { \"value\" : \"Completeness\" , \"ordinal\" : 1 }, { \"value\" : \"Consistency\" , \"ordinal\" : 2 }, { \"value\" : \"Timeliness\" , \"ordinal\" : 3 }, { \"value\" : \"Validity\" , \"ordinal\" : 4 }, { \"value\" : \"Uniqueness\" , \"ordinal\" : 5 } ] } ] } All enumeration definitions must be specified within the enumDefs array. Each definition must be defined with a category set to ENUM . The name you provide for the definition will be used both for the front and back-end. Within the definition, you need to define each valid value for the enumeration within the elementDefs array. Each valid value should have both a string value (as it will appear in the UI) and an integer ordinal . Both must be unique within the enumeration. Create the enumeration from the object Â¶ 1.3.3 4.0.0 Now that the object is built, this enumDef object will have the required information for Atlan to create it.\nYou can then actually create the enumeration in Atlan by calling the create() method on the object itself: Java Python Kotlin Raw REST API Create the enumeration 5 EnumDef response = enumDef . create ( client ); // (1) The create() operation will actually create the enumeration within Atlan, including all the valid values that were defined as part of it. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create the enumeration 7 8 9 10 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . typedef . create ( enum_def ) # (1) The typedef.create() operation will actually create the enumeration definition within Atlan, including all the valid values that were defined as part of it. Create the enumeration 5 val response = enumDef . create ( client ) // (1) The create() operation will actually create the enumeration within Atlan, including all the valid values that were defined as part of it. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Creation implicit in step above The actual creation of the enumeration structure is implicit in the example above. Use an enumeration in a custom metadata definition Â¶ 7.0.0 4.0.0 To use an enumeration to restrain the values for an attribute in a custom metadata definition: Java Python Kotlin Raw REST API Build custom metadata definition for creation 6 7 8 9 10 11 12 13 14 CustomMetadataDef customMetadataDef = CustomMetadataDef . creator ( \"DQ\" ) // (1) . attributeDef ( // (2) AttributeDef . of ( client , \"Dimension\" , // (3) AtlanCustomAttributePrimitiveType . OPTIONS , // (4) \"DataQualityDimensions\" , // (5) false )) // (6) . options ( CustomMetadataOptions . withLogoAsEmoji ( \"ðŸ”–\" )) // (7) . build (); // (8) customMetadataDef . create (); // (9) When creating the custom metadata structure, you must provide a name ( DQ in this example). You can then add as many attributes to that structure as you want. Each attribute must have a name. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Each attribute must have a type. When using the enumeration as the type (to constrain its possible values), use AtlanCustomAttributePrimitiveType.OPTIONS as the type. You must then also specify the enumeration that defines the valid values for this attribute. Carrying on the same example, we give the name of the enumeration here: DataQualityDimensions . You must also specify whether the attribute allows multiple values to be captured on it ( true ) or only a single value ( false ). You can specify how the custom metadata should appear (in this case, with an emoji). As with all other builder patterns, you must build() the object you've defined. Then you can create() the custom metadata definition within Atlan, including this enumeration-constrained attribute that was defined as part of it. Build custom metadata definition for creation 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from pyatlan.model.typedef import AttributeDef , CustomMetadataDef from pyatlan.model.enums import AtlanCustomAttributePrimitiveType from pyatlan.client.atlan import AtlanClient client = AtlanClient () cm_def = CustomMetadataDef . create ( display_name = \"DQ\" ) # (1) cm_def . attribute_defs = [ # (2) AttributeDef . create ( client = client , # (3) display_name = \"Dimension\" , # (4) attribute_type = AtlanCustomAttributePrimitiveType . OPTIONS , # (5) options_name = \"DataQualityDimensions\" , # (6) ), ] cm_def . options = CustomMetadataDef . Options . with_logo_as_emoji ( # (7) emoji = \"ðŸ”–\" ) client . typedef . create ( cm_def ) # (8) When creating the custom metadata structure, you must provide a name ( DQ in this example). You can then add as many attributes to that structure as you want. You must provide a client instance. Each attribute must have a name. Each attribute must have a type. When using the enumeration as the type (to constrain its possible values), use AtlanCustomAttributePrimitiveType.OPTIONS as the type. You must then also specify the enumeration that defines the valid values for this attribute. Carrying on the same example, we give the name of the enumeration here: DataQualityDimensions . You can specify how the custom metadata should appear (in this case, with an emoji). Then you can create the custom metadata definition within Atlan, including this enumeration-constrained attribute that was defined as part of it. Build custom metadata definition for creation 6 7 8 9 10 11 12 13 14 val customMetadataDef = CustomMetadataDef . creator ( \"DQ\" ) // (1) . attributeDef ( // (2) AttributeDef . of ( client , \"Dimension\" , // (3) AtlanCustomAttributePrimitiveType . OPTIONS , // (4) \"DataQualityDimensions\" , // (5) false )) // (6) . options ( CustomMetadataOptions . withLogoAsEmoji ( \"ðŸ”–\" )) // (7) . build () // (8) customMetadataDef . create () // (9) When creating the custom metadata structure, you must provide a name ( DQ in this example). You can then add as many attributes to that structure as you want. Each attribute must have a name. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Each attribute must have a type. When using the enumeration as the type (to constrain its possible values), use AtlanCustomAttributePrimitiveType.OPTIONS as the type. You must then also specify the enumeration that defines the valid values for this attribute. Carrying on the same example, we give the name of the enumeration here: DataQualityDimensions . You must also specify whether the attribute allows multiple values to be captured on it ( true ) or only a single value ( false ). You can specify how the custom metadata should appear (in this case, with an emoji). As with all other builder patterns, you must build() the object you've defined. Then you can create() the custom metadata definition within Atlan, including this enumeration-constrained attribute that was defined as part of it. POST /api/meta/types/typedefs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 { \"businessMetadataDefs\" : [ { \"category\" : \"BUSINESS_METADATA\" , \"name\" : \"DQ\" , // (1) \"attributeDefs\" : [ // (2) { \"name\" : \"\" , // (3) \"displayName\" : \"Dimension\" , \"description\" : \"\" , \"typeName\" : \"DataQualityDimensions\" , // (4) \"isOptional\" : true , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"isUnique\" : false , \"isIndexable\" : true , \"includeInNotification\" : false , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"PowerBIMeasure\\\",\\\"TableauWorkbook\\\",\\\"LookerModel\\\",\\\"MetabaseCollection\\\",\\\"ModeQuery\\\",\\\"GCSBucket\\\",\\\"LookerTile\\\",\\\"Table\\\",\\\"PowerBITile\\\",\\\"PowerBIPage\\\",\\\"SalesforceOrganization\\\",\\\"PresetWorkspace\\\",\\\"TableauDatasource\\\",\\\"PresetDataset\\\",\\\"TableauCalculatedField\\\",\\\"LookerFolder\\\",\\\"TableauWorksheet\\\",\\\"MetabaseQuestion\\\",\\\"AtlasGlossary\\\",\\\"PresetChart\\\",\\\"PowerBITable\\\",\\\"LookerProject\\\",\\\"SnowflakePipe\\\",\\\"PowerBIReport\\\",\\\"SigmaDatasetColumn\\\",\\\"TableauDatasourceField\\\",\\\"TablePartition\\\",\\\"AtlasGlossaryTerm\\\",\\\"SigmaDataElementField\\\",\\\"Schema\\\",\\\"Database\\\",\\\"DbtColumnProcess\\\",\\\"S3Object\\\",\\\"LookerLook\\\",\\\"TableauSite\\\",\\\"SnowflakeStream\\\",\\\"ModeCollection\\\",\\\"LookerDashboard\\\",\\\"PowerBIWorkspace\\\",\\\"Collection\\\",\\\"AtlasGlossaryCategory\\\",\\\"TableauFlow\\\",\\\"LookerView\\\",\\\"TableauProject\\\",\\\"LookerExplore\\\",\\\"ModeReport\\\",\\\"PowerBIColumn\\\",\\\"Query\\\",\\\"ColumnProcess\\\",\\\"SalesforceDashboard\\\",\\\"SalesforceObject\\\",\\\"BIProcess\\\",\\\"DbtModelColumn\\\",\\\"S3Bucket\\\",\\\"SigmaDataElement\\\",\\\"DataStudioAsset\\\",\\\"DbtProcess\\\",\\\"DbtModel\\\",\\\"PowerBIDataset\\\",\\\"Column\\\",\\\"DbtMetric\\\",\\\"TableauDashboard\\\",\\\"SigmaDataset\\\",\\\"LookerQuery\\\",\\\"APISpec\\\",\\\"MetabaseDashboard\\\",\\\"Process\\\",\\\"PowerBIDashboard\\\",\\\"APIPath\\\",\\\"ModeChart\\\",\\\"PowerBIDataflow\\\",\\\"SalesforceField\\\",\\\"GCSObject\\\",\\\"SalesforceReport\\\",\\\"View\\\",\\\"Folder\\\",\\\"TableauMetric\\\",\\\"MaterialisedView\\\",\\\"PresetDashboard\\\",\\\"PowerBIDatasource\\\",\\\"ModeWorkspace\\\",\\\"SigmaPage\\\",\\\"LookerField\\\",\\\"SigmaWorkbook\\\"]\" , \"allowSearch\" : false , \"maxStrLength\" : \"100000000\" , \"allowFiltering\" : true , \"multiValueSelect\" : false , \"showInOverview\" : false , \"primitiveType\" : \"enum\" , // (5) \"isEnum\" : true , \"enumType\" : \"DataQualityDimensions\" }, \"isNew\" : true , \"enumValues\" : [ \"Accuracy\" , \"Completeness\" , \"Consistency\" , \"Timeliness\" , \"Validity\" , \"Uniqueness\" ] } ], \"displayName\" : \"DQ\" , \"options\" : { \"logoType\" : \"emoji\" , \"emoji\" : \"ðŸ”–\" } } ] } When creating the custom metadata structure, you must provide a name ( DQ in this example). You can then add as many attributes to that structure as you want. Each attribute must have a name. Note, however, that the name should be sent as an empty string when creating an attribute (the name will be generated by the back-end), and it is actually the displayName that gives the name as it will appear in the UI. You must specify the enumeration that defines the valid values for this attribute. Carrying on the same example, we give the name of the enumeration here: DataQualityDimensions . Each attribute must also have a primitive type. When using the enumeration as the type (to constrain its possible values), use: enum as the primitive type isEnum set to true and set enumType to the name of the enumeration Update options (enumerations) Â¶ 7.0.0 4.0.0 For example, to update our data quality dimension\nenumeration by adding a new set of valid values. Java Python Kotlin Raw REST API Update existing enum structure 1 2 3 4 5 6 7 EnumDef enumDef = EnumDef . updater ( client , \"DataQualityDimensions\" , // (1) List . of ( \"Unknown\" , \"Others\" ), // (2) false // (3) ). build (); // (4) EnumDef response = enumDef . update ( client ); // (5) When updating the existing enumeration, you must provide a name ( DataQualityDimensions in this example). Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then add as many valid values as you want: always as a list of strings. You must specify whether you want to replace all existing values in the enumeration with the new ones ( true ), or if the new ones will be appended to the existing set ( false ). As with all other builder patterns, you must build() the object you've defined. The update() operation will actually update the enumeration within Atlan, including all the valid values that were defined as part of it. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update existing enum structure 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyatlan.model.typedef import EnumDef from pyatlan.client.atlan import AtlanClient client = AtlanClient () enum_def = EnumDef . update ( client = client , name = \"DataQualityDimensions\" , # (1) values = [ \"Unknown\" , \"Others\" ] # (2) replace_existing = False # (3) ) response = client . typedef . update ( enum_def ) # (4) When updating the existing enumeration, you must provide\na name ( DataQualityDimensions in this example). You can then add as many valid values as you want: always as a list of strings. You must specify whether you want to replace all existing values in the enumeration\nwith the new ones ( True ), or if the new ones will be appended to the existing set ( False ). The client.typedef.update() operation will actually update the enumeration within Atlan,\nincluding all the valid values that were defined as part of it. Update existing enum structure 1 2 3 4 5 6 7 val enumDef = EnumDef . updater ( client , \"DataQualityDimensions\" , // (1) listOf ( \"Unknown\" , \"Others\" ), // (2) false // (3) ). build () // (4) val response = enumDef . update ( client ) // (5) When updating the existing enumeration, you must provide a name ( DataQualityDimensions in this example). Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then add as many valid values as you want: always as a list of strings. You must specify whether you want to replace all existing values in the enumeration with the new ones ( true ), or if the new ones will be appended to the existing set ( false ). As with all other builder patterns, you must build() the object you've defined. The update() operation will actually update the enumeration within Atlan, including all the valid values that were defined as part of it. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. PUT /api/meta/types/typedefs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 { \"enumDefs\" : [ // (1) { \"category\" : \"ENUM\" , // (2) \"name\" : \"DataQualityDimensions\" , // (3) \"elementDefs\" : [ // (4) { \"value\" : \"Accuracy\" , \"ordinal\" : 0 }, { \"value\" : \"Completeness\" , \"ordinal\" : 1 }, { \"value\" : \"Consistency\" , \"ordinal\" : 2 }, { \"value\" : \"Timeliness\" , \"ordinal\" : 3 }, { \"value\" : \"Validity\" , \"ordinal\" : 4 }, { \"value\" : \"Uniqueness\" , \"ordinal\" : 5 }, { \"value\" : \"Unknown\" , \"ordinal\" : 6 }, { \"value\" : \"Others\" , \"ordinal\" : 7 } ] } ] } All enumeration definitions must be specified within the enumDefs array. Each definition must be defined with a category set to ENUM . The name of the enumeration definition you want to update. You must send all valid values in the elementDefs array, as the existing list\nof elements for the enumerations will be entirely replaced by what you provide here. Retrieve options (enumerations) Â¶ 2.1.0 1.0.0 To retrieve options (enumeration) by name: Java Python Kotlin Raw REST API Retrieve existing enum structure 1 TypeDef enumDef = client . typeDefs . get ( \"DataQualityDimensions\" ); // (1) To retrieve the enumeration, you need to call the .typeDefs.get() method on a client, with the human-readable name of the enumeration. Retrieve existing enum structure 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () enum_def = client . typedef . get_by_name ( \"DataQualityDimensions\" ) # (1) To retrieve the enumeration, you need to call the client.typedef.get_by_name() method with its human-readable name. Retrieve existing enum structure 1 val enumDef = client . typeDefs . get ( \"DataQualityDimensions\" ) // (1) To retrieve the enumeration, you need to call the .typeDefs.get() method on a client, with the human-readable name of the enumeration. GET /api/meta/types/typedef/name/DataQualityDimensions 1 Options (enumerations) do not have a hashed-string representation Note that unlike a custom metadata structure, options (enumerations) do not have\na hashed-string name. Therefore, use their human-readable name when retrieving its structure. URL-encoding However, since this name is embedded in the URL for retrieval,\nit does need to be url-encoded. For example, if the name contains\nspaces these need to be replaced with %20 . Retrieve all options (enumerations) Â¶ 1.3.3 4.0.0 To retrieve all options (enumeration): Java Python Kotlin Raw REST API Retrieve all enum structures 1 TypeDefResponse enumDefs = client . typeDefs . list ( AtlanTypeCategory . ENUM ); // (1) To retrieve all enumerations, call the .typeDefs.list() method on a client, with the category AtlanTypeCategory.ENUM . Retrieve all enum structures 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . typedef . get ( type_category = AtlanTypeCategory . ENUM ) # (1) enum_defs = response . enum_defs # (2) To retrieve all enumerations, call the client.typedef.get() method with the definition category AtlanTypeCategory.ENUM . Specifically retrieve the list of enumerations from TypeDefResponse . Retrieve all enum structures 1 val enumDefs = client . typeDefs . list ( AtlanTypeCategory . ENUM ) // (1) To retrieve all enumerations, call the .typeDefs.list() method on a client, with the category AtlanTypeCategory.ENUM . GET /api/meta/types/typedefs/?type=ENUM 1 Delete options (enumerations) Â¶ 1.3.3 4.0.0 To delete options (enumeration): Java Python Kotlin Raw REST API Delete enum structure 1 EnumDef . purge ( client , \"DataQualityDimensions\" ); // (1) You only need to call the EnumDef.purge() method with the human-readable name of the enumeration, and it will be deleted. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete enum structure 1 2 3 4 5 from pyatlan.model.typedef import EnumDef from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . typedef . purge ( \"DataQualityDimensions\" , EnumDef ) # (1) You only need to call the clietn.typedef.purge() method\nwith the human-readable name of the enumeration, and it will be deleted. Delete enum structure 1 EnumDef . purge ( \"DataQualityDimensions\" ) // (1) You only need to call the EnumDef.purge() method with the human-readable name of the enumeration, and it will be deleted. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. DELETE /api/meta/types/typedef/name/DataQualityDimensions 1 Options (enumerations) do not have a hashed-string representation Note that unlike a custom metadata structure, options (enumerations)\ndo not have a hashed-string name. Therefore, use their human-readable name when deleting. URL-encoding However, since this name is embedded in the URL for deletion,\nit does need to be url-encoded. For example, if the name contains\nspaces these need to be replaced with %20 . 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/datacontract/",
    "content": "Data contracts overview Â¶ erDiagram\n    DataContract |o--o{ DataContract : versions\n    DataContract |o--|| Asset : latest\n    DataContract |o--|| Asset : latestCertified Data contracts are agreements between data producers and consumers, that specify requirements for generating and using high-quality, reliable data. You can manage data contracts using Atlan's command-line interface (CLI) . You may also want to refer to the data contracts specification , to understand how data contracts are defined. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/custom-metadata/read/",
    "content": "/api/meta/types/typedefs (GET) Retrieve custom metadata Â¶ 7.0.0 4.0.0 You can retrieve an existing custom metadata structure: Java Python Kotlin Raw REST API Retrieve existing custom metadata structure 1 2 3 CustomMetadataDef existing = client . getCustomMetadataCache () . getCustomMetadataDef ( \"RACI\" ); // (1) You can retrieve the current custom metadata definition using the custom metadata cache from any client. In most cases you can simply use the default client ( Atlan.getDefaultClient() ). Pass the human-readable name of the custom metadata structure to the cache. Retrieve existing custom metadata structure 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () existing = client . custom_metadata_cache . get_custom_metadata_def ( client = client , name = \"RACI\" ) # (1) You can retrieve the current custom metadata definition using the client.custom_metadata_cache.get_custom_metadata_def() method and passing the client and human-readable name of the custom metadata structure. Retrieve existing custom metadata structure 1 2 3 val existing = client . customMetadataCache . getCustomMetadataDef ( \"RACI\" ) // (1) You can retrieve the current custom metadata definition using the custom metadata cache from any client. In most cases you can simply use the default client ( Atlan.getDefaultClient() ). Pass the human-readable name of the custom metadata structure to the cache. GET /api/meta/types/typedefs?type=BUSINESS_METADATA 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 { \"businessMetadataDefs\" : [ // (1) { \"category\" : \"BUSINESS_METADATA\" , // (2) \"guid\" : \"917ffec9-fa84-4c59-8e6c-c7b114d04be3\" , \"name\" : \"MNJ8mpLsIOaP4OQnLNhRta\" , // (3) \"displayName\" : \"RACI\" , // (4) \"description\" : \"\" , \"typeVersion\" : \"1.0\" , \"serviceType\" : \"atlan\" , \"attributeDefs\" : [ // (5) { \"name\" : \"fWMB77RSjRGNYoFeD4FcGi\" , // (6) \"displayName\" : \"Responsible\" , // (7) \"description\" : \"\" , \"typeName\" : \"string\" , // (8) \"includeInNotification\" : false , \"isIndexable\" : true , \"isOptional\" : true , \"isUnique\" : false , \"indexType\" : \"DEFAULT\" , \"searchWeight\" : -1 , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"Database\\\",\\\"Schema\\\",\\\"Table\\\"]\\n\" , \"maxStrLength\" : \"100000000\" , \"isEnum\" : false , \"multiValueSelect\" : false , \"allowFiltering\" : true , \"allowSearch\" : true , \"primitiveType\" : \"string\" , \"customType\" : \"users\" // (9) } } ], \"createdBy\" : \"jsmith\" , \"updatedBy\" : \"jsmith\" , \"createTime\" : 1648852296555 , \"updateTime\" : 1649172284333 , \"version\" : 2 } ] } Each custom metadata structure will be wrapped in the top-level businessMetadataDefs array. Each custom metadata structure object will have a category of BUSINESS_METADATA . The name of a custom metadata structure is a unique hashed-string, but is not human-readable. This is how the custom metadata is uniquely referred to through the raw APIs. The displayName of a custom metadata structure is the human-readable name you see in the UI. Each property defined within the custom metadata structure is nested within an attributeDefs array. As with the overall custom metadata structure, each attribute has a unique hashed-string name that is not human-readable. This is how the custom metadata property is uniquely referred to through the raw APIs. As with the overall custom metadata structure, each attribute also has a displayName that is the human-readable name you see in the UI. The type of the custom metadata property is its simple type, but does not include custom types like SQL, users, groups and so on. For the precise type, you also need to look at the customType within the options , within the attribute definition. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/custom-metadata/update/",
    "content": "/api/meta/types/typedefs (PUT) Update custom metadata Â¶ Custom metadata structure updates are complete replacements You need to send the entire custom metadata structure (all of its attributes) on each update. Retrieve existing structure Â¶ To ensure you have the complete structure, it is easiest to start by retrieving the existing custom metadata structure . Update the custom metadata structure Â¶ Now that you have the existing structure, modify the object. You can add or remove as many properties as you want in a single update, but for simplicity the following describe how to add and remove a single property each. Add a property Â¶ 7.0.0 4.0.0 To add a property: Java Python Kotlin Raw REST API Add a property to the structure 4 5 6 7 8 9 10 existing . toBuilder () // (6) . attributeDef ( AttributeDef . of ( client , // (2) \"Extra\" , // (3) AtlanCustomAttributePrimitiveType . STRING , // (4) false )) // (5) . build (); // (6) CustomMetadataDef updated = existing . update ( client ); // (7) After retrieving the existing custom metadata structure, clone the structure into a mutable one using toBuilder() . You can append a new attribute to its list of attributes by chaining .attributeDef() . Use the AttributeDef.of() factory method to define the attribute with the correct internal settings. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. When using the factory method, you need to provide at least a name; ...a type; ...and whether there can be multiple values for this property (true) or only a single value (false) on a given asset. Then build the mutable structure. And finally call the .update() method on the revised custom metadata structure to actually submit the changes to Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add a property to the structure 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pyatlan.model.typedef import AttributeDef from pyatlan.model.enums import AtlanCustomAttributePrimitiveType from pyatlan.client.atlan import AtlanClient client = AtlanClient () attrs = existing . attribute_defs # (1) attrs . append ( AttributeDef . create ( # (2) client = client , # (3) display_name = \"Extra\" , # (4) attribute_type = AtlanCustomAttributePrimitiveType . STRING , ), ) existing . attribute_defs = attrs # (5) response = client . typedef . update ( existing ) # (6) Create a new list of attributes, starting with the list of existing attributes. Add a new attribute to this list of attributes. Use the AttributeDef.create() factory method to define the attribute with the correct internal settings. You must provide a client instance. When using the factory method, you need to provide at least a name and a type. Then set the attributes on the custom metadata structure to this revised list. And finally call the .typedef.update() method sending the revised custom metadata structure to actually submit the changes to Atlan. Add a property to the structure 4 5 6 7 8 9 10 existing . toBuilder () // (6) . attributeDef ( AttributeDef . of ( client , // (2) \"Extra\" , // (3) AtlanCustomAttributePrimitiveType . STRING , // (4) false )) // (5) . build () // (6) val updated = existing . update ( client ) // (7) After retrieving the existing custom metadata structure, clone the structure into a mutable one using toBuilder() . You can append a new attribute to its list of attributes by chaining .attributeDef() . Use the AttributeDef.of() factory method to define the attribute with the correct internal settings. Because this operation may need to retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. When using the factory method, you need to provide at least a name; ...a type; ...and whether there can be multiple values for this property (true) or only a single value (false) on a given asset. Then build the mutable structure. And finally call the .update() method on the revised custom metadata structure to actually submit the changes to Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. PUT /api/meta/types/typedefs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 { \"businessMetadataDefs\" : [ // (1) { \"category\" : \"BUSINESS_METADATA\" , // (2) \"guid\" : \"917ffec9-fa84-4c59-8e6c-c7b114d04be3\" , \"name\" : \"MNJ8mpLsIOaP4OQnLNhRta\" , \"displayName\" : \"RACI\" , \"description\" : \"\" , \"typeVersion\" : \"1.0\" , \"serviceType\" : \"atlan\" , \"attributeDefs\" : [ // (3) { \"name\" : \"fWMB77RSjRGNYoFeD4FcGi\" , \"displayName\" : \"Responsible\" , \"description\" : \"\" , \"typeName\" : \"string\" , \"includeInNotification\" : false , \"isIndexable\" : true , \"isOptional\" : true , \"isUnique\" : false , \"indexType\" : \"DEFAULT\" , \"searchWeight\" : -1 , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"Database\\\",\\\"Schema\\\",\\\"Table\\\"]\\n\" , \"maxStrLength\" : \"100000000\" , \"isEnum\" : false , \"multiValueSelect\" : false , \"allowFiltering\" : true , \"allowSearch\" : true , \"primitiveType\" : \"string\" , \"customType\" : \"users\" } }, { ... }, { ... }, { ... }, { \"name\" : \"Extra\" , // (4) \"displayName\" : \"Extra\" , // (5) \"description\" : \"\" , \"typeName\" : \"string\" , \"includeInNotification\" : false , \"isIndexable\" : true , \"isOptional\" : true , \"isUnique\" : false , \"indexType\" : \"DEFAULT\" , \"searchWeight\" : -1 , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"Database\\\",\\\"Schema\\\",\\\"Table\\\"]\\n\" , \"maxStrLength\" : \"100000000\" , \"isEnum\" : false , \"multiValueSelect\" : false , \"allowFiltering\" : true , \"allowSearch\" : true , \"primitiveType\" : \"string\" , \"customType\" : \"users\" } } ], \"createdBy\" : \"jsmith\" , \"updatedBy\" : \"jsmith\" , \"createTime\" : 1648852296555 , \"updateTime\" : 1649172284333 , \"version\" : 2 } ] } You need to specify the entire custom metadata structure, within the businessMetadataDefs array. Include all the details of the custom metadata structure definition as you retrieved it. Include all the details of the custom metadata attribute definitions, as you retrieved them. Add the new attribute definition to the list of attribute definitions. Note that for the name during this update you can use any string you want, as it will be replaced by a system-generated hashed-string during creation of the property. However, ensure you use the human-readable name you want for the property for the displayName of the attribute definition. Also ensure you set all the remaining pieces of the attribute definition according to the nature of the attribute you want to define. Change a property Â¶ 1.3.3 4.0.0 To change an existing property: Java Python Kotlin Raw REST API Change the custom metadata definition 4 5 6 7 8 9 10 11 12 13 14 15 16 List < AttributeDef > revised = new ArrayList <> (); // (1) for ( AttributeDef attr : existing . getAttributeDefs ()) { // (2) if ( attr . getDisplayName (). equals ( \"Extra\" )) { revised . add ( attr . toBuilder (). displayName ( \"Something else\" ). build ()); // (3) } else { revised . add ( attr ); // (4) } } existing . toBuilder () // (5) . clearAttributeDefs () // (6) . attributeDefs ( revised ) // (7) . build (); CustomMetadataDef updated = existing . update ( client ); // (8) Create a new (mutable) empty list of attributes. Iterate through the existing attributes in the custom metadata structure... ...When you get to the attribute you want to change, modify it as-needed. Some properties must not be changed Do not change the attribute's primitiveType , isEnum , enumType , customType , multiValueSelect , isArchived , archivedAt , or archivedBy properties. These should only be set at creation or through archival methods. And add all attributes (existing and the modified one) into the list of revised attributes. You must then clone the custom metadata structure into a mutable structure, using toBuilder() . You then need to clear the existing attribute definitions (otherwise the next step will only append the same definitions again). Then you can set the attributes on the custom metadata structure to this revised list, and build the structure. And finally call the .update() method on the revised custom metadata structure to actually submit the changes to Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Change the custom metadata definition 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pyatlan.model.typedef import AttributeDef from pyatlan.model.enums import AtlanCustomAttributePrimitiveType revised = [] # (1) for attr in existing . attribute_defs : # (2) if attr . display_name == \"Extra\" : attr . display_name = \"Something else\" # (3) revised . append ( attr ) # (4) existing . attribute_defs = revised # (5) from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . typedef . update ( existing ) # (6) Create a new empty list of attributes. Iterate through the existing attributes in the custom metadata structure... ...When you get to the attribute you want to change, you can change its display_name , but not much else. (You should not change its type, hashed-string name, etc.) And add all attributes (existing and the modified one) into the list of revised attributes. Then set the attributes on the custom metadata structure to this revised list. And finally call the typedef.update() method sending the revised custom metadata structure to actually submit the changes to Atlan. Change the custom metadata definition 4 5 6 7 8 9 10 11 12 13 14 15 16 val revised = mutableListOf < AttributeDef > () // (1) for ( attr in existing . attributeDefs ) { // (2) if ( attr . displayName == \"Extra\" ) { revised . add ( attr . toBuilder (). displayName ( \"Something else\" ). build ()) // (3) } else { revised . add ( attr ) // (4) } } existing . toBuilder () // (5) . clearAttributeDefs () // (6) . attributeDefs ( revised ) // (7) . build () val updated = existing . update ( client ) // (8) Create a new (mutable) empty list of attributes. Iterate through the existing attributes in the custom metadata structure... ...When you get to the attribute you want to change, modify it as-needed. Some properties must not be changed Do not change the attribute's primitiveType , isEnum , enumType , customType , multiValueSelect , isArchived , archivedAt , or archivedBy properties. These should only be set at creation or through archival methods. And add all attributes (existing and the modified one) into the list of revised attributes. You must then clone the custom metadata structure into a mutable structure, using toBuilder() . You then need to clear the existing attribute definitions (otherwise the next step will only append the same definitions again). Then you can set the attributes on the custom metadata structure to this revised list, and build the structure. And finally call the .update() method on the revised custom metadata structure to actually submit the changes to Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. PUT /api/meta/types/typedefs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 { \"businessMetadataDefs\" : [ // (1) { \"category\" : \"BUSINESS_METADATA\" , // (2) \"guid\" : \"917ffec9-fa84-4c59-8e6c-c7b114d04be3\" , \"name\" : \"MNJ8mpLsIOaP4OQnLNhRta\" , \"displayName\" : \"RACI\" , \"description\" : \"\" , \"typeVersion\" : \"1.0\" , \"serviceType\" : \"atlan\" , \"attributeDefs\" : [ // (3) { \"name\" : \"fWMB77RSjRGNYoFeD4FcGi\" , \"displayName\" : \"Responsible\" , \"description\" : \"\" , \"typeName\" : \"string\" , \"includeInNotification\" : false , \"isIndexable\" : true , \"isOptional\" : true , \"isUnique\" : false , \"indexType\" : \"DEFAULT\" , \"searchWeight\" : -1 , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"Database\\\",\\\"Schema\\\",\\\"Table\\\"]\\n\" , \"maxStrLength\" : \"100000000\" , \"isEnum\" : false , \"multiValueSelect\" : false , \"allowFiltering\" : true , \"allowSearch\" : true , \"primitiveType\" : \"string\" , \"customType\" : \"users\" } }, { ... }, { ... }, { ... }, { \"name\" : \"okm7BDXjTQx4iYPT5u7ilu\" , // (4) \"displayName\" : \"Something else\" , // (5) \"description\" : \"\" , \"typeName\" : \"string\" , \"includeInNotification\" : false , \"isIndexable\" : true , \"isOptional\" : true , \"isUnique\" : false , \"indexType\" : \"DEFAULT\" , \"searchWeight\" : -1 , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"Database\\\",\\\"Schema\\\",\\\"Table\\\"]\\n\" , \"maxStrLength\" : \"100000000\" , \"isEnum\" : false , \"multiValueSelect\" : false , \"allowFiltering\" : true , \"allowSearch\" : true , \"primitiveType\" : \"string\" , \"customType\" : \"users\" } } ], \"createdBy\" : \"jsmith\" , \"updatedBy\" : \"jsmith\" , \"createTime\" : 1648852296555 , \"updateTime\" : 1649172284333 , \"version\" : 2 } ] } You need to specify the entire custom metadata structure, within the businessMetadataDefs array. Include all the details of the custom metadata structure definition as you retrieved it. Include all the details of the custom metadata attribute definitions, as you retrieved them. For the attribute you want to modify, include its hashed-string name as with all the other attribute definitions. However, for the displayName of the attribute you want to modify, change it to the new human-readable name you want to use. Remove a property Â¶ 1.3.3 4.0.0 To remove a property: Java Python Kotlin Raw REST API Remove a property from the structure 4 5 6 7 8 9 10 11 12 13 14 15 16 List < AttributeDef > revised = new ArrayList <> (); // (1) for ( AttributeDef attr : existing . getAttributeDefs ()) { // (2) if ( attr . getDisplayName (). equals ( \"Extra\" )) { revised . add ( attr . toBuilder (). archive ( \"jsmith\" ). build ()); // (3) } else { revised . add ( attr ); // (4) } } existing . toBuilder () // (5) . clearAttributeDefs () // (6) . attributeDefs ( revised ) // (7) . build (); CustomMetadataDef updated = existing . update ( client ); // (8) Create a new (mutable) empty list of attributes. Iterate through the existing attributes in the custom metadata structure... ...When you get to the attribute you want to remove, call the .archive() method against it passing the name of the user deleting the attribute. And add all attributes (existing and the removed one) into the list of revised attributes. You must then clone the custom metadata structure into a mutable structure, using toBuilder() . You then need to clear the existing attribute definitions (otherwise the next step will only append the same definitions again). Then you can set the attributes on the custom metadata structure to this revised list, and build the structure. And finally call the .update() method on the revised custom metadata structure to actually submit the changes to Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Change the custom metadata definition 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pyatlan.model.typedef import AttributeDef from pyatlan.model.enums import AtlanCustomAttributePrimitiveType revised = [] # (1) for attr in existing . attribute_defs : # (2) if attr . display_name == \"Extra\" : attr . archive ( by = \"jsmith\" ) # (3) revised . append ( attr ) # (4) existing . attribute_defs = revised # (5) from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . typedef . update ( existing ) # (6) Create a new empty list of attributes. Iterate through the existing attributes in the custom metadata structure... ...When you get to the attribute you want to remove, call the .archive() method against it passing the name of the user deleting the attribute. And add all attributes (existing and the archived one) into the list of revised attributes. Then set the attributes on the custom metadata structure to this revised list. And finally call the .update() method on the revised custom metadata structure to actually submit the changes to Atlan. Remove a property from the structure 4 5 6 7 8 9 10 11 12 13 14 15 16 val revised = mutableListOf < AttributeDef > () // (1) for ( attr in existing . attributeDefs ) { // (2) if ( attr . displayName == \"Extra\" ) { revised . add ( attr . toBuilder (). archive ( \"jsmith\" ). build ()) // (3) } else { revised . add ( attr ) // (4) } } existing . toBuilder () // (5) . clearAttributeDefs () // (6) . attributeDefs ( revised ) // (7) . build () val updated = existing . update ( client ) // (8) Create a new (mutable) empty list of attributes. Iterate through the existing attributes in the custom metadata structure... ...When you get to the attribute you want to remove, call the .archive() method against it passing the name of the user deleting the attribute. And add all attributes (existing and the removed one) into the list of revised attributes. You must then clone the custom metadata structure into a mutable structure, using toBuilder() . You then need to clear the existing attribute definitions (otherwise the next step will only append the same definitions again). Then you can set the attributes on the custom metadata structure to this revised list, and build the structure. And finally call the .update() method on the revised custom metadata structure to actually submit the changes to Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. PUT /api/meta/types/typedefs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 { \"businessMetadataDefs\" : [ // (1) { \"category\" : \"BUSINESS_METADATA\" , // (2) \"guid\" : \"917ffec9-fa84-4c59-8e6c-c7b114d04be3\" , \"name\" : \"MNJ8mpLsIOaP4OQnLNhRta\" , \"displayName\" : \"RACI\" , \"description\" : \"\" , \"typeVersion\" : \"1.0\" , \"serviceType\" : \"atlan\" , \"attributeDefs\" : [ // (3) { \"name\" : \"fWMB77RSjRGNYoFeD4FcGi\" , \"displayName\" : \"Responsible\" , \"description\" : \"\" , \"typeName\" : \"string\" , \"includeInNotification\" : false , \"isIndexable\" : true , \"isOptional\" : true , \"isUnique\" : false , \"indexType\" : \"DEFAULT\" , \"searchWeight\" : -1 , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"Database\\\",\\\"Schema\\\",\\\"Table\\\"]\\n\" , \"maxStrLength\" : \"100000000\" , \"isEnum\" : false , \"multiValueSelect\" : false , \"allowFiltering\" : true , \"allowSearch\" : true , \"primitiveType\" : \"string\" , \"customType\" : \"users\" } }, { ... }, { ... }, { ... }, { \"name\" : \"okm7BDXjTQx4iYPT5u7ilu\" , // (4) \"displayName\" : \"Extra-archived-1649172285912\" , // (5) \"description\" : \"\" , \"typeName\" : \"string\" , \"includeInNotification\" : false , \"isIndexable\" : true , \"isOptional\" : true , \"isUnique\" : false , \"indexType\" : \"DEFAULT\" , \"searchWeight\" : -1 , \"cardinality\" : \"SINGLE\" , \"valuesMinCount\" : 0 , \"valuesMaxCount\" : 1 , \"options\" : { \"applicableEntityTypes\" : \"[\\\"Asset\\\"]\" , \"customApplicableEntityTypes\" : \"[\\\"Database\\\",\\\"Schema\\\",\\\"Table\\\"]\\n\" , \"maxStrLength\" : \"100000000\" , \"isEnum\" : false , \"multiValueSelect\" : false , \"allowFiltering\" : true , \"allowSearch\" : true , \"primitiveType\" : \"string\" , \"customType\" : \"users\" , \"isArchived\" : true , // (6) \"archivedBy\" : \"jsmith\" , // (7) \"archivedAt\" : 1649172285912 // (8) } } ], \"createdBy\" : \"jsmith\" , \"updatedBy\" : \"jsmith\" , \"createTime\" : 1648852296555 , \"updateTime\" : 1649172284333 , \"version\" : 2 } ] } You need to specify the entire custom metadata structure, within the businessMetadataDefs array. Include all the details of the custom metadata structure definition as you retrieved it. Include all the details of the custom metadata attribute definitions, as you retrieved them. For the attribute you want to remove, include its hashed-string name as with all the other attribute definitions. However, for the displayName of the attribute you want to remove, append -archived- and a millisecond epoch for the current system time. Within the options for the attribute definition, set isArchived to true . Within the options for the attribute definition, set archivedBy to the name of the user who is deleting the attribute. Within the options for the attribute definition, set archivedAt to the millisecond epoch appended to the displayName . Removed properties are only soft-deleted Note that removing a property only archives (soft-deletes) it. When retrieving the custom metadata structure again, you will still see the deleted attribute definition in the structure, but its attributeDefs.options.isArchived property will be set to true and its displayName will have been changed to include the time at which it was archived. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/datacontract/manage-via-sdks/",
    "content": "Manage data contracts via SDKs Â¶ Limited availability Data contracts can currently only be managed for tables , views ,\nand materialized views . Create a new contract Â¶ 2.5.1 4.0.0 To create a contract for an existing asset in Atlan: Java Python Kotlin Raw REST API Create a data contract 1 2 3 4 5 6 7 8 9 10 11 12 Table asset = Table . updater ( \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN\" , \"SALE_TXN\" ) . build (); String spec = client . contracts . generateInitialSpec ( asset ); // (1) DataContractSpec dcs = DataContractSpec . fromString ( spec ) // (2) . toBuilder () . description ( \"Changed description.\" ) . extraProperty ( \"something\" , \"extra\" ) . build (); DataContract contract = DataContract . creator ( spec , asset ) // (3) . build (); AssetMutationResponse response = contract . save ( client ); // (4) Start by initializing a data contract. You can use the .contracts.generateInitialSpec() on any Atlan client to generate the initial YAML data contract specification for a given asset. (Optional) You can translate the YAML string representation into a specification object that you can then programmatically extend, without needing to do direct string manipulations. Loses all comments Be aware that doing this conversion will remove any comments in the YAML. You need to provide the contract specification (YAML), as a string, and the asset the contract will govern to the DataContract.creator() method. Converting an object into the string form If you programmatically modified the specification as an object, you can convert it back to its YAML string form simply by calling .toString() on the object. You are always asked to provide the YAML string form here to ensure that if you want to keep any comments, you have the option to do so (since the object form removes any comments). Finally, you can call the save() method to create the new data contract in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a data contract 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from pyatlan.model.assets import Table from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataContract from pyatlan.model.contract import DataContractSpec client = AtlanClient () asset = Table . updater ( qualified_name = \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN\" , name = \"SALE_TXN\" ) spec = client . contracts . generate_initial_spec ( asset ) # (1) contract_spec = DataContractSpec . from_yaml ( spec ) # (2) contract_spec . description = \"Changed description.\" contract_spec . extra_properties = { \"something\" : \"extra\" } contract = DataContract . creator ( # (3) asset_qualified_name = asset . qualified_name , contract_spec = contract_spec , ) response = client . asset . save ( contract ) # (4) Start by initializing a data contract. You can use the .contracts.generate_initial_spec() on any Atlan client to generate the initial YAML data contract specification for a given asset. (Optional) You can translate the YAML string representation into a specification object\nthat you can then programmatically extend, without needing to do direct string manipulations. Loses all comments Be aware that doing this conversion will remove any comments in the YAML. You need to provide the contract specification (YAML), as a string,\nand the asset the contract will govern to the DataContract.creator() method. Finally, you can call the save() method to create the new data contract in Atlan. Create a data contract 1 2 3 4 5 6 7 8 9 10 11 12 val asset = Table . updater ( \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN\" , \"SALE_TXN\" ) . build () val spec = client . contracts . generateInitialSpec ( asset ) // (1) val dcs = DataContractSpec . fromString ( spec ) // (2) . toBuilder () . description ( \"Changed description.\" ) . extraProperty ( \"something\" , \"extra\" ) . build () val contract = DataContract . creator ( spec , asset ) // (3) . build () val response = contract . save ( client ) // (4) Start by initializing a data contract. You can use the .contracts.generateInitialSpec() on any Atlan client to generate the initial YAML data contract specification for a given asset. (Optional) You can translate the YAML string representation into a specification object that you can then programmatically extend, without needing to do direct string manipulations. Loses all comments Be aware that doing this conversion will remove any comments in the YAML. You need to provide the contract specification (YAML), as a string, and the asset the contract will govern to the DataContract.creator() method. Converting an object into the string form If you programmatically modified the specification as an object, you can convert it back to its YAML string form simply by calling .toString() on the object. You are always asked to provide the YAML string form here to ensure that if you want to keep any comments, you have the option to do so (since the object form removes any comments). Finally, you can call the save() method to create the new data contract in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ { \"typeName\" : \"DataContract\" , // (1) \"attributes\" : { // (2) \"dataContractJson\" : \"{\\\"type\\\": \\\"Table\\\", \\\"status\\\": \\\"DRAFT\\\", \\\"kind\\\": \\\"DataContract\\\", \\\"dataset\\\": \\\"SALE_TXN\\\", \\\"data_source\\\": \\\"snowflake\\\", \\\"description\\\": \\\"Created by Python SDK.\\\", \\\"columns\\\": [{\\\"name\\\": \\\"order_id\\\", \\\"data_type\\\": \\\"BIGNUMERIC\\\", \\\"description\\\": \\\"\\\"}]}\" , \"name\" : \"Data contract for SALE_TXN\" , // (3) \"qualifiedName\" : \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN/contract\" // (4) } } ] } The typeName must be exactly DataContract . Provide the data contract JSON. In this example, we're creating it with only the minimal required properties as specified by the API. Please check the reference section for the complete data contract specification . type of the asset in Atlan ( Table , View , or MaterializedView ). state of the contract ( DRAFT or VERIFIED ). must always be DataContract . name of the asset as it exists inside Atlan. name of the asset connection as it exists inside Atlan. (Optional) description of this dataset, for documentation purposes. (Optional) columns : name of the column as it is defined in the source system (often technical). physical data type of values in this column. description of this column, for documentation purposes. You must provide a human-readable name for your contract. The qualifiedName should follow the pattern: <assetQualifiedName>/contract (where assetQualifiedName is, in this example, the qualifiedName of a Snowflake table). Retrieve a contract Â¶ 2.2.4 4.0.0 By asset: Â¶ To retrieve the latest contract and certified\ncontract of a given asset using its qualified name: Java Python Kotlin Raw REST API Retrieve latest and certified data contract of a asset 1 2 3 Table table = Table . get ( client , \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN\" , true ); // (1) DataContract latest = table . getDataContractLatest (); // (2) DataContract certified = table . getDataContractLatestCertified (); // (3) First, retrieve the asset by its qualifiedName . Because this operation will retrieve the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve the latest data contract by using .getDataContractLatest() . Retrieve the certified data contract by using the .getDataContractLatestCertified() . Retrieve latest and certified data contract of a asset 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataContract client = AtlanClient () table = client . asset . get_by_qualified_name ( # (1) asset_type = Table , qualified_name = \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN\" ) latest_contract = table . data_contract_latest # (2) certified_contract = table . data_contract_latest_certified # (3) First, retrieve the asset by its qualified_name . Retrieve the latest data contract by using the table.data_contract_latest attribute. Retrieve the certified data contract by using the table.data_contract_latest_certified attribute. Retrieve latest and certified data contract of a asset 1 2 3 val table = Table . get ( client , \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN\" , true ) // (1) val latest = table . dataContractLatest // (2) val certified = table . dataContractLatestCertified // (3) First, retrieve the asset by its qualifiedName . Because this operation will retrieve the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve the latest data contract by using .dataContractLatest . Retrieve the certified data contract by using the .dataContractLatestCertified . GET /api/meta/entity/uniqueAttribute/type/Table?attr%3AqualifiedName=default%2Fsnowflake%2F1717514525%2FRAW%2FWIDEWORLD%2FSALE_TXN&minExtInfo=False&ignoreRelationships=False 1 // (1) All details are in the URL itself. URL-encoded filter Note that the filter is URL-encoded. decoded it would be : /api/meta/entity/uniqueAttribute/type/Table?attr:qualifiedName=default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN&minExtInfo=False&ignoreRelationships=False By qualified name: Â¶ To retrieve a contract by its version ( V1 , V2 , etc) using its qualified name: Java Python Kotlin Raw REST API Retrieve a data contract by its version 1 2 3 DataContract contract = DataContract . get ( // (1)! client , \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN/Table/contract/V1\" ); The qualifiedName of the data contract must be in the format: <assetQualifiedName>/<assetType>/contract/V<versionNumber> . For this example: assetQualifiedName : qualifiedName of a Snowflake table. assetType : type of this asset in Atlan, i.e: Table . versionNumber : specific version of the data contract to retrieve, e.g: 1 , 2 , and so on. Retrieve a data contract by its version 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataContract client = AtlanClient () contract = client . asset . get_by_qualified_name ( asset_type = DataContract , # (1) qualified_name = \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN/Table/contract/V1\" ) The qualifiedName of the data contract must be in the format: <assetQualifiedName>/<assetType>/contract/V<versionNumber> .\nFor this example: assetQualifiedName : qualifiedName of a Snowflake table. assetType : type of this asset in Atlan, i.e: Table . versionNumber : specific version of the data\ncontract to retrieve, e.g: 1 , 2 , and so on. Retrieve a data contract by its version 1 2 3 val contract = DataContract . get ( // (1)! client , \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN/Table/contract/V1\" ) The qualifiedName of the data contract must be in the format: <assetQualifiedName>/<assetType>/contract/V<versionNumber> . For this example: assetQualifiedName : qualifiedName of a Snowflake table. assetType : type of this asset in Atlan, i.e: Table . versionNumber : specific version of the data contract to retrieve, e.g: 1 , 2 , and so on. GET /api/meta/entity/uniqueAttribute/type/DataContract?attr%3AqualifiedName=dedefault%2Fsnowflake%2F1717514525%2FRAW%2FWIDEWORLD%2FSALE_TXN%2FTable%2Fcontract%2FV1&minExtInfo=False&ignoreRelationships=False 1 // (1) All details are in the URL itself. URL-encoded filter Note that the filter is URL-encoded. decoded it would be : attr:qualifiedName=default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN/Table/contract/V1&minExtInfo=False&ignoreRelationships=False where the qualifiedName of the data contract must be in the format: <assetQualifiedName>/<assetType>/contract/V<versionNumber> .\nFor this example: assetQualifiedName : qualifiedName of a Snowflake table. assetType : type of this asset in Atlan, i.e: Table . versionNumber : specific version of the data contract to retrieve, e.g: 1 , 2 , and so on. Update a contract Â¶ 2.5.1 4.0.0 In the following example, we are updating the contact certificateStatus field to VERIFIED (shown as PUBLISHED in the UI): Java Python Kotlin Raw REST API Update a data contract 1 2 3 4 5 6 7 8 9 10 11 DataContractSpec updatedContractDetails = DataContractSpec . fromString ( spec ) // (1) . toBuilder () . status ( DataContractStatus . VERIFIED ) // (2) . build (); DataContract contract = DataContract . updater ( // (3) \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN/contract\" , \"Data contract for SALE_TXN\" ) . dataContractSpec ( updatedContractDetails . toString ()) // (4) . build (); AssetMutationResponse response = contract . save ( client ); // (5) Begin by constructing the updated data contract specification. This example assumes you already have the string YAML form in a variable named spec , which you have retrieved from the data contract using one of the retrieval methods above. After converting the specification into a builder (using .toBuilder() ) you can chain any updates you want against it, such as changing its status. Use the updater() method to update a data contract. qualifiedName of the data contract, ie: <assetQualifiedName>/contract (where assetQualifiedName is, in this example, the qualifiedName of a Snowflake table). name of the data contract. ( NOTE: SDKs and CLI always generate it in the format: \"Data contract for dataset ( asset.name )\" ). You can then add any other updates or attributes. In this example, we're updating the contract spec itself (must be string ). To update the data contract in Atlan, call the save() method with the object you've built. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update a data contract 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataContract from pyatlan.model.contract import DataContractSpec from pyatlan.model.enums import DataContractStatus client = AtlanClient () spec = current_contract . data_contract_spec updated_contract_spec = DataContractSpec . from_yaml ( spec ) # (1) updated_contract_spec . status = DataContractStatus . VERIFIED # (2) contract = DataContract . updater ( # (3) qualified_name = \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN/contract\" , name = \"Data contract for SALE_TXN\" , ) contract . data_contract_spec = updated_contract_spec . to_yaml () # (4) response = client . asset . save ( contract ) # (5) Begin by constructing the updated data contract specification.\nThis example assumes you already have the string YAML form in a\nvariable named spec , which you have retrieved from the data\ncontract using one of the retrieval methods above. After converting the specification into DataContractSpec instance, you can then chain any updates you\nwant against it, such as changing its status . Use the updater() method to update a data contract. qualifiedName of the data contract,ie: <assetQualifiedName>/contract (where assetQualifiedName is, in this example, the qualifiedName of a Snowflake table). name of the data contract. ( NOTE: SDKs and CLI always generate it in the format: \"Data contract for dataset ( asset.name )\" ). You can then add any other updates or attributes.\nIn this example, we're updating the contract spec itself\n(make sure to use .to_yaml() to convert spec instance to YAML string) To update the data contract in Atlan, call the save() method with the object you've built. Update a data contract 1 2 3 4 5 6 7 8 9 10 11 val updatedContractDetails = DataContractSpec . fromString ( spec ) // (1) . toBuilder () . status ( DataContractStatus . VERIFIED ) // (2) . build () val contract = DataContract . updater ( // (3) \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN/contract\" , \"Data contract for SALE_TXN\" ) . dataContractSpec ( updatedContractDetails . toString ()) // (4) . build () val response = contract . save ( client ) // (5) Begin by constructing the updated data contract specification. This example assumes you already have the string YAML form in a variable named spec , which you have retrieved from the data contract using one of the retrieval methods above. Will not retain any comments Keep in mind that when programmatically building the specification as an object, no comments will be retained. If you want to have comments in your YAML specification, you must directly manipulate the YAML string yourself. After converting the specification into a builder (using .toBuilder() ) you can chain any updates you want against it, such as changing its status. Use the updater() method to update a data contract. qualifiedName of the data contract, ie: <assetQualifiedName>/contract (where assetQualifiedName is, in this example, the qualifiedName of a Snowflake table). name of the data contract. ( NOTE: SDKs and CLI always generate it in the format: \"Data contract for dataset ( asset.name )\" ). You can then add any other updates or attributes. In this example, we're updating the contract spec itself (must be string ). To update the data contract in Atlan, call the save() method with the object you've built. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ { \"typeName\" : \"DataContract\" , // (1) \"attributes\" : { // (2) \"dataContractJson\" : \"{\\\"type\\\": \\\"Table\\\", \\\"status\\\": \\\"VERIFIED\\\", \\\"kind\\\": \\\"DataContract\\\", \\\"dataset\\\": \\\"SALE_TXN\\\", \\\"data_source\\\": \\\"snowflake\\\", \\\"description\\\": \\\"Created by Python SDK.\\\", \\\"columns\\\": [{\\\"name\\\": \\\"order_id\\\", \\\"data_type\\\": \\\"BIGNUMERIC\\\", \\\"description\\\": \\\"\\\"}]}\" , \"name\" : \"Data contract for SALE_TXN\" , // (3) \"qualifiedName\" : \"default/snowflake/1717514525/RAW/WIDEWORLD/SALE_TXN/contract\" // (4) } } ] } The typeName must be exactly DataContract . Provide the data contract JSON. In this example, we're updating it with only the minimal required properties as specified by the API. Please check the reference section for the complete data contract specification . type of the asset in Atlan ( Table , View , or MaterializedView ). state of the contract ( DRAFT or VERIFIED ). must always be DataContract . name of the asset as it exists inside Atlan. name of the asset connection as it exists inside Atlan. (Optional) description of this dataset, for documentation purposes. (Optional) columns : name of the column as it is defined in the source system (often technical). physical data type of values in this column. description of this column, for documentation purposes. Human-readable name for your contract. The qualifiedName of your contract, ie: <assetQualifiedName>/contract (where assetQualifiedName is, in this example, the qualifiedName of a Snowflake table). Delete a contract Â¶ Soft-delete (archive) Â¶ To soft-delete, or archive, a contract: Java Python Kotlin Raw REST API Coming soon Coming soon Coming soon Coming soon Hard-delete (purge) Â¶ To permanently delete (purge) a contract: Java Python Kotlin Raw REST API Coming soon Coming soon Coming soon Coming soon 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/files/",
    "content": "/api/service/files/presignedUrl (POST) Manage files in tenant object store Â¶ You can use the SDK's FileClient to manage your files\nwithin Atlan's tenant object store by leveraging presigned URLs. Upload a file to the object store Â¶ 0.0.4 2.1.7 0.1.3 To upload a file to the tenant object store: Atlan CLI Java Python Kotlin Go Raw REST API Upload a file to the tenant object store 1 atlan upload -f user/some-folder/my-file.txt -r atlan/object/store/file.txt # (1) To upload the file to the object store, you must specify the following flags: -f or --file : path to the file to be uploaded to the object store. --r or --remote : actual object name where you want to upload the file (e.g: prefix/object_name ). CLI must be configured Make sure you have the CLI configured before running the above command. Coming soon Upload a file to the tenant object store 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pyatlan.client.atlan import AtlanClient from pyatlan.model.file import PresignedURLRequest client = AtlanClient () presigned_url = client . files . generate_presigned_url ( request = PresignedURLRequest ( # (1) key = \"my-folder/my-file.txt\" , expiry = \"30s\" , method = PresignedURLRequest . Method . PUT , ) ) client . files . upload_file ( # (2) presigned_url = presigned_url , file_path = \"user/some-folder/upload-file.txt\" ) Begin by generating a presigned URL for the object store. You need to specify: actual object name where you want to upload the file (e.g: prefix/object_name ). expiration time interval for the presigned URL. presigned URL method ( PUT for upload). Finally, upload the file to the object store by providing: any valid presigned URL. path to the file to be uploaded to the object store. Coming soon Upload a file to the tenant object store 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package main import ( \"fmt\" \"github.com/atlanhq/atlan-go/atlan/assets\" \"github.com/atlanhq/atlan-go/atlan/model\" ) func main () { ctx := assets . NewContext () client := assets . NewFileClient ( ctx ) presignedUrl , err := client . GeneratePresignedURL ( & model . PresignedURLRequest { // (1) Key : \"my-folder/my-file.txt\" , Expiry : \"30s\" , Method : model . PUT , }, ) if err != nil { fmt . Println ( \"Error while generating url:\" , err ) } uploadFilePath := \"user/some-folder/upload-file.txt\" err = client . UploadFile ( presignedUrl , uploadFilePath ) // (2) if err != nil { fmt . Println ( \"Error while uploading file:\" , err ) } } Begin by generating a presigned URL for the object store. You need to specify: actual object name where you want to upload the file (e.g: prefix/object_name ). expiration time interval for the presigned URL. presigned URL method ( PUT for upload). Finally, upload the file to the object store by providing: any valid presigned URL. path to the file to be uploaded to the object store. POST /api/service/files/presignedUrl 1 2 3 4 5 { \"method\" : \"PUT\" , // (1) \"key\" : \"my-folder/my-file.txt\" , // (2) \"expiry\" : \"30s\" // (3) } The presigned URL method ( PUT for upload). The actual object name where you want to upload the file (e.g: prefix/object_name ). An expiration time interval for the presigned URL. Download a file from the object store Â¶ 0.0.4 2.1.7 0.1.3 To download a file from the tenant object store: Atlan CLI Java Python Kotlin Go Raw REST API Download a file to the tenant object store 1 atlan download -r atlan/object/store/file.txt -o user/some-folder/my_file.txt # (1) To download the file from the object store, you must specify the following flags: -r or --remote : actual object name you want to download (e.g: prefix/object_name ). --o or --output : path to the location where you want to save the downloaded file. CLI must be configured Make sure you have the CLI configured before running the above command. Coming soon Download a file from the tenant object store 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pyatlan.client.atlan import AtlanClient from pyatlan.model.file import PresignedURLRequest client = AtlanClient () presigned_url = client . files . generate_presigned_url ( request = PresignedURLRequest ( # (1) key = \"my-folder/my-file.txt\" , expiry = \"30s\" , method = PresignedURLRequest . Method . GET , ) ) client . files . download_file ( # (2) presigned_url = presigned_url , file_path = \"user/some-folder/download-file.txt\" ) Begin by generating a presigned URL for the object store. You need to specify: actual object name you want to download (e.g: prefix/object_name ). expiration time interval for the presigned URL. presigned URL method ( GET for download). Finally, download the file from the object store by providing: any valid presigned URL. path to the location where you want to save the downloaded file. Coming soon Download a file from the tenant object store 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 package main import ( \"fmt\" \"github.com/atlanhq/atlan-go/atlan/assets\" \"github.com/atlanhq/atlan-go/atlan/model\" ) func main () { ctx := assets . NewContext () client := assets . NewFileClient ( ctx ) presignedUrl , err := client . GeneratePresignedURL ( & model . PresignedURLRequest { // (1) Key : \"my-folder/my-file.txt\" , Expiry : \"30s\" , Method : model . GET , }, ) if err != nil { fmt . Println ( \"Error while generating url:\" , err ) } downloadFilePath := \"user/some-folder/download-file.txt\" err = client . DownloadFile ( presignedUrl , downloadFilePath ) // (2) if err != nil { fmt . Println ( \"Error while downloading file:\" , err ) } } Begin by generating a presigned URL for the object store. You need to specify: actual object name you want to download (e.g: prefix/object_name ). expiration time interval for the presigned URL. presigned URL method ( GET for download). Finally, download the file from the object store by providing: any valid presigned URL. path to the location where you want to save the downloaded file. POST /api/service/files/presignedUrl 1 2 3 4 5 { \"method\" : \"GET\" , // (1) \"key\" : \"my-folder/my-file.txt\" , // (2) \"expiry\" : \"30s\" // (3) } The presigned URL method ( GET for download). The actual object name you want to download (e.g: prefix/object_name ). An expiration time interval for the presigned URL. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/datacontract/manage/",
    "content": "Manage data contracts Â¶ Limited availability Data contracts can currently only be managed for tables , views ,\nand materialized views . Initialize a contract Â¶ 0.1.0 To generate a contract for an existing asset in Atlan: Atlan CLI atlan init contract \\ # (1) --asset \"Table@CUST_TXN\" \\ # (2) --data-source \"snowflake\" # (3) Use atlan init contract to initialize a contract. If you provide no other arguments, the CLI will generate a skeletal contract you can fill in yourself. To pre-populate the contract with information about a dataset, you must provide the type and (technical) name of the asset to generate from, in the format TypeName@name . To pre-populate the contract, you must also provide the name of the data source in which to find the asset. This will generate a contract in your current working directory, using the details from the asset in Atlan as a starting point. (This requires you to first configure the Atlan CLI with details about your tenant.) Can I manage contracts without Atlan connectivity? You can also initialize a contract without any connection to Atlan, by leaving out the --asset and --data-source arguments. This will provide you a skeletal contract you can then fill in yourself. Validate contract Â¶ 0.1.0 You can validate the contract file is syntactically correct and refers to an asset known to Atlan: Atlan CLI atlan validate contract \\ # (1) -f \"contract.yaml\" # (2) Use atlan validate contract to validate a contract. You must specify the filename that defines the contract. Push contract Â¶ 0.1.0 To apply the contract in Atlan, you then need to push the contract: Atlan CLI atlan push contract \\ # (1) -f \"contract.yaml\" # (2) Use atlan push contract to push a contract. You must specify the filename that defines the contract. Sync metadata Â¶ 0.1.5 To sync metadata from a contract file to the asset governed by the contract in Atlan: Atlan CLI atlan sync contract \\ # (1) -f \"contract.yaml\" # (2) Use atlan sync contract to sync metadata contained in a contract to the asset governed by that contract. You must specify the filename that defines the contract (containing the metadata to be synced). This command will sync the following from the contract file to the governed asset in Atlan: description : |- # (1) Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus eu orci non arcu placerat tincidunt eu et ligula. Nullam non nisi in risus finibus tristique non quis erat. Phasellus hendrerit finibus velit nec dapibus. Sed non viverra ligula, at dignissim diam. Mauris finibus elementum mi id luctus. Maecenas sit amet lectus placerat, lobortis turpis dictum, semper magna. Nullam sollicitudin ipsum eget felis vulputate, sit amet ultrices nisi posuere. Ut facilisis eu enim id maximus. owners : users : # (2) - jdoe - jsmith groups : # (3) - data_producers_group certification : status : VERIFIED # (4) message : \"\" announcement : type : information # (5) title : \"\" description : \"\" terms : # (6) - \"\" tags : # (7) - name : PII propagate : false restrict_propagation_through_lineage : false restrict_propagation_through_hierarchy : false custom_metadata : # (8) Data Quality : Completeness Score : 100 Failed Checks : - 884438be-82cc-4e04-bfe1-fba59276df38 - afa0e560-a916-4862-a2f2-c491f19f39f5 Updates the user-managed description of the governed asset. Appends individual user owners to the list of existing owning users of the governed asset. Each user should be listed by their username in Atlan. Appends group owners to the list of existing owning groups of the governed asset. Each group should be listed by its internal alias name in Atlan. Updates the certificate of the governed asset. Must be one of: VERIFIED DRAFT DEPRECATED Updates the announcement on the governed asset. Must be one of: information warning issue Appends assigned terms to the list of terms assigned to the governed asset. Each term should be listed by its name in Atlan. If multiple terms exist with the same name If multiple terms are found with the same name in Atlan, these will be returned as a conflict (rather than any being added to the asset). Appends tags to the list of tags assigned to the governed asset. Each tag should be listed by its name in Atlan. Merges the custom metadata provided with any existing custom metadata on the governed asset. Each custom metadata set and its attributes should be keyed by its name in Atlan. Managing via CI/CD Â¶ You can combine the actions above to manage data contracts via automated CI/CD pipelines. For example, a process to automate publication of data contracts could be as follows: Configure CLI for CI/CD Â¶ First, configure the CLI within your CI/CD environment. Separate sensitive and non-sensitive configuration As a general rule, we recommend removing sensitive information (like the API token) from your configuration file. Instead manage this through an environment variable, which your CI/CD environment can inject into the job that runs the CLI. (For example, in GitHub you can use GitHub Secrets to manage the API token and have it automatically injected as an environment variable in GitHub Actions.) The non-sensitive configuration details can remain in the configuration file, and the configuration file itself can then be version-controlled in your source code repository, too. The examples below assume you have stored: your tenant's URL in a repository secret named ATLAN_BASE_URL the API token in a repository secret named ATLAN_API_KEY Publish contracts from CI/CD Â¶ Once configured, you can use the CLI to publish any new contracts or changes to existing contracts: Commit contract file(s) to your revision control repository. Apply any validations or approval processes you like in your revision control repository. (For example, GitHub Actions that are triggered by pull request events.) When the committed changes are merged to a particular branch (for example, main ), trigger an action to publish them to Atlan using the command in the push contract step. GitHub .atlan/config.yaml 1 2 3 4 5 6 7 8 9 10 11 12 atlan_api_key : \"\" # (1) log : enabled : true # (2) level : info data_source snowflake : # (3) type : snowflake connection : name : snowflake-prod qualified_name : \"default/snowflake/1234567890\" database : db schema : analytics Your repository should configure the CLI . The simplest way to do this is to include the configuration file in your repository (it must be at exactly .atlan/config.yaml in your repository to be picked up by the GitHub Action automatically). Leave sensitive information out Leave the sensitive information (like API token and URL of your tenant) out of the configuration file. These can instead be stored as GitHub Secrets and used via environment variables in the GitHub Action. You may want to enable logging, so you'll have debugging information to review if something goes wrong. You will need to define the data sources used by your contracts. .github/workflows/push-contracts.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 name : Push contract to Atlan on : push : branches : - main paths : - 'contracts/**' # (1) env : ATLAN_BINARY_URL : \"https://github.com/atlanhq/atlan-cli-releases/releases/latest/download/atlan_Linux_amd64.tar.gz\" BINARY_FILE_NAME : \"atlan_Linux_amd64.tar.gz\" jobs : push-contract : runs-on : ubuntu-latest steps : - name : Checkout repository uses : actions/checkout@v4 - name : Download atlan CLI run : | curl -LO $ATLAN_BINARY_URL - name : Uncompress CLI archive run : tar -xzf $BINARY_FILE_NAME - name : Give permissions to CLI run : | sudo mv atlan /usr/local/bin/atlan chmod +x /usr/local/bin/atlan - name : Configure the CLI # (2) run : | echo \"atlan_base_url: \\\"$ATLAN_BASE_URL\\\"\" >> .atlan/config.yaml env : ATLAN_BASE_URL : ${{ secrets.ATLAN_BASE_URL }} - name : Run atlan push command # (3) run : | atlan push dc -f \"$GITHUB_WORKSPACE/contracts\" env : ATLAN_API_KEY : ${{ secrets.ATLAN_API_KEY }} # (4) Specify the path where your contract files exist in the GitHub repository, so the action is only triggered when contract files themselves change. Configure the CLI with the URL of your tenant, here pulled from a repository secret with the name ATLAN_BASE_URL . Again, this directory may need to change depending on where the contract files are within your GitHub repository. Include your API token as the ATLAN_API_KEY environment variable, here pulled from a repository secret with the same name. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/datamesh/",
    "content": "Data mesh overview Â¶ erDiagram\n    DataDomain ||--o{ DataDomain : SubDomains\n    DataDomain ||--o{ DataProduct : contains\n    DataProduct }o--o{ Asset : contains\n    DataProduct }o--o{ Asset : OutputPort Data domains are a way to group data products together, for example by business area. Data domains themselves can be organized in a hierarchical structure of subdomains. Data products are a way to group assets together. They both define the assets included in the product, as well as the assets that can be used to consume the product (as output ports). 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/datamesh/dataproducts/",
    "content": "/api/meta/entity/bulk (POST) Manage data products Â¶ Create a new data product Â¶ 4.0.0 2.0.4 To create a new data product: Java Python Kotlin Raw REST API Create a data product 1 2 3 4 5 6 7 8 9 FluentSearch assets = Table . select ( client ) // (1) . where ( Table . CERTIFICATE_STATUS . eq ( CertificateStatus . VERIFIED )) . where ( Table . ATLAN_TAGS . eq ( \"Marketing\" )) . build (); DataProduct dp = DataProduct . creator ( \"Marketing Influence\" , // (2) DataDomain . refByQualifiedName ( \"default/domain/marketing\" ), // (3) assets ) // (4) . build (); // (5) AssetMutationResponse response = dp . save ( client ); // (6) When defining a data product, you must define the assets within it. These are defined through a search, so that the assets included can be automatically managed. In this example, we are selecting all verified tables that have a tag of Marketing . You must provide a human-readable name for your data product. You must also provide the domain in which the data product should exist. And finally the search that was defined earlier, to define which assets to include in the data product. You then need to build the object. And then you can save() the object you've built to create the new data product in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a data product 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataProduct , Table from pyatlan.model.fluent_search import CompoundQuery , FluentSearch from pyatlan.model.enums import CertificateStatus client = AtlanClient () assets = ( FluentSearch () . where ( CompoundQuery . active_assets ()) . where ( CompoundQuery . asset_type ( Table )) . where ( Table . CERTIFICATE_STATUS . eq ( CertificateStatus . VERIFIED . value )) . where ( Table . ATLAN_TAGS . eq ( \"Marketing\" )) ) . to_request () # (1) product = DataProduct . creator ( name = \"Marketing Influence\" , # (2) asset_selection = assets , # (3) domain_qualified_name = \"default/domain/marketing\" # (4) ) response = client . asset . save ( product ) # (5) When defining a data product, you must define the assets within it. These are defined through a search, so that the assets included can be automatically managed. In this example, we are selecting all verified tables that have a tag of Marketing . You must provide a human-readable name for your data product. You must provide the index search request that was defined earlier, to define which assets to include in the data product. You must also provide the domain in which the data product should exist. And then you can save() the object you've built to create the new data product in Atlan. Create a data product 1 2 3 4 5 6 7 8 9 val assets = Table . select ( client ) // (1) . where ( Table . CERTIFICATE_STATUS . eq ( CertificateStatus . VERIFIED )) . where ( Table . ATLAN_TAGS . eq ( \"Marketing\" )) . build () val dp = DataProduct . creator ( \"Marketing Influence\" , // (2) DataDomain . refByQualifiedName ( \"default/domain/marketing\" ), // (3) assets ) // (4) . build () // (5) val response = dp . save ( client ) // (6) When defining a data product, you must define the assets within it. These are defined through a search, so that the assets included can be automatically managed. In this example, we are selecting all verified tables that have a tag of Marketing . You must provide a human-readable name for your data product. You must also provide the domain in which the data product should exist. And finally the search that was defined earlier, to define which assets to include in the data product. You then need to build the object. And then you can save() the object you've built to create the new data product in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 { \"entities\" : [ { \"typeName\" : \"DataProduct\" , // (1) \"attributes\" : { \"name\" : \"Marketing Influence\" , // (2) \"qualifiedName\" : \"default/domain/8S8vbC9hT6iPLMY2ydXqf/super/product/marketingInfluence\" , // (3) \"parentDomainQualifiedName\" : \"default/domain/8S8vbC9hT6iPLMY2ydXqf/super\" , // (4) \"superDomainQualifiedName\" : \"default/domain/8S8vbC9hT6iPLMY2ydXqf/super\" , // (5) \"dataProductAssetsDSL\" : \"{\\\"query\\\": {\\\"attributes\\\": [\\\"__traitNames\\\", \\\"connectorName\\\", \\\"__customAttributes\\\", \\\"certificateStatus\\\", \\\"tenantId\\\", \\\"anchor\\\", \\\"parentQualifiedName\\\", \\\"Query.parentQualifiedName\\\", \\\"AtlasGlossaryTerm.anchor\\\", \\\"databaseName\\\", \\\"schemaName\\\", \\\"parent\\\", \\\"connectionQualifiedName\\\", \\\"collectionQualifiedName\\\", \\\"announcementMessage\\\", \\\"announcementTitle\\\", \\\"announcementType\\\", \\\"announcementUpdatedAt\\\", \\\"announcementUpdatedBy\\\", \\\"allowQuery\\\", \\\"allowQueryPreview\\\", \\\"adminGroups\\\", \\\"adminRoles\\\", \\\"adminUsers\\\", \\\"category\\\", \\\"credentialStrategy\\\", \\\"connectionSSOCredentialGuid\\\", \\\"certificateStatus\\\", \\\"certificateUpdatedAt\\\", \\\"certificateUpdatedBy\\\", \\\"classifications\\\", \\\"connectionId\\\", \\\"connectionQualifiedName\\\", \\\"connectorName\\\", \\\"dataType\\\", \\\"defaultDatabaseQualifiedName\\\", \\\"defaultSchemaQualifiedName\\\", \\\"description\\\", \\\"displayName\\\", \\\"links\\\", \\\"link\\\", \\\"meanings\\\", \\\"name\\\", \\\"ownerGroups\\\", \\\"ownerUsers\\\", \\\"qualifiedName\\\", \\\"typeName\\\", \\\"userDescription\\\", \\\"displayDescription\\\", \\\"subDataType\\\", \\\"rowLimit\\\", \\\"queryTimeout\\\", \\\"previewCredentialStrategy\\\", \\\"policyStrategy\\\", \\\"policyStrategyForSamplePreview\\\", \\\"useObjectStorage\\\", \\\"objectStorageUploadThreshold\\\", \\\"outputPortDataProducts\\\"], \\\"dsl\\\": {\\\"from\\\": 0, \\\"query\\\": {\\\"bool\\\": {\\\"filter\\\": {\\\"bool\\\": {\\\"filter\\\": [{\\\"term\\\": {\\\"__state\\\": {\\\"value\\\": \\\"ACTIVE\\\", \\\"case_insensitive\\\": false}}}, {\\\"term\\\": {\\\"__typeName.keyword\\\": {\\\"value\\\": \\\"Table\\\", \\\"case_insensitive\\\": false}}}, {\\\"term\\\": {\\\"certificateStatus\\\": {\\\"value\\\": \\\"VERIFIED\\\", \\\"case_insensitive\\\": false}}}, {\\\"term\\\": {\\\"__traitNames\\\": {\\\"value\\\": \\\"Marketing\\\", \\\"case_insensitive\\\": false}}}]}}}}}, \\\"suppressLogs\\\": true}, \\\"filterScrubbed\\\": true}\" , // (6) \"dataProductAssetsPlaybookFilter\" : \"{\\\"condition\\\":\\\"AND\\\",\\\"isGroupLocked\\\":false,\\\"rules\\\":[]}\" // (7) }, \"relationshipAttributes\" : { \"dataDomain\" : { \"typeName\" : \"DataDomain\" , \"uniqueAttributes\" : { // (8) \"qualifiedName\" : \"default/domain/8S8vbC9hT6iPLMY2ydXqf/super\" } } } } ] } The typeName must be exactly DataProduct . Provide a human-readable name for your data product. Ensure the qualifiedName follows the pattern: <parentDomainQualifiedName>/product/<lowerCamelCaseName> . Provide a parentDomainQualifiedName for the data domain under which you want to create this product. Provide a superDomainQualifiedName for the data domain under which you want to create this product. If creating a product under sub-domains, this should be the qualified name of the root-level domain. Provide the DSL that defines the assets to include in the data product as an embedded JSON string. Use SDK to create data products The above data products assets DSL requires a filter as a nested object construct within an outer bool, rather than a list or array. It's recommended to create data products via SDK as it handles this complexity automatically. Specify the default playbook filter to define which assets are shown in the data product UI. Specify the qualifiedName of the data domain under which you want to create this product. Retrieve a data product Â¶ 4.0.0 2.2.1 To retrieve a data product by its human-readable name: Java Python Kotlin Raw REST API Retrieve a data product by its human-readable name 1 2 3 DataProduct product = DataProduct . findByName ( // (1) client , \"marketingInfluence\" , List . of ( \"certificateStatus\" ) ). get ( 0 ); Use DataProduct.findByName() method to retrieve a data product by its human-readable name: client through which to access a tenant. name of the data product. (optional) a list of attributes to retrieve for the data product, for example certificateStatus . Retrieve a data product by its human-readable name 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataProduct client = AtlanClient () product = client . asset . find_product_by_name ( # (1) name = \"marketingInfluence\" , attributes = [ \"certificateStatus\" ] ) assert product assert product . certificate_status Use client.asset.find_product_by_name() method to retrieve a data product by its human-readable name: name of the data product. (optional) a list of attributes to retrieve\nfor the data product, for example certificateStatus . Retrieve a data product by its human-readable name 1 2 3 val product = DataProduct . findByName ( // (1) client , \"marketingInfluence\" , listOf ( \"certificateStatus\" ) ). get ( 0 ) Use DataProduct.findByName() method to retrieve a data product by its human-readable name: client through which to access a tenant. name of the data product. (optional) a list of attributes to retrieve for the data product, for example certificateStatus . POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \"dsl\" : { \"from\" : 0 , \"size\" : 100 , \"aggregations\" : {}, \"track_total_hits\" : true , \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"name.keyword\" : { \"value\" : \"marketingInfluence\" // (1) } } }, { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"DataProduct\" } } } ] } }, \"sort\" : [ { \"__guid\" : { \"order\" : \"asc\" } } ] }, \"attributes\" : [ \"certificateStatus\" // (2) ] } Human-readable name of the data product. (optional) a list of attributes to retrieve\nfor the data product, for example certificateStatus . Retrieve all assets linked to a data product Â¶ 4.2.2 7.0.0 To retrieve list of all assets associated with a data product: Java Python Kotlin Raw REST API Retrieve list of all assets linked to a data product 1 2 3 4 5 6 DataProduct dp = DataProduct . get ( client , \"49abc625-9a03-4733-bdfb-ec0cb5315cac\" , true ); // (1) IndexSearchResponse response = dp . getAssets ( client ); // (2) for ( Asset result : response ) { // Do something with the assets retrieved... } Use DataProduct.get() method to retrieve the data product by its GUID . Use DataProduct.getAssets() method to retrieve all assets linked to the data product, providing the client used to access the tenant. Retrieve list of all assets linked to a data product 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataProduct client = AtlanClient () product = client . asset . get_by_guid ( # (1) guid = \"49abc625-9a03-4733-bdfb-ec0cb5315cac\" , asset_type = DataProduct ) response = product . get_assets ( client ) # (2) for assets in response : // Do something with the assets retrieved ... assert product assert response Use client.asset.get_by_guid() method to retrieve the data product by its GUID . Use DataProduct.get_assets() method to retrieve all assets linked to the data product. Retrieve list of all assets linked to a data product 1 2 3 4 5 6 val product = DataProduct . get ( client , \"49abc625-9a03-4733-bdfb-ec0cb5315cac\" , true ); // (1) val response = dp . getAssets ( client ); // (2) for ( result in response ) { // Do something with the assets retrieved... } Use DataProduct.get() method to retrieve the data product by its GUID . Use DataProduct.getAssets() method to retrieve all assets linked to the data product, providing the client used to access the tenant. POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 { \"dsl\" : { \"from\" : 0 , \"size\" : 100 , \"aggregations\" : {}, \"track_total_hits\" : true , \"query\" : { \"bool\" : { \"filter\" : { \"bool\" : { \"filter\" : [ // (1) { \"term\" : { \"__state\" : { \"value\" : \"ACTIVE\" , \"case_insensitive\" : false } } }, { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"Table\" , \"case_insensitive\" : false } } } ] } } } }, \"sort\" : [] }, \"attributes\" : [ // (2) \"certificateStatus\" , \"connectorName\" , \"__customAttributes\" , \"certificateStatus\" , ] } List the asset type and active status for all assets included in the data product. (optional) a list of attributes to retrieve\nfor the data product's list of all assets, for example certificateStatus . Update a data product Â¶ 4.0.0 2.4.5 To update a data product: Java Python Kotlin Raw REST API Update a data product 1 2 3 4 5 6 7 8 9 10 11 FluentSearch assets = Table . select ( client ) . where ( Table . CERTIFICATE_STATUS . eq ( CertificateStatus . VERIFIED )) . where ( Table . ATLAN_TAGS . eq ( \"Digital Marketing\" )) . build (); // (1) DataProduct dp = DataProduct . updater ( \"default/product/marketingInfluence\" , // (2) \"Marketing Influence\" ) . userDescription ( \"Now with a description!\" ) // (3) . assetSelection ( Atlan . getDefaultClient (), assets ) . build (); // (4) AssetMutationResponse response = dp . save ( client ); // (5) (Optional) You can also update the assets within an existing product.\nThese assets are defined through a search, allowing for automatic management.\nIn this example, we select all verified tables that are tagged with Digital Marketing . Use the updater() method to update a data product,\nproviding the qualifiedName and name of the data product. You can chain additional enrichments onto the updater, such as: userDescription : updating the product's description. assetSelection : modifying the assets within the product. You then need to build the object. You can then save() the object you've built to update the data product in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update a data product 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataProduct client = AtlanClient () assets = ( FluentSearch () . where ( CompoundQuery . active_assets ()) . where ( CompoundQuery . asset_type ( Table )) . where ( Table . CERTIFICATE_STATUS . eq ( CertificateStatus . VERIFIED . value )) . where ( Table . ATLAN_TAGS . eq ( \"Digital Marketing\" )) ) . to_request () # (1) product = DataProduct . updater ( # (2) qualified_name = \"default/product/marketingInfluence\" , # (3) name = \"Marketing Influence\" , # (4) asset_selection = assets , # (5) ) product . user_description = \"Now with a description!\" # (6) response = client . asset . save ( product ) # (7) (Optional) You can also update the assets within an existing product.\nThese assets are defined through a search, allowing for automatic management.\nIn this example, we select all verified tables that are tagged with Digital Marketing . Use the updater() method to update a data product. You must provide the qualifiedName of the data product. You must provide the name of the data product. Optionally, you can provide an index search request to define the assets to include in the data product. You can then add on any other updates, such as changing the user description of the data product. To update the data product in Atlan, call the save() method with the object you've built. Update a data product 1 2 3 4 5 6 7 8 9 10 11 val assets = Table . select ( client ) . where ( Table . CERTIFICATE_STATUS . eq ( CertificateStatus . VERIFIED )) . where ( Table . ATLAN_TAGS . eq ( \"Digital Marketing\" )) . build (); // (1) val dp = DataProduct . updater ( \"default/product/marketingInfluence\" , // (2) \"Marketing Influence\" ) . userDescription ( \"Now with a description!\" ) // (3) . assetSelection ( Atlan . getDefaultClient (), assets ) . build () // (4) val response = dp . save ( client ) // (5) (Optional) You can also update the assets within an existing product.\nThese assets are defined through a search, allowing for automatic management.\nIn this example, we select all verified tables that are tagged with Digital Marketing . Use the updater() method to update a data product,\nproviding the qualifiedName and name of the data product. You can chain additional enrichments onto the updater, such as: userDescription : updating the product's description. assetSelection : modifying the assets within the product. You then need to build the object. You can then save() the object you've built to update the data product in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"entities\" : [ { \"typeName\" : \"DataProduct\" , // (1) \"attributes\" : { \"name\" : \"Marketing Influence\" , // (2) \"qualifiedName\" : \"default/product/marketingInfluence\" , // (3) \"userDescription\" : \"Now with a description!\" , // (4) \"dataProductAssetsDSL\" : \"{\\\"query\\\": {\\\"attributes\\\": [\\\"__traitNames\\\", \\\"connectorName\\\", \\\"__customAttributes\\\", \\\"certificateStatus\\\", \\\"tenantId\\\", \\\"anchor\\\", \\\"parentQualifiedName\\\", \\\"Query.parentQualifiedName\\\", \\\"AtlasGlossaryTerm.anchor\\\", \\\"databaseName\\\", \\\"schemaName\\\", \\\"parent\\\", \\\"connectionQualifiedName\\\", \\\"collectionQualifiedName\\\", \\\"announcementMessage\\\", \\\"announcementTitle\\\", \\\"announcementType\\\", \\\"announcementUpdatedAt\\\", \\\"announcementUpdatedBy\\\", \\\"allowQuery\\\", \\\"allowQueryPreview\\\", \\\"adminGroups\\\", \\\"adminRoles\\\", \\\"adminUsers\\\", \\\"category\\\", \\\"credentialStrategy\\\", \\\"connectionSSOCredentialGuid\\\", \\\"certificateStatus\\\", \\\"certificateUpdatedAt\\\", \\\"certificateUpdatedBy\\\", \\\"classifications\\\", \\\"connectionId\\\", \\\"connectionQualifiedName\\\", \\\"connectorName\\\", \\\"dataType\\\", \\\"defaultDatabaseQualifiedName\\\", \\\"defaultSchemaQualifiedName\\\", \\\"description\\\", \\\"displayName\\\", \\\"links\\\", \\\"link\\\", \\\"meanings\\\", \\\"name\\\", \\\"ownerGroups\\\", \\\"ownerUsers\\\", \\\"qualifiedName\\\", \\\"typeName\\\", \\\"userDescription\\\", \\\"displayDescription\\\", \\\"subDataType\\\", \\\"rowLimit\\\", \\\"queryTimeout\\\", \\\"previewCredentialStrategy\\\", \\\"policyStrategy\\\", \\\"policyStrategyForSamplePreview\\\", \\\"useObjectStorage\\\", \\\"objectStorageUploadThreshold\\\", \\\"outputPortDataProducts\\\"], \\\"dsl\\\": {\\\"from\\\": 0, \\\"query\\\": {\\\"bool\\\": {\\\"filter\\\": {\\\"bool\\\": {\\\"filter\\\": [{\\\"term\\\": {\\\"__state\\\": {\\\"value\\\": \\\"ACTIVE\\\", \\\"case_insensitive\\\": false}}}, {\\\"term\\\": {\\\"__typeName.keyword\\\": {\\\"value\\\": \\\"Table\\\", \\\"case_insensitive\\\": false}}}, {\\\"term\\\": {\\\"certificateStatus\\\": {\\\"value\\\": \\\"VERIFIED\\\", \\\"case_insensitive\\\": false}}}, {\\\"term\\\": {\\\"__traitNames\\\": {\\\"value\\\": \\\"Digital Marketing\\\", \\\"case_insensitive\\\": false}}}]}}}}}, \\\"suppressLogs\\\": true}, \\\"filterScrubbed\\\": true}\" , // (5) } } ] } The typeName must be exactly DataProduct . Human-readable name for your data product. You must provide the the qualifiedName of the data product to update. You can add on any other updates, such as changing the user description of the data product. (Optional) You can update the product's assets by providing\na DSL that defines the assets to include in the data product, as an embedded JSON string. Use SDK to update data products The above data products assets DSL requires a filter as a\nnested object construct within an outer bool, rather than a list or array.\nIt's recommended to update data products via SDK as it handles this complexity automatically. Delete a data product Â¶ 4.0.0 1.8.1 Soft-delete (archive) Â¶ To soft-delete, or archive, a data product: Java Python Kotlin Raw REST API Delete a data product 1 AssetDeletionResponse response = DataProduct . delete ( client , \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ); // (1) To archive a data product in Atlan, call the DataProduct.delete() method with the GUID of the data product. Because this operation will archive the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a data product 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . asset . delete_by_guid ( \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ) # (1) To archive a data product in Atlan, call the asset.delete_by_guid() method with the GUID of the data product. Delete a data product 1 val response = DataProduct . delete ( client , \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ) // (1) To archive a data product in Atlan, call the DataProduct.delete() method with the GUID of the data product. Because this operation will archive the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. DELETE /api/meta/entity/bulk?guid=218c8144-dc39-43a5-b0c0-9eeb4d11e74a&deleteType=SOFT 1 // (1) All the details for deleting the data product are specified in the URL directly. Note that you must provide the GUID of the data product to delete it. Hard-delete (purge) Â¶ To permanently delete (purge) a data product: Java Python Kotlin Raw REST API Delete a data product 1 AssetDeletionResponse response = DataProduct . purge ( client , \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ); // (1) To permanently delete a data product in Atlan, call the DataProduct.purge() method with the GUID of the data product. Because this operation will remove the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a product 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . asset . purge_by_guid ( \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ) # (1) To permanently delete a data product in Atlan, call the asset.purge_by_guid() method with the GUID of the data product. Delete a data product 1 val response = DataProduct . purge ( client , \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ) // (1) To permanently delete a data product in Atlan, call the DataProduct.purge() method with the GUID of the data product. Because this operation will remove the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. DELETE /api/meta/entity/bulk?guid=218c8144-dc39-43a5-b0c0-9eeb4d11e74a&deleteType=PURGE 1 // (1) All the details for deleting the data product are specified in the URL directly. Note that you must provide the GUID of the data product to delete it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/datamesh/datadomains/",
    "content": "/api/meta/entity/bulk (POST) Manage data domains Â¶ Create a new data domain Â¶ 4.0.0 2.0.4 To create a new data domain: Java Python Kotlin Raw REST API Create a data domain 1 2 3 4 5 DataDomain domain = DataDomain . creator ( \"Marketing\" ) // (1) . assetIcon ( AtlanIcon . ROCKET ) // (2) . assetThemeHex ( AtlanMeshColor . MAGENTA ) . build (); // (3) AssetMutationResponse response = domain . save ( client ); // (4) You must provide a human-readable name for your data domain. You can chain onto the creator any other enrichment, for example choosing a different icon or color to represent the domain. You then need to build the object. You can then save() the object you've built to create the new data domain in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a data domain 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataDomain from pyatlan.model.enums import AtlanIcon , AtlanMeshColor client = AtlanClient () domain = DataDomain . creator ( name = \"Marketing\" , # (1) ) domain . asset_icon = AtlanIcon . ROCKET # (2) domain . asset_theme_hex = AtlanMeshColor . MAGENTA response = client . asset . save ( domain ) # (3) You must provide a human-readable name for your data domain. You can apply any other enrichment, for example choosing a different icon or color to represent the domain. You can then save() the object to create the new data domain in Atlan. Create a data domain 1 2 3 4 5 val domain = DataDomain . creator ( \"Marketing\" ) // (1) . assetIcon ( AtlanIcon . ROCKET ) // (2) . assetThemeHex ( AtlanMeshColor . MAGENTA ) . build () // (3) val response = domain . save ( client ) // (4) You must provide a human-readable name for your data domain. You can chain onto the creator any other enrichment, for example choosing a different icon or color to represent the domain. You then need to build the object. You can then save() the object you've built to create the new data domain in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"entities\" : [ { \"typeName\" : \"DataDomain\" , // (1) \"attributes\" : { \"name\" : \"Marketing\" , // (2) \"assetIcon\" : \"PhRocket\" , // (3) \"assetThemeHex\" : \"#F34D77\" , \"qualifiedName\" : \"default/domain/marketing\" // (4) } } ] } The typeName must be exactly DataDomain . Human-readable name for your data domain. You can specify other enrichment, for example choosing a different icon or color to represent the domain. The qualifiedName should follow the pattern: default/domain/<lowerCamelCaseName> . Create a new subdomain Â¶ 4.0.0 2.0.0 To create a new subdomain: Java Python Kotlin Raw REST API Create a subdomain 1 2 3 4 DataDomain sub = DataDomain . creator ( \"Social Marketing\" , // (1) DataDomain . refByQualifiedName ( \"default/domain/marketing\" )) // (2) . build (); // (3) AssetMutationResponse response = sub . save ( client ); // (4) You must provide a human-readable name for your data domain. To create subdomain, you must provide the parent domain with at least its qualifiedName . You can chain on other enrichment, like above, but ultimately then need to build the object. You can then save() the object you've built to create the new data subdomain in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a subdomain 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataDomain client = AtlanClient () sub_domain = DataDomain . creator ( name = \"Social Marketing\" , # (1) parent_domain_qualified_name = \"default/domain/marketing\" , # (2) ) response = client . asset . save ( sub_domain ) Human-readable name for your data domain. To create subdomain, you must provide the parent domain qualifiedName . Create a subdomain 1 2 3 4 val sub = DataDomain . creator ( \"Social Marketing\" , // (1) DataDomain . refByQualifiedName ( \"default/domain/marketing\" )) // (2) . build () // (3) val response = sub . save ( client ) // (4) You must provide a human-readable name for your data domain. To create subdomain, you must provide the parent domain with at least its qualifiedName . You can chain on other enrichment, like above, but ultimately then need to build the object. You can then save() the object you've built to create the new data subdomain in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \"entities\" : [ { \"typeName\" : \"DataDomain\" , // (1) \"attributes\" : { \"name\" : \"Social Marketing\" , // (2) \"qualifiedName\" : \"default/domain/gAbQGZNrFjG2F9lGB3hYp/super/domain/socialMarketing\" , // (3) \"parentDomainQualifiedName\" : \"default/domain/gAbQGZNrFjG2F9lGB3hYp/super\" , // (4) \"superDomainQualifiedName\" : \"default/domain/gAbQGZNrFjG2F9lGB3hYp/super\" // (5) }, \"relationshipAttributes\" : { \"parentDomain\" : { // (6) \"typeName\" : \"DataDomain\" , \"uniqueAttributes\" : { \"qualifiedName\" : \"default/domain/gAbQGZNrFjG2F9lGB3hYp\" } } } } ] } The typeName must be exactly DataDomain . Human-readable name for your data sub-domain. The qualifiedName should follow the pattern: <parentQualifiedName>/domain/<lowerCamelCaseName> . You must provide the qualifiedName of the parent domain. Provide a superDomainQualifiedName for the data domain under which you want to create this sub-domain.\nIf creating a sub-domain under another sub-domains (ie. nested sub-domains), this should be the qualified name of the root-level domain. You must also specify a relationship to the parent domain, in this example through its qualifiedName . Retrieve a data domain Â¶ 4.0.0 2.2.1 To retrieve a data domain by its human-readable name: Java Python Kotlin Raw REST API Retrieve a data domain by its human-readable name 1 2 3 DataDomain domain = DataDomain . findByName ( // (1) client , \"marketing\" , List . of ( \"certificateStatus\" ) ). get ( 0 ); Use DataDomain.findByName() method to retrieve a data domain by its human-readable name: client through which to access a tenant. name of the data domain. (optional) a list of attributes to retrieve for the data domain, for example certificateStatus . Retrieve a data domain by its human-readable name 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataDomain client = AtlanClient () domain = client . asset . find_domain_by_name ( # (1) name = \"marketing\" , attributes = [ \"certificateStatus\" ] ) assert domain assert domain . certificate_status Use client.asset.find_domain_by_name() method to retrieve a data domain by its human-readable name: name of the data domain. (optional) a list of attributes to retrieve\nfor the data domain, for example certificateStatus . Retrieve a data domain by its human-readable name 1 2 3 val domain = DataDomain . findByName ( // (1) client , \"marketing\" , listOf ( \"certificateStatus\" ) ). get ( 0 ) Use DataDomain.findByName() method to retrieve a data domain by its human-readable name: client through which to access a tenant. name of the data domain. (optional) a list of attributes to retrieve for the data domain, for example certificateStatus . POST /api/meta/search/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 { \"dsl\" : { \"from\" : 0 , \"size\" : 100 , \"aggregations\" : {}, \"track_total_hits\" : true , \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"name.keyword\" : { \"value\" : \"marketing\" // (1) } } }, { \"term\" : { \"__typeName.keyword\" : { \"value\" : \"DataDomain\" } } } ] } }, \"sort\" : [ { \"__guid\" : { \"order\" : \"asc\" } } ] }, \"attributes\" : [ \"certificateStatus\" // (2) ] } Human-readable name of the data domain. (optional) a list of attributes to retrieve\nfor the data domain, for example certificateStatus . Update a data domain Â¶ 4.0.0 2.0.0 To update a data domain or subdomain: Java Python Kotlin Raw REST API Update a data domain 1 2 3 4 5 DataDomain domain = DataDomain . updater ( \"default/domain/marketing\" , // (1) \"Marketing\" ) . userDescription ( \"Now with a description!\" ) // (2) . build (); // (3) AssetMutationResponse response = domain . save ( client ); // (4) Use the updater() method to update a data domain, providing the qualifiedName and name of the data domain. You can chain onto the updater any other enrichment, for example changing the domain's description. You then need to build the object. You can then save() the object you've built to update the data domain in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update a data domain 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.assets import DataDomain client = AtlanClient () data_domain = DataDomain . updater ( # (1) qualified_name = \"default/domain/marketing\" , # (2) name = \"Marketing\" , # (3) ) data_domain . user_description = \"Now with a description!\" # (4) response = client . asset . save ( data_domain ) # (5) Use the updater() method to update a data domain. You must provide the qualifiedName of the data domain. You must provide the name of the data domain. You can then add on any other updates, such as changing the user description of the data domain. To update the data domain in Atlan, call the save() method with the object you've built. Update a data domain 1 2 3 4 5 val domain = DataDomain . updater ( \"default/domain/marketing\" , // (1) \"Marketing\" ) . userDescription ( \"Now with a description!\" ) // (2) . build () // (3) val response = domain . save ( client ) // (4) Use the updater() method to update a data domain, providing the qualifiedName and name of the data domain. You can chain onto the updater any other enrichment, for example changing the domain's description. You then need to build the object. You can then save() the object you've built to update the data domain in Atlan. Because this operation will persist the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. POST /api/meta/entity/bulk 1 2 3 4 5 6 7 8 9 10 11 12 { \"entities\" : [ { \"typeName\" : \"DataDomain\" , // (1) \"attributes\" : { \"name\" : \"Marketing\" , // (2) \"qualifiedName\" : \"default/domain/marketing\" , // (3) \"userDescription\" : \"Now with a description!\" // (4) }, } ] } The typeName must be exactly DataDomain . Human-readable name for your data domain. You must provide the the qualifiedName of the domain to update. You can add on any other updates, such as changing the user description of the data domain. Delete a data domain Â¶ 4.0.0 1.8.1 Soft-delete (archive) Â¶ To soft-delete, or archive, a data domain: Java Python Kotlin Raw REST API Delete a data domain 1 AssetDeletionResponse response = DataDomain . delete ( client , \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ); // (1) To archive a data domain in Atlan, call the DataDomain.delete() method with the GUID of the data domain. Because this operation will archive the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a data domain 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . asset . delete_by_guid ( \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ) # (1) To archive a data domain in Atlan, call the asset.delete_by_guid() method with the GUID of the data domain. Delete a data domain 1 val response = DataDomain . delete ( client , \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ) // (1) To archive a data domain in Atlan, call the DataDomain.delete() method with the GUID of the data domain. Because this operation will archive the asset in Atlan, you must provide it an AtlanClient through which to connect to the tenant. DELETE /api/meta/entity/bulk?guid=218c8144-dc39-43a5-b0c0-9eeb4d11e74a&deleteType=SOFT 1 // (1) All the details for deleting the data domain are specified in the URL directly. Note that you must provide the GUID of the data domain to delete it. Hard-delete (purge) Â¶ To permanently delete (purge) a data domain: Java Python Kotlin Raw REST API Purge a data domain 1 AssetDeletionResponse response = DataDomain . purge ( client , \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ); // (1) To permanently delete a data domain in Atlan, call the DataDomain.purge() method with the GUID of the data domain. Because this operation will remove the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Purge a data domain 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . asset . purge_by_guid ( \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ) # (1) To permanently delete a data domain in Atlan, call the asset.purge_by_guid() method with the GUID of the data domain. Purge a data domain 1 val response = DataDomain . purge ( \"218c8144-dc39-43a5-b0c0-9eeb4d11e74a\" ) // (1) To permanently delete a data domain in Atlan, call the DataDomain.purge() method with the GUID of the data domain. Because this operation will remove the asset from Atlan, you must provide it an AtlanClient through which to connect to the tenant. DELETE /api/meta/entity/bulk?guid=218c8144-dc39-43a5-b0c0-9eeb4d11e74a&deleteType=PURGE 1 // (1) All the details for deleting the data domain are specified in the URL directly. Note that you must provide the GUID of the data domain to delete it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/search-logs/",
    "content": "/api/meta/search/searchlog (POST) Accessing the search logs of an asset Â¶ Search logs provide valuable insights for analysts to monitor the popularity of assets within Atlan, including details such as the most visited assets, user interactions, and emerging search patterns. Accessing the search log of an asset is a flexible operation, which may seem more intricate compared to other operations. To harness the complete flexibility of Atlan's search, the SDK introduces a dedicated SearchLogRequest object. Similar but not identical to searching in general Atlan's search log, which contains the search logs of an asset, utilizes Elasticsearch. This makes the approach to accessing search logs similar to searching . However, there are differences, as the search log uses a distinct index from the broader search. If you're feeling adventurous, feel free to experiment with the more complex search mechanisms outlined in the searching section. Nevertheless, this should be sufficient to help you get started with accessing asset search logs. Most recent viewers of an asset Â¶ 2.0.4 4.0.0 To retrieve the most recent viewers of an asset: Java Python Kotlin Raw REST API Retrieve the most recent viewers of an asset 1 2 3 4 5 6 7 8 9 List < UserViews > viewers = SearchLog . mostRecentViewers ( // (1) client , \"955c455d-cfea-4c9c-844d-e226edf8b6da\" , 20 , List . of ( \"atlansupport\" ) ); for ( UserViews viewer : viewers ) { // (2) String user = viewer . getUsername () // (3) Long viewCount = viewer . getViewCount () // (4) Long lastAccessed = viewer . getMostRecentView () // (5) } You must provide GUID of the asset and specify the maximum number of recent users to be considered for the search log request. Optionally, you may provide a list of usernames to be excluded from the search log results. Because this operation will directly look up the asset's views in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through each recent viewers. Name of the user who viewed the asset. Number of times the user viewed the asset. When the user most recently viewed the asset (epoch-style), in milliseconds. Retrieve the most recent viewers of an asset 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.search_log import SearchLogRequest client = AtlanClient () request = SearchLogRequest . most_recent_viewers ( # (1) guid = \"955c455d-cfea-4c9c-844d-e226edf8b6da\" , max_users = 20 , exclude_users = [ \"atlansupport\" ], ) response = client . search_log . search ( request ) for viewer in response . user_views : # (2) user = viewer . username # (3) view_count = viewer . view_count # (4) last_accessed = viewer . most_recent_view # (5) You must provide a GUID and specify the maximum number of recent users to be considered for the search log request. Optionally, you may provide a list of usernames\n  to be excluded from the search log results. You can then iterate through each recent viewers. Name of the user who viewed the asset. Number of times the user viewed the asset. When the user most recently viewed the asset (epoch-style), in milliseconds. Retrieve the most recent viewers of an asset 1 2 3 4 5 6 7 8 9 val viewers = SearchLog . mostRecentViewers ( // (1) client , \"955c455d-cfea-4c9c-844d-e226edf8b6da\" , maxUsers = 20 , listOf ( \"atlansupport\" ) ) for ( viewer in viewers ) { // (2) val user = viewer . username // (3) val viewCount = viewer . viewCount // (4) val lastAccessed = viewer . mostRecentView // (5) } You must provide a GUID and specify the maximum number of recent users to be considered for the search log request. Optionally, you may provide a list of usernames to be excluded from the search log results. Because this operation will directly look up the asset's views in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through each recent viewers. Name of the user who viewed the asset. Number of times the user viewed the asset. When the user most recently viewed the asset (epoch-style), in milliseconds. POST /api/meta/search/searchlog 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 { \"dsl\" : { \"from\" : 0 , \"size\" : 0 , \"aggregations\" : { \"uniqueUsers\" : { \"aggregations\" : { \"latestTimestamp\" : { \"max\" : { \"field\" : \"timestamp\" } } }, \"terms\" : { \"field\" : \"userName\" , \"order\" : [ { \"latestTimestamp\" : \"desc\" } ], \"size\" : 20 // (2) } }, \"totalDistinctUsers\" : { \"cardinality\" : { \"field\" : \"userName\" , \"precision_threshold\" : 1000 } } }, \"query\" : { \"bool\" : { \"must_not\" : [ { \"terms\" : { \"userName\" : [ \"atlansupport\" ] // (3) } } ], \"filter\" : [ { \"term\" : { \"utmTags\" : { \"value\" : \"action_asset_viewed\" } } }, { \"term\" : { \"entityGuidsAll\" : { \"value\" : \"955c455d-cfea-4c9c-844d-e226edf8b6da\" , // (1) \"case_insensitive\" : false } } }, { \"bool\" : { \"minimum_should_match\" : 1 , \"should\" : [ { \"term\" : { \"utmTags\" : { \"value\" : \"ui_profile\" } } }, { \"term\" : { \"utmTags\" : { \"value\" : \"ui_sidebar\" } } } ] } } ] } }, \"sort\" : [ { \"timestamp\" : { \"order\" : \"asc\" } } ], \"track_total_hits\" : true } } GUID of the asset for which you are seeking the details of recent viewers. Maximum number of recent users to be considered for the search log request. Optionally, you may provide a list of usernames to be excluded from the search log results. Most viewed assets Â¶ 2.0.4 4.0.0 To retrieve the most viewed assets by its total views: Java Python Kotlin Raw REST API Retrieve the most viewed assets by its total views 1 2 3 4 5 6 7 8 9 List < AssetViews > topAssetsByViews = SearchLog . mostViewedAssets ( client , 10 , false , List . of ( \"atlansupport\" ) // (1) ); for ( AssetViews detail : topAssetsByViews ) { // (2) String guid = detail . getGuid () // (3) Long totalViews = detail . getTotalViews () // (4) Long distinctUsers = detail . getDistinctUsers () // (5) } To retrieve the most viewed assets, specify the maximum number of assets you want to retrieve. Then specify whether you want the most-viewed based on total number of views, irrespective of distinct users ( false ); or based on the most distinct users, irrespective of total views ( true ). Optionally, you may provide a list of usernames to be excluded from the search log results. Because this operation will directly look up the asset's views in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through each asset's views. GUID of the asset that was viewed. Number of times the asset has been viewed (in total). Number of distinct users that have viewed the asset. Retrieve the most viewed assets by its total view 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient from pyatlan.model.search_log import SearchLogRequest client = AtlanClient () request = SearchLogRequest . most_viewed_assets ( # (1) max_assets = 10 , by_different_user = False , exclude_users = [ \"username\" ], ) response = client . search_log . search ( request ) for detail in response . asset_views : # (2) guid = detail . guid # (3) total_views = detail . total_views # (4) distinct_users = detail . distinct_users # (5) To retrieve the most viewed assets, specify the maximum number of assets you want to retrieve.\nThen specify whether you want the most-viewed based on total number of views, irrespective\nof distinct users ( False ); or based on the most distinct users, irrespective of total views ( True ). Optionally, you may provide a list of usernames\n  to be excluded from the search log results. You can then iterate through each asset's viewers. Name of the user who viewed the asset. Number of times the user viewed the asset. Number of distinct users that have viewed the asset. Retrieve the most viewed assets by its total view 1 2 3 4 5 6 7 8 9 val topAssetsByViews = SearchLog . mostViewedAssets ( 10 , byDifferentUsers = false , excludeUsers = listOf ( \"atlansupport\" ) ) // (1) for ( detail in topAssetsByViews ) { // (2) val guid = detail . username // (3) val totalViews = detail . viewCount // (4) val distinctUsers = detail . mostRecentView // (5) } To retrieve the most viewed assets, specify the maximum number of assets you want to retrieve. Then specify whether you want the most-viewed based on total number of views, irrespective of distinct users ( false ); or based on the most distinct users, irrespective of total views ( true ). Optionally, you may provide a list of usernames to be excluded from the search log results. Because this operation will directly look up the asset's views in Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through each asset's viewers. Name of the user who viewed the asset. Number of times the user viewed the asset. Number of distinct users that have viewed the asset. POST /api/meta/search/searchlog 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 { \"dsl\" : { \"from\" : 0 , \"size\" : 0 , \"aggregations\" : { \"uniqueAssets\" : { \"aggregations\" : { \"uniqueUsers\" : { \"cardinality\" : { \"field\" : \"userName\" , \"precision_threshold\" : 1000 } } }, \"terms\" : { \"field\" : \"entityGuidsAll\" , \"size\" : 10 // (1) // (2) } }, \"totalDistinctUsers\" : { \"cardinality\" : { \"field\" : \"userName\" , \"precision_threshold\" : 1000 } } }, \"query\" : { \"bool\" : { \"must_not\" : [ { \"terms\" : { \"userName\" : [ \"atlansupport\" ] // (3) } } ], \"filter\" : [ { \"term\" : { \"utmTags\" : { \"value\" : \"action_asset_viewed\" } } }, { \"bool\" : { \"minimum_should_match\" : 1 , \"should\" : [ { \"term\" : { \"utmTags\" : { \"value\" : \"ui_profile\" } } }, { \"term\" : { \"utmTags\" : { \"value\" : \"ui_sidebar\" } } } ] } } ] } }, \"sort\" : [ { \"timestamp\" : { \"order\" : \"asc\" } } ], \"track_total_hits\" : true } } Maximum number of assets to be considered for the search log request If you want to retrieve most viewed assets based on the most distinct users,\nregardless of total views, include \"order\": [{\"uniqueUsers\": \"desc\"}] here. Optionally, you may provide a list of usernames to be excluded from the search log results. Detailed search log entries Â¶ 3.1.0 4.0.0 To retrieve detailed search log entries (paged via lazy fetching): Java Python Kotlin Raw REST API To retrieve detailed search log entries 1 2 3 4 5 6 7 8 9 10 SearchLog . viewsByGuid ( client , \"955c455d-cfea-4c9c-844d-e226edf8b6da\" , List . of ( \"atlansupport\" ) ) // (1) . stream () // (2) . limit ( 100 ) // (3) . forEach ( entry -> { // (4) logger . info ( entry . getUserName () + \" from \" + entry . getIpAddress ()); }); You must provide the GUID of the asset for which you are seeking the detailed search log entries. Optionally, you may provide a list of usernames to be excluded from the search log results. Because this operation will directly look up the asset's views in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The search will only run when you call the stream() method,\nwhich will then lazily-load each page of results into a stream With streaming, you can apply your own limits to the maximum number of results you want to process. Independent of page size Note that this is independent of page size. You could page through results 50 at a time, but only process a maximum of 100 total results this way. Since the results are lazily-loaded when streaming, only the first two pages of results would be retrieved in such a scenario. You can then iterate through each log entry to retrieve details\nsuch as the username and IP address of the search logs. To retrieve detailed search log entries 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pyatlan.client.atlan import AtlanClient from pyatlan.model.search_log import SearchLogRequest client = AtlanClient () request = SearchLogRequest . views_by_guid ( # (1) guid = \"955c455d-cfea-4c9c-844d-e226edf8b6da\" , size = 20 , exclude_users = [ \"atlansupport\" ], ) response = client . search_log . search ( criteria = request , bulk = False ) # (2) for entry in response : # (3) LOGGER . info ( f \" { entry . user_name } from { entry . ip_address } \" ) You must provide the GUID of the asset and specify\nthe maximum number of log entries per page for the search log request. Optionally, you may provide a list of usernames\n  to be excluded from the search log results. client.search_log.search() method takes following parameters: criteria : defines the search query to execute the search. bulk ( default: False ): specifies whether to execute the search in bulk mode for retrieving the search logs matching the criteria. This mode is optimized for handling large results (more than 10,000 ). When enabled ( True ), the results will be reordered based on the creation timestamp to facilitate iterating through large datasets. Note If the number of results exceeds the predefined threshold\n( 10,000 assets) search log search will be automatically converted into a bulk search. You can then iterate through each log entry to\nretrieve details such as the username and IP address of the search logs. To retrieve detailed search log entries 1 2 3 4 5 6 7 8 9 10 SearchLog . viewsByGuid ( client , \"955c455d-cfea-4c9c-844d-e226edf8b6da\" , listOf ( \"atlansupport\" ) ) // (1) . stream () // (2) . limit ( 100 ) // (3) . forEach { entry -> // (4) logger . info { \" ${ entry . userName } from ${ entry . ipAddress } \" } } You must provide the GUID of the asset for which you are seeking the detailed search log entries. Optionally, you may provide a list of usernames to be excluded from the search log results. Because this operation will directly look up the asset's views in Atlan, you must provide it an AtlanClient through which to connect to the tenant. The search will only run when you call the stream() method,\nwhich will then lazily-load each page of results into a stream With streaming, you can apply your own limits to the maximum number of results you want to process. Independent of page size Note that this is independent of page size. You could page through results 50 at a time, but only process a maximum of 100 total results this way. Since the results are lazily-loaded when streaming, only the first two pages of results would be retrieved in such a scenario. You can then iterate through each log entry to retrieve details\nsuch as the username and IP address of the search logs. POST /api/meta/search/searchlog 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 { \"dsl\" : { \"from\" : 0 , \"size\" : 100 , // (1) \"query\" : { \"bool\" : { \"must_not\" : [ { \"terms\" : { \"userName\" : [ \"atlansupport\" ] // (3) } } ], \"filter\" : [ { \"term\" : { \"utmTags\" : { \"value\" : \"action_asset_viewed\" } } }, { \"term\" : { \"entityGuidsAll\" : { \"value\" : \"955c455d-cfea-4c9c-844d-e226edf8b6da\" , // (2) \"case_insensitive\" : false } } }, { \"bool\" : { \"minimum_should_match\" : 1 , \"should\" : [ { \"term\" : { \"utmTags\" : { \"value\" : \"ui_profile\" } } }, { \"term\" : { \"utmTags\" : { \"value\" : \"ui_sidebar\" } } } ] } } ] } }, \"sort\" : [ { \"timestamp\" : { \"order\" : \"asc\" } } ], \"track_total_hits\" : true } } Page size of the search log request. GUID of the asset for which you are seeking the detailed search log entries. Optionally, you may provide a list of usernames to be excluded from the search log results. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/tags/",
    "content": "Atlan tag definitions overview Â¶ Tags managed in Atlan are structurally very simple: A name An (optional) description An icon (or image) A color Through Atlan's APIs, you can create your own tags programmatically. Once the structures exist, you can then also tag assets . 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/tags/monitor-propagation/",
    "content": "/api/meta/task/search (POST) Monitor tag propagation Â¶ Atlan's background tasks queue provides essential insights for monitoring tag propagation of assets, detailing completed, pending, in-progress, and deleted tasks. In Atlan's SDK you can use the FluentTasks object to search the tasks queue. Run the search Â¶ 2.0.2 4.0.0 For example, to initiate a search for pending tag propagation tasks related to a specific asset after a tag has been added: Java Python Kotlin Raw REST API Search for background tag propagation tasks 1 2 3 4 5 6 7 8 9 client . tasks . select () // (1) // (2) . where ( AtlanTask . ENTITY_GUID . eq ( \"f65e3da2-6ec2-4ff5-8f0b-b6eba640df24\" )) . where ( AtlanTask . TYPE . eq ( AtlanTaskType . CLASSIFICATION_PROPAGATION_ADD )) . where ( AtlanTask . STATUS . match ( AtlanTaskStatus . PENDING . getValue ())) . stream () // (3) . forEach ( task -> { // (4) log . info ( \"Task: {}\" , task ); }); To search across all tasks, you can use the tasks.select() convenience method on a client. The .where() method allows you to limit to only tasks that have a particular value in a particular field: GUID of the asset for which you want to retrieve tag propagation tasks. Specify the task type; in this example, we're retrieving tasks for monitoring propagation after a tag has been added to the asset. Specify the task status; here, we're checking for any pending tag propagation tasks for the given asset. Note: There's no need to try to remember or even know the precise string values for the above constants. Enums for these values are available in the SDK, making it easier for you. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Search for background tag propagation tasks 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from pyatlan.client.atlan import AtlanClient from pyatlan.model.task import AtlanTask from pyatlan.model.fluent_tasks import FluentTasks from pyatlan.model.enums import AtlanTaskStatus , AtlanTaskType client = AtlanClient () task_request = ( FluentTasks () # (1) . where ( # (2) AtlanTask . ENTITY_GUID . eq ( \"f65e3da2-6ec2-4ff5-8f0b-b6eba640df24\" ) ) . where ( AtlanTask . TYPE . eq ( AtlanTaskType . CLASSIFICATION_PROPAGATION_ADD . value ) ) . where ( AtlanTask . STATUS . match ( AtlanTaskStatus . PENDING . value ) ) . to_request () # (3) ) response = client . tasks . search ( request = task_request ) # (4) for task in response : # (5) ... You can use FluentTasks() to simplify the most common searches against the Atlan task queue. The .where() method allows you to limit to only tasks that have a particular value in a particular field: GUID of the asset for which you want to retrieve tag propagation tasks. Specify the task type; in this example, we're retrieving tasks for monitoring propagation after a tag has been added to the asset. Specify the task status; here, we're checking for any pending tag propagation tasks for the given asset. Note: There's no need to try to remember or even know the precise string values for the above constants. Enums for these values are available in the SDK, making it easier for you. Build the task request object using the provided search criteria. Initiate the task request search by providing the created request object. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Search for background tag propagation tasks 1 2 3 4 5 6 7 8 9 client . assets . select () // (1) // (2) . where ( AtlanTask . ENTITY_GUID . eq ( \"f65e3da2-6ec2-4ff5-8f0b-b6eba640df24\" )) . where ( AtlanTask . TYPE . eq ( AtlanTaskType . CLASSIFICATION_PROPAGATION_ADD )) . where ( AtlanTask . STATUS . match ( AtlanTaskStatus . PENDING . value )) . stream () // (3) . forEach { // (4) log . info { \"Task: $ it \" } } To search across all tasks, you can use the tasks.select() convenience method on a client. The .where() method allows you to limit to only tasks that have a particular value in a particular field: GUID of the asset for which you want to retrieve tag propagation tasks. Specify the task type; in this example, we're retrieving tasks for monitoring propagation after a tag has been added to the asset. Specify the task status; here, we're checking for any pending tag propagation tasks for the given asset. Note: There's no need to try to remember or even know the precise string values for the above constants. Enums for these values are available in the SDK, making it easier for you. The search will only run when you call the stream() method, which will then lazily-load each page of results into a stream. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. POST /api/meta/task/search 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 { \"dsl\" : { \"from\" : 0 , \"size\" : 20 , \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"__task_entityGuid\" : { \"value\" : \"f65e3da2-6ec2-4ff5-8f0b-b6eba640df24\" , // (1) \"case_insensitive\" : false } } }, { \"term\" : { \"__task_type\" : { \"value\" : \"CLASSIFICATION_PROPAGATION_ADD\" // (2) } } }, { \"match\" : { \"__task_status\" : { \"query\" : \"PENDING\" // (3) } } } ] } }, \"sort\" : [ { \"__task_startTime\" : { \"order\" : \"asc\" // (4) } } ], \"track_total_hits\" : true } } GUID of the asset for which you want to retrieve tag propagation tasks. Specify the task type; in this example, we're retrieving tasks for monitoring propagation after a tag has been added to the asset. Specify the task status; here, we're checking for any pending tag propagation tasks for the given asset. This is the default sort order for tag propagation tasks search. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/users-groups/",
    "content": "Users and groups introduction Â¶ Operations on users and groups. Users and groups are peer objects A user can be a member of multiple groups, and a group can have multiple users. At the same time, both can exist without the other. In other words, neither is a parent or container for the other. erDiagram\n    User }o..o{ Group : related 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/users-groups/sso-group-mapping/",
    "content": "/api/service/idp/{sso_alias}/mappers (GET) /api/service/idp/{sso_alias}/mappers (POST) /api/service/idp/{sso_alias}/mappers/{group_map_id} (GET) /api/service/idp/{sso_alias}/mappers/{group_map_id} (POST) /api/service/idp/{sso_alias}/mappers/{group_map_id}/delete (POST) Manage SSO group mapping Â¶ You can use the SDK's SSO client to manage your SSO group mapping in Atlan. Create a new group mapping Â¶ 6.1.0 To create a new SSO group mapping: Java Python Kotlin Raw REST API Coming soon Create a new SSO group mapping 1 2 3 4 5 6 7 8 9 10 11 12 13 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import AtlanSSO client = AtlanClient () atlan_group = client . group . get_by_name ( \"atlan-group\" ) # (1) atlan_group = atlan_group . records [ 0 ] response = client . sso . create_group_mapping ( # (2) sso_alias = AtlanSSO . AZURE_AD , atlan_group = atlan_group , sso_group_name = \"sso_group_name\" , ) Begin by retrieving the Atlan group for which you wish to create a group mapping.\nIn this example, we retrieve an existing Atlan group by its name. To create a new group mapping, provide the following: name of the SSO provider. existing Atlan group. name of the existing SSO group. Coming soon POST /api/service/idp/azure/mappers 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"identityProviderAlias\" : \"azure\" , // (1) \"identityProviderMapper\" : \"saml-group-idp-mapper\" , \"name\" : \"0d9b0028-513c-4536-af90-d594ef2d549c--1713772147406\" , // (2) \"config\" : { \"syncMode\" : \"FORCE\" , \"attributes\" : \"[]\" , \"are.attribute.values.regex\" : \"\" , \"attribute.name\" : \"memberOf\" , \"group\" : \"atlan_group_name\" , // (3) \"attribute.value\" : \"sso_group_name\" // (4) } } Specify the SSO provider; here, we create group mapping for Azure AD SSO. Set the group mapping name in the format <atlan_group_id>--<epoch_timestamp> . Provide the name of the existing Atlan group. Provide the name of the existing SSO group. Retrieve group mapping Â¶ 2.1.6 Retrieve group mapping by ID Â¶ To retrieve an existing SSO group mapping: Java Python Kotlin Raw REST API Coming soon Retrieve an existing SSO group mapping 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import AtlanSSO client = AtlanClient () response = client . sso . get_group_mapping ( # (1) sso_alias = AtlanSSO . AZURE_AD , group_map_id = \"0637576a-5419-40d7-b6cb-fe5841b1da4b\" , ) To retrieve an existing group mapping, provide the following: name of the SSO provider. existing SSO group map identifier. Coming soon GET /api/service/idp/azure/mappers/0637576a-5419-40d7-b6cb-fe5841b1da4b 1 All details are present the URL itself Note that you need to specify the SSO alias and map identifier\ndirectly in the URL. For this example, we're retrieving a group mapping for Azure AD SSO. Retrieve all group mappings Â¶ To retrieve all existing SSO group mappings: Java Python Kotlin Raw REST API Coming soon Retrieve all existing SSO group mappings 1 2 3 4 5 6 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import AtlanSSO client = AtlanClient () response = client . sso . get_all_group_mappings ( sso_alias = AtlanSSO . AZURE_AD ) # (1) To retrieve all existing group mappings,\nyou need to provide the name of the SSO provider.\nHere, we're retrieving all group mappings for Azure AD SSO. Coming soon GET /api/service/idp/azure/mappers 1 All details are present the URL itself Note that you need to specify the SSO alias directly in the URL. \nFor this example, we're retrieving all group mappings for Azure AD SSO. Update an existing group mapping Â¶ 6.1.0 To update an existing SSO group mapping: Java Python Kotlin Raw REST API Coming soon Update an existing SSO group mapping 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import AtlanSSO client = AtlanClient () atlan_group = client . group . get_by_name ( \"atlan-group\" ) # (1) atlan_group = atlan_group . records [ 0 ] response = client . sso . update_group_mapping ( # (2) sso_alias = AtlanSSO . AZURE_AD , atlan_group = atlan_group , group_map_id = \"0637576a-5419-40d7-b6cb-fe5841b1da4b\" , sso_group_name = \"sso_group_name_updated\" , ) Begin by retrieving the Atlan group for which you wish to update a group mapping.\nIn this example, we retrieve an existing Atlan group by its name. To update an existing group mapping, provide the following: name of the SSO provider. existing Atlan group. existing SSO group map identifier. updated name of the existing SSO group. Coming soon POST /api/service/idp/azure/mappers/0637576a-5419-40d7-b6cb-fe5841b1da4b 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"identityProviderAlias\" : \"azure\" , // (1) \"identityProviderMapper\" : \"saml-group-idp-mapper\" , \"id\" : \"0637576a-5419-40d7-b6cb-fe5841b1da4b\" , // (2) \"name\" : \"0d9b0028-513c-4536-af90-d594ef2d549c--1713772147406\" , // (3) \"config\" : { \"syncMode\" : \"FORCE\" , \"attributes\" : \"[]\" , \"are.attribute.values.regex\" : \"\" , \"attribute.name\" : \"memberOf\" , \"group\" : \"atlan_group_name\" , // (4) \"attribute.value\" : \"sso_group_name_updated\" // (5) } } Specify the SSO provider; here, we update group mapping for Azure AD SSO. Specify the existing SSO group map identifier. Specify the name of the existing SSO group map. Provide the name of the existing Atlan group. Provide the updated name of the existing SSO group. Delete a group mapping Â¶ To delete an existing SSO group mapping: Java Python Kotlin Raw REST API Coming soon Delete an existing SSO group mapping 1 2 3 4 5 6 7 8 9 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import AtlanSSO client = AtlanClient () response = client . sso . delete_group_mapping ( # (1) sso_alias = AtlanSSO . AZURE_AD , group_map_id = \"0637576a-5419-40d7-b6cb-fe5841b1da4b\" ) To delete an existing group mapping,\nyou need to provide the SSO alias and map identifier.\nHere, we're deleting the group mapping for Azure AD SSO. Coming soon POST /api/service/idp/azure/mappers/0637576a-5419-40d7-b6cb-fe5841b1da4b/delete 1 All details are present the URL itself Note that you need to specify the SSO alias and map identifier\ndirectly in the URL. For this example, we're deleting a group mapping for Azure AD SSO. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/tags/manage/",
    "content": "/api/meta/types/typedef/name/{name} (GET) /api/meta/types/typedef/{name} (DELETE) /api/meta/types/typedefs (POST) /api/meta/types/typedefs (PUT) /api/meta/types/typedefs/?type=CLASSIFICATION (GET) Manage Atlan tags Â¶ Similar to other objects you can create in the SDK, Atlan tags implement the builder pattern. Atlan tags vs tags in general Note that we intentionally use the phrase Atlan tag here to differentiate\ntags you can structurally maintain in Atlan vs other tags in general.\nFor example, Snowflake tags are not managed this way, since they are owned and managed in Snowflake. Build minimal object needed Â¶ 1.3.3 1.0.0 For example, to create an Atlan tag to identify personally-identifiable information: Java Python Kotlin Raw REST API Build Atlan tag object for creation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 AtlanTagDef color = AtlanTagDef . creator ( // (1) \"PII\" , // (2) AtlanTagColor . RED ) // (3) . description ( \"Personally-Identifiable Information\" ) // (4) . build (); // (5) AtlanTagDef icon = AtlanTagDef . creator ( \"PII\" , AtlanIcon . PASSWORD , // (6) AtlanTagColor . RED ) . build (); AtlanTagDef image = AtlanTagDef . creator ( \"PII\" , \"http://some.example.com/image.png\" , // (7) AtlanTagColor . RED ) . build (); Use the creator() method to start building up the Atlan tag. You must provide a name for the Atlan tag ( PII in this example). You must also specify the color you want to use for the Atlan tag. (Optional) You can also give the Atlan tag a description. As with all other builder patterns, you must build() the object you've defined. As an alternative, you can also specify a built-in icon to use for the tag. As an alternative, you can also specify your own image to use for the tag. Build Atlan tag object for creation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import urllib.request from pyatlan.model.typedef import AtlanTagDef from pyatlan.model.enums import AtlanTagColor , AtlanIcon from pyatlan.client.atlan import AtlanClient client = AtlanClient () atlan_tag_def = AtlanTagDef . create ( # (1) name = \"PII\" , # (2) color = AtlanTagColor . RED ) # (3) atlan_tag_def . description = \"Personally-Identifiable Information\" # (4) atlan_tag_def = AtlanTagDef . create ( name = \"PII\" , icon = AtlanIcon . PASSWORD , # (5) color = AtlanTagColor . RED ) urllib . request . urlretrieve ( \"http://some.example.com/image.png\" , \"image.png\" ) # (6) with open ( \"image.png\" , \"rb\" ) as img_file : image = client . upload_image ( file = img_file , filename = \"image.png\" ) # (7) atlan_tag_def = AtlanTagDef . create ( name = \"PII\" , image = image , # (8) color = AtlanTagColor . RED ) Use the create() method to set up the Atlan tag with its minimal necessary inputs. You must provide a name for the Atlan tag ( PII in this example). You must also specify the color you want to use for the Atlan tag. (Optional) You can also give the Atlan tag a description. As an alternative, you can also specify a built-in icon to use for the tag. As an alternative, you can download or use your own image file for the tag. Before you can use the image, you must upload it to Atlan. You can then specify the uploaded image to use for the tag. Build Atlan tag object for creation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 val color = AtlanTagDef . creator ( // (1) \"PII\" , // (2) AtlanTagColor . RED ) // (3) . description ( \"Personally-Identifiable Information\" ) // (4) . build () // (5) val icon = AtlanTagDef . creator ( \"PII\" , AtlanIcon . PASSWORD , // (6) AtlanTagColor . RED ) . build () val image = AtlanTagDef . creator ( \"PII\" , \"http://some.example.com/image.png\" , // (7) AtlanTagColor . RED ) . build () Use the creator() method to start building up the Atlan tag. You must provide a name for the Atlan tag ( PII in this example). You must also specify the color you want to use for the Atlan tag. (Optional) You can also give the Atlan tag a description. As with all other builder patterns, you must build() the object you've defined. As an alternative, you can also specify a built-in icon to use for the tag. As an alternative, you can also specify your own image to use for the tag. Image option not shown The option to use your own image is significantly more complicated, as it involves constructing a multipart form-encoded upload of the binary image data first, and then using the resulting uploaded object's details to use the image within the tag. POST /api/meta/types/typedefs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { \"classificationDefs\" : [ // (1) { \"category\" : \"CLASSIFICATION\" , // (2) \"name\" : \"PII\" , // (3) \"description\" : \"Personally-Identifiable Information\" , \"displayName\" : \"PII\" , // (4) \"options\" : { \"color\" : \"Red\" , // (5) \"icon\" : \"PhPassword\" , // (6) \"iconType\" : \"icon\" }, \"skipDisplayNameUniquenessCheck\" : false } ] } All Atlan tag definitions must be specified within the classificationDefs array. Each definition must be defined with a category set to CLASSIFICATION . Whatever name you provide for the definition will be replaced by a hashed-string generated name by the back-end. Specify the name of the Atlan tag, as it should appear in the UI, to the displayName . Set the color to use for the Atlan tag within the options object. (Optional) Set a built-in icon to use within the options object. When defining an icon, you must also set options.iconType to \"icon\" . Where can I see each icon? We use Phosphor for the icons. They have a beautiful icon browser on their site to search and preview the icons. Create the Atlan tag from the object Â¶ 1.3.3 4.0.0 Now that the object is built, it will have the required information for Atlan to create it: Java Python Kotlin Raw REST API Create the Atlan tag 6 AtlanTagDef response = atlanTagDef . create ( client ); // (1) The create() operation will actually create the Atlan tag within Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create the Atlan tag 8 response = client . typedef . create ( atlan_tag_def ) # (1) The typedef.create() operation will actually create the Atlan tag within Atlan. Create the Atlan tag 6 val response = color . create ( client ) // (1) The create() operation will actually create the Atlan tag within Atlan. Because this operation will persist the structure in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Creation implicit in step above The actual creation of the Atlan tag is implicit in the example above. Now that the Atlan tag has been created, you can use it to tag assets . Update Atlan tags Â¶ 2.1.0 To update Atlan tags: Java Python Kotlin Raw REST API Coming soon Update existing tag structure 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient client = AtlanClient () atlan_tag = client . typedef . get_by_name ( \"S7qnUqZ5mBMBpzQ3Wzt6yD\" ) # (1) atlan_tag . options [ \"color\" ] = \"Green\" # (2) atlan_tag . options [ \"emoji\" ] = \"ðŸ’ª\" atlan_tag . display_name = \"MyTagName\" response = client . typedef . update ( atlan_tag ) # (3) To ensure you have the complete structure, it is easiest to start by retrieving the existing Atlan tag structure by its hashed-string name . In this example, we're updating the following properties of an Atlan tag: color of the tag. emoji of the tag. display name of the tag. Now use client.typedef.update() with the updated tag definition. Coming soon PUT /api/meta/types/typedefs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \"enumDefs\" : [], \"structDefs\" : [], \"classificationDefs\" : [ // (1) { \"category\" : \"CLASSIFICATION\" , \"createTime\" : 1716829706708 , \"createdBy\" : \"service-account-apikey-2e721c86-2814-4c98-8e1a-fc3fdf6b0489\" , \"guid\" : \"884091b2-4fbc-4c8e-85d1-173ad90547cf\" , \"name\" : \"S7qnUqZ5mBMBpzQ3Wzt6yD\" , \"typeVersion\" : \"1.0\" , \"updateTime\" : 1716835149934 , \"updatedBy\" : \"service-account-apikey-2e721c86-2814-4c98-8e1a-fc3fdf6b0489\" , \"version\" : 16 , \"attributeDefs\" : [], \"entityTypes\" : [], \"displayName\" : \"MyTagName\" , // (2) \"options\" : { \"color\" : \"Green\" , \"emoji\" : \"ðŸ’ª\" , \"imageID\" : \"\" , \"iconName\" : \"PhRocketLaunch\" , \"iconType\" : \"emoji\" }, \"subTypes\" : [], \"superTypes\" : [] } ], \"entityDefs\" : [], \"relationshipDefs\" : [], \"businessMetadataDefs\" : [] } All Atlan tag definitions must be specified within the classificationDefs array. In this example, we're updating the following properties of an Atlan tag: display name of the tag. color of the tag. emoji of the tag. Retrieve Atlan tags Â¶ 2.1.0 4.0.0 To retrieve Atlan tag by its hashed-string name eg: S7qnUqZ5mBMBpzQ3Wzt6yD : Java Python Kotlin Raw REST API Retrieve existing tag structure 1 TypeDef atlanTag = client . typeDefs . get ( \"S7qnUqZ5mBMBpzQ3Wzt6yD\" ); // (1) To retrieve the tag, you need to call the .typeDefs.get() method on a client, with the hashed-string name of the tag. Retrieve existing tag structure 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () atlan_tag = client . typedef . get_by_name ( \"S7qnUqZ5mBMBpzQ3Wzt6yD\" ) # (1) To retrieve the tag, you need to call the client.typedef.get_by_name() method with its hashed-string name. Retrieve existing tag structure 1 val atlanTag = client . typeDefs . get ( \"S7qnUqZ5mBMBpzQ3Wzt6yD\" ); // (1) To retrieve the tag, you need to call the .typeDefs.get() method on a client, with the hashed-string name of the tag. GET /api/meta/types/typedef/name/S7qnUqZ5mBMBpzQ3Wzt6yD 1 Atlan tag have a hashed-string representation Use their hashed-string name when retrieving its structure eg: S7qnUqZ5mBMBpzQ3Wzt6yD . Retrieve all Atlan tags Â¶ 1.3.3 4.0.0 To retrieve all Atlan tags: Java Python Kotlin Raw REST API Retrieve all tag structures 1 TypeDefResponse atlanTags = client . typeDefs . list ( AtlanTypeCategory . CLASSIFICATION ); // (1) To retrieve all tags, call the .typeDefs.list() method on a client, with the category AtlanTypeCategory.CLASSIFICATION . Retrieve all tag structures 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . typedef . get ( type_category = AtlanTypeCategory . CLASSIFICATION ) # (1) atlan_tags = response . atlan_tag_defs # (2) To retrieve all tags, call the client.typedef.get() method with the definition category AtlanTypeCategory.CLASSIFICATION . Specifically retrieve the list of tags from TypeDefResponse . Retrieve all tag structures 1 val atlanTags = client . typeDefs . list ( AtlanTypeCategory . CLASSIFICATION ) // (1) To retrieve all tags, call the .typeDefs.list() method on a client, with the category AtlanTypeCategory.CLASSIFICATION . GET /api/meta/types/typedefs/?type=CLASSIFICATION 1 Delete Atlan tags Â¶ 1.3.3 4.0.0 Delete Atlan tags by its human-readable name: Java Python Kotlin Raw REST API Delete tag structure 1 AtlanTagDef . purge ( client , \"MyTagName\" ); // (1) You only need to call the AtlanTagDef.purge() method with the human-readable name of the tag, and it will be deleted. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete tag structure 1 2 3 4 5 from pyatlan.model.typedef import AtlanTagDef from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . typedef . purge ( \"MyTagName\" , AtlanTagDef ) # (1) You only need to call the clietn.typedef.purge() method\nwith the human-readable name of the tag, and it will be deleted. Delete tag structure 1 AtlanTagDef . purge ( client , \"MyTagName\" ) // (1) You only need to call the AtlanTagDef.purge() method with the hashed-string name of the tag, and it will be deleted. Because this operation will remove the structure from Atlan, you must provide it an AtlanClient through which to connect to the tenant. DELETE /api/meta/types/typedef/name/S7qnUqZ5mBMBpzQ3Wzt6yD 1 Atlan tag have a hashed-string representation Use their hashed-string name when deleting its structure eg: S7qnUqZ5mBMBpzQ3Wzt6yD . 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/users-groups/delete/",
    "content": "/api/service/groups/{guid}/delete (POST) Deleting users and groups Â¶ Deleting users and groups uses a similar pattern to the retrieval operations. For this you can use static methods provided by the AtlanUser and AtlanGroup classes. All delete operations are permanent All delete operations on users and groups are permanent, hard-deletes. There is no way to archive (soft-delete) users and groups. Delete a group Â¶ 0.0.13 1.3.3 4.0.0 For example, to delete a group: Java Python Kotlin Go Raw REST API Delete a group 1 AtlanGroup . delete ( client , \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" ); // (1) To delete a group, you must specify its GUID and can simply call the AtlanGroup.delete() method. Because this operation will remove the group from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a group 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . group . purge ( \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" ) # (1) To delete a group, you must specify its GUID and can simply call the group.purge() method. Delete a group 1 AtlanGroup . delete ( client , \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" ) // (1) To delete a group, you must specify its GUID and can simply call the AtlanGroup.delete() method. Because this operation will remove the group from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Delete a group 1 ctx . GroupClient . Purge ( \"a99f50bc-46bf-4d08-a987-3411ef5cfc33\" ) // (1) To delete a group, you must specify its GUID and can simply call the GroupClient.Purge() method. POST /api/service/groups/e79cb8eb-2bb6-4821-914c-f8dfd21fedc7/delete 1 // (1) All details are in the URL itself. Group ID in the URL Note that you must provide the unique ID (GUID) of the group to delete it. Delete a user Â¶ 0.0.16 Irreversible operation that requires admin user's bearer token Deleting a user is irreversible â€” be certain you want to fully remove the user and all references to the user (their ownership of assets, workflows, and so on) before running this operation. This operation can only be done using an admin user's bearer token, not an API token. Go Delete a user 1 2 3 4 5 6 7 8 response , atlanErr := ctx . UserClient . RemoveUser ( \"test-user-1\" , \"test-user-2\" , \"test-user-3\" , ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error deleting user: %v\" , atlanErr ) // (2) } Call the ctx.UserClient.RemoveUser() method with the following parameters : userName : The username of the user to be removed ( \"test-user-1\" in this example). transferToUserName : The username of the user to whom the assets should be transferred ( \"test-user-2\" in this example). wfCreatorUserName (optional) : The username of the workflow creator ( \"test-user-3\" in this example). If nil , it defaults to transferToUserName . If an error occurs during the deletion, it will be logged. Note: A user can only be removed if they have fewer permissions than an admin. An admin cannot remove another admin. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/users-groups/read/",
    "content": "/api/service/groups (GET) /api/service/groups/{guid}/members (GET) /api/service/users (GET) /api/service/users/{guid}/groups (GET) Retrieving users and groups Â¶ You can retrieve users and groups through different helper methods. Retrieve all groups Â¶ 0.0.13 6.1.0 4.0.0 For example, to retrieve all groups in Atlan: Java Python Kotlin Go Raw REST API Retrieve all groups 1 2 3 4 List < AtlanGroup > groups = AtlanGroup . list ( client ); // (1) for ( AtlanGroup group : groups ) { // (2) // Do something with the group... } You can retrieve all groups in Atlan using the AtlanGroup.list() method. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through the groups to do whatever you like with them. Retrieve all groups 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient client = AtlanClient () groups = client . group . get_all ( # (1) limit = 10 , offset = 1 , sort = \"createdAt\" , columns = [ \"roles\" , \"path\" ]) for group in groups : # (2) # Do something with the group... The get_all() method retrieves all groups defined in Atlan. Returns a GroupResponse object. Optional parameters include: (Optional) limit : Specifies the maximum number of results to return. Defaults to 20 . (Optional) offset : Indicates the starting point for the results when paging. Defaults to 0 . (Optional) sort : Allows sorting by a specific property, such as \"createdAt\" . (Optional) columns : Restricts the fields returned for each group, providing column projection support. Example: [\"roles\", \"path\"] . This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Retrieve all groups 1 2 3 4 val groups = AtlanGroup . list ( client ) // (1) for ( group in groups ) { // (2) // Do something with the group... } You can retrieve all groups in Atlan using the AtlanGroup.list() method. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through the groups to do whatever you like with them. Retrieve all groups 1 2 3 4 5 6 7 8 groups , atlanErr := ctx . GroupClient . GetAll ( // (1) 10 , 1 , \"createdAt\" , ) for _ , group := range groups { // (2) // Do Something with the group... } The GetAll() method retrieves all groups defined in Atlan. Optional parameters include: (Optional) limit : Specifies the maximum number of results to return. Defaults to 20 . (Optional) offset : Indicates the starting point for the results when paging. Defaults to 0 . (Optional) sort : Allows sorting by a specific property, such as \"createdAt\" . You can then iterate through the groups to do whatever you like with them. GET /api/service/v2/groups?sort=createdAt&imit=10&offset=0&columns=path&columns=roles 1 // (1) All details are in the URL itself. Paging results Note that you have a limit to control page size, and an offset to control where to start a page. Retrieve group by name Â¶ 0.0.13 6.1.0 4.0.0 To retrieve a specific group in Atlan by its name: Java Python Kotlin Go Raw REST API Retrieve group by name 1 2 List < AtlanGroup > list = AtlanGroup . get ( client , \"Example\" ); // (1) AtlanGroup group = list . get ( 0 ); // (2) You can retrieve a specific group by its name using the AtlanGroup.get() method. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Still returns a list Note that this still returns a list of groups, as it actually runs a contains search for the specified name. You could therefore use this same method to retrieve many groups that all follow the same naming convention, for example. If you were expecting only a single group to match, however, you can still retrieve that from the list directly, of course. Retrieve group by name 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () groups = client . group . get_by_name ( \"Example\" ) # (1) group = groups . records [ 0 ] # (2) You can retrieve a specific group by its name using the group.get_by_name() method. Returns a GroupResponse object. Still returns a list Note that this still returns a list of groups, as it actually runs a contains search for the specified name. You could therefore use this same method to retrieve many groups that all follow the same naming convention, for example. If you were expecting only a single group to match, however, you can still retrieve that from the list directly, of course. Retrieve group by name 1 2 val list = AtlanGroup . get ( client , \"Example\" ) // (1) val group = list [ 0 ] // (2) You can retrieve a specific group by its name using the AtlanGroup.get() method. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Still returns a list Note that this still returns a list of groups, as it actually runs a contains search for the specified name. You could therefore use this same method to retrieve many groups that all follow the same naming convention, for example. If you were expecting only a single group to match, however, you can still retrieve that from the list directly, of course. Retrieve group by name 1 2 groups , atlanErr := ctx . GroupClient . GetByName ( \"Example\" , 20 , 0 ) // (1) group := groups [ 0 ] // (2) You can retrieve a specific group by its name using the GroupClient.GetByName() method. You can also set the limit (default is 20) and offset (default is 0). Still returns a list Note that this still returns a list of groups, as it actually runs a contains search for the specified name. You could therefore use this same method to retrieve many groups that all follow the same naming convention, for example. If you were expecting only a single group to match, however, you can still retrieve that from the list directly, of course. GET /api/service/groups?filter=%7B%22%24and%22%3A[%7B%22alias%22%3A%7B%22%24ilike%22%3A%22%25Example%25%22%7D%7D]%7D 1 // (1) All details are in the URL itself. URL-encoded filter Note that the filter is URL-encoded. Decoded it would be: {\"$and\":[{\"alias\":{\"$ilike\":\"%Example%\"}}]} Retrieve all users Â¶ 0.0.13 6.1.0 4.0.0 To retrieve all users in Atlan: Java Python Kotlin Go Raw REST API Retrieve all users 1 2 3 4 List < AtlanUser > users = AtlanUser . list ( client ); // (1) for ( AtlanUser user : users ) { // (2) // Do something with the user... } You can retrieve all users in Atlan using the AtlanUser.list() method. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through the users to do whatever you like with them. Retrieve all users 1 2 3 4 5 6 from pyatlan.client.atlan import AtlanClient client = AtlanClient () users = client . user . get_all () # (1) for user in users : # (2) # Do something with the user... You can retrieve all users in Atlan using the get_all() method under the user attribute of the AtlanClient instance. Returns a UserResponse object. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Retrieve all users 1 2 3 4 val users = AtlanUser . list ( client ) // (1) for ( user in users ) { // (2) // Do something with the user... } You can retrieve all users in Atlan using the AtlanUser.list() method. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through the users to do whatever you like with them. Retrieve all users 1 2 3 4 users , atlanErr := ctx . UserClient . GetAll ( 20 , 0 , \"\" ) // (1) for _ , user := range users { // (2) // Do something with the user... } You can retrieve all users in Atlan using the GetAll() method under the user attribute of the AtlanClient instance. You can also set the limit (default is 20), offset (default is 0) and sort (default is by username). You can then iterate through the users to do whatever you like with them. GET /api/service/users?sort=username&limit=100&offset=0 1 // (1) All details are in the URL itself. Paging results Note that you have a limit to control page size, and an offset to control where to start a page. Retrieve user by username Â¶ 0.0.13 1.3.3 4.0.0 To retrieve a specific user in Atlan by their username: Java Python Kotlin Go Raw REST API Retrieve user by username 1 AtlanUser user = AtlanUser . getByUsername ( client , \"jdoe\" ); // (1) You can retrieve a specific user by their username using the AtlanUser.getByUsername() method. This runs an exact match for the provided username, so only returns a single user (if found). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve user by username 1 2 3 4 from pyatlan.client.atlan import AtlanClient client = AtlanClient () user = client . user . get_by_username ( \"jdoe\" ) # (1) You can retrieve a specific user by their username using the user.get_by_username() method. This runs an exact match for the provided username, so only returns a single user (if found). Retrieve user by username 1 val user = AtlanUser . getByUsername ( client , \"jdoe\" ) // (1) You can retrieve a specific user by their username using the AtlanUser.getByUsername() method. This runs an exact match for the provided username, so only returns a single user (if found). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve user by username 1 users , atlanErr := ctx . UserClient . GetByUsername ( \"jdoe\" ) // (1) You can retrieve a specific user by their username using the UserClient.GetByUsername() method. This runs an exact match for the provided username, so only returns a single user (if found). GET /api/service/users?filter=%7B%22username%22%3A%22jdoe%22%7D 1 // (1) All details are in the URL itself. URL-encoded filter Note that the filter is URL-encoded. Decoded it would be: {\"username\":\"jdoe\"} Retrieve user by email Â¶ 0.0.13 6.1.0 4.0.0 To retrieve a specific user in Atlan by their email address: Java Python Kotlin Go Raw REST API Retrieve user by username 1 2 List < AtlanUser > users = AtlanUser . getByEmail ( client , \"@example.com\" ); // (1) AtlanUser user = users . get ( 0 ); // (2) You can retrieve a specific user by their email address using the AtlanUser.getByEmail() method. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Still returns a list Note that this still returns a list of users, as it actually runs a contains search for the specified email address. You could therefore use this same method to retrieve many users that all have the same email domain, for example. If you were expecting only a single user to match, however, you can still retrieve that from the list directly, of course. Retrieve user by username 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () users = client . user . get_by_email ( \"@example.com\" ) # (1) user = users . records [ 0 ] # (2) You can retrieve a specific user by their email address using the user.get_by_email() method. Returns a UserResponse object. Still returns a list Note that this still returns a list of users, as it actually runs a contains search for the specified email address. You could therefore use this same method to retrieve many users that all have the same email domain, for example. If you were expecting only a single user to match, however, you can still retrieve that from the list directly, of course. Retrieve user by username 1 2 val users = AtlanUser . getByEmail ( client , \"@example.com\" ) // (1) val user = users [ 0 ] // (2) You can retrieve a specific user by their email address using the AtlanUser.getByEmail() method. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Still returns a list Note that this still returns a list of users, as it actually runs a contains search for the specified email address. You could therefore use this same method to retrieve many users that all have the same email domain, for example. If you were expecting only a single user to match, however, you can still retrieve that from the list directly, of course. Retrieve user by username 1 2 users , atlanErr := ctx . UserClient . GetByEmail ( \"@example.com\" , 20 , 0 ) // (1) user := users [ 0 ] // (2) You can retrieve a specific user by their email address using the UserClient.GetByEmail() method. Still returns a list Note that this still returns a list of users, as it actually runs a contains search for the specified email address. You could therefore use this same method to retrieve many users that all have the same email domain, for example. If you were expecting only a single user to match, however, you can still retrieve that from the list directly, of course. GET /api/service/users?filter=%7B%22email%22%3A%7B%22%24ilike%22%3A%22%25%40example.com%25%22%7D%7D 1 // (1) All details are in the URL itself. URL-encoded filter Note that the filter is URL-encoded. Decoded it would be: {\"email\":{\"$ilike\":\"%@example.com%\"}} Retrieve multiple users Â¶ 0.0.13 6.1.0 4.0.0 By usernames Â¶ To retrieve multiple users in Atlan by their usernames: Java Python Kotlin Go Raw REST API Retrieve users by usernames 1 2 3 List < AtlanUser > users = client . users . getByUsernames ( List . of ( \"john.doe\" , \"jane.doe\" ) ); // (1) Retrieve users with specified usernames using the users.getByUsernames() method. This method performs an exact match for the provided username in the list. Retrieve users by usernames 1 2 3 4 5 6 from pyatlan.client.atlan import AtlanClient client = AtlanClient () users = client . user . get_by_usernames ([ 'john.doe' , 'jane.doe' ]) # (1) for user in users : # (2) # Do something with the user... Retrieve users with specified usernames using the user.get_by_usernames() method.\nThis method performs an exact match for the provided username in the list. Returns a UserResponse object. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Retrieve users by usernames 1 2 3 val users = client . users . getByUsernames ( listOf ( \"john.doe\" , \"jane.doe\" ) ); // (1) Retrieve users with specified usernames using the users.getByUsernames() method. This method performs an exact match for the provided username in the list. Retrieve users by usernames 1 users , atlanErr := ctx . UserClient . GetByUsernames ([] string { \"john.doe\" , \"jane.doe\" }, 20 , 0 ) // (1) Retrieve users with specified usernames using the UserClient.GetByUsernames() method.\nThis method performs an exact match for the provided username in the list. GET /api/service/users?filter={%22username%22:{%22$in%22:[%22john.doe%22,%22jane.doe%22]}} 1 // (1) All details are in the URL itself. URL-encoded filter Note that the filter is URL-encoded. Decoded it\nwould be: {\"username\":{\"$in\":[\"john.doe\",\"jane.doe\"]}} By emails Â¶ 6.1.0 To retrieve multiple users in Atlan by their emails: Java Python Kotlin Go Raw REST API Retrieve users by emails 1 2 3 List < AtlanUser > users = client . users . getByEmails ( List . of ( \"john@atlan.com\" , \"jane@atlan.com\" ) ); // (1) Retrieve users with specified emails using the users.getByEmails() method. This method performs an exact match for the provided email in the list. Retrieve users by emails 1 2 3 4 5 6 from pyatlan.client.atlan import AtlanClient client = AtlanClient () users = client . user . get_by_emails ([ 'john@atlan.com' , 'jane@atlan.com' ]) # (1) for user in users : # (2) # Do something with the user... Retrieve users with specified emails using the user.get_by_emails() method.\nThis method performs an exact match for the provided email in the list. Returns a UserResponse object. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Retrieve users by emails 1 2 3 val users = client . users . getByEmails ( listOf ( \"john@atlan.com\" , \"jane@atlan.com\" ) ); // (1) Retrieve users with specified emails using the users.getByEmails() method. This method performs an exact match for the provided email in the list. Retrieve users by emails 1 users , atlanErr := ctx . UserClient . GetByEmails ([] string { \"john@atlan.com\" , \"jane@atlan.com\" }, 20 , 0 ) // (1) Retrieve users with specified emails using the UserClient.GetByEmails() method.\nThis method performs an exact match for the provided email in the list. GET /api/service/users?filter={%22email%22:{%22$in%22:[%22john@atlan.com%22,%20%22jane@atlan.com%22]}} 1 // (1) All details are in the URL itself. URL-encoded filter Note that the filter is URL-encoded. Decoded it\nwould be: {\"email\":{\"$in\":[\"john@atlan.com\",\"jane@atlan.com\"]}} Retrieve user group membership Â¶ Retrieve groups for a user Â¶ 0.0.13 2.0.1 4.0.0 To retrieve the groups a user is a member of: Java Python Kotlin Go Raw REST API Retrieve groups for a user 3 4 5 6 GroupResponse response = user . fetchGroups ( client ); // (1) for ( AtlanGroup group : response ) { // (2) // Do something with each group... } You can retrieve the groups the user is a member of using the fetchGroups() method, after you have an AtlanUser object (for example, by first retrieving it). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through the groups the user is a member of. Retrieve groups for a user 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . user . get_groups ( user . id ) # (1) for group in response : # (2) # Do something with each group... You can retrieve the groups the user is a member of using the user.get_groups() method, by providing the GUID of the user. You can then iterate through the groups the user is a member of. Retrieve groups for a user 3 4 5 6 val response = user . fetchGroups ( client ) // (1) for ( group in response ) { // (2) // Do something with each group... } You can retrieve the groups the user is a member of using the fetchGroups() method, after you have an AtlanUser object (for example, by first retrieving it). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through the groups the user is a member of. Retrieve groups for a user 3 4 5 6 response , atlanErr := ctx . UserClient . GetGroups ( user . ID , nil ) // (1) for _ , group := range response { // (2) // Do something with each group... } You can retrieve the groups the user is a member of using the UserClient.GetGroups() method, by providing the GUID of the user. You can then iterate through the groups the user is a member of. GET /api/service/users/f06122f4-7279-4e42-b9e0-46f9b470e659/groups 1 // (1) All details are in the URL itself. User ID in the URL Note that you must provide the unique ID (GUID) of the user to retrieve its associated groups. Retrieve users in a group Â¶ 0.0.13 2.0.1 4.0.0 To retrieve the users that are members of a group: Java Python Kotlin Go Raw REST API Retrieve users in a group 3 4 5 6 UserResponse response = group . fetchUsers ( client ); // (1) for ( AtlanUser user : response ) { // (2) // Do something with each user... } You can retrieve the users a group has as members using the fetchUsers() method, after you have an AtlanGroup object (for example, by first retrieving it). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through the users that are members of the group. Retrieve users in a group 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . group . get_members ( group . id ) # (1) for user in response : # (2) # Do something with each user... You can retrieve the users a group has as members using the group.get_members() method, by providing the GUID of the group. You can then iterate through the users that are members of the group. Retrieve users in a group 3 4 5 6 val response = group . fetchUsers ( client ) // (1) for ( user in response ) { // (2) // Do something with each user... } You can retrieve the users a group has as members using the fetchUsers() method, after you have an AtlanGroup object (for example, by first retrieving it). Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. You can then iterate through the users that are members of the group. Retrieve users in a group 3 4 5 6 response , atlanErr := ctx . GroupClient . GetMembers ( group . ID , nil ) for _ , user := range response { // Do something with each user } You can retrieve the users a group has as members using the GroupClient.GetMembers() method, by providing the GUID of the group. You can then iterate through the users that are members of the group. GET /api/service/groups/e79cb8eb-2bb6-4821-914c-f8dfd21fedc7/members 1 // (1) All details are in the URL itself. Group ID in the URL Note that you must provide the unique ID (GUID) of the group to retrieve its associated members. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/users-groups/create/",
    "content": "/api/service/groups (POST) /api/service/users (POST) Creating users and groups Â¶ Like other objects in the SDK, you can create users and groups using the builder pattern. Create a group Â¶ 0.0.13 1.3.3 4.0.0 For example, to create a group: Java Python Kotlin Go Raw REST API Create a group 1 2 3 AtlanGroup group = AtlanGroup . creator ( \"Example Group\" ) // (1) . build (); // (2) String guid = group . create ( client ); // (3) When creating a group, you must specify at least its name. Like other builder patterns, you build the object to make it ready for creation. To actually create the group in Atlan, call the create() method on the built object. Note that it will return the GUID of the group that was created when successful. Because this operation will persist the group in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a group 1 2 3 4 5 6 from pyatlan.model.group import AtlanGroup from pyatlan.client.atlan import AtlanClient client = AtlanClient () group = AtlanGroup . create ( alias = \"Example Group\" ) # (1) guid = client . group . create ( group ) . group # (2) When creating a group, you must specify at least its name. To actually create the group in Atlan, call the group.create() method with the built object. Note that it will return a minimal object that includes the GUID of the group that was created in the group property. Create a group 1 2 3 val group = AtlanGroup . creator ( \"Example Group\" ) // (1) . build () // (2) val guid = group . create ( client ) // (3) When creating a group, you must specify at least its name. Like other builder patterns, you build the object to make it ready for creation. To actually create the group in Atlan, call the create() method on the built object. Note that it will return the GUID of the group that was created when successful. Because this operation will persist the group in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Create a group 1 2 3 atlanGroup := assets . AtlanGroup {} group , _ := atlanGroup . Create ( \"Example Group\" ) // (1) response , atlanErr := ctx . GroupClient . Create ( group , nil ) // (2) When creating a group, you must specify at least its name. To actually create the group in Atlan, call the GroupClient.Create() method with the built object. Note that it will return a minimal object that includes the GUID of the group that was created in the group property. POST /api/service/groups 1 2 3 4 5 6 7 8 9 10 11 12 13 { \"group\" : { // (1) \"attributes\" : { // (2) \"alias\" : [ // (3) \"Example Group\" ], \"isDefault\" : [ // (4) \"false\" ] }, \"name\" : \"example_group\" // (5) } } All the details for the group must be wrapped in a group object. The details of the group are further nested within an attributes object. Provide an alias for the group, which will be the name that appears in the UI. Note that it must be specified as an array, even though there is only a single value. Specify whether the group should be applied to all new users (true) or not (false). Note that this must be specified as a string within an array. Provide an internal name for the group. Must follow certain constraints The internal name for the group must be unique, all lowercase, and include only alphanumeric characters and the _ (no spaces or special characters). Create a user Â¶ To create a user, what you're really doing is inviting them. The users will need to verify their email address to activate their account, and will be able to specify their own password as part of that process. 0.0.13 2.0.4 4.0.0 To invite a user: Java Python Kotlin Go Raw REST API Invite a user 1 2 3 4 5 AtlanUser user = AtlanUser . creator ( \"someone@somewhere.org\" , // (1) \"$member\" ) // (2) . build (); // (3) user . create ( client ); // (4) When inviting a user, you must specify at least their email address... ...and the workspace role you want to give that user (one of $guest , $member , or $admin ). Like other builder patterns, you build the object to make it ready for creation. To actually invite the user to Atlan, call the create() method on the built object. Note that this does not return any information. Because this operation will persist the user in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Invite a user 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.model.user import AtlanUser from pyatlan.client.atlan import AtlanClient client = AtlanClient () users = [ AtlanUser . create ( email = \"someone@somewhere.org\" , # (1) role_name = \"$member\" # (2) ) ] client . user . create ( users , return_info = False ) # (3) When inviting a user, you must specify at least their email address... ...and the workspace role you want to give that user (one of $guest , $member , or $admin ). To invite the user to Atlan, simply call the user.create() method with the built object.\nNote that if return_info is set to True , this will return a list\ncontaining details of the created users; otherwise, it will return None . Invite a user 1 2 3 4 5 val user = AtlanUser . creator ( \"someone@somewhere.org\" , // (1) \"\\ $ member \" ) // (2) . build () // (3) user . create ( client ) // (4) When inviting a user, you must specify at least their email address... ...and the workspace role you want to give that user (one of \\$guest , \\$member , or \\$admin ). Like other builder patterns, you build the object to make it ready for creation. To actually invite the user to Atlan, call the create() method on the built object. Note that this does not return any information. Because this operation will persist the user in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Invite a user 1 2 3 4 5 6 7 users := [] assets . AtlanUser { { Email : \"someone@somewhere.org\" , // (1) WorkspaceRole : \"$member\" , // (2) }, } ctx . UserClient . CreateUsers ( users , false ) // (3) When inviting a user, you must specify at least their email address... ...and the workspace role you want to give that user (one of $guest , $member , or $admin ). To invite the user to Atlan, simply call the UserClient.CreateUsers() method with the built object.\nNote that if returnInfo is set to True , this will return a list\ncontaining details of the created users; otherwise, it will return None . POST /api/service/users 1 2 3 4 5 6 7 8 9 { \"users\" : [ // (1) { \"email\" : \"someone@somewhere.org\" , // (2) \"roleName\" : \"$member\" , // (3) \"roleId\" : \"be5f0df7-dab8-4107-8bf0-56ce7131623a\" // (4) } ] } All user details must be wrapped in a users array, where each object in the array defines a single user. You must provide a valid email address for each user. Atlan will send an invitation to this email address. You must specify the role for the user, one of: $admin , $member , or $guest . You must also provide the unique ID (GUID) of the role. You probably need to look this up first When using the raw API, you will need to lookup the role GUID yourself. You can GET /api/service/roles , and the GUID will be the id field in the response for each role. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/",
    "content": "Packages and workflows introduction Â¶ In Atlan, packages define the workflows you can run to retrieve metadata from various sources. Workflows run asynchronously This means the helper method to run a workflow will return immediately, before the workflow itself has finished running. If you want to wait until the workflow is finished you'll need to use other helper methods to check the status and wait accordingly. Supported packages Explore the list of individual packages currently supported through our SDKs .\nEach package section includes examples demonstrating how to build a workflow from scratch and execute it on Atlan. Block until workflow completion Â¶ 6.0.3 4.0.0 To block until the workflow has completed running: Java Python Kotlin Block until workflow has completed 1 2 3 ... WorkflowResponse response = workflow . run ( client ); // (1) AtlanWorkflowPhase state = response . monitorStatus ( log ); // (2) Every package returns a Workflow object, from which you can run() the workflow. This call will return almost immediately with some metadata about the workflow run â€” it will not wait until the workflow has completed running. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. There is a monitorStatus() method on the response of a workflow run that you can use to wait until the workflow has completed. When this method finally returns, it will give the state of the workflow when it completed (for example, success or failure). The method comes in two variations: one that takes an slf4j logger (in this example) and will log its status periodically and another that takes no arguments and does not do any logging Block until workflow has completed 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import logging from pyatlan.client.atlan import AtlanClient client = AtlanClient () LOGGER = logging . getLogger ( __name__ ) LOGGER . setLevel ( logging . DEBUG ) ... response = client . workflow . run ( workflow ) # (1) state = client . workflow . monitor ( # (2) workflow_response = response , logger = LOGGER , workflow_name = \"atlan-snowflake-1744600804\" # (3) ) Each package returns a Workflow object, which you can subsequently pass to the run() method of the workflow client. This call will return almost immediately with some metadata\nabout the workflow run â€” it will not wait until the workflow has completed running. Use the monitor() method on the workflow client to wait until the workflow\nhas completed. When this method returns, it provides the final state of the workflow,\nindicating whether it was successful or failed. The method comes in two variations: one that takes a logger (in this example) and will log its status periodically. and another that takes no arguments and does not do any logging. You can now monitor any existing workflow directly by specifying its workflow_name ,  as displayed in the Atlan UI. In this case, you only need to pass the workflow_name as a parameter to the monitor() methodâ€”no need for a workflow_response . Block until workflow has completed 1 2 3 ... val response = workflow . run ( client ) // (1) val state = response . monitorStatus ( log ) // (2) Every package returns a Workflow object, from which you can run() the workflow. This call will return almost immediately with some metadata about the workflow run â€” it will not wait until the workflow has completed running. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. There is a monitorStatus() method on the response of a workflow run that you can use to wait until the workflow has completed. When this method finally returns, it will give the state of the workflow when it completed (for example, success or failure). The method comes in two variations: one that takes an slf4j logger (in this example) and will log its status periodically and another that takes no arguments and does not do any logging 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/users-groups/update/",
    "content": "/api/service/groups/{guid} (POST) /api/service/groups/{guid}/members/remove (POST) /api/service/users/{guid} (POST) /api/service/users/{guid}/groups (POST) /api/service/users/{guid}/update (POST) Updating users and groups Â¶ You can update basic properties of both users and groups, again using the builder pattern. Update a group Â¶ 0.0.13 1.3.3 4.0.0 For example, to update a group: Java Python Kotlin Go Raw REST API Update a group 1 2 3 4 5 6 7 8 AtlanGroup group = AtlanGroup . updater ( // (1) \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" , // (2) \"/example_group\" ) // (3) . attributes ( AtlanGroup . GroupAttributes . builder () // (4) . description ( List . of ( \"Now with a description!\" )) // (5) . build ()) // (6) . build (); // (7) group . update ( client ); // (8) To update a group, start a builder using the updater() method. You must provide the GUID of the group... ...and the path of the group you want to update. (Note that the path is different from the name â€” you're best retrieving a group first and then getting the path from that retrieved object if you are unsure.) You can then specify anything you want to update. In the case of a group, most of the properties are in an embedded attributes object that can be built-up through its own builder. For example, you can add or change the description of the group. (Note that all objects in the attributes of a group are lists, even when they only have a single value.) Like other builder patterns, you need to build the attributes object. Like other builder patterns, you need to build the updated group object itself. Finally, you can call the update() method on the built-up group object to actually update the group in Atlan. Note that this method does not return anything. Because this operation will persist the group in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update a group 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.model.group import AtlanGroup from pyatlan.client.atlan import AtlanClient client = AtlanClient () group = AtlanGroup . create_for_modification ( # (1) guid = \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" , # (2) path = \"/example_group\" # (3) ) group . attributes = AtlanGroup . Attributes ( # (4) description = [ \"Now with a description!\" ] # (5) ) client . group . update ( group ) # (6) To update a group, you could start by retrieving the group. Alternatively, you can use AtlanGroup.create_for_modification() to start building a minimal update request. You must provide the GUID of the group... ...and the path of the group you want to update. (Note that the path is different from the name â€” you're best retrieving a group first and then getting the path from that retrieved object if you are unsure.) You can then specify anything you want to update. In the case of a group, most of the properties are in an embedded Attributes class that can be built-up. For example, you can add or change the description of the group. (Note that all objects in the attributes of a group are lists, even when they only have a single value.) Finally, you can call the group.update() method with the built-up group object to actually update the group in Atlan. Note that this method does not return anything. Update a group 1 2 3 4 5 6 7 8 val group = AtlanGroup . updater ( // (1) \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" , // (2) \"/example_group\" ) // (3) . attributes ( AtlanGroup . GroupAttributes . builder () // (4) . description ( listOf ( \"Now with a description!\" )) // (5) . build ()) // (6) . build () // (7) group . update ( client ) // (8) To update a group, start a builder using the updater() method. You must provide the GUID of the group... ...and the path of the group you want to update. (Note that the path is different from the name â€” you're best retrieving a group first and then getting the path from that retrieved object if you are unsure.) You can then specify anything you want to update. In the case of a group, most of the properties are in an embedded attributes object that can be built-up through its own builder. For example, you can add or change the description of the group. (Note that all objects in the attributes of a group are lists, even when they only have a single value.) Like other builder patterns, you need to build the attributes object. Like other builder patterns, you need to build the updated group object itself. Finally, you can call the update() method on the built-up group object to actually update the group in Atlan. Note that this method does not return anything. Because this operation will persist the group in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update a group 1 2 3 4 5 6 7 8 AtlanGroup := assets . AtlanGroup {} group , atlanErr := AtlanGroup . Updater ( // (1) \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" , // (2) \"/example_group\" , // (3) ) description := [] string { \"Now with a description!\" } group . Attributes . Description = description // (4) ctx . GroupClient . Update ( group ) // (5) To update a group, you could start by retrieving the group. Alternatively, you can use AtlanGroup.Updater() to start building a minimal update request. You must provide the GUID of the group... ...and the path of the group you want to update. (Note that the path is different from the name â€” you're best retrieving a group first and then getting the path from that retrieved object if you are unsure.) You can then specify anything you want to update. In the case of a group, most of the properties are in an embedded Attributes class that can be built-up. For example, you can add or change the description of the group. (Note that all objects in the attributes of a group are lists, even when they only have a single value.) Finally, you can call the GroupClient.Update() method with the built-up group object to actually update the group in Atlan. Note that this method does not return anything. POST /api/service/groups/e79cb8eb-2bb6-4821-914c-f8dfd21fedc7 1 2 3 4 5 6 7 8 9 10 11 12 { \"id\" : \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" , // (1) \"path\" : \"/example_group\" , // (2) \"attributes\" : { // (3) \"description\" : [ \"Now with a description!\" ], \"isDefault\" : [ \"false\" ] } } You must provide the GUID of the group within the request payload. You must provide the internal name of the group, prefixed by / , as the path . You can provide any attributes to update on the group in the attributes object. Values are all arrays of strings Note that every value for an attribute is an array of strings, even when there is only a single value. Remove users from group Â¶ 0.0.13 1.3.3 4.0.0 To remove one or more users from a group: Java Python Kotlin Go Raw REST API Remove users from a group 1 2 3 4 5 AtlanGroup group = AtlanGroup . updater ( // (1) \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" , // (2) \"/example_group\" ) // (3) . build (); // (4) group . removeUsers ( client , List . of ( \"da213751-95de-4f96-8bee-a2c73e2ef8c8\" )); // (5) To update group membership, start a builder using the updater() method. You must provide the GUID of the group... ...and the path of the group you want to update. (Note that the path is different from the name â€” you're best retrieving a group first and then getting the path from that retrieved object if you are unsure.) Like other builder patterns, you need to build the updated group object itself. Use the removeUsers() method to remove one or more users from the group. Specify the GUID of each user you want to remove as a member of the group. Because this operation will persist the group in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Remove users from a group 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . group . remove_users ( # (1) guid = \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" , # (2) user_ids = [ \"da213751-95de-4f96-8bee-a2c73e2ef8c8\" ] # (3) ) Use the group.remove_users() method to remove one or more users from the group. Specify the GUID of the group from which you want to remove users. Specify the GUID of each user you want to remove as a member of the group. Remove users from a group 1 2 3 4 5 val group = AtlanGroup . updater ( // (1) \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" , // (2) \"/example_group\" ) // (3) . build () // (4) group . removeUsers ( client , listOf ( \"da213751-95de-4f96-8bee-a2c73e2ef8c8\" )) // (5) To update group membership, start a builder using the updater() method. You must provide the GUID of the group... ...and the path of the group you want to update. (Note that the path is different from the name â€” you're best retrieving a group first and then getting the path from that retrieved object if you are unsure.) Like other builder patterns, you need to build the updated group object itself. Use the removeUsers() method to remove one or more users from the group. Specify the GUID of each user you want to remove as a member of the group. Because this operation will persist the group in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Remove users from a group 1 2 3 4 ctx . GroupClient . RemoveUsers ( // (1) \"a99f50bc-46bf-4d08-a987-3411ef5cfc33\" , // (2) [] string { \"b060a754-4d16-4e13-b5a8-ba42f10aee39\" }, // (3) ) Use the GroupClient.RemoveUsers() method to remove one or more users from the group. Specify the GUID of the group from which you want to remove users. Specify the GUID of each user you want to remove as a member of the group. POST /api/service/groups/e79cb8eb-2bb6-4821-914c-f8dfd21fedc7/members/remove 1 2 3 4 5 { \"users\" : [ // (1) \"da213751-95de-4f96-8bee-a2c73e2ef8c8\" // (2) ] } You must provide the list of users to remove from the group in a users array. Specify each user by its unique ID (GUID). Update a user Â¶ 0.0.13 1.3.3 1.0.0 To update a user, begin by building the minimal update object: Java Python Kotlin Go Raw REST API Build the minimal update object 1 2 3 AtlanUser user = AtlanUser . updater ( // (1) \"da213751-95de-4f96-8bee-a2c73e2ef8c8\" ) // (2) . build (); // (3) To update a user, start a builder using the updater() method. You must provide the GUID of the user. Like other builder patterns, you need to build the updated user object itself. Specific operations below The specific operations for updating a user are all listed below - there is no update object to build in the Python SDK. Build the minimal update object 1 2 3 val user = AtlanUser . updater ( // (1) \"da213751-95de-4f96-8bee-a2c73e2ef8c8\" ) // (2) . build () // (3) To update a user, start a builder using the updater() method. You must provide the GUID of the user. Like other builder patterns, you need to build the updated user object itself. Specific operations below The specific operations for updating a user are all listed below - there is no update object to build in the Go SDK. Implicit in the API calls below There is nothing specific to do for this step when using the raw APIs â€” constructing the object is simply what you place in the payload of the API calls in the steps below. Add user to groups Â¶ 0.0.13 1.3.3 4.0.0 Once you have the update object, to add a user to one or more groups: Java Python Kotlin Go Raw REST API Add user to groups 4 user . addToGroups ( client , List . of ( \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" )); // (1) Use the addToGroups() method to add the user to one or more groups. Specify the GUID of each group you want to make the user a member of. Because this operation will persist the user in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add user to groups 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . user . add_to_groups ( # (1) guid = \"da213751-95de-4f96-8bee-a2c73e2ef8c8\" , # (2) group_ids = [ \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" ] # (3) ) Use the user.add_to_groups() method to add the user to one or more groups. Specify the GUID of the user you want to add to one or more groups. Specify the GUID of each group you want to make the user a member of. Add user to groups 4 user . addToGroups ( client , listOf ( \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" )) // (1) Use the addToGroups() method to add the user to one or more groups. Specify the GUID of each group you want to make the user a member of. Because this operation will persist the user in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Add user to groups 1 2 3 4 ctx . UserClient . AddUserToGroups ( // (1) \"b060a754-4d16-4e13-b5a8-ba42f10aee39\" , // (2) [] string { \"a99f50bc-46bf-4d08-a987-3411ef5cfc33\" }, // (3) ) Use the UserClient.AddUserToGroups() method to add the user to one or more groups. Specify the GUID of the user you want to add to one or more groups. Specify the GUID of each group you want to make the user a member of. POST /api/service/users/da213751-95de-4f96-8bee-a2c73e2ef8c8/groups 1 2 3 4 5 { \"groups\" : [ // (1) \"e79cb8eb-2bb6-4821-914c-f8dfd21fedc7\" // (2) ] } You must provide the list of groups to remove the user from in a groups array. Specify each group by its unique ID (GUID). Change role of user Â¶ 0.0.13 6.0.0 4.0.0 Once you have the update object, to change the role of a user: Java Python Kotlin Go Raw REST API Change role of user 4 user . changeRole ( client , client . getRoleCache (). getIdForName ( \"$guest\" )); // (1) Use the changeRole() method to change the role of a user. Because this operation will persist the user in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Use the RoleCache to find the right GUID The changeRole() method requires the GUID of the role you want to move the user to. In order to find that GUID, you can use the RoleCache.getIdForName() and provide the name of the role. Change role of user 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . user . change_role ( # (1) guid = \"da213751-95de-4f96-8bee-a2c73e2ef8c8\" , # (2) role_id = client . role_cache . get_id_for_name ( \"$guest\" ) # (3) ) Use the user.change_role() method to change the role of a user. Specify the GUID of the user whose role you want to change. Specify the GUID of the role you want to change the user to. Use the RoleCache to find the right GUID The user.change_role() method requires the GUID of the role you want to move the user to. In order to find that GUID, you can use the RoleCache.get_id_for_name() and provide the name of the role. Change role of user 4 user . changeRole ( client , client . roleCache . getIdForName ( \"\\$guest\" )) // (1) Use the changeRole() method to change the role of a user. Because this operation will persist the user in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Use the RoleCache to find the right GUID The changeRole() method requires the GUID of the role you want to move the user to. In order to find that GUID, you can use the RoleCache.getIdForName() and provide the name of the role. Change role of user 1 2 3 4 5 roleID , atlanErr := assets . GetRoleIDForRoleName ( \"$guest\" ) ctx . UserClient . ChangeUserRole ( // (1) \"b060a754-4d16-4e13-b5a8-ba42f10aee39\" , // (2) roleID , // (3) ) Use the UserClient.ChangeUserRole() method to change the role of a user. Specify the GUID of the user whose role you want to change. Specify the GUID of the role you want to change the user to. Use the RoleCache to find the right GUID The UserClient.ChangeUserRole() method requires the GUID of the role you want to move the user to. In order to find that GUID, you can use the assets.GetRoleIDForRoleName() and provide the name of the role. POST /api/service/users/da213751-95de-4f96-8bee-a2c73e2ef8c8/update 1 2 3 { \"roleId\" : \"0d1c39de-7323-4490-98d9-43240307eea7\" // (1) } You must provide the unique ID (GUID) of the new role for the user. You probably need to look this up first When using the raw API, you will need to lookup the role GUID yourself. You can GET /api/service/roles , and the GUID will be the id field in the response for each role. Deactivate a user Â¶ This cannot be done programmatically You can only deactivate users as an Admin user (via the UI), API tokens do not have access to deactivate users. Reactivate a user Â¶ This cannot be done programmatically You can only reactivate users as an Admin user (via the UI), API tokens do not have access to reactivate users. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/manage/schedules/",
    "content": "Manage workflow schedules Schedule a workflow run Â¶ 0.0.16 2.1.8 Directly on run: Â¶ You can directly add a schedule to a workflow run.\nFor example, with Snowflake Miner : Java Python Kotlin Go Coming soon Add schedule directly on run 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import SnowflakeMiner from pyatlan.model.workflow import WorkflowSchedule client = AtlanClient () miner = ( # (1) SnowflakeMiner ( connection_qualified_name = \"default/snowflake/1234567890\" ) . s3 ( s3_bucket = \"test-s3-bucket\" , s3_prefix = \"test-s3-prefix\" , s3_bucket_region = \"test-s3-bucket-region\" , sql_query_key = \"TEST_QUERY\" , default_database_key = \"TEST_SNOWFLAKE\" , default_schema_key = \"TEST_SCHEMA\" , session_id_key = \"TEST_SESSION_ID\" , ) . popularity_window ( days = 15 ) . native_lineage ( enabled = True ) . custom_config ( config = { \"test\" : True , \"feature\" : 1234 }) . to_workflow () ) schedule = WorkflowSchedule ( cron_schedule = \"45 5 * * *\" , timezone = \"Europe/Paris\" , ) # (2) response = client . workflow . run ( workflow = miner , workflow_schedule = schedule ) # (3) Begin by constructing the Snowflake miner workflow. To create a new schedule for the workflow, specify: cron schedule expression , \nfor example: 45 5 * * * (scheduled for tomorrow at 04:05:00 ). time zone for the cron schedule, such as Europe/Paris . Finally, use the client.workflow.run() method to add this new schedule.\nIt will both add the schedule and execute the workflow in Atlan. Coming soon Add schedule directly on run 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 miner := assets . NewSnowflakeMiner ( \"default/snowflake/1234567890\" ). // (1) S3 ( \"test-s3-bucket\" , \"test-s3-prefix\" , \"TEST_QUERY\" , \"TEST_SNOWFLAKE\" , \"TEST_SCHEMA\" , \"TEST_SESSION_ID\" , structs . StringPtr ( \"test-s3-bucket-region\" ), ). PopularityWindow ( 30 ). NativeLineage ( true ). CustomConfig ( map [ string ] interface {}{ \"test\" : true , \"feature\" : 1234 , }). ToWorkflow () Schedule := structs . WorkflowSchedule { CronSchedule : \"45 5 * * *\" , Timezone : \"Europe/Paris\" } // (2) response , atlanErr := ctx . WorkflowClient . Run ( miner , & Schedule ) // (3) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } Begin by constructing the Snowflake miner workflow. To create a new schedule for the workflow, specify: cron schedule expression , \nfor example: 45 5 * * * (scheduled for tomorrow at 04:05:00 ). time zone for the cron schedule, such as Europe/Paris . Finally, use the ctx.WorkflowClient.Run() method to add this new schedule.\nIt will both add the schedule and execute the workflow in Atlan. Existing workflow: Â¶ You can also add a schedule to an existing workflow run: Java Python Kotlin Go Coming soon Schedule an existing workflow run 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 from pyatlan.client.atlan import AtlanClient from pyatlan.model.workflow import WorkflowSchedule client = AtlanClient () existing_workflow = client . workflow . find_by_type ( prefix = WorkflowPackage . SNOWFLAKE_MINER )[ 0 ] # (1) schedule = WorkflowSchedule ( cron_schedule = \"45 5 * * *\" , timezone = \"Europe/Paris\" , ) # (2) response = client . workflow . add_schedule ( workflow = existing_workflow , workflow_schedule = schedule ) # (3) You can retrieve workflows based on their type (prefix). Note: Only workflows that have been run will be found. To create a new schedule for an existing workflow, provide: cron schedule expression , e.g: 45 5 * * * (tomorrow at 04:05:00 ). time zone for the cron schedule, e.g: Europe/Paris . Finally, to apply this new schedule to the existing workflow,\nuse client.workflow.add_schedule() method. Coming soon Schedule an existing workflow run 1 2 3 existingWorkflow , _ := ctx . WorkflowClient . FindByType ( atlan . WorkflowPackageSnowflakeMiner , 1 ) // (1) Schedule := structs . WorkflowSchedule { CronSchedule : \"45 5 * * *\" , Timezone : \"Europe/Paris\" } // (2) response , atlanErr := ctx . WorkflowClient . AddSchedule ( existingWorkflow [ 0 ], & Schedule ) You can retrieve workflows based on their type (prefix). (You can also specify the maximum number of resulting\nworkflows you want to retrieve as results.) Note: Only workflows that have been run will be found. To create a new schedule for an existing workflow, provide: cron schedule expression , e.g: 45 5 * * * (tomorrow at 04:05:00 ). time zone for the cron schedule, e.g: Europe/Paris . Finally, to apply this new schedule to the existing workflow,\nuse ctx.WorkflowClient.AddSchedule() method. Note: The AddSchedule() method returns an error in case of failure, manage it accordingly by returning it. Retrieve a scheduled workflow run Â¶ 0.0.16 2.1.8 Retrieve by name: Â¶ To retrieve an existing scheduled workflow run by its name: Java Python Go Coming soon Retrieve scheduled workflow run by its name 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . workflow . get_scheduled_run ( workflow_name = \"atlan-snowflake-miner-1714638976\" ) # (1) To retrieve an existing scheduled workflow runs, specify: name of the workflow as displayed in the\nUI (e.g: atlan-snowflake-miner-1714638976 ). Retrieve scheduled workflow run by its name 1 2 3 4 response , atlanErr := ctx . WorkflowClient . GetScheduledRun ( \"atlan-snowflake-miner-1714638976\" ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } To retrieve an existing scheduled workflow runs, specify: name of the workflow as displayed in the\nUI (e.g: atlan-snowflake-miner-1714638976 ). Retrieve all scheduled runs: Â¶ To retrieve all existing scheduled workflow runs: Java Python Kotlin Go Coming soon Retrieve all scheduled workflow runs 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . workflow . get_all_scheduled_runs () # (1) To retrieve all scheduled runs for workflows,\nuse client.workflow.get_all_scheduled_runs() method. Coming soon Retrieve all scheduled workflow runs 1 response , atlanErr := ctx . WorkflowClient . GetAllScheduledRuns () // (1) To retrieve all scheduled runs for workflows,\nuse ctx.WorkflowClient.GetAllScheduledRuns() method. Remove a schedule from workflow run Â¶ 0.0.16 2.1.8 To remove a schedule from an existing workflow run: Java Python Kotlin Go Coming soon Remove a schedule from an existing workflow run 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient client = AtlanClient () existing_workflow = client . workflow . find_by_type ( prefix = WorkflowPackage . SNOWFLAKE_MINER )[ 0 ] # (1) response = client . workflow . remove_schedule ( workflow = existing_workflow ) # (2) You can retrieve workflows based on their type (prefix). Note: Only workflows that have been run will be found. Finally, to remove a schedule from this workflow,\nuse the client.workflow.remove_schedule() method. Coming soon Remove a schedule from an existing workflow run 1 2 existingWorkflow , _ := ctx . WorkflowClient . FindByType ( atlan . WorkflowPackageSnowflakeMiner , 1 ) // (1) response , atlanErr := ctx . WorkflowClient . RemoveSchedule ( existingWorkflow [ 0 ]) // (2) You can retrieve workflows based on their type (prefix). (You can also specify the maximum number of resulting\nworkflows you want to retrieve as results.) Note: Only workflows that have been run will be found. Finally, to remove a schedule from this workflow,\nuse the ctx.WorkflowClient.RemoveSchedule() method. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/manage/workflows/",
    "content": "Manage workflows Â¶ Retrieve workflow Â¶ By ID Â¶ 0.0.16 2.3.1 4.0.0 Retrieve an existing workflow by its ID: Java Python Kotlin Go Raw REST API Retrieve workflows by its type 1 2 WorkflowSearchResult result = WorkflowSearchRequest // (1) . findById ( client , \"atlan-snowflake-miner-1714638976\" ); // (2) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their ID using the findById() helper method and providing the ID for one of the packages. In this example, we're retrieving a specific Snowflake miner package. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve workflow by its ID 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () result = client . workflow . find_by_id ( # (1) id = \"atlan-snowflake-miner-1714638976\" ) You can find a workflow by its identifier using the find_by_id() method\nof the workflow client, providing the id for the specific workflow.\nIn this example, we're retrieving the SnowflakeMiner workflow. Retrieve workflows by its type 1 2 val result = WorkflowSearchRequest // (1) . findById ( client , \"atlan-snowflake-miner-1714638976\" ) // (2) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their ID using the findById() helper method and providing the ID for one of the packages. In this example, we're retrieving a specific Snowflake miner package. Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve workflow by its ID 1 2 3 4 result , atlanErr := ctx . WorkflowClient . FindByID ( \"atlan-snowflake-miner-1714638976\" ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } You can find a workflow by its identifier using the FindByID() method\nof the workflow client, providing the id for the specific workflow.\nIn this example, we're retrieving the SnowflakeMiner workflow. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 { \"from\" : 0 , \"size\" : 1 , \"track_total_hits\" : true , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"bool\" : { \"must\" : [ { \"term\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-snowflake-miner-1714638976\" // (1) } } } ] } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"order\" : \"desc\" , \"nested\" : { \"path\" : \"metadata\" } } } ] } You can find a workflow by its identifier. In this example, we're retrieving the SnowflakeMiner workflow. By type Â¶ 0.0.16 1.9.5 4.0.0 Retrieve existing workflows by its type: Java Python Kotlin Go Raw REST API Retrieve workflows by its type 1 2 List < WorkflowSearchResult > results = WorkflowSearchRequest // (1) . findByType ( client , SnowflakeMiner . PREFIX , 5 ); // (2) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SnowflakeMiner . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve workflows by its type 1 2 3 4 5 6 7 8 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () results = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . SNOWFLAKE_MINER , max_results = 5 ) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the SnowflakeMiner .\n(You can also specify the maximum number of resulting\nworkflows you want to retrieve as results.) Retrieve workflows by its type 1 2 var results = WorkflowSearchRequest // (1) . findByType ( client , SnowflakeMiner . PREFIX , 5 ); // (2) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SnowflakeMiner . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Because this operation will retrieve information from Atlan, you must provide it an AtlanClient through which to connect to the tenant. Retrieve workflows by its type 1 2 3 4 result , atlanErr := ctx . WorkflowClient . FindByType ( atlan . WorkflowPackageSnowflakeMiner , 5 ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } You can find workflows by their type using the workflow client FindByType() method and providing the prefix for one of the packages.\nIn this example, we do so for the SnowflakeMiner .\n(You can also specify the maximum number of resulting\nworkflows you want to retrieve as results.) POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , // (1) \"track_total_hits\" : true , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"regexp\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan[-]snowflake[-]miner[-][0-9]{10}\" } // (2) } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"order\" : \"desc\" , \"nested\" : { \"path\" : \"metadata\" } } } ] } Specify the maximum number of resulting workflows you want to retrieve as results. In this example, we do so for the SnowflakeMiner with regexp: atlan[-]snowflake[-]miner[-][0-9]{10} . Create workflow credentials Â¶ 4.2.1 To create workflow credentials for example, for Snowflake : Java Python Kotlin Raw REST API Coming soon Create workflow credentials 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from pyatlan.client.atlan import AtlanClient from pyatlan.model.credential import Credential client = AtlanClient () snowflake_credential = Credential () # (1) snowflake_credential . name = \"snowflake-credential\" # (2) snowflake_credential . connector_config_name = \"atlan-connectors-snowflake\" # (3) snowflake_credential . connector = \"snowflake\" # (4) snowflake_credential . auth_type = \"basic\" # (5) snowflake_credential . username = \"username\" # (6) snowflake_credential . password = \"password\" snowflake_credential . extras = { \"role\" : \"role-here\" , \"warehouse\" : \"warehouse-here\" , } snowflake_credential . host = \"test-host\" # (7) snowflake_credential . port = 1234 response = client . credentials . creator ( credential = snowflake_credential , test = True # (8) ) Initialize the credential object for credential creation. You must provide a name for the credential being created. You must specify the connector_config_name for the credential. You must specify the connector name for the credential. You must specify the authentication type of the credential. You can provide the sensitive details such as the username , password , and extras when creating credentials. This behavior aligns with the Atlan workflow config. You can specify the host and port being used. To create workflow credentials using the creator() method. You need to provide the below params: credential : the credential object is passed to create new credentials in Atlan. For example, in this case, snowflake_credential serves as the credential object. test : specify whether to validate the credentials ( True ) or skip validation ( False ) before creations. Defaults to True Coming soon POST /api/service/credentials?testCredential=true 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"authType\" : \"basic\" , // (1) \"name\" : \"snowflake-credential\" , //(2) \"connector\" : \"snowflake\" , // (3) \"connectorConfigName\" : \"atlan-connectors-snowflake\" , // (4) \"username\" : \"username\" , // (5) \"password\" : \"password\" , \"extra\" : { \"role\" : \"role-here\" , \"warehouse\" : \"warehouse-here\" , }, \"host\" : \"test-host\" , // (6) \"port\" : 1234 , } You must specify the authType for the credential. You must provide a Human-readable name for your credential. You must specify the connector for the credential. You must specify the connectorConfigName for the credential. You can provide the sensitive details such as the username , password , and extras when creating credentials. You can specify the host and port being used. Retrieve all workflow credentials Â¶ 6.0.3 To retrive all workflow credentials for example, for Snowflake : Java Python Kotlin Raw REST API Coming soon Retrieve all workflow credentials 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . credentials . get_all ( #(1) filter = { \"connector\" : \"snowflake\" }, limit = 5 , offset = 1 , workflow_name = \"atlan-bigquery-1735837155\" , ) To retrieve workflow credentials using the get_all() method. When run without any parameters, it returns all existing records. You can also use following optional parameters to filter, limit, or paginate through the results: (Optional) filter : filters records based on specific key-value criteria, such as {\"connector\": \"snowflake\"} returns credentials for workflows using the snowflake connector. (Optional) limit : restricts the maximum number of records returned in a single call, for example, limit=5 retrieves up to 5 records only. (Optional) offset : skips a specified number of records before starting retrieval, such as offset=10 to skip the first 10 records and retrieve from the 11th onward. (Optional) workflow_name : retrieves credentials for a specific workflow. The name should match the workflow name as shown in the Atlan UI. Coming soon GET api/service/credentials?filter=%7B%22name%22%3A%22atlan-snowflake-17891%22%7D&limit=1&offset=1 1 //(1) All details are in the URL itself. URL-encoded filter Note that the filter is URL-encoded. Decoded it would be: {\"name\":\"atlan-snowflake-17891\"} Update workflow source credentials Â¶ 1.8.4 4.0.0 To update workflow source credentials for example, for Snowflake : Java Python Kotlin Raw REST API Update workflow source credentials 1 2 3 4 5 6 7 8 9 10 11 Credential snowflakeCredential = client . credentials . get ( // (1) \"972a87c1-28d7-8bf2-896d-ea5bd3e9c691\" ). toCredential () . authType ( \"basic\" ) // (2) . username ( \"username\" ) // (3) . password ( \"password\" ) . extra ( \"role\" , \"role-here\" ) . extra ( \"warehouse\" , \"warehouse-here\" ) . build () // (4) CredentialResponse response = snowflakeCredential . update ( client ) // (5) You can retrieve the workflow credential object by providing its GUID . You must specify the authentication type of the credential. You must provide the sensitive details such as the username , password , and extra when updating credentials. This behavior aligns with the Atlan workflow config update UI. Build the minimal Credential object. Now, use the update() method of the Credential object to update this new credentials in Atlan after initially testing it for successful validation. Because this operation will update details in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Update workflow source credentials 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from pyatlan.client.atlan import AtlanClient client = AtlanClient () snowflake_credential = client . credentials . get ( guid = \"972a87c1-28d7-8bf2-896d-ea5bd3e9c691\" ) . to_credential () # (1) # Basic Authentication snowflake_credential . auth_type = \"basic\" # (2) snowflake_credential . username = \"username\" # (3) snowflake_credential . password = \"password\" snowflake_credential . extras = { \"role\" : \"role-here\" , \"warehouse\" : \"warehouse-here\" , } response = client . credentials . test_and_update ( # (4) credential = snowflake_credential ) You can retrieve the workflow credential object by providing its GUID . You must specify the authentication type of the credential. You must provide the sensitive details such as the username , password , and extras when updating credentials.\nThis behavior aligns with the Atlan workflow config update UI. Now, pass the credential object to the test_and_update() method to update this new credentials in Atlan after\ninitially testing it to confirm its successful validation. Update workflow source credentials 1 2 3 4 5 6 7 8 9 10 11 val snowflakeCredential = client . credentials . get ( // (1) \"972a87c1-28d7-8bf2-896d-ea5bd3e9c691\" ). toCredential () . authType ( \"basic\" ) // (2) . username ( \"username\" ) // (3) . password ( \"password\" ) . extra ( \"role\" , \"role-here\" ) . extra ( \"warehouse\" , \"warehouse-here\" ) . build () // (4) val response = snowflakeCredential . update () // (5) You can retrieve the workflow credential object by providing its GUID . You must specify the authentication type of the credential. You must provide the sensitive details such as the username , password , and extra when updating credentials. This behavior aligns with the Atlan workflow config update UI. Build the minimal Credential object. Now, use the update() method of the Credential object to update this new credentials in Atlan after initially testing it for successful validation. Because this operation will update details in Atlan, you must provide it an AtlanClient through which to connect to the tenant. GET /api/service/credentials/972a87c1-28d7-8bf2-896d-ea5bd3e9c691 1 // (1) You can retrieve the workflow credential object by providing its GUID . POST /api/service/credentials/972a87c1-28d7-8bf2-896d-ea5bd3e9c691/test 1 // (1) You can also test the existing credential authentication by providing its GUID . POST /api/service/credentials/test 1 2 3 4 5 6 7 8 9 10 11 12 13 14 { \"name\" : \"default-snowflake-1735595539-0\" , // (1) \"host\" : \"test.snowflake.com\" , \"port\" : 443 , \"authType\" : \"basic\" , \"connectorType\" : \"jdbc\" , \"username\" : \"test-username\" , // (2) \"password\" : \"test-password\" , \"extra\" : { \"role\" : \"test-role\" , \"warehouse\" : \"test-warehouse\" , }, \"connectorConfigName\" : \"atlan-connectors-snowflake\" } This example demonstrates how to test & update the source\ncredentials for the Snowflake crawler (basic authentication). You can update the following credentials fields: username : update with the new username. password : update with the new password. role : update with the new role. warehouse : update with the new warehouse. Hard-delete an workflow credentials Â¶ 4.2.0 Hard-deletes (also called a purge) are irreversible operations. The workflow credential is removed from Atlan entirely, so no longer appears in the UI and also no longer exists in Atlan's back-end. To hard-delete (purge) an asset, you only need to provide the GUID for Snowflake : Java Python Kotlin Raw REST API Coming soon Delete workflow credentials 1 2 3 4 5 from pyatlan.client.atlan import AtlanClient client = AtlanClient () response = client . credentials . purge_by_guid ( guid = \"972a87c1-28d7-8bf2-896d-ea5bd3e9c691\" ) #(1) The credentials.purge_by_guid() method returns None when the credentials deleted sucessfully. Coming soon POST api/service/credentials/972a87c1-28d7-8bf2-896d-ea5bd3e9c691/archive 1 //(1) Specify the GUID of the credential to be deleted: api/service/credentials/{credential-guid}/archive Update workflow configuration Â¶ 0.0.16 2.3.1 To update workflow configuration for example, for Snowflake : Java Python Kotlin Go Coming soon Update workflow configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from pyatlan.client.atlan import AtlanClient client = AtlanClient () result = client . workflow . find_by_id ( # (1) id = \"atlan-snowflake-1714638976\" ) workflow_task = result . source . spec . templates [ 0 ] . dag . tasks [ 0 ] workflow_params = workflow_task . arguments . parameters # (2) for option in workflow_params : if option . name == \"enable-lineage\" : # (3) option . value = True response = client . workflow . update ( workflow = result . to_workflow ()) # (4) You can find a workflow by its identifier using the find_by_id() method\nof the workflow client, providing the id for the specific workflow.\nIn this example, we're retrieving the Snowflake workflow for an update. Retrieve the workflow template and specific task that you need to update. Update the specific workflow parameter. In this example,\nwe're enabling lineage for the Snowflake workflow. Convert the workflow search result object to a workflow object\nand pass that to the update() method to actually perform the workflow update in Atlan. Coming soon Update workflow configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 result , _ := ctx . WorkflowClient . FindByID ( \"atlan-snowflake-1714638976\" ) // (1) workflowTask := result . Source . Spec . Templates [ 0 ]. DAG . Tasks [ 0 ] // (2) workflowParams := workflowTask . Arguments . Parameters for _ , option := range workflowParams { if option . Name == \"enable-lineage\" { // (3) option . Value = true } } response , atlanErr := ctx . WorkflowClient . Update ( result . ToWorkflow ()) // (4) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } You can find a workflow by its identifier using the FindByID() method\nof the workflow client, providing the id for the specific workflow.\nIn this example, we're retrieving the Snowflake workflow for an update. Retrieve the workflow template and specific task that you need to update. Update the specific workflow parameter. In this example,\nwe're enabling lineage for the Snowflake workflow. Convert the workflow search result object to a workflow object\nand pass that to the Update() method to actually perform the workflow update in Atlan. Retrieve workflow run Â¶ By ID Â¶ 0.0.16 2.4.2 Retrieve an existing workflow run by its ID: Java Python Kotlin Go Raw REST API Coming soon Retrieve workflow run by its ID 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () result = client . workflow . find_run_by_id ( # (1) id = \"atlan-snowflake-miner-1714638976-mzdza\" ) You can find a workflow run by its identifier using the find_run_by_id() method\nof the workflow client, providing the id for the specific workflow run.\nIn this example, we're retrieving the existing SnowflakeMiner workflow run. Coming soon Retrieve workflow run by its ID 1 2 3 4 result , atlanErr := ctx . WorkflowClient . FindRunByID ( \"atlan-snowflake-miner-1714638976-mzdza\" ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } You can find a workflow run by its identifier using the FindRunByID() method\nof the workflow client, providing the id for the specific workflow run.\nIn this example, we're retrieving the existing SnowflakeMiner workflow run. POST /api/service/runs/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 { \"from\" : 0 , \"size\" : 1 , \"track_total_hits\" : true , \"query\" : { \"bool\" : { \"filter\" : [ { \"term\" : { \"_id\" : { \"value\" : \"atlan-snowflake-miner-1714638976-mzdza\" } // (1) } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"order\" : \"desc\" , \"nested\" : { \"path\" : \"metadata\" } } } ] } You can find a workflow run by its identifier.\nIn this example, we're retrieving the existing SnowflakeMiner workflow run. By status and time range Â¶ 6.1.0 Retrieve existing workflow runs by their status and time range: Java Python Kotlin Go Raw REST API Coming soon Retrieve workflow runs by status and time range 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import AtlanWorkflowPhase client = AtlanClient () results = client . workflow . find_runs_by_status_and_time_range ( # (1) status = [ AtlanWorkflowPhase . SUCCESS , AtlanWorkflowPhase . FAILED ], started_at = \"now-6h\" , finished_at = \"now-1h\" , from_ = 0 , size = 100 , ) for result in results : # (2) # Do something with the workflow... To search for workflow runs based on their status and time range, use the find_runs_by_status_and_time_range() method with the following parameters: status (required): filters workflow runs by their status. Acceptable values are defined in the AtlanWorkflowPhase enum. For example, setting status=[AtlanWorkflowPhase.SUCCESS, AtlanWorkflowPhase.FAILED] will retrieve workflow runs that have either a success or failed status. started_at (optional): Filters workflow runs based on their start time. For example, setting started_at=\"now-6h\" will retrieve runs that started within the last 6 hours. finished_at (optional): Filters workflow runs based on their finish time. For example, setting finished_at=\"now-1h\" will retrieve runs that finished within the last hour. from_ (optional): starting index of the search results (default: 0 ). size (optional): maximum number of search results to return (default: 100 ). Returns a WorkflowSearchResponse object. Not sure about Elasticsearch time range format? Check out the Elasticsearch date math guide for more details. This is the pattern for iterating through all results (across pages) covered in the Searching for assets portion of the SDK documentation. Coming soon Coming soon POST /api/service/runs/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 { \"from\" : 0 , // (1) \"size\" : 100 , // (2) \"track_total_hits\" : true , \"query\" : { \"bool\" : { \"must\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"terms\" : { \"metadata.labels.workflows.argoproj.io/phase.keyword\" : [ // (3) \"Succeeded\" , \"Failed\" ] } } } }, { \"range\" : { \"status.startedAt\" : { // (4) \"gte\" : \"now-6h\" } } }, { \"range\" : { \"status.finishedAt\" : { // (5) \"gte\" : \"now-1h\" } } }, { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"exists\" : { \"field\" : \"metadata.labels.workflows.argoproj.io/creator\" } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"order\" : \"desc\" , \"nested\" : { \"path\" : \"metadata\" } } } ] } Starting index of the search results (default: 0 ). Maximum number of search results to return (default: 100 ). You can find workflow runs by their status.\nIn this example, we're retrieving workflow runs that have either a Succeeded or Failed status. You can use a Range query to filter workflow runs based on their start time. \nFor example, setting \"status.startedAt\": {\"gte\": \"now-6h\"} will retrieve runs that started within the last 6 hours. You can use a Range query to filter workflow runs based on their finish time. For example, setting \"status.finishedAt\": {\"gte\": \"now-1h\"} will retrieve runs that finished within the last hour. Not sure about Elasticsearch time range format? Check out the Elasticsearch date math guide for more details. Retrieve all workflow runs Â¶ 0.0.16 2.1.8 By their phase: Â¶ To retrieve all existing workflow runs based on \ntheir phase, such as Succeeded , Running , Failed , etc Java Python Kotlin Go Raw REST API Coming soon Retrieve all workflow runs by their phase 1 2 3 4 5 6 7 8 9 10 11 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import AtlanWorkflowPhase client = AtlanClient () response = client . workflow . get_runs ( workflow_name = \"atlan-snowflake-miner-1714638976\" , workflow_phase = AtlanWorkflowPhase . RUNNING , from_ = 0 , size = 100 , ) # (1) To retrieve all existing workflow runs\nbased on their phase, you need to specify: name of the workflow as displayed in the UI, eg: atlan-snowflake-miner-1714638976 . phase of the given workflow (e.g: Succeeded , Running , Failed , etc) starting index of the search results (default: 0 ). maximum number of search results to return (default: 100 ). Coming soon Retrieve all workflow runs by their phase 1 2 3 4 5 6 7 8 9 result , atlanErr := ctx . WorkflowClient . GetRuns ( \"atlan-snowflake-miner-1714638976\" , atlan . AtlanWorkflowPhaseSuccess , 0 , 100 , ) // (1) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } To retrieve all existing workflow runs\nbased on their phase, you need to specify: name of the workflow as displayed in the UI, eg: atlan-snowflake-miner-1714638976 . phase of the given workflow (e.g: AtlanWorkflowPhaseSuccess , AtlanWorkflowPhaseRunning , AtlanWorkflowPhaseFailed , etc) starting index of the search results (default: 0 ). maximum number of search results to return (default: 100 ). POST /api/service/runs/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 { \"from\" : 0 , // (1) \"size\" : 100 , // (2) \"track_total_hits\" : true , \"query\" : { \"bool\" : { \"must\" : [ { \"nested\" : { \"path\" : \"spec\" , \"query\" : { \"term\" : { \"spec.workflowTemplateRef.name.keyword\" : { \"value\" : \"atlan-snowflake-miner-1714638976\" } // (3) } } } } ], \"filter\" : [ { \"term\" : { \"status.phase.keyword\" : { \"value\" : \"Succeeded\" } // (4) } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"order\" : \"desc\" , \"nested\" : { \"path\" : \"metadata\" } } } ] } Starting index of the search results (default: 0 ). Maximum number of search results to return (default: 100 ). Name of the workflow as displayed in the UI, eg: atlan-snowflake-miner-1714638976 . Phase of the given workflow (e.g: Succeeded , Running , Failed , etc) Stop a running workflow Â¶ 0.0.16 2.1.8 To stop a running workflow: Java Python Kotlin Go Coming soon Retrieve all workflow runs by their phase 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient client = AtlanClient () runs = client . workflow . get_runs ( workflow_name = \"atlan-snowflake-miner-1714638976\" , workflow_phase = AtlanWorkflowPhase . RUNNING , ) # (1) response = client . workflow . stop ( workflow_run_id = runs [ 0 ] . id ) # (2) First, retrieve all existing running workflows. From the list of existing running workflows, provide \nthe identifier of the specific workflow run to the client.workflow.stop() method, e.g: atlan-snowflake-miner-1714638976-9wfxz . Coming soon Retrieve all workflow runs by their phase 1 2 3 4 5 6 7 8 9 10 runs , _ := ctx . WorkflowClient . GetRuns ( \"atlan-snowflake-miner-1714638976-9wfxz\" , atlan . AtlanWorkflowPhaseRunning , 0 , 100 , ) // (1) response , atlanErr := ctx . WorkflowClient . Stop ( runs [ 0 ]. ID ) // (2) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } First, retrieve all existing running workflows. From the list of existing running workflows, provide \nthe identifier of the specific workflow run to the ctx.WorkflowClient.Stop() method, e.g: atlan-snowflake-miner-1714638976-9wfxz . Delete a workflow Â¶ 0.0.16 2.1.8 4.0.0 To delete a workflow: Java Python Kotlin Go Delete a workflow 1 2 3 client . workflows . archive ( \"atlan-snowflake-miner-1714638976\" ); // (1) To delete an existing workflow, specify the name of the workflow as displayed in the UI (e.g: atlan-snowflake-miner-1714638976 ). Delete a workflow 1 2 3 4 5 6 7 from pyatlan.client.atlan import AtlanClient client = AtlanClient () client . workflow . delete ( workflow_name = \"atlan-snowflake-miner-1714638976\" ) # (1) To delete an existing workflow, specify: name of the workflow as displayed in the\nUI (e.g: atlan-snowflake-miner-1714638976 ). Delete a workflow 1 2 3 client . workflows . archive ( \"atlan-snowflake-miner-1714638976\" ) // (1) To delete an existing workflow, specify the name of the workflow as displayed in the UI (e.g: atlan-snowflake-miner-1714638976 ). Delete a workflow 1 ctx . WorkflowClient . Delete ( \"atlan-snowflake-miner-1714638976\" ) // (1) To delete an existing workflow, specify: name of the workflow as displayed in the\nUI (e.g: atlan-snowflake-miner-1714638976 ). 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/",
    "content": "Supported packages Â¶ In this section, you'll find a comprehensive list of individual packages currently supported through our SDKs.\nEach package section includes examples demonstrating how to build a workflow from scratch and execute it on Atlan. Can't find the package you're looking for? Python Go 4.1.0 Don't worry! If the workflow package isn't listed here, you can still run it by passing your workflow JSON configuration string directly to the WorkflowClient.run() method in Atlan. workflow_run_example.py 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient client = AtlanClient () workflow_json = r \"\"\" { \"metadata\": {\"name\": \"test-name\", \"namespace\": \"test-namespace\"}, \"spec\": {} , \"payload\": [{\"parameter\": \"test-param\", \"type\": \"test-type\", \"body\": {} }] } \"\"\" response = client . workflow . run ( workflow_json ) 0.0.16 Don't worry! If the workflow package isn't listed here, you can still run it by passing your workflow JSON configuration string directly to the ctx.WorkflowClient.Run() method in Atlan. workflow_run_example.py 1 2 3 4 5 6 7 8 9 10 workflowJSON := `{ \"metadata\": {\"name\": \"test-name\", \"namespace\": \"test-namespace\"}, \"spec\": {}, \"payload\": [{\"parameter\": \"test-param\", \"type\": \"test-type\", \"body\": {}}] }` response , atlanErr := ctx . WorkflowClient . Run ( workflowJSON , nil ) if atlanErr != nil { logger . Log . Errorf ( \"Error : %v\" , atlanErr ) } Connectors Â¶ Athena assets BigQuery assets Connection delete Confluent Kafka assets dbt assets DynamoDB assets Databricks assets Databricks miner Fivetran enrichment Glue assets Looker assets Oracle assets Postgres assets PowerBI assets Redshift assets Snowflake assets Snowflake miner Sigma assets SQL Server assets Tableau assets MongoDB assets Utilities Â¶ Asset import Asset export (basic) API token connection admin Lineage builder Lineage generator (no transformation) Relational assets builder 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/api-token-connection-admin/",
    "content": "API token connection admin package Â¶ The API token connection admin package allows you to assign an API token to a connection as a connection admin. This is a necessary step when: A connection is created through a workflow, run by a user You want to use an API token to programmatically administrate\nthe connection or its assets (in particular, to manage policies in a persona) Configuration Â¶ 4.1.0 To set up the API token connection admin with the specified configuration. Java Python Kotlin Raw REST API Coming soon API token connection admin with the specified configuration 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import APITokenConnectionAdmin client = AtlanClient () workflow = ( APITokenConnectionAdmin () # (1) . config ( # (2) connection_qualified_name = \"default/snowflake/1234567890\" , api_token_guid = \"92588c67-5ddf-4a45-8b5c-dd92f4b84e99\" , ) . to_workflow () # (3) ) response = client . workflow . run ( workflow ) # (4) The API token connection admin package allows you \nto assign an API token to a connection as a connection admin. Set up the API token connection admin with the specified configuration. connection_qualified_name: connection qualified name\nto which you want to add the API token as a connection admin api_token_guid: GUID of the API token Convert the package into a Workflow object. Run the workflow by invoking the run() method\non the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how to check the status and wait\nuntil the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 4.1.0 To re-run an existing api token connection admin workflow: Java Python Kotlin Raw REST API Coming soon Re-run existing api token connection admin workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . API_TOKEN_CONNECTION_ADMIN , max_results = 5 ) # Determine which api token connection admin workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the APITokenConnectionAdmin . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"csa-api-token-connection-admin\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the csa-api-token-connection-admin prefix will ensure you only find existing api token connection admin workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"csa-api-token-connection-admin-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/asset-export-basic/",
    "content": "Asset export (basic) package Â¶ The asset export (basic) package identifies all assets that could have been enriched in some way through Atlan's UI and extracts them. The resulting CSV file can be modified or enriched, and then loaded back using the asset import package . All assets Â¶ 2.6.0 In this example, weâ€™re building and running the asset-export workflow to export all assets. However, you can also use one of the following methods to customize the scope of your asset export workflow: enriched_only() : sets up the package to export only assets enriched by users. glossaries_only() : sets up the package to export only glossaries. products_only() : sets up the package to export only data products. all_assets() : sets up the package to export all assets, whether enriched by users or not, will be exported. Java Python Kotlin Raw REST API Coming soon Import assets from the object store 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import AssetExportBasic client = AtlanClient () workflow = ( AssetExportBasic () # (1) . all_assets ( # (2) prefix = \"default\" , include_description = True , include_glossaries = True , include_data_products = True , include_archived = True , ) . object_store ( prefix = \"/test/prefix\" ) # (3) . s3 ( # (4) access_key = \"test-access-key\" , secret_key = \"test-secret-key\" , bucket = \"my-bucket\" , region = \"us-west-1\" , ) ) . to_workflow () # (5) response = client . workflow . run ( workflow ) # (6) The AssetExportBasic package exports assets from Atlan. In this example, weâ€™re building a workflow to export all_assets() .\nHowever, you can also use one of the following methods to\ncustomize the scope of your asset export workflow: enriched_only() : sets up the package to export only assets enriched by users. glossaries_only() : sets up the package to export only glossaries. products_only() : sets up the package to export only data products. all_assets() : sets up the package to export all assets, whether\n    enriched by users or not, will be exported. For all_assets() , you need to provide following: prefix : starting value for a qualifiedName that\n    will determine which assets to export, default: default (all data assets). include_description : whether to extract only user-entered description\n    ( False ), or to also include system-level description ( True ). include_glossaries : whether glossaries (and their terms\n    and categories) should be exported ( True ) or not ( False ). include_data_products : whether data products\n    (and their domains) should be exported ( True ) or not ( False ). include_archived : whether to include archived\n    assets in the export ( True ) or only active assets ( False ). To set up the package to export to an object storage location, you need to provide prefix : directory (path) within the object store\n   where the exported file will be uploaded. In this example, we're exporting assets to an object storage location using s3() .\nHowever, you can use different object storage methods such as gcs() or adls() .\nYou can also configure different export delivery methods using one of the following methods: email() : sets up the package to deliver the export via email. direct() : sets up the package to deliver the export via direct download. For s3() , you need to provide following: access_key : AWS access key. secret_key : AWS secret key. bucket : S3 bucket to upload the export file to. region : name of the AWS region. Convert the package into a Workflow object. Run the workflow by invoking the run() method\non the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how to check the status and wait\nuntil the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 2.6.0 To re-run an existing asset export basic workflow: Java Python Kotlin Raw REST API Coming soon Re-run existing asset export basic workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . ASSET_EXPORT_BASIC , max_results = 5 ) # Determine which asset export basic workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the AssetExportBasic . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"csa-asset-export-basic\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the csa-asset-export-basic prefix will ensure you only find existing asset export basic workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"csa-asset-export-basic-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/confluent-kafka-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Confluent Kafka assets package Â¶ The Confluent Kafka assets package crawls Confluent Kafka assets and publishes them to Atlan for discovery. Direct extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method\nit will create a new connection and new assets within that connection â€” which could lead to duplicate\nassets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 4.0.0 To crawl assets directly from Confluent Kafka: Java Python Kotlin Raw REST API Direct extraction from Confluent Kafka 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 Workflow crawler = ConfluentKafkaCrawler . creator ( // (1) client , // (2) \"production\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null ) . direct ( // (5) \"dev-south.aws.confluent.cloud:9092\" , true ) . apiToken ( \"api-key-here\" , // (6) \"api-secret-here\" // (7) ) . skipInternal ( true ) // (8) . include ( \".*_DEV_TOPICS\" ) // (9) . exclude ( \".*_TEST\" ) // (10) . build () // (11) . toWorkflow (); // (12) WorkflowResponse response = crawler . run ( client ); // (13) The ConfluentKafkaCrawler package will create a workflow to crawl assets from Confluent Kafka. You must provide Atlan client. You must provide a name for the connection that the Confluent Kafka assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. When crawling assets directly from Confluent Kafka,\nyou are required to provide the following information: hostname and port number (host.example.com:9092) for the Kafka bootstrap server. whether to use an encrypted SSL connection ( true ), or plaintext ( false ) You must provide an API key through which to access Kafka. You must provide API secret through which to access Kafka. You can also optionally set whether to skip internal\ntopics when crawling ( true ) or include them ( false ). You can also optionally provide the regular expression\nto use for including topics when crawling. You can also optionally provide the regular expression\nto use for excluding topics when crawling. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Direct extraction from Confluent Kafka 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import ConfluentKafkaCrawler client = AtlanClient () crawler = ( ConfluentKafkaCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , ) . direct ( # (5) bootstrap = \"dev-south.aws.confluent.cloud:9092\" , encrypted = True ) . api_token ( api_key = \"api-key-here\" , # (6) api_secret = \"api-secret-here\" # (7) ) . skip_internal ( True ) # (8) . include ( regex = \".*_DEV_TOPICS\" ) # (9) . exclude ( regex = \".*_TEST\" ) # (10) . to_workflow () # (11) ) response = client . workflow . run ( crawler ) # (12) Base configuration for a new Confluent Kafka crawler. You must provide a client instance. You must provide a name for the connection that the Confluent Kafka assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. When crawling assets directly from Confluent Kafka,\nyou are required to provide the following information: hostname and port number (host.example.com:9092) for the Kafka bootstrap server. whether to use an encrypted SSL connection ( True ), or plaintext ( False ) You must provide an API key through which to access Kafka. You must provide API secret through which to access Kafka. You can also optionally set whether to skip internal\ntopics when crawling ( True ) or include them ( False ). You can also optionally provide the regular expression\nto use for including topics when crawling.\n(If set to None, all topics will be crawled.) You can also optionally provide the regular expression\nto use for excluding topics when crawling.\n(If set to None, no topics will be excluded.) Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Direct extraction from Confluent Kafka 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 val crawler = ConfluentKafkaCrawler . creator ( // (1) client , // (2) \"production\" , // (3) listOf ( client . roleCache . getIdForName ( \"\\ $ admin \" )), // (4) null , null ) . direct ( // (5) \"dev-south.aws.confluent.cloud:9092\" , true ) . apiToken ( \"api-key-here\" , // (6) \"api-secret-here\" // (7) ) . skipInternal ( true ) // (8) . include ( \".*_DEV_TOPICS\" ) // (9) . exclude ( \".*_TEST\" ) // (10) . build () // (11) . toWorkflow () // (12) val response = crawler . run ( client ) // (13) The ConfluentKafkaCrawler package will create a workflow to crawl assets from Confluent Kafka. You must provide Atlan client. You must provide a name for the connection that the Confluent Kafka assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. When crawling assets directly from Confluent Kafka,\nyou are required to provide the following information: hostname and port number (host.example.com:9092) for the Kafka bootstrap server. whether to use an encrypted SSL connection ( true ), or plaintext ( false ) You must provide an API key through which to access Kafka. You must provide API secret through which to access Kafka. You can also optionally set whether to skip internal\ntopics when crawling ( true ) or include them ( false ). You can also optionally provide the regular expression\nto use for including topics when crawling. You can also optionally provide the regular expression\nto use for excluding topics when crawling. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for Confluent Kafka assets: Java Python Kotlin Raw REST API Re-run existing Confluent Kafka workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , ConfluentKafkaCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the Confluent Kafka workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the ConfluentKafkaCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Confluent Kafka workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . KAFKA_CONFLUENT_CLOUD , max_results = 5 ) # Determine which Confluent Kafka workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the ConfluentKafkaCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Confluent Kafka workflow 1 2 3 4 5 val existing = WorkflowSearchRequest // (1) . findByType ( client , ConfluentKafkaCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // Confluent Kafka workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the ConfluentKafkaCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-kafka-confluent-cloud\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-kafka-confluent-cloud prefix will ensure you only find existing Confluent Kafka assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-kafka-confluent-cloud-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/asset-import/",
    "content": "Asset import package Â¶ The asset import package loads metadata\nfrom a CSV file that matches the format of one extracted using either of the asset\nexport packages ( basic or advanced ). Import assets from object store Â¶ 2.6.0 To import assets directly from the object store: Java Python Kotlin Raw REST API Coming soon Import assets from the object store 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import AssetImport from pyatlan.model.assets import Asset from pyatlan.model.enums import AssetInputHandling client = AtlanClient () workflow = ( AssetImport () # (1) . object_store () # (2) . s3 ( # (3) access_key = \"test-access-key\" , secret_key = \"test-secret-key\" , bucket = \"my-bucket\" , region = \"us-west-1\" , ) . assets ( # (4) prefix = \"/test/prefix\" , object_key = \"assets-test.csv\" , input_handling = AssetInputHandling . UPSERT , ) . assets_advanced ( # (5) remove_attributes = [ Asset . CERTIFICATE_STATUS , Asset . ANNOUNCEMENT_TYPE ], fail_on_errors = True , case_sensitive_match = False , field_separator = \",\" , batch_size = 20 , ) . glossaries ( # (6) prefix = \"/test/prefix\" , object_key = \"glossaries-test.csv\" , input_handling = AssetInputHandling . UPDATE , ) . glossaries_advanced ( # (7) remove_attributes = [ Asset . CERTIFICATE_STATUS , Asset . ANNOUNCEMENT_TYPE ], fail_on_errors = True , field_separator = \",\" , batch_size = 20 , ) . data_products ( # (8) prefix = \"/test/prefix\" , object_key = \"data-products-test.csv\" , input_handling = AssetInputHandling . UPDATE , ) . data_product_advanced ( # (9) remove_attributes = [ Asset . CERTIFICATE_STATUS , Asset . ANNOUNCEMENT_TYPE ], fail_on_errors = True , field_separator = \",\" , batch_size = 20 , ) ) . to_workflow () # (10) response = client . workflow . run ( workflow ) # (11) The AssetImport loads metadata from a CSV file. Set up the package to import metadata directly from the object store. You can use different object store methods (e.g: s3() , gcs() , adls() ). In this example,\nwe're building a workflow using s3() and for that, youâ€™ll need to provide the following information: AWS access key. AWS secret key. name of the bucket/storage that contains the metadata CSV files. name of the AWS region. (Optional) To set up the package for importing assets, provide the following information: prefix : directory (path) within the object store from\n    which to retrieve the file containing asset metadata. object_key : object key (filename),\n    including its extension, within the object store and prefix input_handling : specifies whether to allow the creation\n    of new assets from the input CSV with full ( AssetInputHandling.UPSERT )\n    or partial assets ( AssetInputHandling.PARTIAL )\n    or only update ( AssetInputHandling.UPDATE ) existing assets in Atlan. (Optional) To set up the package for importing assets with\nadvanced configuration, provide the following information: remove_attributes : list of attributes to clear (remove)\n    from assets if their value is blank in the provided file. fail_on_errors : specifies whether an invalid value\n    in a field should cause the import to fail ( True ) or\n    log a warning, skip that value, and proceed ( False ). case_sensitive_match : indicates whether to use\n    case-sensitive matching when running in update-only mode ( True )\n    or to try case-insensitive matching ( False ). is_table_view_agnostic : specifies whether to treat\n    tables, views, and materialized views as interchangeable ( True )\n    or to strictly adhere to specified types in the input ( False ). field_separator : character used to separate\n    fields in the input file (e.g: ',' or ';' ). batch_size : maximum number of rows\n    to process at a time (per API request). (Optional) To set up the package for importing glossaries, provide the following information: prefix : directory (path) within the object store from\n    which to retrieve the file containing glossaries, categories and terms. object_key : object key (filename),\n    including its extension, within the object store and prefix input_handling : specifies whether to allow the creation\n    of new glossaries, categories and terms from the input CSV ( AssetInputHandling.UPSERT )\n    or or ensure these are only updated ( AssetInputHandling.UPDATE ) if they already exist in Atlan. (Optional) To set up the package for importing glossaries with\nadvanced configuration, provide the following information: remove_attributes : list of attributes to clear (remove)\n    from assets if their value is blank in the provided file. fail_on_errors : specifies whether an invalid value\n    in a field should cause the import to fail ( True ) or\n    log a warning, skip that value, and proceed ( False ). field_separator : character used to separate\n    fields in the input file (e.g: ',' or ';' ). batch_size : maximum number of rows\n    to process at a time (per API request). (Optional) To set up the package for importing data products, provide the following information: prefix : directory (path) within the object store from\n    which to retrieve the file containing data domains, and data products. object_key : object key (filename),\n    including its extension, within the object store and prefix input_handling : specifies whether to allow the creation\n    of new data domains, and data products from the input CSV ( AssetInputHandling.UPSERT )\n    or or ensure these are only updated ( AssetInputHandling.UPDATE ) if they already exist in Atlan. (Optional) To set up the package for importing data domain\n    and data products with advanced configuration, provide the following information: remove_attributes : list of attributes to clear (remove)\n    from assets if their value is blank in the provided file. fail_on_errors : specifies whether an invalid value\n    in a field should cause the import to fail ( True ) or\n    log a warning, skip that value, and proceed ( False ). field_separator : character used to separate\n    fields in the input file (e.g: ',' or ';' ). batch_size : maximum number of rows\n    to process at a time (per API request). Convert the package into a Workflow object. Run the workflow by invoking the run() method\non the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how to check the status and wait\nuntil the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 2.6.0 To re-run an existing asset import workflow: Java Python Kotlin Raw REST API Coming soon Re-run existing asset import workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . ASSET_IMPORT , max_results = 5 ) # Determine which asset import workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the AssetImport . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"csa-asset-import\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the csa-asset-import prefix will ensure you only find existing asset import workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"csa-asset-import-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/bigquery-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) BigQuery assets package Â¶ The BigQuery assets package crawls Google BigQuery assets and publishes them to Atlan for discovery. Service account Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you\nrun this method it will create a new connection and new assets within that connection\nâ€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 To crawl assets from BigQuery using service account authentication: Java Python Kotlin Raw REST API Coming soon BigQuery assets crawling using service account auth 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import BigQueryCrawler client = AtlanClient () crawler = ( BigQueryCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . service_account_auth ( # (8) project_id = \"test-project-id\" , service_account_json = \"test-account-json\" , service_account_email = \"test@test.com\" , ) . include ( assets = { \"test-include\" : [ \"test-asset-1\" , \"test-asset-2\" ]}) # (9) . exclude ( assets = None ) # (10) . exclude_regex ( regex = \".*_TEST\" ) # (11) . custom_config ( config = { \"ignore-all-case\" : True }) # (12) . to_workflow () # (13) ) response = client . workflow . run ( crawler ) # (14) Base configuration for a new BigQuery crawler. You must provide a client instance. You must provide a name for the connection that the BigQuery assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). When using service_account_auth() , you need to provide the following information: project ID of your Google Cloud project. entire service account json. service account email. You can also optionally specify the set of assets to include in crawling.\nFor BigQuery assets, this should be specified as a dict keyed by project name\nwith each value being a list of tables to include. (If set to None, all table will be crawled.) You can also optionally specify the list of assets to exclude from crawling.\nFor BigQuery assets, this should be specified as a dict keyed by project name\nwith each value being a list of tables to exclude. (If set to None, no table will be excluded.) You can also optionally specify the exclude regex\nfor crawler ignore tables and views based on a naming convention. You can also optionally specify the custom JSON configuration\ncontrolling experimental feature flags for the crawler,\neg: {\"ignore-all-case\": True} to enable crawling assets\nwith case-sensitive identifiers. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for BigQuery assets: Java Python Kotlin Raw REST API Re-run existing BigQuery workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , BigQueryCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the BigQuery workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the BigQueryCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(true) method with idempotency\nto avoid re-running a workflow that is already in running or in a pending state.\nThis will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing BigQuery workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . BIGQUERY , max_results = 5 ) # Determine which BigQuery workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the BigQueryCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing BigQuery workflow 1 2 3 4 5 val existing = WorkflowSearchRequest // (1) . findByType ( client , BigQueryCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // BigQuery workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the BigQueryCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(true) method with idempotency\nto avoid re-running a workflow that is already in running or in a pending state.\nThis will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-bigquery\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-bigquery prefix will ensure you only find existing BigQuery assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. \n(Remember since this is a search, there could be multiple results, so you may want to use the other details \nin each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-bigquery-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/athena-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Athena assets package Â¶ The Athena assets package crawls Athena assets and publishes them to Atlan for discovery. Direct extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it will create a new connection and new assets within that connection â€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 4.0.0 To crawl Athena assets directly from Athena: Java Python Kotlin Raw REST API Direct extraction from Athena 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Workflow athena = AthenaCrawler . iamUserAuth ( // (1) \"production\" , // (2) \"athena.ap-south-1.amazonaws.com\" , // (3) 443 , // (4) \"pEBDcOauyVfCtQacdvJUL\" , // (5) \"eKnUIFK2JvmVO5mruqbxjnzXf\" , // (6) \"s3://bucket/location\" , // (7) \"workgroup\" , // (8) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (8) null , null , true , // (9) true , // (10) 10000L , // (11) Map . of ( \"AwsDataCatalog\" , List . of ( \"schema_one\" , \"schema_two\" )), // (12) null ); // (13) WorkflowResponse response = athena . run ( client ); // (14) The AthenaCrawler package will create a workflow to crawl assets from Athena. The iamUserAuth() method creates a workflow for crawling assets directly from Athena using IAM-based authentication. You must provide a name for the connection that the Athena assets will exist within. You must provide the hostname of your Athena instance. You must specify the port number of the Athena instance (use 443 for the default). You must provide your IAM access key for accessing Athena. You must provide your IAM secret key for accessing Athena. You must provide an S3 location these IAM credentials can access for storing Athena query results. You must specify at least one connection admin, either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins a list of users (names) that will be connection admins You can also specify whether to allow queries against these Athena assets (true) or disable all querying to these assets (false). You can also specify whether to allow data in these Athena assets to be previewed (true) or not (false). You can also specify the maximum number of rows you'll allow to be returned from a query against these Athena assets. You can also optionally specify the list of schemas to include in crawling. For Athena assets, this should be specified as a list of schema names provided as the value in a map with the key AwsDataCatalog . (If set to null, all schemas will be crawled.) You can also optionally specify the list of schemas to exclude from crawling. For Athena assets, this should be specified as a list of schema names provided as the value in a map with the key AwsDataCatalog . (If set to null, no schemas will be excluded.) You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Direct extraction from Athena 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 val athena = AthenaCrawler . iamUserAuth ( // (1) \"production\" , // (2) \"athena.ap-south-1.amazonaws.com\" , // (3) 443 , // (4) \"pEBDcOauyVfCtQacdvJUL\" , // (5) \"eKnUIFK2JvmVO5mruqbxjnzXf\" , // (6) \"s3://bucket/location\" , // (7) \"workgroup\" , // (8) listOf ( client . roleCache . getIdForName ( \"\\ $ admin \" )), // (8) null , null , true , // (9) true , // (10) 10000L , // (11) mapOf ( \"AwsDataCatalog\" to listOf ( \"schema_one\" , \"schema_two\" )), // (12) null ); // (13) WorkflowResponse response = athena . run ( client ); // (14) The AthenaCrawler package will create a workflow to crawl assets from Athena. The iamUserAuth() method creates a workflow for crawling assets directly from Athena using IAM-based authentication. You must provide a name for the connection that the Athena assets will exist within. You must provide the hostname of your Athena instance. You must specify the port number of the Athena instance (use 443 for the default). You must provide your IAM access key for accessing Athena. You must provide your IAM secret key for accessing Athena. You must provide an S3 location these IAM credentials can access for storing Athena query results. You must specify at least one connection admin, either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins a list of users (names) that will be connection admins You can also specify whether to allow queries against these Athena assets (true) or disable all querying to these assets (false). You can also specify whether to allow data in these Athena assets to be previewed (true) or not (false). You can also specify the maximum number of rows you'll allow to be returned from a query against these Athena assets. You can also optionally specify the list of schemas to include in crawling. For Athena assets, this should be specified as a list of schema names provided as the value in a map with the key AwsDataCatalog . (If set to null, all schemas will be crawled.) You can also optionally specify the list of schemas to exclude from crawling. For Athena assets, this should be specified as a list of schema names provided as the value in a map with the key AwsDataCatalog . (If set to null, no schemas will be excluded.) You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 4.0.0 To re-run an existing workflow for Athena assets: Java Python Kotlin Raw REST API Re-run existing Athena workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , AthenaCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the Athena workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the AthenaCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Re-run existing Athena workflow 1 2 3 4 val existing = WorkflowSearchRequest // (1) . findByType ( client , AthenaCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the Athena workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the AthenaCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-athena\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-athena prefix will ensure you only find existing Athena assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. (Remember since this is a search, there could be multiple results, so you may want to use the other details in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-athena-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/dynamodb-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) DynamoDB assets package Â¶ The DynamoDB assets package crawls Amazon DynamoDB assets and publishes them to Atlan for discovery. IAM user authentication Â¶ Will create a new connection This should only be used to create the workflow the first time.\nEach time you run this method it will create a new connection\nand new assets within that connection â€” which could lead to duplicate\nassets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 To crawl assets from DynamoDB using IAM user authentication: Java Python Kotlin Raw REST API Coming soon DynamoDB assets crawling using IAM user authentication 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import DynamoDBCrawler client = AtlanClient () crawler = ( DynamoDBCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . direct ( region = \"test-south-1\" ) # (8) . iam_user_auth ( # (9) access_key = \"test-access-key\" , secret_key = \"test-secret-key\" ) . include_regex ( regex = \".*_TEST_INCLUDE\" ) # (10) . exclude_regex ( regex = \".*_TEST_EXCLUDE\" ) # (11) . to_workflow () # (12) ) response = client . workflow . run ( crawler ) # (13) Base configuration for a new DynamoDB crawler. You must provide a client instance. You must provide a name for the connection that the DynamoDB assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). You can specify the region to extract directly from the DynamoDB. When using iam_user_auth() , you need to provide the following information: access key through which to access DynamoDB. secret key through which to access DynamoDB. You can also optionally specify the regex of\ntables to include. By default, everything will be included. You can also optionally specify the regex of tables to ignore.\nBy default, nothing will be excluded. NOTE : This takes priority over include regex. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status\nand wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. IAM role authentication Â¶ Will create a new connection This should only be used to create the workflow the first time.\nEach time you run this method it will create a new connection\nand new assets within that connection â€” which could lead to duplicate\nassets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 To crawl assets from DynamoDB using IAM role authentication: Java Python Kotlin Raw REST API Coming soon DynamoDB assets crawling using IAM role authentication 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import DynamoDBCrawler client = AtlanClient () crawler = ( DynamoDBCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . direct ( region = \"test-south-1\" ) # (8) . iam_role_auth ( # (9) arn = \"arn:aws:iam::123456789012:user/test\" , external_id = \"test-ext-id\" ) . include_regex ( regex = \".*_TEST_INCLUDE\" ) # (10) . exclude_regex ( regex = \".*_TEST_EXCLUDE\" ) # (11) . to_workflow () # (12) ) response = client . workflow . run ( crawler ) # (13) Base configuration for a new DynamoDB crawler. You must provide a client instance. You must provide a name for the connection that the DynamoDB assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). You can specify the region to extract directly from the DynamoDB. When using iam_role_auth() , you need to provide the following information: ARN of the AWS role. AWS external ID. You can also optionally specify the regex of\ntables to include. By default, everything will be included. You can also optionally specify the regex of tables to ignore.\nBy default, nothing will be excluded. NOTE : This takes priority over include regex. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status\nand wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for DynamoDB assets: Java Python Kotlin Raw REST API Re-run existing DynamoDB workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , DynamoDBCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the DynamoDB workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the DynamoDBCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing DynamoDB workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . DYNAMODB , max_results = 5 ) # Determine which DynamoDB workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the DynamoDBCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing DynamoDB workflow 1 2 3 4 5 val existing = WorkflowSearchRequest // (1) . findByType ( client , DynamoDBCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // DynamoDB workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the DynamoDBCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-dynamodb\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-dynamodb prefix will ensure you only find existing DynamoDB assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. \n(Remember since this is a search, there could be multiple results, so you may want to use the other details \nin each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-dynamodb-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/connection-delete/",
    "content": "Connection delete package Â¶ The connection delete package deletes a connection and all its related assets. Soft-delete (archive) assets Â¶ 2.2.3 4.0.0 To soft-delete (archive) all assets in a connection: Java Python Kotlin Raw REST API Archive assets 1 2 3 4 5 6 Workflow workflow = ConnectionDelete . creator ( // (1) \"default/snowflake/1234567890\" , false // (2) ). build () // (3) . toWorkflow (); // (4) WorkflowResponse response = workflow . run ( client ); // (5) The ConnectionDelete package will create a workflow to delete a connection and its assets using the creator() method. You need to provide the following: qualified name of the connection whose assets should be deleted. whether to permanently delete the connection and its assets (hard-delete) ( true ), or only archive (soft-delete) them ( false ). Build the minimal package object. Convert the package into a Workflow object. Run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how to check the status and wait until the workflow has been completed. Archive assets 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import ConnectionDelete client = AtlanClient () workflow = ConnectionDelete ( # (1) qualified_name = \"default/snowflake/1234567890\" , purge = False # (2) ) . to_workflow () # (3) response = client . workflow . run ( workflow ) # (4) The ConnectionDelete package will create\na workflow to delete a connection and its assets. You need to provide the following: qualified name of the connection whose assets should be deleted. whether to permanently delete the connection and its assets\n(hard-delete) ( True ), or only archive (soft-delete) them ( False ). Convert the package into a Workflow object. Run the workflow by invoking the run() method\non the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how to check the status and wait\nuntil the workflow has been completed. Archive assets 1 2 3 4 5 6 val workflow = ConnectionDelete . creator ( // (1) \"default/snowflake/1234567890\" , false // (2) ). build () // (3) . toWorkflow () // (4) val response = workflow . run ( client ) // (5) The ConnectionDelete package will create a workflow to delete a connection and its assets using the creator() method. You need to provide the following: qualified name of the connection whose assets should be deleted. whether to permanently delete the connection and its assets (hard-delete) ( true ), or only archive (soft-delete) them ( false ). Build the minimal package object. Convert the package into a Workflow object. Run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how to check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Hard-delete (purge) assets Â¶ Permanent and irreversible A hard-delete (purge) is permanent and irreversible.\nBe certain that you want to entirely remove all of the\nassets in a connection before running in this way! 2.2.3 4.0.0 To hard-delete (purge) all assets in a connection: Java Python Kotlin Raw REST API Purge assets 1 2 3 4 5 6 Workflow workflow = ConnectionDelete . creator ( // (1) \"default/snowflake/1234567890\" , true // (2) ). build () // (3) . toWorkflow (); // (4) WorkflowResponse response = workflow . run ( client ); // (5) The ConnectionDelete package will create a workflow to delete a connection and its assets using the creator() method. You need to provide the following: qualified name of the connection whose assets should be deleted. whether to permanently delete the connection and its assets (hard-delete) ( true ), or only archive (soft-delete) them ( false ). Build the minimal package object. Convert the package into a Workflow object. Run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how to check the status and wait until the workflow has been completed. Purge assets 1 2 3 4 5 6 7 8 9 10 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import ConnectionDelete client = AtlanClient () workflow = ConnectionDelete ( # (1) qualified_name = \"default/snowflake/1234567890\" , purge = True # (2) ) . to_workflow () # (3) response = client . workflow . run ( workflow ) # (4) The ConnectionDelete package will create\na workflow to delete a connection and its assets. You need to provide the following: qualified name of the connection whose assets should be deleted. whether to permanently delete the connection and its assets\n(hard-delete) ( True ), or only archive (soft-delete) them ( False ). Convert the package into a Workflow object. Run the workflow by invoking the run() method\non the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how to check the status and wait\nuntil the workflow has been completed. Purge assets 1 2 3 4 5 6 val workflow = ConnectionDelete . creator ( // (1) \"default/snowflake/1234567890\" , true // (2) ). build () // (3) . toWorkflow () // (4) val response = workflow . run ( client ) // (5) The ConnectionDelete package will create a workflow to delete a connection and its assets using the creator() method. You need to provide the following: qualified name of the connection whose assets should be deleted. whether to permanently delete the connection and its assets (hard-delete) ( true ), or only archive (soft-delete) them ( false ). Build the minimal package object. Convert the package into a Workflow object. Run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how to check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing connection delete workflow: Java Python Kotlin Raw REST API Re-run existing connection delete workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , ConnectionDelete . PREFIX , 5 ); // (2) // Determine which of the results is the Connection delete workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the ConnectionDelete . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing connection delete workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . CONNECTION_DELETE , max_results = 5 ) # Determine which Connection delete workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the ConnectionDelete . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing connection delete workflow 1 2 3 4 5 val existing = WorkflowSearchRequest // (1) . findByType ( client , ConnectionDelete . PREFIX , 5 ) // (2) // Determine which of the results is the // connection delete workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the ConnectionDelete . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-connection-delete\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-connection-delete prefix will ensure you only find existing connection delete workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. \n(Remember since this is a search, there could be multiple results, so you may want to use the other details \nin each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-connection-delete-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/databricks-miner/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Databricks miner package Â¶ The Databricks miner package extract lineage and usage from databricks to Atlan for discovery. Will create a new connection This should only be used to create the workflow the first time. Each time you run this method\nit will create a new connection and new assets within that connection â€” which could lead to duplicate\nassets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 6.0.0 To extract lineage and usage from databricks to Atlan for discovery. Java Python Kotlin Raw REST API Coming soon Extract lineage and usage from databricks 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import DatabricksMiner client = AtlanClient () crawler = ( DatabricksMiner ( # (1) connection_qualified_name = \"default/databricks/1234567890\" # (2) ) . rest_api () # (3) . popularity_configuration ( # (4) start_date = \"1234567890\" , extraction_method = DatabricksMiner . ExtractionMethod . SYSTEM_TABLE , window_days = 30 , excluded_users = [ \"test-user-1\" , \"test-user-2\" ], warehouse_id = \"test-warehouse-id\" , ) . to_workflow () # (5) ) response = client . workflow . run ( crawler ) # (6) Base configuration for a new Databricks miner. You must provide the exact qualified_name of the Databricks\nconnection in Atlan for which you want to mine query history. You can sets up the Databricks miner to use the REST API method for fetching lineage. You can also utilize any of the following methods for fetching lineage: offline() bucket_name: name of the S3 bucket to extract data from. bucket_prefix: prefix within the S3 bucket to narrow the extraction scope. system_table() warehouse_id: unique identifier of the SQL warehouse to be used for system table extraction. Optionally, you can define popularity_configuration() : epoch timestamp from which queries will be fetched\nfor calculating popularity. This does not affect lineage generation. method used to fetch popularity data. Defaults to ExtractionMethod.REST_API . (Optional) number of days to consider for calculating popularity metrics. (Optional) list of usernames to exclude from usage metrics calculations. (Optional) unique identifier of the SQL warehouse to use for popularity calculations.\n   Required if extraction_method is ExtractionMethod.SYSTEM_TABLE . Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status\nand wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.1.0 To re-run an existing workflow for databricks assets: Java Python Kotlin Raw REST API Coming soon Re-run existing databricks miner workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . DATABRICKS_LINEAGE , max_results = 5 ) # Determine which Databricks workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the DatabricksMiner . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-databricks-lineage\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-databricks-lineage prefix will ensure you only find existing Databricks assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-databricks-lineage-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/databricks-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Databricks assets package Â¶ The Databricks assets package crawls databricks assets and publishes them to Atlan for discovery. Direct extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method\nit will create a new connection and new assets within that connection â€” which could lead to duplicate\nassets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 To crawl assets directly from databricks: Java Python Kotlin Raw REST API Coming soon Direct extraction from databricks 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import DatabricksCrawler client = AtlanClient () crawler = ( DatabricksCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . direct ( hostname = \"test-hostname\" , port = 443 ) # (8) . basic_auth ( # (9) personal_access_token = \"test-pat\" , http_path = \"test-http-path\" , ) . metadata_extraction_method ( type = DatabricksCrawler . ExtractionMethod . JDBC ) # (10) . include ( assets = { \"test-include\" : [ \"ti1\" , \"ti2\" ]}) # (11) . exclude ( assets = { \"test-exclude\" : [ \"te1\" , \"te2\" ]}) # (12) . exclude_regex ( regex = \"TEST*\" ) # (13) . enable_view_lineage ( False ) # (14) . enable_source_level_filtering ( True ) # (15) . to_workflow () # (16) ) response = client . workflow . run ( crawler ) # (17) Base configuration for a new Databricks crawler. You must provide a client instance. You must provide a name for the connection that the Databricks assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. defaults to 10000 . You can specify whether you want to allow queries to this connection\n(default: True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n(default: True , as in this example) or deny all sample data previews to the connection ( False ). When crawling assets directly from Databricks,\nyou are required to provide the following information: hostname of the Databricks instance. port number of the Databricks instance. default: 443 . When using basic authentication,\nyou are required to provide the following information: personal access token through which to access Databricks instance. HTTP path of your Databricks instance You can also utilize any of the following authentication methods: aws_service() client_id: client ID for your AWS service principal. client_secret: client secret for your AWS service principal. azure_service() client_id: client ID for Azure service principal. client_secret: client secret for your Azure service principal. tenant_id: tenant ID (directory ID) for Azure service principal. Determines the interface that the package\nwill use to extract metadata from Databricks.\nJDBC is the recommended method ( default ).\nREST API method is supported only by Unity Catalog enabled instances. You can also optionally specify the list of assets to include in crawling.\nFor Databricks assets, this should be specified as a list of database names.\nIf set to [] , all databases will be crawled. Recommendation When using the DatabricksCrawler.ExtractionMethod.REST extraction method, ensure that you use the include_for_rest_api() method, which accepts a list of database names to include during crawling. You can also optionally specify the list of assets to exclude from crawling.\nFor Databricks assets, this should be specified as a list of database GUIDs.\nIf set to [] , no databases will be excluded. Recommendation When using the DatabricksCrawler.ExtractionMethod.REST extraction method, ensure that you use the exclude_for_rest_api() method, which accepts a list of database names to exclude during crawling. You can also optionally specify the exclude regex for the crawler\nto ignore collections based on a naming convention. You can configure whether to enable view lineage as part of crawling Databricks (default: True ). You can configure advanced settings to enable\n( True ) or disable ( False ) schema-level filtering on the source. \nSchemas specified in the include filter will be fetched. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status\nand wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Offline extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method\nit will create a new connection and new assets within that connection â€” which could lead to duplicate\nassets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 To crawl databricks assets from the S3 bucket: Java Python Kotlin Raw REST API Coming soon Crawling databricks assets from a bucket 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import DatabricksCrawler client = AtlanClient () crawler = ( DatabricksCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . s3 ( # (8) bucket_name = \"test-bucket\" , bucket_prefix = \"test-prefix\" , bucket_region = \"test-region\" , ) . to_workflow () # (9) ) response = client . workflow . run ( crawler ) # (10) Base configuration for a new Databricks crawler. You must provide a client instance. You must provide a name for the connection that the Databricks assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. defaults to 10000 . You can specify whether you want to allow queries to this connection\n(default: True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n(default: True , as in this example) or deny all sample data previews to the connection ( False ). When using s3() , you need to provide the following information: name of the bucket/storage that contains the extracted metadata files. prefix is everything after the bucket/storage name, including the path . (Optional) name of the region if applicable. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status\nand wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.1.0 To re-run an existing workflow for databricks assets: Java Python Kotlin Raw REST API Coming soon Re-run existing databricks workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . DATABRICKS , max_results = 5 ) # Determine which Databricks workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the DatabricksCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-databricks\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-databricks prefix will ensure you only find existing Databricks assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-databricks-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/dbt-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) dbt assets package Â¶ The dbt assets package crawls dbt assets and publishes them to Atlan for discovery. Cloud Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method\nit will create a new connection and new assets within that connection â€” which could lead to\nduplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 4.0.0 To create a new crawl of dbt assets from a multi-tenant dbt Cloud account: Java Python Kotlin Raw REST API dbt Cloud multi-tenant 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Workflow crawler = DbtCrawler . creator ( // (1) client , // (2) \"dbt-snowflake\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null ) . cloud ( // (5) \"https://cloud.getdbt.com\" , \"example-token\" , true ) . limitToConnection ( // (6) \"default/snowflake/1234567890\" ) . include ( \"{\\\"24670\\\":{\\\"211208\\\":{\\\"163013\\\":{\\\"207502\\\":{}}}}}\" ) // (7) . exclude ( \"{\\\"24670\\\":{}}\" ) // (8) . tags ( true ) // (9) . enrichMaterializedAssets ( true ) // (10) . build () // (11) . toWorkflow (); // (12) WorkflowResponse response = crawler . run ( client ); // (13) The DbtCrawler package will create a workflow to crawl assets from dbt cloud. You must provide Atlan client. You must provide a name for the connection that the dbt assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. To configure the crawler for extracting dbt assets directly from dbt cloud account then you must provide the following information: hostname of your dbt cloud instance. token to use to authenticate against dbt cloud instance. whether to use a multi-tenant cloud config ( true ), otherwise a single-tenant cloud config ( false ). You can also optionally specify the qualifiedName of a connection to a source (such as Snowflake), to limit the crawling of dbt assets to that existing connection in Atlan. You can also optionally specify the list of assets to include in crawling. This is a highly-nested map structure of numeric IDs for the account, project, etc. You will almost certainly need to set up the filter you want through the UI first, and look at the developer console of your browser to see the structure. You can also optionally specify the list of assets to exclude from crawling. This follows the same structure as the inclusion filter. You can also optionally specify whether to enable dbt tag syncing as part of crawling dbt. You can also optionally set whether to enable the enrichment of materialized SQL assets as part of crawling dbt. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. dbt Cloud multi-tenant 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import DbtCrawler client = AtlanClient () crawler = ( DbtCrawler ( # (1) client = client , # (2) connection_name = \"dbt-snowflake\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , ) . cloud ( # (5) hostname = \"https://cloud.getdbt.com\" , service_token = \"example-token\" , multi_tenant = True , ) . limit_to_connection ( # (6) connection_qualified_name = \"default/snowflake/1234567890\" ) . include ( filter = '{\"24690\":{\"327645\": {} }}' ) # (7) . exclude ( filter = '' ) # (8) . tags ( True ) # (9) . enrich_materialized_assets ( True ) # (10) . to_workflow () # (11) ) response = client . workflow . run ( crawler ) # (12) Base configuration for a new dbt crawler. You must provide a client instance. You must provide a name for the connection that the dbt assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. To configure the crawler for extracting dbt assets directly from\ndbt cloud account then you must provide the following information: hostname of your dbt cloud instance. token to use to authenticate against dbt cloud instance. whether to use a multi-tenant cloud config ( True ), otherwise a single-tenant cloud config ( False ). You can also optionally specify the qualifiedName of a connection to a source (such as Snowflake),\nto limit the crawling of dbt assets to that existing connection in Atlan. You can also optionally specify the list of assets to include in crawling.\nThis is a highly-nested map structure of numeric IDs for the account, project, etc.\nYou will almost certainly need to set up the filter you want through the UI first,\nand look at the developer console of your browser to see the structure.\n(If set to None, all assets will be crawled.) You can also optionally specify the list of assets to exclude from crawling.\nThis follows the same structure as the inclusion filter.\n(If set to None, no assets will be excluded.) You can also optionally specify whether to\nenable dbt tag syncing as part of crawling dbt. You can also optionally set whether to enable\nthe enrichment of materialized SQL assets as part of crawling dbt. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. dbt Cloud multi-tenant 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 val crawler = DbtCrawler . creator ( // (1) client , // (2) \"dbt-snowflake\" , // (3) listOf ( client . getRoleCache (). getIdForName ( \"\\ $ admin \" )), // (4) null , null ) . cloud ( // (5) \"https://cloud.getdbt.com\" , \"example-token\" , true ) . limitToConnection ( // (6) \"default/snowflake/1234567890\" ) . include ( \"{\\\"24670\\\":{\\\"211208\\\":{\\\"163013\\\":{\\\"207502\\\":{}}}}}\" ) // (7) . exclude ( \"{\\\"24670\\\":{}}\" ) // (8) . tags ( true ) // (9) . enrichMaterializedAssets ( true ) // (10) . build () // (11) . toWorkflow () // (12) val response = crawler . run ( client ) // (13) The DbtCrawler package will create a workflow to crawl assets from dbt cloud. You must provide Atlan client. You must provide a name for the connection that the dbt assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. To configure the crawler for extracting dbt assets directly from dbt cloud account then you must provide the following information: hostname of your dbt cloud instance. token to use to authenticate against dbt cloud instance. whether to use a multi-tenant cloud config ( true ), otherwise a single-tenant cloud config ( false ). You can also optionally specify the qualifiedName of a connection to a source (such as Snowflake), to limit the crawling of dbt assets to that existing connection in Atlan. You can also optionally specify the list of assets to include in crawling. This is a highly-nested map structure of numeric IDs for the account, project, etc. You will almost certainly need to set up the filter you want through the UI first, and look at the developer console of your browser to see the structure. You can also optionally specify the list of assets to exclude from crawling. This follows the same structure as the inclusion filter. You can also optionally specify whether to enable dbt tag syncing as part of crawling dbt. You can also optionally set whether to enable the enrichment of materialized SQL assets as part of crawling dbt. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Core Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it\nwill create a new connection and new assets within that connection â€” which could lead to duplicate assets\nif you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 4.0.0 To create a new crawl of dbt assets from a dbt files location: Java Python Kotlin Raw REST API dbt Core 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Workflow crawler = DbtCrawler . creator ( // (1) client , // (2) \"dbt-snowflake\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null ) . core ( // (5) \"dbt-bucket\" , \"dbt-data\" , \"ap-south-1\" ) . limitToConnection ( // (6) \"default/snowflake/1234567890\" ) . tags ( true ) // (7) . enrichMaterializedAssets ( true ) // (8) . build () // (9) . toWorkflow (); // (10) WorkflowResponse response = crawler . run ( client ); // (11) The DbtCrawler package will create a workflow to crawl assets from dbt files location. You must provide Atlan client. You must provide a name for the connection that the dbt assets will exist within You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. To configure the crawler for extracting dbt assets directly from the dbt files location, you must provide the following information: s3 bucket containing the dbt Core files. prefix within the S3 bucket where the dbt Core files are located. s3 region where the bucket is located. You can also optionally specify the qualifiedName of a connection to a source (such as Snowflake), to limit the crawling of dbt assets to that existing connection in Atlan. You can also optionally specify the list of assets to include in crawling. This is a highly-nested map structure of numeric IDs for the account, project, etc. You will almost certainly need to set up the filter you want through the UI first, and look at the developer console of your browser to see the structure. You can also optionally set whether to enable the enrichment of materialized SQL assets as part of crawling dbt. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. dbt Core 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import DbtCrawler client = AtlanClient () crawler = ( DbtCrawler ( # (1) client = client , # (2) connection_name = \"dbt-snowflake\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , ) . core ( # (5) s3_bucket = \"dbt-bucket\" , s3_prefix = \"dbt-data\" , s3_region = \"ap-south-1\" , ) . limit_to_connection ( # (6) connection_qualified_name = \"default/snowflake/1234567890\" ) . tags ( True ) # (7) . enrich_materialized_assets ( True ) # (8) . to_workflow () # (9) ) response = client . workflow . run ( crawler ) # (10) Base configuration for a new dbt crawler. You must provide a client instance. You must provide a name for the connection that the dbt assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. To configure the crawler for extracting dbt assets directly\nfrom the dbt files location, you must provide the following information: s3 bucket containing the dbt Core files. prefix within the S3 bucket where the dbt Core files are located. s3 region where the bucket is located. You can also optionally specify the qualifiedName of a connection to a source (such as Snowflake),\nto limit the crawling of dbt assets to that existing connection in Atlan. You can also optionally specify whether to\nenable dbt tag syncing as part of crawling dbt. You can also optionally set whether to enable\nthe enrichment of materialized SQL assets as part of crawling dbt. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. dbt Core 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 val crawler = DbtCrawler . creator ( // (1) client , // (2) \"dbt-snowflake\" , // (3) listOf ( client . getRoleCache (). getIdForName ( \"\\ $ admin \" )), // (4) null , null ) . core ( // (5) \"dbt-bucket\" , \"dbt-data\" , \"ap-south-1\" ) . limitToConnection ( // (6) \"default/snowflake/1234567890\" ) . tags ( true ) // (7) . enrichMaterializedAssets ( true ) // (8) . build () // (9) . toWorkflow () // (10) val response = crawler . run ( client ) // (11) The DbtCrawler package will create a workflow to crawl assets from dbt files location. You must provide Atlan client. You must provide a name for the connection that the dbt assets will exist within You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. To configure the crawler for extracting dbt assets directly from the dbt files location, you must provide the following information: s3 bucket containing the dbt Core files. prefix within the S3 bucket where the dbt Core files are located. s3 region where the bucket is located. You can also optionally specify the qualifiedName of a connection to a source (such as Snowflake), to limit the crawling of dbt assets to that existing connection in Atlan. You can also optionally specify the list of assets to include in crawling. This is a highly-nested map structure of numeric IDs for the account, project, etc. You will almost certainly need to set up the filter you want through the UI first, and look at the developer console of your browser to see the structure. You can also optionally set whether to enable the enrichment of materialized SQL assets as part of crawling dbt. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for dbt assets: Java Python Kotlin Raw REST API Re-run existing dbt workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , DbtCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the dbt workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the DbtCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing dbt workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . DBT , max_results = 5 ) # Determine which dbt workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the DbtCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing dbt workflow 1 2 3 4 5 val existing = WorkflowSearchRequest // (1) . findByType ( client , DbtCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // dbt workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the DbtCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-dbt\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-dbt prefix will ensure you only find existing dbt assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-dbt-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/lineage-builder/",
    "content": "Lineage builder package Â¶ The lineage builder package allows you to create lineage between any source and any target asset. Retrieve the lineage file from object store Â¶ To retrieve the lineage file from cloud object storage: Java Python Kotlin Raw REST API Coming soon Retrieve the lineage file from object store 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import LineageBuilder from pyatlan.model.enums import AssetInputHandling client = AtlanClient () workflow = ( LineageBuilder () # (1) . object_store ( # (2) prefix = \"text-prefix\" , object_key = \"test-object-key\" ) . s3 ( # (3) access_key = \"test-access-key\" , secret_key = \"test-secret-key\" , region = \"test-region\" , bucket = \"test-bucket\" , ) . options ( # (4) input_handling = AssetInputHandling . UPSERT , fail_on_errors = True , case_sensitive_match = False , field_separator = \",\" , batch_size = 20 , ) ) . to_workflow () # (5) response = client . workflow . run ( workflow ) # (6) Lineage builder package allows you to create lineage between any source and any target asset. Set up the package to retrieve the lineage file from cloud object storage. You can use different object store methods (e.g: s3() , gcs() , adls() ). In this example,\nwe're building a workflow using s3() and for that, youâ€™ll need to provide the following information: AWS access key. AWS secret key. name of the bucket/storage that contains the metadata CSV files. name of the AWS region. You can provide other following options() : input_handling : specifies whether to allow the creation\n    of new assets from the input CSV (full ( UPSERT )\n    or partial ( PARTIAL ) assets) or only update existing\n    ( UPDATE ) assets in Atlan. fail_on_errors : specifies whether an invalid value\n    in a field should cause the import to fail ( True ) or\n    log a warning, skip that value, and proceed ( False ). case_sensitive_match :  indicates whether to use\n    case-sensitive matching when running in update-only mode ( True )\n    or to try case-insensitive matching ( False ). field_separator : character used to separate\n    fields in the input file (e.g: ',' or ';' ). batch_size : maximum number of rows\n    to process at a time (per API request). Convert the package into a Workflow object. Run the workflow by invoking the run() method\non the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how to check the status and wait\nuntil the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ To re-run an existing lineage builder workflow: Java Python Kotlin Raw REST API Coming soon Re-run existing lineage builder workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . LINEAGE_BUILDER , max_results = 5 ) # Determine which lineage builder workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the LineageBuilder . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"csa-lineage-builder\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the csa-lineage-builder prefix will ensure you only find existing asset import workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"csa-lineage-builder-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/lineage-generator-nt/",
    "content": "Lineage generator package Â¶ The lineage generator (no transformation) package automatically detects assets with the same (or similar) name between two connections and creates the lineage between them. Configuration Â¶ Recommendation To avoid to blindly let the package to create the lineage,\nan option to preview the ouput is provided. The typical path to use this package would be: Ask the package to generate the lineage preview: If happy with the output, ask the package to generate the lineage on Atlan. The package also provides a method to delete lineage created by the package itself. To generate lineage by automatically detecting assets with similar names between two connections: Java Python Kotlin Raw REST API Coming soon Generate lineage for assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import LineageGenerator from pyatlan.model.assets import Asset from pyatlan.model.enums import AssetInputHandling client = AtlanClient () workflow = ( LineageGenerator () # (1) . config ( # (2) source_asset_type = LineageGenerator . SourceAssetType . Table , source_qualified_name = \"default/snowflake/1737456702/DB/SCHEMA/TABLE\" , target_asset_type = LineageGenerator . TargetAssetType . View , target_qualified_name = \"default/mssql/1737456702/DB/SCHEMA/VIEW\" , case_sensitive_match = False , match_on_schema = False , output_type = LineageGenerator . OutputType . PREVIEW , generate_on_child_assets = False , regex_match = \"regex_match/*\" , regex_replace = \"regex_replace/*\" , regex_match_schema = \"regex_match_schema/*\" , regex_replace_schema = \"regex_replace_schema/*\" , regex_match_schema_name = \"regex_match_schema_name/*\" , regex_replace_schema_name = \"regex_replace_schema_name/*\" , match_prefix = \"test-prefix\" , match_suffix = \"test-suffix\" , file_advanced_seperator = \"/\" , file_advanced_position = 3 , process_connection_qn = \"default/mssql/1737456702/DB/SCHEMA/MVIEW\" , ) ) . to_workflow () # (3) response = client . workflow . run ( workflow ) # (4) Lineage generator (no transformation) package automatically detects assets\nwith the same (or similar) name between two connections and creates the lineage between them. Set up the lineage generator using config() with the following: source_asset_type : type name of the lineage input assets (sources). source_qualified_name : qualified name prefix of the lineage input assets (sources). target_asset_type : type name of the lineage output assets (targets). target_qualified_name : qualified name prefix of the lineage output assets (targets). case_sensitive_match : whether to match asset names using case-sensitive logic, default: False . match_on_schema : whether to include the schema name to match source and target assets, default: False . Ignored if one of the asset types is not relational (e.g., Table , View , Materialized View , Calculation View , Column , or MongoDB Collection). output_type : determines the type of lineage generation, default: Preview . PREVIEW : generates a CSV preview of the lineage. GENERATE : creates the lineage on Atlan. DELETE : removes the lineage on Atlan. generate_on_child_assets : whether to generate lineage on child assets of the specified source a target types, default: False . regex_match (optional): regex pattern to identify renaming between source and target. regex_replace (optional): replacement characters for renaming identified by regex_match . regex_match_schema (optional): regex pattern for renaming between source and target schemas (used only if match_on_schema is False ). regex_replace_schema (optional): replacement characters for schema renaming identified by regex_match_schema . regex_match_schema_name (optional): regex pattern for renaming source and target name + schema (used only if match_on_schema is True ; overrides other regex patterns). regex_replace_schema_name (optional): replacement characters for schema name renaming identified by regex_match_schema_name . match_prefix (optional): prefix added to source assets to match with target assets. match_suffix (optional): suffix added to source assets to match with target assets. file_advanced_separator (optional): separator used to split the qualified name (applicable to file-based assets). eg: / splits default/s3/1707397085/arn:aws:s3:::mybucket/prefix/myobject.csv into [\"default\", \"s3\", \"1707397085\", \"arn:aws:s3:::mybucket\", \"prefix\", \"myobject.csv\"] . file_advanced_position (optional): number of substrings (from the right) to use for matching (applies to file-based assets). eg: if the value is 3 , it results in [\"arn:aws:s3:::mybucket\", \"prefix\", \"myobject.csv\"] . process_connection_qn (optional): connection for process assets. Defaults to the source asset connection if blank. Convert the package into a Workflow object. Run the workflow by invoking the run() method\non the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how to check the status and wait\nuntil the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ To re-run an existing lineage generator workflow: Java Python Kotlin Raw REST API Coming soon Re-run existing lineage generator workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . LINEAGE_GENERATOR , max_results = 5 ) # Determine which lineage generator workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the LineageGenerator . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"csa-lineage-generator\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the csa-lineage-generator prefix will ensure you only find existing asset import workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"csa-lineage-generator-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/looker-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Looker assets package Â¶ The Looker assets package crawls Looker assets and publishes them to Atlan for discovery. Direct extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it will create a new connection and new assets within that connection â€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 4.0.0 To crawl Looker assets directly from Looker: Java Python Kotlin Raw REST API Direct extraction from Looker 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Workflow looker = LookerCrawler . directResourceOwner ( // (1) \"production\" , // (2) \"https://example.cloud.looker.com\" , // (3) 443 , // (4) \"pEBDcOauyVfCtQacdvJUL\" , // (5) \"eKnUIFK2JvmVO5mruqbxjnzXf\" , // (6) \"-----BEGIN OPENSSH PRIVATE KEY-----\\n-99+PmSlex0FmY9ov1J8H1H9Y3IMWXbL...\\n-----END OPENSSH PRIVATE KEY-----\" , // (7) \"b3WnMIjaZJaTYZd\" , // (8) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (9) null , null , List . of ( \"67\" ), // (10) List . of ( \"dbt_food_beverage\" ), // (11) null , // (12) null ); // (13) WorkflowResponse response = looker . run ( client ); // (14) The LookerCrawler package will create a workflow to crawl assets from Looker. The directResourceOwner() method creates a workflow for crawling assets directly from Looker. You must provide a name for the connection that the Looker assets will exist within. You must provide the hostname of your Looker instance. You must specify the port number of the Looker instance (use 443 for the default). You must provide your admin client ID. You must provide your admin client secret. If you want to crawl field-level lineage, you must provide your SSH private key. If you want to crawl field-level lineage, you must provide the passphrase for your SSH private key (if any). You must specify at least one connection admin, either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins a list of users (names) that will be connection admins You can also optionally specify the list of folders to include in crawling. For Looker assets, this should be specified as a list of numeric folder IDs. (If set to null, all folders will be crawled.) You can also optionally specify the list of projects to include in crawling. For Looker assets, this should be specified as a list of project names. (If set to null, all projects will be crawled.) You can also optionally specify the list of folders to exclude from crawling. For Looker assets, this should be specified as a list of numeric folder IDs. (If set to null, no folders will be excluded.) You can also optionally specify the list of projects to exclude from crawling. For Looker assets, this should be specified as a list of project names. (If set to null, no projects will be excluded.) You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Direct extraction from Looker 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 val looker = LookerCrawler . directResourceOwner ( // (1) \"production\" , // (2) \"https://example.cloud.looker.com\" , // (3) 443 , // (4) \"pEBDcOauyVfCtQacdvJUL\" , // (5) \"eKnUIFK2JvmVO5mruqbxjnzXf\" , // (6) \"-----BEGIN OPENSSH PRIVATE KEY-----\\n-99+PmSlex0FmY9ov1J8H1H9Y3IMWXbL...\\n-----END OPENSSH PRIVATE KEY-----\" , // (7) \"b3WnMIjaZJaTYZd\" , // (8) listOf ( client . roleCache . getIdForName ( \"\\ $ admin \" )), // (9) null , null , List . of ( \"67\" ), // (10) List . of ( \"dbt_food_beverage\" ), // (11) null , // (12) null ) // (13) val response = looker . run ( client ) // (14) The LookerCrawler package will create a workflow to crawl assets from Looker. The directResourceOwner() method creates a workflow for crawling assets directly from Looker. You must provide a name for the connection that the Looker assets will exist within. You must provide the hostname of your Looker instance. You must specify the port number of the Looker instance (use 443 for the default). You must provide your admin client ID. You must provide your admin client secret. If you want to crawl field-level lineage, you must provide your SSH private key. If you want to crawl field-level lineage, you must provide the passphrase for your SSH private key (if any). You must specify at least one connection admin, either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins a list of users (names) that will be connection admins You can also optionally specify the list of folders to include in crawling. For Looker assets, this should be specified as a list of numeric folder IDs. (If set to null, all folders will be crawled.) You can also optionally specify the list of projects to include in crawling. For Looker assets, this should be specified as a list of project names. (If set to null, all projects will be crawled.) You can also optionally specify the list of folders to exclude from crawling. For Looker assets, this should be specified as a list of numeric folder IDs. (If set to null, no folders will be excluded.) You can also optionally specify the list of projects to exclude from crawling. For Looker assets, this should be specified as a list of project names. (If set to null, no projects will be excluded.) You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 4.0.0 To re-run an existing workflow for Looker assets: Java Python Kotlin Raw REST API Re-run existing Looker workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , LookerCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the Looker workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the LookerCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Re-run existing Looker workflow 1 2 3 4 val existing = WorkflowSearchRequest // (1) . findByType ( client , LookerCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the Looker workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the LookerCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-looker\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-looker prefix will ensure you only find existing Looker assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. (Remember since this is a search, there could be multiple results, so you may want to use the other details in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-looker-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/glue-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Glue assets package Â¶ The Glue assets package crawls AWS glue assets and publishes them to Atlan for discovery. Will create a new connection This should only be used to create the workflow the first time. Each time you run this \nmethod it will create a new connection and new assets within that connection â€” which could\nlead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 7.0.0 4.0.0 To crawl assets from AWS glue using the IAM user authentication: Java Python Kotlin Raw REST API Glue assets crawling using IAM user auth 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Workflow crawler = GlueCrawler . creator ( // (1) client , // (2) \"production\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null , false , // (5) false , // (6) 0 L // (7) ) . iamUserAuth ( // (8) \"your-access-key\" , \"your-secret-key\" ) . direct ( \"ap-south-1\" ) // (9) . include ( // (10) List . of ( \"dev-project\" ) ) . exclude ( List . of ()) // (11) . build () // (12) . toWorkflow (); // (13) WorkflowResponse response = crawler . run ( client ); // (14) The GlueCrawler package will create a workflow to crawl assets from AWS Glue. You must provide Atlan client. You must provide a name for the connection that the AWS Glue assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify whether you want to allow queries to this connection ( true ) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true ) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. When using iamUserAuth() , you need to provide the following information: access key for accessing AWS Glue. secret key for accessing AWS Glue. You must provide AWS region where Glue is set up. You can also optionally specify the list of schema names to include in crawling. (If set to null, all schemas will be crawled.) You can also optionally specify the list of schema names to exclude from crawling. (If set to null, no schemas will be excluded.) Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Glue assets crawling using IAM user auth 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import GlueCrawler client = AtlanClient () crawler = ( GlueCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 0 , # (5) allow_query = False , # (6) allow_query_preview = False , # (7) ) . iam_user_auth ( # (8) access_key = \"your-access-key\" , secret_key = \"your-secret-key\" , ) . direct ( region = \"ap-south-1\" ) # (9) . include ( assets = [ 'dev-project' ]) # (10) . exclude ( assets = []) # (11) . to_workflow () # (12) ) response = client . workflow . run ( crawler ) # (13) Base configuration for a new AWS Glue crawler. You must provide a client instance. You must provide a name for the connection that the AWS Glue assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection\n( True ) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True ) or deny all sample data previews to the connection ( False ). When using iam_user_auth() , you need to provide the following information: access key for accessing AWS Glue. secret key for accessing AWS Glue. You must provide AWS region where Glue is set up. You can also optionally specify the list of schema names to include in crawling.\n(If set to None, all schemas will be crawled.) You can also optionally specify the list of schema names to exclude from crawling.\n(If set to None, no schemas will be excluded.) Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Glue assets crawling using IAM user auth 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 val crawler = GlueCrawler . creator ( // (1) client , // (2) \"production\" , // (3) listOf ( client . getRoleCache (). getIdForName ( \"\\ $ admin \" )), // (4) null , null , false , // (5) false , // (6) 0L // (7) ) . iamUserAuth ( // (8) \"your-access-key\" , \"your-secret-key\" ) . direct ( \"ap-south-1\" ) // (9) . include ( // (10) listOf ( \"dev-project\" ) ) . exclude ( emptyList ()) // (11) . build () // (12) . toWorkflow () // (13) val response = crawler . run ( client ) // (14) The GlueCrawler package will create a workflow to crawl assets from AWS Glue. You must provide Atlan client. You must provide a name for the connection that the AWS Glue assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify whether you want to allow queries to this connection ( true ) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true ) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. When using iamUserAuth() , you need to provide the following information: access key for accessing AWS Glue. secret key for accessing AWS Glue. You must provide AWS region where Glue is set up. You can also optionally specify the list of schema names to include in crawling. (If set to null, all schemas will be crawled.) You can also optionally specify the list of schema names to exclude from crawling. (If set to null, no schemas will be excluded.) Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for Glue assets: Java Python Kotlin Raw REST API Re-run existing Glue workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , GlueCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the Glue workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the GlueCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Glue workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . GLUE , max_results = 5 ) # Determine which Glue workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the GlueCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Glue workflow 1 2 3 4 5 val existing = WorkflowSearchRequest // (1) . findByType ( client , GlueCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // Glue workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the GlueCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-glue\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-glue prefix will ensure you only find existing Glue assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-glue-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/fivetran-enrichment/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Fivetran enrichment package Â¶ The Fivetran enrichment package enriches existing assets in Atlan associated with Fivetran connectors with column-level lineage. Requires access to Fivetran's metadata API Direct API Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it will create a new connection and new assets within that connection â€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 1.0.0 To enrich assets from Fivetran via API: Java Python Kotlin Raw REST API Fivetran enrichment via API 1 2 3 4 5 Workflow fivetran = FivetranCrawler . directApiAuth ( // (1) \"production\" , // (2) \"E77yqOsBPrRXpVp0\" , // (3) \"XLQR73AKwGYmjzk5vlBMAUG4wo13VyY\" ); // (4) WorkflowResponse response = fivetran . run ( client ); // (5) The FivetranCrawler package will create a workflow to enrich assets from Fivetran. The directApiAuth() method creates a workflow for crawling assets from Fivetran's API. You must provide a name for the connection for Fivetran enrichment. (Though this is currently unused since the workflow only enriches existing assets, and does not create any new assets.) You must provide your API key for access to Fivetran's metadata API. You must specify your API secret for access to Fivetran's metadata API. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Fivetran enrichment via API 1 2 3 4 5 val fivetran = FivetranCrawler . directApiAuth ( // (1) \"production\" , // (2) \"E77yqOsBPrRXpVp0\" , // (3) \"XLQR73AKwGYmjzk5vlBMAUG4wo13VyY\" ) // (4) val response = fivetran . run ( client ) // (5) The FivetranCrawler package will create a workflow to enrich assets from Fivetran. The directApiAuth() method creates a workflow for crawling assets from Fivetran's API. You must provide a name for the connection for Fivetran enrichment. (Though this is currently unused since the workflow only enriches existing assets, and does not create any new assets.) You must provide your API key for access to Fivetran's metadata API. You must specify your API secret for access to Fivetran's metadata API. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 4.0.0 To re-run an existing workflow for Fivetran assets: Java Python Kotlin Raw REST API Re-run existing Fivetran workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , FivetranCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the Fivetran workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the FivetranCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Re-run existing Fivetran workflow 1 2 3 4 val existing = WorkflowSearchRequest // (1) . findByType ( client , FivetranCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the Fivetran workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the FivetranCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-fivetran\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-fivetran prefix will ensure you only find existing Fivetran assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. (Remember since this is a search, there could be multiple results, so you may want to use the other details in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-fivetran-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/postgres-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Postgres assets package Â¶ The Postgres assets package crawls PostgreSQL assets and publishes them to Atlan for discovery. Direct extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it will create a new connection and new assets within that connection â€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 4.0.0 7.0.0 To crawl assets directly from PostgreSQL using basic authentication: Java Python Kotlin Raw REST API Direct extraction from PostgreSQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Workflow postgres = PostgreSQLCrawler . directBasicAuth ( // (1) \"production\" , // (2) \"postgres.x9f0ve2k1kvy.ap-south-1.rds.amazonaws.com\" , // (3) 5432 , // (4) \"postgres\" , // (5) \"nCkM685ZH9g4fVICMs6H\" , // (6) \"demo_db\" , // (7) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (8) null , null , true , // (9) true , // (10) 10000L , // (11) Map . of ( \"demo_db\" , List . of ( \"demo\" )), // (12) null ); // (13) WorkflowResponse response = postgres . run (); // (14) The PostgreSQLCrawler package will create a workflow to crawl assets from PostgreSQL. The directBasicAuth() method creates a workflow for crawling assets directly from PostgreSQL. You must provide a name for the connection that the PostgreSQL assets will exist within. You must provide the hostname of your PostgreSQL instance. You must specify the port number of the PostgreSQL instance (use 5432 for the default). You must provide your PostgreSQL username. You must provide your PostgreSQL password. You must specify the name of the PostgreSQL database you want to crawl. You must specify at least one connection admin, either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins a list of users (names) that will be connection admins You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. You can also optionally specify the set of assets to include in crawling. For PostgreSQL assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the list of assets to exclude from crawling. For PostgreSQL assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Direct extraction from PostgreSQL using basic auth 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import PostgresCrawler client = AtlanClient () crawler = ( PostgresCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . direct ( hostname = \"test.com\" , database = \"test-db\" ) # (8) . basic_auth ( # (9) username = \"test-user\" , password = \"test-password\" , ) . include ( assets = { \"test-include\" : [ \"test-asset-1\" , \"test-asset-2\" ]}) # (10) . exclude ( assets = None ) # (11) . exclude_regex ( regex = \".*_TEST\" ) # (12) . source_level_filtering ( enable = True ) # (13) . jdbc_internal_methods ( enable = True ) # (14) . to_workflow () # (15) ) response = client . workflow . run ( crawler ) # (16) Base configuration for a new PostgresCrawler crawler. You must provide a client instance. You must provide a name for the connection that the PostgreSQL assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). You can specify the hostname of your Postgres instance and database name for direct extraction. When using basic_auth() , you need to provide the following information: username through which to access PostgreSQL. password through which to access PostgreSQL. You can also optionally specify the set of assets to\ninclude in crawling. For Postgres assets, this should be specified\nas a dict keyed by database name with each value being\na list of schemas to include. (If set to None, all table will be crawled.) You can also optionally specify the list of assets to\nexclude from crawling. For Postgres assets, this should be\nspecified as a dict keyed by database name with each value being\na list of schemas to exclude. (If set to None, no table will be excluded.) You can also optionally specify the exclude regex for\ncrawler ignore tables and views based on a naming convention. You can also optionally specify whether to enable ( True ) or disable ( False ) schema\nlevel filtering on source, schemas selected in the include filter will be fetched. You can also optionally specify whether to enable ( True ) or disable ( False ) JDBC\ninternal methods for data extraction. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status\nand wait until the workflow has been completed. Direct extraction from PostgreSQL 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 val postgres = PostgreSQLCrawler . directBasicAuth ( // (1) \"production\" , // (2) \"postgres.x9f0ve2k1kvy.ap-south-1.rds.amazonaws.com\" , // (3) 5432 , // (4) \"postgres\" , // (5) \"nCkM685ZH9g4fVICMs6H\" , // (6) \"demo_db\" , // (7) listOf ( client . roleCache . getIdForName ( \"\\ $ admin \" )), // (8) null , null , true , // (9) true , // (10) 10000L , // (11) mapOf ( \"demo_db\" to listOf ( \"demo\" )), // (12) null ) // (13) WorkflowResponse response = postgres . run (); // (14) The PostgreSQLCrawler package will create a workflow to crawl assets from PostgreSQL. The directBasicAuth() method creates a workflow for crawling assets directly from PostgreSQL. You must provide a name for the connection that the PostgreSQL assets will exist within. You must provide the hostname of your PostgreSQL instance. You must specify the port number of the PostgreSQL instance (use 5432 for the default). You must provide your PostgreSQL username. You must provide your PostgreSQL password. You must specify the name of the PostgreSQL database you want to crawl. You must specify at least one connection admin, either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins a list of users (names) that will be connection admins You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. You can also optionally specify the set of assets to include in crawling. For PostgreSQL assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the list of assets to exclude from crawling. For PostgreSQL assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. IAM user authentication Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it will create a new connection and new assets within that connection â€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 7.0.0 To crawl assets directly from PostgreSQL using IAM user authentication: Java Python Raw REST API Coming soon PostgreSQL assets crawling using IAM user authentication 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import PostgresCrawler client = AtlanClient () crawler = ( PostgresCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . direct ( hostname = \"test.com\" , database = \"test-db\" ) # (8) . iam_user_auth ( # (9) username = \"test-user\" , access_key = \"test-access-key\" , secret_key = \"test-secret-key\" , ) . include ( assets = { \"test-include\" : [ \"test-asset-1\" , \"test-asset-2\" ]}) # (10) . exclude ( assets = None ) # (11) . exclude_regex ( regex = \".*_TEST\" ) # (12) . source_level_filtering ( enable = True ) # (13) . jdbc_internal_methods ( enable = True ) # (14) . to_workflow () # (15) ) response = client . workflow . run ( crawler ) # (16) Base configuration for a new PostgresCrawler crawler. You must provide a client instance. You must provide a name for the connection that the PostgreSQL assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). You can specify the hostname of your Postgres instance and database name for direct extraction. When using iam_user_auth() , you need to provide the following information: database username to extract from. access key through which to access PostgreSQL. secret key through which to access PostgreSQL. You can also optionally specify the set of assets to\ninclude in crawling. For Postgres assets, this should be specified\nas a dict keyed by database name with each value being\na list of schemas to include. (If set to None, all table will be crawled.) You can also optionally specify the list of assets to\nexclude from crawling. For Postgres assets, this should be\nspecified as a dict keyed by database name with each value being\na list of schemas to exclude. (If set to None, no table will be excluded.) You can also optionally specify the exclude regex for\ncrawler ignore tables and views based on a naming convention. You can also optionally specify whether to enable ( True ) or disable ( False ) schema\nlevel filtering on source, schemas selected in the include filter will be fetched. You can also optionally specify whether to enable ( True ) or disable ( False ) JDBC\ninternal methods for data extraction. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status\nand wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. IAM role authentication Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it will create a new connection and new assets within that connection â€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 7.0.0 To crawl assets directly from PostgreSQL using IAM role authentication: Java Python Raw REST API Coming soon PostgreSQL assets crawling using IAM role authentication 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import PostgresCrawler client = AtlanClient () crawler = ( PostgresCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . direct ( hostname = \"test.com\" , database = \"test-db\" ) # (8) . iam_role_auth ( # (9) username = \"test-user\" , access_key = \"test-access-key\" , secret_key = \"test-secret-key\" , ) . include ( assets = { \"test-include\" : [ \"test-asset-1\" , \"test-asset-2\" ]}) # (10) . exclude ( assets = None ) # (11) . exclude_regex ( regex = \".*_TEST\" ) # (12) . source_level_filtering ( enable = True ) # (13) . jdbc_internal_methods ( enable = True ) # (14) . to_workflow () # (15) ) response = client . workflow . run ( crawler ) # (16) Base configuration for a new PostgresCrawler crawler. You must provide a client instance. You must provide a name for the connection that the PostgreSQL assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). You can specify the hostname of your Postgres instance and database name for direct extraction. When using iam_role_auth() , you need to provide the following information: database username to extract from. ARN of the AWS role. AWS external ID. You can also optionally specify the set of assets to\ninclude in crawling. For Postgres assets, this should be specified\nas a dict keyed by database name with each value being\na list of schemas to include. (If set to None, all table will be crawled.) You can also optionally specify the list of assets to\nexclude from crawling. For Postgres assets, this should be\nspecified as a dict keyed by database name with each value being\na list of schemas to exclude. (If set to None, no table will be excluded.) You can also optionally specify the exclude regex for\ncrawler ignore tables and views based on a naming convention. You can also optionally specify whether to enable ( True ) or disable ( False ) schema\nlevel filtering on source, schemas selected in the include filter will be fetched. You can also optionally specify whether to enable ( True ) or disable ( False ) JDBC\ninternal methods for data extraction. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status\nand wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Offline extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it will create a new connection and new assets within that connection â€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 7.0.0 To crawl PostgeSQL assets from the S3 bucket: Java Python Raw REST API Coming soon Crawling PostgreSQL assets from a bucket 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import PostgresCrawler client = AtlanClient () crawler = ( PostgresCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . s3 ( # (8) bucket_name = \"test-bucket\" , bucket_prefix = \"test-prefix\" , bucket_region = \"test-region\" , ) . source_level_filtering ( enable = True ) # (9) . jdbc_internal_methods ( enable = True ) # (10) . to_workflow () # (11) ) response = client . workflow . run ( crawler ) # (12) Base configuration for a new PostgresCrawler crawler. You must provide a client instance. You must provide a name for the connection that the PostgreSQL assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). When using s3() , you need to provide the following information: name of the bucket/storage that contains the extracted metadata files. prefix is everything after the bucket/storage name, including the path . (Optional) name of the region if applicable. You can also optionally specify whether to enable ( True ) or disable ( False ) schema\nlevel filtering on source, schemas selected in the include filter will be fetched. You can also optionally specify whether to enable ( True ) or disable ( False ) JDBC\ninternal methods for data extraction. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status\nand wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 4.0.0 To re-run an existing workflow for PostgreSQL assets: Java Python Kotlin Raw REST API Re-run existing PostgreSQL workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , PostgreSQLCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the PostgreSQL workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the PostgreSQLCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing DynamoDB workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . POSTGRES , max_results = 5 ) # Determine which DynamoDB workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the PostgreSQLCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing PostgreSQL workflow 1 2 3 4 val existing = WorkflowSearchRequest // (1) . findByType ( client , PostgreSQLCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the PostgreSQL workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the PostgreSQLCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-postgres\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-postgres prefix will ensure you only find existing PostgreSQL assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. (Remember since this is a search, there could be multiple results, so you may want to use the other details in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-postgres-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/mongodb-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) MongoDB assets package Â¶ The MongoDB assets package crawls MongoDB assets and publishes them to Atlan for discovery. Direct extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method\nit will create a new connection and new assets within that connection â€” which could lead to duplicate\nassets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 To crawl assets directly from MongoDB: Java Python Kotlin Raw REST API Coming soon Direct extraction from MongoDB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import MongoDBCrawler client = AtlanClient () crawler = ( MongoDBCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . direct ( hostname = \"test.host.mongodb.net\" , port = 27017 ) # (8) . basic_auth ( # (9) username = \"test-user\" , password = \"test-pass\" , native_host = \"test.native.mongodb.net\" , default_db = \"test-default-db\" , auth_db = \"test-auth-db\" , is_ssl = True , ) . include ( assets = [ \"test-asset-1\" , \"test-asset-2\" ]) # (10) . exclude ( assets = [ \"test-asset-1\" , \"test-asset-2\" ]) # (11) . exclude_regex ( regex = \"TEST*\" ) # (12) . to_workflow () # (13) ) response = client . workflow . run ( crawler ) # (14) Base configuration for a new MongoDB crawler. You must provide a client instance. You must provide a name for the connection that the MongoDB assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. defaults to 10000 . You can specify whether you want to allow queries to this connection\n(default: True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n(default: True , as in this example) or deny all sample data previews to the connection ( False ). When crawling assets directly from MongoDB,\nyou are required to provide the following information: hostname of the Atlas SQL connection. port number of the Atlas SQL connection. default: 27017 . When using basic authentication,\nyou are required to provide the following information: username through which to access Atlas SQL connection. password through which to access Atlas SQL connection. native host address for the MongoDB connection. default database to connect to. authentication database to use (default: \"admin\" ). whether to use SSL for the connection (default: True ). You can also optionally specify the list of assets to include in crawling.\nFor MongoDB assets, this should be specified as a list of database names.\nIf set to [] , all databases will be crawled. You can also optionally specify the list of assets to exclude from crawling.\nFor MongoDB assets, this should be specified as a list of database GUIDs.\nIf set to [] , no databases will be excluded. You can also optionally specify the exclude regex for the crawler\nto ignore collections based on a naming convention. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 4.0.0 To re-run an existing workflow for MongoDB assets: Java Python Kotlin Raw REST API Coming soon Re-run existing MongoDB workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . MONGODB , max_results = 5 ) # Determine which MongoDB workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the MongoDBCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-mongodb\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-mongodb prefix will ensure you only find existing MongoDB assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-mongodb-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/oracle-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Oracle assets package Â¶ The Oracle assets package crawls Oracle assets and publishes them to Atlan for discovery. Direct extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it \nwill create a new connection and new assets within that connection â€” which could lead to duplicate assets\nif you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 6.0.0 To crawl assets directly from Oracle: Java Python Kotlin Raw REST API Coming soon Direct extraction from Oracle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import OracleCrawler client = AtlanClient () crawler = ( OracleCrawler ( # (1) connection_name = \"production\" , # (2) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (3) admin_groups = None , admin_users = None , row_limit = 10000 , # (4) allow_query = True , # (5) allow_query_preview = True , # (6) ) . direct ( hostname = \"test.oracle.com\" , port = 1521 ) # (7) . basic_auth ( # (8) username = \"test-username\" , password = \"test-password\" , sid = \"test-sid\" , database_name = \"test-db\" , ) . include ( assets = { \"ANALYTICS\" : [ \"SALES\" , \"CUSTOMER\" ]}) # (9) . exclude ( assets = {}) # (10) . exclude_regex ( \"TEST*\" ) # (11) . jdbc_internal_methods ( True ) # (12) . source_level_filtering ( False ) # (13) . to_workflow () # (14) ) response = client . workflow . run ( crawler ) # (15) Base configuration for a new Oracle crawler. You must provide a name for the connection that the Oracle assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). To configure the crawler for extracting data directly from Oracle\nthen you must provide the following information: hostname of your Oracle instance. port number of the Oracle instance (use 1521 for the default). When using basic_auth() , you must provide the following information: username through which to access Oracle password through which to access Oracle SID (system identifier) of the Oracle instance database name to crawl You can also optionally specify the set of assets to include in crawling. For Oracle assets,\nthis should be specified as a dict keyed by database name with values as a list of schemas within that\ndatabase to crawl. (If set to None , all databases and schemas will be crawled.) You can also optionally specify the list of assets to exclude from crawling.\nFor Oracle assets, this should be specified as a dict keyed by database name with values\nas a list of schemas within the database to exclude. (If set to None , no assets will be excluded.) You can also optionally specify the exclude regex for\ncrawler ignore tables and views based on a naming convention. You can also optionally specify whether to enable ( True ) or disable ( False ) JDBC\ninternal methods for data extraction. You can also optionally specify whether to enable ( True ) or disable ( False ) schema\nlevel filtering on source, schemas selected in the include filter will be fetched. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Offline extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it \nwill create a new connection and new assets within that connection â€” which could lead to duplicate assets\nif you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 To crawl Oracle assets from the S3 bucket: Java Python Kotlin Raw REST API Coming soon Crawl assets from the S3 bucket 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import OracleCrawler client = AtlanClient () crawler = ( OracleCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . s3 ( # (8) bucket_name = \"test-bucket\" , bucket_prefix = \"test-prefix\" , ) . jdbc_internal_methods ( True ) # (9) . source_level_filtering ( False ) # (10) . to_workflow () # (11) ) response = client . workflow . run ( crawler ) # (12) Base configuration for a new Oracle crawler. You must provide a client instance. You must provide a name for the connection that the Oracle assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). When using s3() , you need to provide the following information: name of the bucket/storage that contains the extracted metadata files. prefix is everything after the bucket/storage name, including the path . You can also optionally specify whether to enable ( True ) or disable ( False ) JDBC\ninternal methods for data extraction. You can also optionally specify whether to enable ( True ) or disable ( False ) schema\nlevel filtering on source, schemas selected in the include filter will be fetched. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Secure agent extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it \nwill create a new connection and new assets within that connection â€” which could lead to duplicate assets\nif you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 In this example, we demonstrates creating an Oracle Secure Agent workflow with basic authentication ( OracleCrawler.AuthType.BASIC ). You can create a similar workflow for Kerberos authentication ( OracleCrawler.AuthType.KERBEROS ) by providing the necessary configuration. Java Python Kotlin Raw REST API Coming soon To crawl Oracle assets in the offline agent 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import OracleCrawler client = AtlanClient () crawler = ( OracleCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . agent_config ( # (8) hostname = \"test.oracle.com\" , port = 1521 , auth_type = OracleCrawler . AuthType . BASIC , default_db_name = \"test-db\" , sid = \"test-sid\" , agent_name = \"test-agent\" , aws_region = \"us-east-1\" , aws_auth_method = OracleCrawler . AwsAuthMethod . IAM , secret_store = OracleCrawler . SecretStore . AWS_SECRET_MANAGER , secret_path = \"test/path/secret.json\" , agent_custom_config = { \"test\" : \"config\" }, ) . include ( assets = { \"ANALYTICS\" : [ \"SALES\" , \"CUSTOMER\" ]}) # (9) . exclude ( assets = {}) # (10) . jdbc_internal_methods ( True ) # (11) . source_level_filtering ( False ) # (12) . to_workflow () # (13) ) response = client . workflow . run ( crawler ) # (14) Base configuration for a new Oracle crawler. You must provide a client instance. You must provide a name for the connection that the Oracle assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). When using secure agent_config() , you need to provide the following information: host address of the Oracle instance. port number for the Oracle instance. Defaults to 1521 . authentication type ( basic or kerberos ). Defaults to basic . default database name. SID (system identifier) of the Oracle instance. name of the agent. AWS region where secrets are stored. Defaults to us-east-1 . AWS authentication method ( iam , iam-assume-role , access-key ). Defaults to iam . secret store to use (e.g AWS , Azure , GCP , etc) (optional) path to the secret in the secret manager. (optional) Custom JSON configuration for the agent. You can also optionally specify the set of assets to include in crawling. For Oracle assets,\nthis should be specified as a dict keyed by database name with values as a list of schemas within that\ndatabase to crawl. (If set to None , all databases and schemas will be crawled.) You can also optionally specify the list of assets to exclude from crawling.\nFor Oracle assets, this should be specified as a dict keyed by database name with values\nas a list of schemas within the database to exclude. (If set to None , no assets will be excluded.) You can also optionally specify whether to enable ( True ) or disable ( False ) JDBC\ninternal methods for data extraction. You can also optionally specify whether to enable ( True ) or disable ( False ) schema\nlevel filtering on source, schemas selected in the include filter will be fetched. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 To re-run an existing workflow for Oracle assets: Java Python Kotlin Raw REST API Coming soon Re-run existing Oracle workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . ORACLE , max_results = 5 ) # Determine which Oracle workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the TableauCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-oracle\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-oracle prefix will ensure you only find existing Oracle assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-oracle-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/powerbi-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Power BI assets package Â¶ The Power BI assets package crawls Microsoft Power BI assets and publishes them to Atlan for discovery. Will create a new connection This should only be used to create the workflow the first time. Each time you run this method\nit will create a new connection and new assets within that connection â€” which could lead to duplicate\nassets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 4.0.0 To crawl assets from Microsoft Power BI using the delegated user authentication: Java Python Kotlin Raw REST API Power BI assets crawling using delegated user auth 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Workflow crawler = PowerBICrawler . creator ( // (1) client , // (2) \"production\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null ) . direct () // (5) . delegatedUser ( // (6) (7) \"username\" , \"password\" , \"tenant-id-here\" , \"client-id-here\" , \"client-secret-here\" ) . directEndorsements ( true ) // (8) . include ( // (9) List . of ( \"fc2923bd-cf94-2b29-9cdd-9cc2f7a4f029\" ) ) . exclude ( List . of ()) // (10) . build () // (11) . toWorkflow (); // (12) WorkflowResponse response = crawler . run ( client ); // (13) The PowerBICrawler package will create a workflow to crawl assets from Power BI. You must provide Atlan client. You must provide a name for the connection that the Snowflake assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. Set up the crawler to extract directly from Power BI. When using delegatedUser() authentication, you are required to provide the following information: username for accessing Power BI. password for accessing Power BI. unique ID (GUID) of the Power BI tenant. unique ID (GUID) of the Power BI client. client secret for accessing Power BI. When utilizing servicePrincipal() authentication, you must provide the following information: unique ID (GUID) of the Power BI tenant. unique ID (GUID) of the Power BI client. client secret for accessing Power BI. You can also optionally set whether to directly attach endorsements as certificates ( true ), or instead raise these as requests ( false ). You can also optionally specify the list of workspaces to include in crawling. For Power BI assets, this should be specified as a list of workspaces GUIDs. (If set to null, all workspaces will be crawled.) You can also optionally specify the list of workspaces to exclude from crawling. For Power BI assets, this should be specified as a list of workspaces GUIDs.\n(If set to null, no workspaces will be excluded.) Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Power BI assets crawling using delegated user auth 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import PowerBICrawler client = AtlanClient () crawler = ( PowerBICrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , ) . direct () # (5) . delegated_user ( # (6) # (7) username = \"user\" , password = \"pass\" , tenant_id = \"tenant-id-here\" , client_id = \"client-id-here\" , client_secret = \"client-secret-here\" ) . direct_endorsements ( True ) # (8) . include ( workspaces = [ \"9622cbfb-9b0e-412c-9fcf-2ac1bbc1ad9e\" ]) # (9) . exclude ( workspaces = []) # (10) . to_workflow () # (11) ) response = client . workflow . run ( crawler ) # (12) Base configuration for a new Power BI crawler. You must provide a client instance. You must provide a name for the connection that the Power BI assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. Set up the crawler to extract directly from Power BI. When using delegated_user() authentication,\nyou are required to provide the following information: username for accessing Power BI. password for accessing Power BI. unique ID (GUID) of the Power BI tenant. unique ID (GUID) of the Power BI client. client secret for accessing Power BI. When utilizing service_principal() authentication,\nyou must provide the following information: unique ID (GUID) of the Power BI tenant. unique ID (GUID) of the Power BI client. client secret for accessing Power BI. You can also optionally set whether to directly attach endorsements\nas certificates ( True ), or instead raise these as requests ( False ). You can also optionally specify the list of workspaces to include in crawling.\nFor Power BI assets, this should be specified as a list of workspaces GUIDs.\n(If set to None, all workspaces will be crawled.) You can also optionally specify the list of workspaces to exclude from crawling.\nFor Power BI assets, this should be specified as a list of workspaces GUIDs.\n(If set to None, no workspaces will be excluded.) Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Power BI assets crawling using delegated user auth 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 val crawler = PowerBICrawler . creator ( // (1) client , // (2) \"production\" , // (3) listOf ( client . roleCache . getIdForName ( \"\\ $ admin \" )), // (4) null , null ) . direct () // (5) . delegatedUser ( // (6) (7) \"username\" , \"password\" , \"tenant-id-here\" , \"client-id-here\" , \"client-secret-here\" ) . directEndorsements ( true ) // (8) . include ( // (9) listOf ( \"fc2923bd-cf94-2b29-9cdd-9cc2f7a4f029\" ) ) . exclude ( emptyList ()) // (10) . build () // (11) . toWorkflow () // (12) val response = crawler . run ( client ) // (13) The PowerBICrawler package will create a workflow to crawl assets from Power BI. You must provide Atlan client. You must provide a name for the connection that the Snowflake assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. Set up the crawler to extract directly from Power BI. When using delegatedUser() authentication, you are required to provide the following information: username for accessing Power BI. password for accessing Power BI. unique ID (GUID) of the Power BI tenant. unique ID (GUID) of the Power BI client. client secret for accessing Power BI. When utilizing servicePrincipal() authentication, you must provide the following information: unique ID (GUID) of the Power BI tenant. unique ID (GUID) of the Power BI client. client secret for accessing Power BI. You can also optionally set whether to directly attach endorsements as certificates ( true ), or instead raise these as requests ( false ). You can also optionally specify the list of workspaces to include in crawling. For Power BI assets, this should be specified as a list of workspaces GUIDs. (If set to null, all workspaces will be crawled.) You can also optionally specify the list of workspaces to exclude from crawling. For Power BI assets, this should be specified as a list of workspaces GUIDs. (If set to null, no workspaces will be excluded.) Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for Power BI assets: Java Python Kotlin Raw REST API Re-run existing Power BI workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , PowerBICrawler . PREFIX , 5 ); // (2) // Determine which of the results is the Power BI workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the PowerBICrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Power BI workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . POWERBI , max_results = 5 ) # Determine which Power BI workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the PowerBICrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Power BI workflow 1 2 3 4 5 val existing = WorkflowSearchRequest // (1) . findByType ( client , PowerBICrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // Power BI workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the PowerBICrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-powerbi\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-powerbi prefix will ensure you only find existing Power BI assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-powerbi-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/redshift-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Redshift assets package Â¶ The Redshift assets package crawls Amazon Redshift assets and publishes them to Atlan for discovery. Basic authentication Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it will create a new connection and new assets within that connection â€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 4.0.0 To crawl assets from Redshift using basic authentication: Java Python Kotlin Raw REST API Basic authentication crawling of Redshift 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Workflow redshift = RedshiftCrawler . basicAuth ( // (1) \"production\" , // (2) \"abc123.kXufh3dOH2lg.ap-south-1.redshift.amazonaws.com\" , // (3) 5439 , // (4) \"atlan_user\" , // (5) \"pU9ygRpgiAy2Iph3gQn5\" , // (6) \"dev\" , // (7) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (8) null , null , true , // (9) true , // (10) 10000L , // (11) Map . of ( \"dev\" , List . of ( \"public\" )), // (12) null ); // (13) WorkflowResponse response = redshift . run ( client ); // (14) The RedshiftCrawler package will create a workflow to crawl assets from Amazon Redshift. The basicAuth() method creates a workflow for crawling assets directly from Redshift using basic authentication. You must provide a name for the connection that the Redshift assets will exist within. You must provide the hostname of your Redshift instance. You must specify the port number of the Redshift instance (use 5439 for the default). You must provide your Redshift username. You must provide your Redshift password. You must specify the name of the Redshift database you want to crawl. You must specify at least one connection admin, either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins a list of users (names) that will be connection admins You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. You can also optionally specify the set of assets to include in crawling. For Redshift assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the list of assets to exclude from crawling. For Redshift assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Basic authentication crawling of Redshift 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 val redshift = RedshiftCrawler . basicAuth ( // (1) \"production\" , // (2) \"abc123.kXufh3dOH2lg.ap-south-1.redshift.amazonaws.com\" , // (3) 5439 , // (4) \"atlan_user\" , // (5) \"pU9ygRpgiAy2Iph3gQn5\" , // (6) \"dev\" , // (7) listOf ( client . roleCache . getIdForName ( \"\\ $ admin \" )), // (8) null , null , true , // (9) true , // (10) 10000L , // (11) mapOf ( \"dev\" to listOf ( \"public\" )), // (12) null ) // (13) val response = redshift . run ( client ) // (14) The RedshiftCrawler package will create a workflow to crawl assets from Amazon Redshift. The basicAuth() method creates a workflow for crawling assets directly from Redshift using basic authentication. You must provide a name for the connection that the Redshift assets will exist within. You must provide the hostname of your Redshift instance. You must specify the port number of the Redshift instance (use 5439 for the default). You must provide your Redshift username. You must provide your Redshift password. You must specify the name of the Redshift database you want to crawl. You must specify at least one connection admin, either: everyone in a role (in this example, all $admin users) a list of groups (names) that will be connection admins a list of users (names) that will be connection admins You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. You can also optionally specify the set of assets to include in crawling. For Redshift assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the list of assets to exclude from crawling. For Redshift assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 4.0.0 To re-run an existing workflow for Redshift assets: Java Python Kotlin Raw REST API Re-run existing Redshift workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , RedshiftCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the Redshift workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the RedshiftCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Re-run existing Redshift workflow 1 2 3 4 val existing = WorkflowSearchRequest // (1) . findByType ( client , RedshiftCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the Redshift workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the RedshiftCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-redshift\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-redshift prefix will ensure you only find existing Redshift assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. (Remember since this is a search, there could be multiple results, so you may want to use the other details in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-redshift-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/relational-assets-builder/",
    "content": "Relational assets builder package Â¶ The relational assets builder package allows you to create (and update) net-new relational assets: connections, databases, schemas, tables, views, materialized views and columns. Import relational assets from object store Â¶ 2.6.0 To import relational assets directly from the object store: Java Python Kotlin Raw REST API Coming soon Import relational assets from the object store 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import RelationalAssetsBuilder from pyatlan.model.assets import Asset from pyatlan.model.enums import AssetInputHandling , AssetDeltaHandling , AssetRemovalType client = AtlanClient () workflow = ( RelationalAssetsBuilder () # (1) . object_store ( # (2) prefix = \"/test/prefix\" , object_key = \"assets-test.csv\" , ) . s3 ( # (3) access_key = \"test-access-key\" , secret_key = \"test-secret-key\" , bucket = \"my-bucket\" , region = \"us-west-1\" , ) . assets_semantics ( # (4) input_handling = AssetInputHandling . UPSERT , delta_handling = AssetDeltaHandling . INCREMENTAL , removal_type = AssetRemovalType . ARCHIVE , ) . options ( # (5) remove_attributes = [ Asset . CERTIFICATE_STATUS , Asset . ANNOUNCEMENT_TYPE ], fail_on_errors = True , field_separator = \",\" , batch_size = 20 , ) ) . to_workflow () # (6) response = client . workflow . run ( workflow ) # (7) The RelationalAssetsBuilder allows you to create (and update) net-new relational assets. To set up the package for importing metadata directly from the object store, provide the following information: prefix : directory (path) within the bucket/container\n   from which to retrieve the object(s). object_key : object key (filename), including its extension,\n   within the bucket/container and prefix. You can use different object store methods (e.g: s3() , gcs() , adls() ). In this example,\nwe're building a workflow using s3() and for that, youâ€™ll need to provide the following information: AWS access key. AWS secret key. name of the bucket/storage that contains the metadata CSV files. name of the AWS region. To set up the package to import metadata with semantics, you need to provide: input_handling : whether to allow the creation of new full ( AssetInputHandling.UPSERT )\n    or partial ( AssetInputHandling.PARTIAL ) assets from the input CSV, or ensure assets\n    are only updated ( AssetInputHandling.UPDATED ) if they already exist in Atlan. delta_handling : whether to treat the input file as an initial load, full replacement\n    [ AssetDeltaHandling.FULL_REPLACEMENT ] (deleting any existing assets not in the file) or only incremental [ AssetDeltaHandling.INCREMENTAL ] (no deletion of existing assets). removal_type : if delta_handling is set to FULL_REPLACEMENT , this parameter specifies whether to\n    delete any assets not found in the latest file by archive (recoverable) [ AssetRemovalType.ARCHIVE ] or purge (non-recoverable) [ AssetRemovalType.PURGE ].\n    If delta_handling is set to INCREMENTAL , this parameter is ignored and assets are archived. (Optional) To set up the package for importing relational assets with\nadvanced configuration, provide the following information: remove_attributes : list of attributes to clear (remove)\n    from assets if their value is blank in the provided file. fail_on_errors : specifies whether an invalid value\n    in a field should cause the import to fail ( True ) or\n    log a warning, skip that value, and proceed ( False ). field_separator : character used to separate\n    fields in the input file (e.g: ',' or ';' ). batch_size : maximum number of rows\n    to process at a time (per API request). Convert the package into a Workflow object. Run the workflow by invoking the run() method\non the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how to check the status and wait\nuntil the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 2.6.0 To re-run an existing relational assets builder workflow: Java Python Kotlin Raw REST API Coming soon Re-run existing relational assets builder workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . RELATIONAL_ASSETS_BUILDER , max_results = 5 ) # Determine which relational assets builder workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the RelationalAssetsBuilder . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"csa-relational-assets-builder\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the csa-relational-assets-builder prefix will ensure you only find existing relational assets builder workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"csa-relational-assets-builder-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/sigma-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Sigma assets package Â¶ The Sigma assets package crawls Sigma assets and publishes them to Atlan for discovery. Will create a new connection This should only be used to create the workflow the first time. Each time you run this \nmethod it will create a new connection and new assets within that connection â€” which could\nlead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 7.0.0 4.0.0 To crawl assets directly from Sigma: Java Python Kotlin Raw REST API Direct extraction from Sigma 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Workflow crawler = SigmaCrawler . creator ( // (1) client , // (2) \"production\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null , true , // (5) true , // (6) 10000L // (7) ) . direct ( // (8) \"aws-api.sigmacomputing.com\" , ) . apiToken ( \"client-id\" , // (9) \"api-token\" // (10) ) . include ( // (11) List . of ( \"995aecc2-fecf-497a-b169-5b3f96073618\" ) ) . exclude ( List . of ()) // (12) . build () // (13) . toWorkflow (); // (14) WorkflowResponse response = crawler . run ( client ); // (15) The SigmaCrawler package will create a workflow to crawl assets from Sigma. You must provide Atlan client. You must provide a name for the connection that the Sigma assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. When crawling assets directly from Sigma, you are required to provide the following information: hostname of the Sigma host, for example aws-api.sigmacomputing.com You must provide client ID through which to access Sigma. You must provide API token through which to access Sigma. You can also optionally specify the list of workbooks to include in crawling. For Sigma assets, this should be specified as a list of workbook GUIDs. If set to null, all workbooks will be crawled. You can also optionally specify the list of workbooks to exclude from crawling. For Sigma assets, this should be specified as a list of workbook GUIDs. If set to null, no workbooks will be excluded. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Direct extraction from Sigma 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import SigmaCrawler client = AtlanClient () crawler = ( SigmaCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . direct ( hostname = SigmaCrawler . Hostname . AWS ) # (8) . api_token ( client_id = \"client-id\" , # (9) api_token = \"api-token\" # (10) ) . include ( workbooks = [ \"995aecc2-fecf-497a-b169-5b3f96073618\" ]) # (11) . exclude ( workbooks = []) # (12) . to_workflow () # (13) ) response = client . workflow . run ( crawler ) # (14) Base configuration for a new Sigma crawler. You must provide a client instance. You must provide a name for the connection that the Sigma assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). When crawling assets directly from Sigma,\nyou are required to provide the following information: hostname of the Sigma host, for example aws-api.sigmacomputing.com You must provide client ID through which to access Sigma. You must provide API token through which to access Sigma. You can also optionally specify the list of workbooks to include in crawling.\nFor Sigma assets, this should be specified as a list of workbook GUIDs.\nIf set to None, all workbooks will be crawled. You can also optionally specify the list of workbooks to exclude from crawling.\nFor Sigma assets, this should be specified as a list of workbook GUIDs.\nIf set to None, no workbooks will be excluded. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Direct extraction from Sigma 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 val crawler = SigmaCrawler . creator ( // (1) client , // (2) \"production\" , // (3) listOf ( client . roleCache . getIdForName ( \"\\ $ admin \" )), // (4) null , null , true , // (5) true , // (6) 10000L // (7) ) . direct ( // (8) \"aws-api.sigmacomputing.com\" , ) . apiToken ( \"client-id\" , // (9) \"api-token\" // (10) ) . include ( // (11) listOf ( \"995aecc2-fecf-497a-b169-5b3f96073618\" ) ) . exclude ( emptyList ()) // (12) . build () // (13) . toWorkflow () // (14) val response = crawler . run ( client ) // (15) The SigmaCrawler package will create a workflow to crawl assets from Sigma. You must provide Atlan client. You must provide a name for the connection that the Sigma assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. When crawling assets directly from Sigma, you are required to provide the following information: hostname of the Sigma host, for example aws-api.sigmacomputing.com You must provide client ID through which to access Sigma. You must provide API token through which to access Sigma. You can also optionally specify the list of workbooks to include in crawling. For Sigma assets, this should be specified as a list of workbook GUIDs. If set to null, all workbooks will be crawled. You can also optionally specify the list of workbooks to exclude from crawling. For Sigma assets, this should be specified as a list of workbook GUIDs. If set to null, no workbooks will be excluded. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for Sigma assets: Java Python Kotlin Raw REST API Re-run existing Sigma workflow 1 2 3 4 5 6 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , SigmaCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the // Sigma workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SigmaCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Sigma workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . Sigma , max_results = 5 ) # Determine which Sigma workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the SigmaCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Sigma workflow 1 2 3 4 5 6 val existing = WorkflowSearchRequest // (1) . findByType ( client , SigmaCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // Sigma workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SigmaCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-sigma\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-sigma prefix will ensure you only find existing Sigma assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-sigma-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/snowflake-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Snowflake assets package Â¶ The Snowflake assets package crawls Snowflake assets and publishes them to Atlan for discovery. Information schema Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you\nrun this method it will create a new connection and new assets within that connection\nâ€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 6.0.0 4.0.0 To crawl assets from Snowflake using the built-in information schema and basic authentication: Java Python Kotlin Raw REST API Information schema crawling of Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Workflow crawler = SnowflakeCrawler . creator ( // (1) client , // (2) \"production\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null , true , // (5) true , // (6) 10000L // (7) ) . basicAuth ( // (8) \"atlan-user\" , // (9) \"atlan-pass\" , // (10) \"Transformer\" , // (11) \"COMPUTE_WH\" // (12) ) . informationSchema ( \"dev.ap-south.snowflakecomputing.com\" ) // (13) . include ( // (14) Map . of ( \"ANALYTICS\" , List . of ( \"WIDE_WORLD_IMPORTERS\" ) ) ) . exclude ( Map . of ()) // (15) . lineage ( true ) // (16) . tags ( false ) // (17) . build () // (18) . toWorkflow (); // (19) WorkflowResponse response = crawler . run ( client ); // (20) The SnowflakeCrawler package will create a workflow to crawl assets from Snowflake. You must provide Atlan client. You must provide a name for the connection that the Snowflake assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. You can also use .keypairAuth for information schema crawling. You must provide your Snowflake username. You must provide your Snowflake password. You must specify the name of the Snowflake role you want to use for crawling. You must specify the name of the Snowflake warehouse you want to use for crawling. You must provide the hostname of your Snowflake instance. You can also optionally specify the set of assets to include in crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the list of assets to exclude from crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) You can also optionally specify whether to enable lineage as part of crawling Snowflake. You can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Information schema crawling of Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import SnowflakeCrawler client = AtlanClient () crawler = ( SnowflakeCrawler ( # (1) connection_name = \"production\" , # (2) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (3) admin_groups = None , admin_users = None , row_limit = 10000 , # (4) allow_query = True , # (5) allow_query_preview = True , # (6) ) . basic_auth ( # (7) username = \"atlan-user\" , # (8) password = \"atlan-pass\" , # (9) role = \"Transformer\" , # (10) warehouse = \"COMPUTE_WH\" , # (11) ) . information_schema ( hostname = \"dev.ap-south.snowflakecomputing.com\" ) # (12) . include ( assets = { \"ANALYTICS\" : [ \"WIDE_WORLD_IMPORTERS\" ]}) # (13) . exclude ( assets = {}) # (14) . lineage ( True ) # (15) . tags ( False ) # (15) . to_workflow () # (16) ) response = client . workflow . run ( crawler ) # (17) Base configuration for a new Snowflake crawler. You must provide a name for the connection that the Snowflake assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection.\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). You can also use .keypair_auth for information schema crawling. You must provide your Snowflake username. You must provide your Snowflake password. You must specify the name of the Snowflake role you want to use for crawling. You must specify the name of the Snowflake warehouse you want to use for crawling. When using the built-in information schema, you must provide the hostname of your Snowflake instance. You can also optionally specify the set of assets to include in crawling. For Snowflake assets,\nthis should be specified as a dict keyed by database name with values as a list of schemas within that\ndatabase to crawl. (If set to None, all databases and schemas will be crawled.) You can also optionally specify the list of assets to exclude from crawling.\nFor Snowflake assets, this should be specified as a dict keyed by database name with values\nas a list of schemas within the database to exclude. (If set to None, no assets will be excluded.) You can also optionally specify whether to enable lineage as part of crawling Snowflake. You can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Information schema crawling of Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 val crawler = SnowflakeCrawler . creator ( client , // (2) \"production\" , // (3) listOf ( client . roleCache . getIdForName ( \"\\ $ admin \" )), // (4) null , null , true , // (5) true , // (6) 10000L // (7) ) . basicAuth ( // (9) \"atlan-user\" , // (9) \"atlan-pass\" , // (10) \"Transformer\" , // (11) \"COMPUTE_WH\" // (12) ) . informationSchema ( \"dev.ap-south.snowflakecomputing.com\" ) // (13) . include ( // (14) mapOf ( \"ANALYTICS\" to listOf ( \"WIDE_WORLD_IMPORTERS\" ) ) ) . exclude ( mapOf ()) // (15) . lineage ( true ) // (16) . tags ( false ) // (17) . build () // (18) . toWorkflow () // (19) val response = crawler . run ( client ) // (20) The SnowflakeCrawler package will create a workflow to crawl assets from Snowflake. You must provide Atlan client. You must provide a name for the connection that the Snowflake assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. You can also use .keypairAuth for information schema crawling. You must provide your Snowflake username. You must provide your Snowflake password. You must specify the name of the Snowflake role you want to use for crawling. You must specify the name of the Snowflake warehouse you want to use for crawling. You must provide the hostname of your Snowflake instance. You can also optionally specify the set of assets to include in crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the list of assets to exclude from crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) You can also optionally specify whether to enable lineage as part of crawling Snowflake. You can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Account usage Â¶ Will create a new connection This should only be used to create the workflow the first time\nEach time you run this method it will create a new connection and new assets within that connection\nâ€” which could lead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow \n(see Re-run existing workflow below). 7.0.0 4.0.0 To crawl assets from Snowflake using the account usage and keypair authentication: Java Python Kotlin Raw REST API Account usage crawling of Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 Workflow crawler = SnowflakeCrawler . creator ( // (1) client , // (2) \"production\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null , true , // (5) true , // (6) 10000L // (7) ) . keypairAuth ( // (8) \"atlan-user\" , // (9) \"private-key\" , // (10) \"private-key-pass\" , // (11) \"Transformer\" , // (12) \"COMPUTE_WH\" , // (13) ) . accountUsage ( // (14) \"dev.ap-south.snowflakecomputing.com\" , \"db\" , \"schema\" , ) . include ( // (15) Map . of ( \"ANALYTICS\" , List . of ( \"WIDE_WORLD_IMPORTERS\" ) ) ) . exclude ( Map . of ()) // (16) . lineage ( true ) // (17) . tags ( false ) // (18) . build () // (19) . toWorkflow (); // (20) WorkflowResponse response = crawler . run ( client ); // (21) The SnowflakeCrawler package will create a workflow to crawl assets from Snowflake. You must provide Atlan client. You must provide a name for the connection that the Snowflake assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. You can also use .basicAuth for account usage crawling. You must provide your Snowflake username. You must provide encrypted private key for authenticating with Snowflake. You must provide password for the encrypted private key. You must specify the name of the Snowflake role you want to use for crawling. You must specify the name of the Snowflake warehouse you want to use for crawling. To configure the crawler for extracting data from Snowflake's account usage database and schema, provide the following information: hostname of your Snowflake instance. name of the database to use. name of the schema to use. You can also optionally specify the set of assets to include in crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the set of assets to exclude from crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) You can also optionally specify whether to enable lineage as part of crawling Snowflake. You can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Account usage crawling of Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import SnowflakeCrawler client = AtlanClient () crawler = ( SnowflakeCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , row_limit = 10000 , # (5) allow_query = True , # (6) allow_query_preview = True , # (7) ) . keypair_auth ( # (8) username = \"atlan-user\" , # (9) private_key = \"private-key\" , # (10) private_key_password = \"private-key-pass\" , # (11) role = \"Transformer\" , # (12) warehouse = \"COMPUTE_WH\" , # (13) ) . account_usage ( # (14) hostname = \"dev.ap-south.snowflakecomputing.com\" , database_name = \"db\" , schema_name = \"schema\" , ) . include ( assets = { \"ANALYTICS\" : [ \"WIDE_WORLD_IMPORTERS\" ]}) # (15) . exclude ( assets = {}) # (16) . lineage ( True ) # (17) . tags ( False ) # (18) . to_workflow () # (19) ) response = client . workflow . run ( crawler ) # (20) Base configuration for a new Snowflake crawler. You must provide a client instance. You must provide a name for the connection that the Snowflake assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify a maximum number of rows that can be accessed for any asset in the connection. You can specify whether you want to allow queries to this connection\n( True , as in this example) or deny all query access to the connection ( False ). You can specify whether you want to allow data previews on this connection\n( True , as in this example) or deny all sample data previews to the connection ( False ). You can also use .basic_auth for account usage crawling. You must provide your Snowflake username. You must provide encrypted private key for authenticating with Snowflake. You must provide password for the encrypted private key. You must specify the name of the Snowflake role you want to use for crawling. You must specify the name of the Snowflake warehouse you want to use for crawling. To configure the crawler for extracting data from Snowflake's\naccount usage database and schema, provide the following information: hostname of your Snowflake instance. name of the database to use. name of the schema to use. You can also optionally specify the set of assets to include in crawling. For Snowflake assets,\nthis should be specified as a dict keyed by database name with values as a list of schemas within that\ndatabase to crawl. (If set to None, all databases and schemas will be crawled.) You can also optionally specify the set of assets to exclude from crawling.\nFor Snowflake assets, this should be specified as a dict keyed by database name with values\nas a list of schemas within the database to exclude. (If set to None, no assets will be excluded.) You can also optionally specify whether to enable lineage as part of crawling Snowflake. You can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Account usage crawling of Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 val crawler = SnowflakeCrawler . creator ( client , // (2) \"production\" , // (3) listOf ( client . getRoleCache (). getIdForName ( \"\\ $ admin \" )), // (4) null , null , true , // (5) true , // (6) 10000L // (7) ) . keypairAuth ( // (8) \"atlan-user\" , // (9) \"private-key\" , // (10) \"private-key-pass\" , // (11) \"Transformer\" , // (12) \"COMPUTE_WH\" , // (13) ) . accountUsage ( // (14) \"dev.ap-south.snowflakecomputing.com\" , \"db\" , \"schema\" , ) . include ( // (15) mapOf ( \"ANALYTICS\" to listOf ( \"WIDE_WORLD_IMPORTERS\" ) ) ) . exclude ( emptyMap ()) // (16) . lineage ( true ) // (17) . tags ( false ) // (18) . build () // (19) . toWorkflow () // (20) val response = crawler . run ( client ) // (21) The SnowflakeCrawler package will create a workflow to crawl assets from Snowflake. You must provide Atlan client. You must provide a name for the connection that the Snowflake assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. You can specify whether you want to allow queries to this connection ( true , as in this example) or deny all query access to the connection ( false ). You can specify whether you want to allow data previews on this connection ( true , as in this example) or deny all sample data previews to the connection ( false ). You can specify a maximum number of rows that can be accessed for any asset in the connection. You can also use .basicAuth for account usage crawling. You must provide your Snowflake username. You must provide encrypted private key for authenticating with Snowflake. You must provide password for the encrypted private key. You must specify the name of the Snowflake role you want to use for crawling. You must specify the name of the Snowflake warehouse you want to use for crawling. To configure the crawler for extracting data from Snowflake's account usage database and schema, provide the following information: hostname of your Snowflake instance. name of the database to use. name of the schema to use. You can also optionally specify the set of assets to include in crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the set of assets to exclude from crawling. For Snowflake assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) You can also optionally specify whether to enable lineage as part of crawling Snowflake. You can also optionally specify whether to enable Snowflake tag syncing as part of crawling Snowflake. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for Snowflake assets: Java Python Kotlin Raw REST API Re-run existing Snowflake workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , SnowflakeCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the Snowflake workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SnowflakeCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Snowflake workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . SNOWFLAKE , max_results = 5 ) # Determine which Snowflake workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the SnowflakeCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Snowflake workflow 1 2 3 4 5 val existing = WorkflowSearchRequest // (1) . findByType ( client , SnowflakeCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // Snowflake workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SnowflakeCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-snowflake\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-snowflake prefix will ensure you only find existing Snowflake assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. \n(Remember since this is a search, there could be multiple results, so you may want to use the other details \nin each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-snowflake-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/sql-server-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) SQL Server assets package Â¶ The SQL Server assets package crawls Microsoft SQL Server assets and publishes them to Atlan for discovery. Will create a new connection This should only be used to create the workflow the first time. Each time you run this \nmethod it will create a new connection and new assets within that connection â€” which could\nlead to duplicate assets if you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow (see Re-run existing workflow below). 7.0.0 4.0.0 To crawl assets directly from Microsoft SQL Server: Java Python Kotlin Raw REST API Direct extraction from SQL Server 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Workflow crawler = SQLServerCrawler . creator ( // (1) client , // (2) \"production\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null ) . direct ( // (5) \"11.22.33.44\" , \"WideWorldImporters\" ) . basicAuth ( // (6) \"username\" , \"password\" ) . include ( // (7) Map . of ( \"WideWorldImporters\" , List . of ( \"Application\" , \"Purchasing\" , \"Sales\" , \"Warehouse\" , \"Website\" ) ) ) . exclude ( Map . of ()) // (8) . build () // (9) . toWorkflow (); // (10) WorkflowResponse response = crawler . run ( client ); // (11) The SQLServerCrawler package will create a workflow to crawl assets from SQL Server. You must provide Atlan client. You must provide a name for the connection that the SQL Server assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. When crawling assets directly from SQL Server, you are required to provide the following information: hostname of the SQL Server host name of the database to extract When using basic authentication, you are required to provide the following information: username for accessing SQL Server. password for accessing SQL Server. You can also optionally specify the set of assets to include in crawling. For SQL Server assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the set of assets to exclude from crawling. For SQL Server assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Direct extraction from SQL Server 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import SQLServerCrawler client = AtlanClient () crawler = ( SQLServerCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , ) . direct ( hostname = \"11.22.33.44\" , database = \"WideWorldImporters\" ) # (5) . basic_auth ( # (6) username = \"username\" , password = \"password\" ) . include ( # (7) assets = { \"WideWorldImporters\" : [ \"Application\" , \"Purchasing\" , \"Sales\" , \"Warehouse\" , \"Website\" , ] } ) . exclude ( assets = {}) # (8) . to_workflow () # (9) ) response = client . workflow . run ( crawler ) # (10) Base configuration for a new SQL Server crawler. You must provide a client instance. You must provide a name for the connection that the SQL Server assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. When crawling assets directly from SQL Server,\nyou are required to provide the following information: hostname of the SQL Server host name of the database to extract When using basic authentication,\nyou are required to provide the following information: username for accessing SQL Server. password for accessing SQL Server. You can also optionally specify the set of assets to include in crawling. For SQL Server assets,\nthis should be specified as a dict keyed by database name with values as a list of schemas within that\ndatabase to crawl. (If set to None, all databases and schemas will be crawled.) You can also optionally specify the set of assets to exclude from crawling. \nFor SQL Server assets, this should be specified as a dict keyed by database name with values\nas a list of schemas within the database to exclude. (If set to None, no assets will be excluded.) Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Direct extraction from SQL Server 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 val crawler = SQLServerCrawler . creator ( // (1) client , // (2) \"production\" , // (3) listOf ( client . roleCache . getIdForName ( \"\\ $ admin \" )), // (4) null , null ) . direct ( // (5) \"11.22.33.44\" , \"WideWorldImporters\" ) . basicAuth ( // (6) \"username\" , \"password\" ) . include ( // (7) mapOf ( \"WideWorldImporters\" to listOf ( \"Application\" , \"Purchasing\" , \"Sales\" , \"Warehouse\" , \"Website\" ) ) ) . exclude ( mapOf ()) // (8) . build () // (9) . toWorkflow () // (10) WorkflowResponse response = crawler . run ( client ) // (11) The SQLServerCrawler package will create a workflow to crawl assets from SQL Server. You must provide Atlan client. You must provide a name for the connection that the SQL Server assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. When crawling assets directly from SQL Server, you are required to provide the following information: hostname of the SQL Server host name of the database to extract When using basic authentication, you are required to provide the following information: username for accessing SQL Server. password for accessing SQL Server. You can also optionally specify the set of assets to include in crawling. For SQL Server assets, this should be specified as a map keyed by database name with values as a list of schemas within that database to crawl. (If set to null, all databases and schemas will be crawled.) You can also optionally specify the set of assets to exclude from crawling. For SQL Server assets, this should be specified as a map keyed by database name with values as a list of schemas within the database to exclude. (If set to null, no assets will be excluded.) Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for SQL Server assets: Java Python Kotlin Raw REST API Re-run existing SQL Server workflow 1 2 3 4 5 6 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , SQLServerCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the // SQL Server workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SQLServerCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing SQL Server workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . MSSQL , max_results = 5 ) # Determine which SQL Server workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the SQLServerCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing SQL Server workflow 1 2 3 4 5 6 val existing = WorkflowSearchRequest // (1) . findByType ( client , SQLServerCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // SQL Server workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SQLServerCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-mssql\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-mssql prefix will ensure you only find existing SQL Server assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-mssql-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/tableau-assets/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Tableau assets package Â¶ The Tableau assets package crawls Tableau assets and publishes them to Atlan for discovery. Direct extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it \nwill create a new connection and new assets within that connection â€” which could lead to duplicate assets\nif you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 4.0.0 To crawl Tableau assets directly from Tableau: Java Python Kotlin Raw REST API Direct extraction from Tableau 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Workflow crawler = TableauCrawler . creator ( // (1) client , // (2) \"production\" , // (3) List . of ( client . getRoleCache (). getIdForName ( \"$admin\" )), // (4) null , null ) . direct ( // (5) \"example.online.tableau.com\" , \"atlan-site\" , true ) . basicAuth ( // (6) (7) \"atlan-user\" , \"atlan-pass\" ) . include ( // (8) List . of ( \"fc2923bd-cf94-2b29-9cdd-9cc2f7a4f029\" ) ) . exclude ( List . of ()) // (9) . crawlUnpublished ( true ) // (10) . crawlHiddenFields ( false ) // (11) . alternateHost ( \"https://alternate.tableau.com\" ) // (12) . build () // (13) . toWorkflow (); // (14) WorkflowResponse response = crawler . run ( client ); // (15) The TableauCrawler package will create a workflow to crawl assets from Tableau. You must provide Atlan client. You must provide a name for the connection that the Tableau assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. To configure the crawler for extracting data directly from Tableau then you must provide the following information: hostname of your Tableau instance. site to crawl within Tableau. whether to use SSL for the connection. When using basicAuth() , you must provide the following information: your username for accessing Tableau. your password for accessing Tableau. or you also can use personalAccessToken() auth, then you must provide the following information: your username for accessing Tableau. your access token for accessing Tableau. You can also optionally specify the list of projects to include in crawling. For Tableau assets, this should be specified as a list of project GUIDs. (If set to null, all projects will be crawled.) You can also optionally specify the list of projects to exclude from crawling. For Tableau assets, this should be specified as a list of project GUIDs. (If set to null, no projects will be excluded.) You can also optionally set whether to crawl unpublished worksheets and dashboards ( true ) or not ( false ). You can also optionally set whether to crawl hidden datasource fields ( true ) or not ( false ). You can also optionally set an alternate host to use for the \"View in Tableau\" button for assets in the UI. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Direct extraction from Tableau 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import TableauCrawler client = AtlanClient () crawler = ( TableauCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , ) . direct ( # (5) hostname = \"example.online.tableau.com\" , site = \"atlan-site\" , port = 443 , ssl_enabled = True , ) . basic_auth ( # (6) # (7) username = \"atlan-user\" , password = \"atlan-pass\" , ) . include ( projects = [ \"fc2923bd-cf94-2b29-9cdd-9cc2f7a4f029\" ]) # (8) . exclude ( projects = []) # (9) . crawl_unpublished ( True ) # (10) . crawl_hidden_fields ( False ) # (11) . alternate_host ( hostname = \"https://alternate.tableau.com\" ) # (12) . to_workflow () # (13) ) response = client . workflow . run ( crawler ) # (14) Base configuration for a new Tableau crawler. You must provide a client instance. You must provide a name for the connection that the Tableau assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. To configure the crawler for extracting data directly from Tableau\nthen you must provide the following information: hostname of your Tableau instance. port number of the Tableau instance (use 443 for the default). site to crawl within Tableau. whether to use SSL for the connection. When using basic_auth() , you must provide the following information: your username for accessing Tableau. your password for accessing Tableau. or you also can use personal_access_token() auth, then you must provide the following information: your username for accessing Tableau. your access token for accessing Tableau. You can also optionally specify the list of projects to include in crawling.\nFor Tableau assets, this should be specified as a list of project GUIDs.\n(If set to None, all projects will be crawled.) You can also optionally specify the list of projects to exclude from crawling.\nFor Tableau assets, this should be specified as a list of project GUIDs.\n(If set to None, no projects will be excluded.) You can also optionally set whether to\ncrawl hidden datasource fields ( True ) or not ( False ). You can also optionally set whether to\ncrawl unpublished worksheets and dashboards ( True ) or not ( False ). You can also optionally set an alternate host to\nuse for the \"View in Tableau\" button for assets in the UI. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Direct extraction from Tableau 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 val crawler = TableauCrawler . creator ( // (1) client , // (2) \"production\" , // (3) listOf ( client . getRoleCache (). getIdForName ( \"\\ $ admin \" )), // (4) null , null ) . direct ( // (5) \"example.online.tableau.com\" , \"atlan-site\" , true ) . basicAuth ( // (6) (7) \"atlan-user\" , \"atlan-pass\" ) . include ( // (8) listOf ( \"fc2923bd-cf94-2b29-9cdd-9cc2f7a4f029\" ) ) . exclude ( emptyList ()) // (9) . crawlUnpublished ( true ) // (10) . crawlHiddenFields ( false ) // (11) . alternateHost ( \"https://alternate.tableau.com\" ) // (12) . build () // (13) . toWorkflow () // (14) val response = crawler . run ( client ) // (15) The TableauCrawler package will create a workflow to crawl assets from Tableau. You must provide Atlan client. You must provide a name for the connection that the Tableau assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. To configure the crawler for extracting data directly from Tableau then you must provide the following information: hostname of your Tableau instance. site to crawl within Tableau. whether to use SSL for the connection. When using basicAuth() , you must provide the following information: your username for accessing Tableau. your password for accessing Tableau. or you also can use personalAccessToken() auth, then you must provide the following information: your username for accessing Tableau. your access token for accessing Tableau. You can also optionally specify the list of projects to include in crawling. For Tableau assets, this should be specified as a list of project GUIDs. (If set to null, all projects will be crawled.) You can also optionally specify the list of projects to exclude from crawling. For Tableau assets, this should be specified as a list of project GUIDs.\n(If set to null, no projects will be excluded.) You can also optionally set whether to crawl unpublished worksheets and dashboards ( true ) or not ( false ). You can also optionally set whether to crawl hidden datasource fields ( true ) or not ( false ). You can also optionally set an alternate host to use for the \"View in Tableau\" button for assets in the UI. Build the minimal package object. Now, you can convert the package into a Workflow object. You can then run the workflow using the run() method on the object you've created. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Offline extraction Â¶ Will create a new connection This should only be used to create the workflow the first time. Each time you run this method it \nwill create a new connection and new assets within that connection â€” which could lead to duplicate assets\nif you run the workflow this way multiple times with the same settings. Instead, when you want to re-crawl assets, re-run the existing workflow\n(see Re-run existing workflow below). 7.0.0 To crawl Tableau assets from the S3 bucket: Java Python Kotlin Raw REST API Coming soon Crawl assets from the S3 bucket 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import TableauCrawler client = AtlanClient () crawler = ( TableauCrawler ( # (1) client = client , # (2) connection_name = \"production\" , # (3) admin_roles = [ client . role_cache . get_id_for_name ( \"$admin\" )], # (4) admin_groups = None , admin_users = None , ) . s3 ( # (5) bucket_name = \"test-bucket\" , bucket_prefix = \"test-prefix\" , bucket_region = \"test-region\" , ) . to_workflow () # (6) ) response = client . workflow . run ( crawler ) # (7) Base configuration for a new Tableau crawler. You must provide a client instance. You must provide a name for the connection that the Tableau assets will exist within. You must specify at least one connection admin , either: everyone in a role (in this example, all $admin users). a list of groups (names) that will be connection admins. a list of users (names) that will be connection admins. When using s3() , you need to provide the following information: name of the bucket/storage that contains the extracted metadata files. prefix is everything after the bucket/storage name, including the path . (Optional) name of the region if applicable. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Coming soon Create the workflow via UI only We recommend creating the workflow only via the UI. To rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 1.9.5 4.0.0 To re-run an existing workflow for Tableau assets: Java Python Kotlin Raw REST API Re-run existing Tableau workflow 1 2 3 4 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , TableauCrawler . PREFIX , 5 ); // (2) // Determine which of the results is the Tableau workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the TableauCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Tableau workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . TABLEAU , max_results = 5 ) # Determine which Tableau workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the TableauCrawler . (You can also specify\nthe maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Tableau workflow 1 2 3 4 5 val existing = WorkflowSearchRequest // (1) . findByType ( client , TableauCrawler . PREFIX , 5 ) // (2) // Determine which of the results is the // Tableau workflow you want to re-run... val response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the TableauCrawler . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-tableau\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-tableau prefix will ensure you only find existing Tableau assets workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object.\n(Remember since this is a search, there could be multiple results, so you may want to use the other\ndetails in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-tableau-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/snippets/workflows/packages/snowflake-miner/",
    "content": "/api/service/workflows/indexsearch (POST) /api/service/workflows/submit (POST) Snowflake miner package Â¶ The Snowflake miner package mines query history from Snowflake. This data is used for generating lineage and usage metrics. Source extraction Â¶ 0.0.16 2.1.8 4.0.0 To mine query history directly from Snowflake using its built-in database: Java Python Kotlin Go Raw REST API Mine query history direct from Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Workflow miner = SnowflakeMiner . creator ( // (1) \"default/snowflake/1234567890\" // (2) ) . direct ( // (3) \"TEST_DB\" , \"TEST_SCHEMA\" , 1713225600 ) . excludeUsers ( // (4) List . of ( \"test-user-1\" , \"test-user-2\" ) ) . nativeLineage ( true ) // (5) . build () // (6) . toWorkflow (); // (7) WorkflowResponse response = miner . run ( client ); // (8) Base configuration for a new Snowflake miner. You must provide the exact qualifiedName of the Snowflake connection in Atlan for which you want to mine query history. To create a workflow for mining history directly from Snowflake using its built-in database you need to provide: name of the database to extract from. name of the schema to extract from. date and time from which to start mining, as an epoch. Optionally, you can specify list of users who should be excluded when calculating usage metrics for assets (for example, system accounts). Optionally, you can specify whether to enable native lineage from Snowflake, using Snowflake's ACCESS_HISTORY.OBJECTS_MODIFIED Column. Note: this is only available only for Snowflake Enterprise customers. Build the minimal package object. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Mine query history direct from Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import SnowflakeMiner miner = ( SnowflakeMiner ( # (1) connection_qualified_name = \"default/snowflake/1234567890\" # (2) ) . direct ( # (3) start_epoch = 1713225600 , database = \"TEST_DB\" , schema = \"TEST_SCHEMA\" , ) . exclude_users ( # (4) users = [ \"test-user-1\" , \"test-user-2\" , ] ) . popularity_window ( days = 30 ) # (5) . native_lineage ( enabled = True ) # (6) . custom_config ( # (7) config = { \"test\" : True , \"feature\" : 1234 } ) . to_workflow () # (8) ) response = client . workflow . run ( miner ) # (9) Base configuration for a new Snowflake miner. You must provide the exact qualified_name of the Snowflake\nconnection in Atlan for which you want to mine query history. To create a workflow for mining history directly from Snowflake\nusing its built-in database you need to provide: date and time from which to start mining, as an epoch. name of the database to extract from. name of the schema to extract from. Optionally, you can specify list of users who should be excluded\nwhen calculating usage metrics for assets (for example, system accounts). Optionally, you can provide number of days to consider for calculating popularity. Optionally, you can specify whether to enable native lineage from Snowflake,\nusing Snowflake's ACCESS_HISTORY.OBJECTS_MODIFIED Column. Note: this is only available only for Snowflake Enterprise customers. Optionally, you can provide custom configuration\ncontrolling experimental feature flags for the miner. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the\nworkflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status and wait until\nthe workflow has been completed. Mine query history direct from Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 val miner = SnowflakeMiner . creator ( // (1) \"default/snowflake/1234567890\" // (2) ) . direct ( // (3) \"TEST_DB\" , \"TEST_SCHEMA\" , 1713225600 ) . excludeUsers ( // (4) listOf ( \"test-user-1\" , \"test-user-2\" ) ) . nativeLineage ( true ) // (5) . build () // (6) . toWorkflow () // (7) val response = miner . run ( client ) // (8) Base configuration for a new Snowflake miner. You must provide the exact qualifiedName of the Snowflake connection in Atlan for which you want to mine query history. To create a workflow for mining history directly from Snowflake using its built-in database you need to provide: name of the database to extract from. name of the schema to extract from. date and time from which to start mining, as an epoch. Optionally, you can specify list of users who should be excluded when calculating usage metrics for assets (for example, system accounts). Optionally, you can specify whether to enable native lineage from Snowflake, using Snowflake's ACCESS_HISTORY.OBJECTS_MODIFIED Column. Note: this is only available only for Snowflake Enterprise customers. Build the minimal package object. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Mine query history direct from Snowflake 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 miner := assets . NewSnowflakeMiner ( // (1) \"default/snowflake/1234567890\" , // (2) ). Direct ( // (3) 1713225600 , \"TEST_DB\" , \"TEST_SCHEMA\" , ). ExcludeUsers ([] string { \"test-user-1\" , \"test-user-2\" }). // (4) PopularityWindow ( 30 ). // (5) NativeLineage ( true ). // (6) CustomConfig ( map [ string ] interface {}{ // (7) \"test\" : true , \"feature\" : 1234 , }). ToWorkflow () // (8) response , atlanErr := ctx . WorkflowClient . Run ( miner , nil ) // (9) Base configuration for a new Snowflake miner. You must provide the exact qualifiedName of the Snowflake\nconnection in Atlan for which you want to mine query history. To create a workflow for mining history directly from Snowflake\nusing its built-in database you need to provide: date and time from which to start mining, as an epoch. name of the database to extract from. name of the schema to extract from. Optionally, you can specify list of users who should be excluded\nwhen calculating usage metrics for assets (for example, system accounts). Optionally, you can provide number of days to consider for calculating popularity. Optionally, you can specify whether to enable native lineage from Snowflake,\nusing Snowflake's ACCESS_HISTORY.OBJECTS_MODIFIED Column. Note: this is only available only for Snowflake Enterprise customers. Optionally, you can provide custom configuration\ncontrolling experimental feature flags for the miner. Now, you can convert the package into a Workflow object. Run the workflow by invoking the ctx.WorkflowClient.Run() method on the\nworkflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status and wait until\nthe workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Offline extraction Â¶ 0.0.16 2.1.8 4.0.0 To mine query history from the S3 bucket: Java Python Kotlin Go Raw REST API Mine query history from the S3 bucket 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Workflow miner = SnowflakeMiner . creator ( // (1) \"default/snowflake/1234567890\" // (2) ) . s3 ( // (3) \"test-s3-bucket\" , \"test-s3-prefix\" , \"TEST_QUERY\" , \"TEST_DB\" , \"TEST_SCHEMA\" , \"TEST_SESSION_ID\" ) . nativeLineage ( true ) // (4) . build () // (5) . toWorkflow (); // (6) WorkflowResponse response = miner . run ( client ); // (7) Base configuration for a new Snowflake miner. You must provide the exact qualifiedName of the Snowflake connection in Atlan for which you want to mine query history. To create a workflow for mining history from S3 bucket you need to provide: S3 bucket where the JSON line-separated files are located. prefix within the S3 bucket in which the JSON line-separated files are located. JSON key containing the query definition. JSON key containing the default database name to use if a query is not qualified with database name. JSON key containing the default schema name to use if a query is not qualified with schema name. JSON key containing the session ID of the SQL query. Optionally, you can specify whether to enable native lineage from Snowflake, using Snowflake's ACCESS_HISTORY.OBJECTS_MODIFIED Column. Note: this is only available only for Snowflake Enterprise customers. Build the minimal package object. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Mine query history from the S3 bucket 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from pyatlan.client.atlan import AtlanClient from pyatlan.model.packages import SnowflakeMiner miner = ( SnowflakeMiner ( # (1) connection_qualified_name = \"default/snowflake/1234567890\" # (2) ) . s3 ( # (3) s3_bucket = \"test-s3-bucket\" , s3_prefix = \"test-s3-prefix\" , s3_bucket_region = \"test-s3-bucket-region\" , sql_query_key = \"TEST_QUERY\" , default_database_key = \"TEST_DB\" , default_schema_key = \"TEST_SCHEMA\" , session_id_key = \"TEST_SESSION_ID\" , ) . popularity_window ( days = 30 ) # (4) . native_lineage ( enabled = True ) # (5) . custom_config ( # (6) config = { \"test\" : True , \"feature\" : 1234 } ) . to_workflow () # (7) ) response = client . workflow . run ( miner ) # (8) Base configuration for a new Snowflake miner. You must provide the exact qualified_name of the Snowflake\nconnection in Atlan for which you want to mine query history. To create a workflow for mining history\nfrom S3 bucket you need to provide: S3 bucket where the JSON line-separated files are located. prefix within the S3 bucket in which the JSON line-separated files are located. (Optional) region of the S3 bucket if applicable. JSON key containing the query definition. JSON key containing the default database name\n  to use if a query is not qualified with database name. JSON key containing the default schema name\nto use if a query is not qualified with schema name. JSON key containing the session ID of the SQL query. Optionally, you can provide number of days to consider for calculating popularity. Optionally, you can specify whether to enable native lineage from Snowflake,\nusing Snowflake's ACCESS_HISTORY.OBJECTS_MODIFIED Column. Note: this is only available only for Snowflake Enterprise customers. Optionally, you can provide custom configuration\ncontrolling experimental feature flags for the miner. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the\nworkflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status and wait until\nthe workflow has been completed. Mine query history from the S3 bucket 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 val miner = SnowflakeMiner . creator ( // (1) \"default/snowflake/1234567890\" // (2) ) . s3 ( // (3) \"test-s3-bucket\" , \"test-s3-prefix\" , \"TEST_QUERY\" , \"TEST_DB\" , \"TEST_SCHEMA\" , \"TEST_SESSION_ID\" ) . nativeLineage ( true ) // (4) . build () // (5) . toWorkflow () // (6) val response = miner . run ( client ) // (7) Base configuration for a new Snowflake miner. You must provide the exact qualifiedName of the Snowflake connection in Atlan for which you want to mine query history. To create a workflow for mining history from S3 bucket you need to provide: S3 bucket where the JSON line-separated files are located. prefix within the S3 bucket in which the JSON line-separated files are located. JSON key containing the query definition. JSON key containing the default database name to use if a query is not qualified with database name. JSON key containing the default schema name to use if a query is not qualified with schema name. JSON key containing the session ID of the SQL query. Optionally, you can specify whether to enable native lineage from Snowflake, using Snowflake's ACCESS_HISTORY.OBJECTS_MODIFIED Column. Note: this is only available only for Snowflake Enterprise customers. Build the minimal package object. Now, you can convert the package into a Workflow object. Run the workflow by invoking the run() method on the workflow client, passing the created object. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Mine query history from the S3 bucket 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 miner := assets . NewSnowflakeMiner ( // (1) \"default/snowflake/1234567890\" // (2) ). S3 ( // (3) \"test-s3-bucket\" , \"test-s3-prefix\" , \"TEST_QUERY\" , \"TEST_SNOWFLAKE\" , \"TEST_SCHEMA\" , \"TEST_SESSION_ID\" , structs . StringPtr ( \"test-s3-bucket-region\" ), ). PopularityWindow ( 30 ). // (4) NativeLineage ( true ). // (5) CustomConfig ( map [ string ] interface {}{ // (6) \"test\" : true , \"feature\" : 1234 , }). ToWorkflow () // (7) response , atlanErr := ctx . WorkflowClient . Run ( miner , & Schedule ) // (8) Base configuration for a new Snowflake miner. You must provide the exact qualifiedName of the Snowflake\nconnection in Atlan for which you want to mine query history. To create a workflow for mining history\nfrom S3 bucket you need to provide: S3 bucket where the JSON line-separated files are located. prefix within the S3 bucket in which the JSON line-separated files are located. (Optional) region of the S3 bucket if applicable. JSON key containing the query definition. JSON key containing the default database name\n  to use if a query is not qualified with database name. JSON key containing the default schema name\nto use if a query is not qualified with schema name. JSON key containing the session ID of the SQL query. Optionally, you can provide number of days to consider for calculating popularity. Optionally, you can specify whether to enable native lineage from Snowflake,\nusing Snowflake's ACCESS_HISTORY.OBJECTS_MODIFIED Column. Note: this is only available only for Snowflake Enterprise customers. Optionally, you can provide custom configuration\ncontrolling experimental feature flags for the miner. Now, you can convert the package into a Workflow object. Run the workflow by invoking the ctx.WorkflowClient.Run() method on the\nworkflow client, passing the created object. Workflows run asynchronously Remember that workflows run asynchronously.\nSee the packages and workflows introduction for details on how you can check the status and wait until\nthe workflow has been completed. Create the workflow via UI only We recommend creating the workflow only via the UI.\nTo rerun an existing workflow, see the steps below. Re-run existing workflow Â¶ 0.0.16 1.9.5 4.0.0 To re-run an existing workflow for Snowflake query mining: Java Python Kotlin Go Raw REST API Re-run existing Snowflake workflow 1 2 3 4 5 List < WorkflowSearchResult > existing = WorkflowSearchRequest // (1) . findByType ( client , SnowflakeMiner . PREFIX , 5 ); // (2) // Determine which of the results is the // Snowflake workflow you want to re-run... WorkflowRunResponse response = existing . get ( n ). rerun ( client ); // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SnowflakeMiner . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Snowflake workflow 1 2 3 4 5 6 7 8 9 10 11 12 from pyatlan.client.atlan import AtlanClient from pyatlan.model.enums import WorkflowPackage client = AtlanClient () existing = client . workflow . find_by_type ( # (1) prefix = WorkflowPackage . SNOWFLAKE_MINER , max_results = 5 ) # Determine which Snowflake workflow (n) # from the list of results you want to re-run. response = client . workflow . rerun ( existing [ n ]) # (2) You can find workflows by their type using the workflow client find_by_type() method and providing the prefix for one of the packages.\nIn this example, we do so for the SnowflakeMiner .\n(You can also specify the maximum number of resulting\nworkflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client rerun() method. Optionally, you can use rerun(idempotent=True) to avoid\n  re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Snowflake workflow 1 2 3 4 5 var existing = WorkflowSearchRequest // (1) . findByType ( client , SnowflakeMiner . PREFIX , 5 ) // (2) // Determine which of the results is the // Snowflake workflow you want to re-run... var response = existing . get ( n ). rerun ( client ) // (3) You can search for existing workflows through the WorkflowSearchRequest class. You can find workflows by their type using the findByType() helper method and providing the prefix for one of the packages. In this example, we do so for the SnowflakeMiner . (You can also specify the maximum number of resulting workflows you want to retrieve as results.) Once you've found the workflow you want to re-run, you can simply call the rerun() helper method on the workflow search result. The WorkflowRunResponse is just a subtype of WorkflowResponse so has the same helper method to monitor progress of the workflow run. Because this operation will execute work in Atlan, you must provide it an AtlanClient through which to connect to the tenant. Optionally, you can use the rerun(client, true) method with idempotency to avoid re-running a workflow that is already in running or in a pending state. This will return details of the already running workflow if found, and by default, it is set to false Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Re-run existing Snowflake workflow 1 2 3 4 5 existingWorkflow , _ := ctx . WorkflowClient . FindByType ( // (1) atlan . WorkflowPackageSnowflakeMiner , 1 , ) response , atlanErr := ctx . WorkflowClient . Rerun ( existingWorkflow [ 0 ], true ) // (2) You can find workflows by their type using the workflow client FindByType() method and providing the prefix for one of the packages.\nIn this example, we do so for the SnowflakeMiner .\n(You can also specify the maximum number of resulting\nworkflows you want to retrieve as results.) Once you've found the workflow you want to re-run,\nyou can simply call the workflow client Rerun() method. Optionally, you can use Rerun(idempotent=True) to avoid\n  re-running a workflow that is already in running or in a pending state.\n  This will return details of the already running workflow if found, and by default, it is set to False . Workflows run asynchronously Remember that workflows run asynchronously. See the packages and workflows introduction for details on how you can check the status and wait until the workflow has been completed. Requires multiple steps through the raw REST API Find the existing workflow. Send through the resulting re-run request. POST /api/service/workflows/indexsearch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 { \"from\" : 0 , \"size\" : 5 , \"query\" : { \"bool\" : { \"filter\" : [ { \"nested\" : { \"path\" : \"metadata\" , \"query\" : { \"prefix\" : { \"metadata.name.keyword\" : { \"value\" : \"atlan-snowflake-miner\" // (1) } } } } } ] } }, \"sort\" : [ { \"metadata.creationTimestamp\" : { \"nested\" : { \"path\" : \"metadata\" }, \"order\" : \"desc\" } } ], \"track_total_hits\" : true } Searching by the atlan-snowflake-miner prefix will ensure you only find existing Snowflake miner workflows. Name of the workflow The name of the workflow will be nested within the _source.metadata.name property of the response object. (Remember since this is a search, there could be multiple results, so you may want to use the other details in each result to determine which workflow you really want.) POST /api/service/workflows/submit 100 101 102 103 104 { \"namespace\" : \"default\" , \"resourceKind\" : \"WorkflowTemplate\" , \"resourceName\" : \"atlan-snowflake-miner-1684500411\" // (1) } Send the name of the workflow as the resourceName to rerun it. 2"
  },
  {
    "url": "https://developer.atlan.com/toolkits/typedef/bind-sdks/",
    "content": "Bind the SDKs Â¶ Of course, to actually test the UX you should create some assets of the new type(s) and test. The simplest way to do this is to make them programmatically accessible â€” via an SDK. Clone SDK repository Â¶ To actually implement the SDK bindings, you will first need to clone the SDK's code repository: Java Python Set up the development environment for the Java SDK . Open the terminal and clone the latest atlan-java repository: git clone https://github.com/atlanhq/atlan-java.git Ensure you have a working Java 21 JDK as your $JAVA_HOME : java -version echo $JAVA_HOME export JAVA_HOME = /Library/Java/JavaVirtualMachines/openjdk-21.jdk/Contents/Home Ensure Gradle is set up and able to compile (this will also ensure all dependencies are downloaded): ./gradlew assemble shadowJar Set up the development environment for pyatlan . Open the terminal and clone the latest pyatlan repository: git clone https://github.com/atlanhq/atlan-python.git We recommend creating a new virtual environment : python3 -m venv venv Activate the virtual environment: source venv/bin/activate In the project root directory, install the required dependencies: pip install -e . && pip install -r requirements-dev.txt Implement creator methods Â¶ Each SDK contains a generator package that generates SDK code based on the latest typedefs available in your Atlan instance. However, each new asset type will also have a specific set of minimal attributes required when creating any instances of that type. This minimal set of attributes is defined for developers to understand and consume through a creator method. qualifiedName Â¶ All asset types require a qualifiedName , and this must be unique for all instances of that asset type. Consider carefully how the qualifiedName should be constructed Since it must be unique across all instances of that asset type, carefully consider how the qualifiedName should be constructed â€” and automate or enforce this as much as possible in your creator implementation. While names are often used in the qualifiedName , if the system you are representing does not have unique names at a given level (or these are subject to change without the object itself changing) you may need to use some other string in the qualifiedName to ensure it remains unique. Typically this will be: default/ {{ connectorType }} / {{ epoch }} / {{ uniqueName }} # (1) default/ {{ connectorType }} / {{ epoch }} /.../ {{ uniqueName }} # (2) For a top-level asset, the connection's qualifiedName concatenated with the (unique) name of the top-level asset: For a child asset, the parent's qualifiedName concatenated with the (unique) name of the child asset: Running example (expand for details) In our running example, this would mean qualifiedName s that look like this: default/genericdb/1234567890/some-dataset # (1) default/genericdb/1234567890/some-dataset/some-table # (2) default/genericdb/1234567890/some-dataset/some-table/some-field # (3) An example qualifiedName for a dataset named some-dataset , in a generic database connection (that happened to be created on February 13, 2009 at 23:31:30 GMT). An example qualifiedName for a table named some-table created within that dataset. An example qualifiedName for a field named some-field created within that table. From the examples above, you can see all assets require at least the following information at creation (even top-level assets): typeName â€” to define the type of asset being created qualifiedName â€” which must be unique across all instances of the asset type, since it is used to determine whether to update or create a new instance connectionQualifiedName â€” for UX filtering and access control purposes, which has embedded within it a connectorType (which in turn determines which icon to use to represent each instance of an asset) Running example (expand for details) Sample creation payload (CustomDataset) { \"entities\" : [ { \"typeName\" : \"CustomDataset\" , \"attributes\" : { \"name\" : \"some-dataset\" , \"qualifiedName\" : \"default/genericdb/1234567890/some-dataset\" , \"connectionQualifiedName\" : \"default/genericdb/1234567890\" , \"connectorName\" : \"genericdb\" } } ] } The sections below walkthrough writing the code-generator templates for the running example. CustomDataset template Â¶ Now that we know the minimal required attributes for creation, we can define these in a template for each SDK. These templates define only the unique portion of the generated code in each SDK â€” any standard code will still continue to be generated automatically: Java Python sdk/src/main/resources/templates/CustomDataset.ftl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 < # macro all > /** * Builds the minimal object necessary to create a CustomDataset. * * @param name of the CustomDataset * @param connectionQualifiedName unique name of the connection through which the spec is accessible * @return the minimal object necessary to create the CustomDataset, as a builder */ public static CustomDatasetBuilder <? , ?> creator ( String name , String connectionQualifiedName ) { // (1) return CustomDataset . _internal () . guid ( \"-\" + ThreadLocalRandom . current (). nextLong ( 0 , Long . MAX_VALUE - 1 )) // (2) . qualifiedName ( connectionQualifiedName + \"/\" + name ) . name ( name ) . connectionQualifiedName ( connectionQualifiedName ) . connectorType ( Connection . getConnectorTypeFromQualifiedName ( connectionQualifiedName )); } /** * Builds the minimal object necessary to update a CustomDataset. * * @param qualifiedName of the CustomDataset * @param name of the CustomDataset * @return the minimal request necessary to update the CustomDataset, as a builder */ public static CustomDatasetBuilder <? , ?> updater ( String qualifiedName , String name ) { // (3) return CustomDataset . _internal () . guid ( \"-\" + ThreadLocalRandom . current (). nextLong ( 0 , Long . MAX_VALUE - 1 )) . qualifiedName ( qualifiedName ) . name ( name ); } /** * Builds the minimal object necessary to apply an update to a CustomDataset, from a potentially * more-complete CustomDataset object. * * @return the minimal object necessary to update the CustomDataset, as a builder * @throws InvalidRequestException if any of the minimal set of required properties for CustomDataset are not found in the initial object */ @Override public CustomDatasetBuilder <? , ?> trimToRequired () throws InvalidRequestException { // (4) validateRequired ( TYPE_NAME , Map . of ( \"qualifiedName\" , this . getQualifiedName (), \"name\" , this . getName () )); return updater ( this . getQualifiedName (), this . getName ()); } </ # macro > Even though we require 4 attributes to create an CustomDataset , we can derive all of them from just 2 inputs. So to keep the interface as simple as possible, we will only request the 2 inputs we need to derive (automatically) the rest. Always set the guid to a random, negative integer. This allows the SDK (and Atlan's back-end) to handle referential integrity when multiple inter-related assets are submitted in a single request â€” even if some of them need to be created. Also implement an updater() method that takes the minimal set of attributes required to update an asset of this type. In almost all cases this will be qualifiedName and name , but in some very rare cases could require other attributes. Finally, implement the trimToRequired method to validate that an object of this type has the minimal set of attributes required to be used to update such an asset in Atlan. Like the updater method this will in almost all cases just validate qualifiedName and name are present, but in some very rare cases could validate other attributes. In Python, we need to create two templates: pyatlan/generator/templates/methods/asset/custom_dataset.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 @classmethod @init_guid def creator ( cls , * , name : str , connection_qualified_name : str ) -> CustomDataset : # (1) validate_required_fields ( [ \"name\" , \"connection_qualified_name\" ], [ name , connection_qualified_name ] ) attributes = CustomDataset . Attributes . create ( name = name , connection_qualified_name = connection_qualified_name ) return cls ( attributes = attributes ) @classmethod @init_guid def create ( cls , * , name : str , connection_qualified_name : str ) -> CustomDataset : warn ( ( \"This method is deprecated, please use 'creator' \" \"instead, which offers identical functionality.\" ), DeprecationWarning , stacklevel = 2 , ) return cls . creator ( name = name , connection_qualified_name = connection_qualified_name ) Even though we require 4 attributes to create an CustomDataset , we can derive all of them from just 2 inputs. So to keep the interface as simple as possible, we will only request the 2 inputs we need to derive (automatically) the rest. pyatlan/generator/templates/methods/attribute/custom_dataset.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 @classmethod @init_guid def create ( cls , * , name : str , connection_qualified_name : str ) -> {{ entity_def . name }} . Attributes : validate_required_fields ( [ \"name\" , \"connection_qualified_name\" ], [ name , connection_qualified_name ] ) return {{ entity_def . name }} . Attributes ( # (1) name = name , qualified_name = f \" { connection_qualified_name } / { name } \" , connection_qualified_name = connection_qualified_name , connector_name = AtlanConnectorType . get_connector_name ( connection_qualified_name ), ) It is within the attributes template that we derive all the required attributes from the minimal inputs requested in the asset-level template. CustomTable template Â¶ As stated earlier, the qualifiedName should be a concatenation onto the parent's qualifiedName (in our running example, the parent of a CustomTable is a CustomDataset ). Java Python sdk/src/main/resources/templates/CustomTable.ftl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 < # macro all > /** * Builds the minimal object necessary to create a CustomTable. * * @param name of the CustomTable * @param customDataset in which the CustomTable should be created, which must have at least *                a qualifiedName * @return the minimal request necessary to create the CustomTable, as a builder * @throws InvalidRequestException if the CustomDataset provided is without a qualifiedName */ public static CustomTableBuilder <? , ?> creator ( String name , CustomDataset customDataset ) throws InvalidRequestException { // (1) validateRelationship ( CustomDataset . TYPE_NAME , Map . of ( \"qualifiedName\" , customDataset . getQualifiedName () )); return creator ( name , customDataset . getQualifiedName () ). customDataset ( customDataset . trimToReference ()); // (2) } /** * Builds the minimal object necessary to create an CustomTable. * * @param name unique name of the CustomTable * @param customDatasetQualifiedName unique name of the CustomDataset through which the table is accessible * @return the minimal object necessary to create the CustomTable, as a builder */ public static CustomTableBuilder <? , ?> creator ( String name , String customDatasetQualifiedName ) { // (3) String connectionQualifiedName = StringUtils . getParentQualifiedNameFromQualifiedName ( customDatasetQualifiedName ); return CustomTable . _internal () . guid ( \"-\" + ThreadLocalRandom . current (). nextLong ( 0 , Long . MAX_VALUE - 1 )) . qualifiedName ( customDatasetQualifiedName + \"/\" + name ) . name ( name ) . customDataset ( CustomDataset . refByQualifiedName ( customDatasetQualifiedName )) . connectionQualifiedName ( connectionQualifiedName ) . connectorType ( Connection . getConnectorTypeFromQualifiedName ( connectionQualifiedName )); } /** * Builds the minimal object necessary to update a CustomTable. * * @param qualifiedName of the CustomTable * @param name of the CustomTable * @return the minimal request necessary to update the CustomTable, as a builder */ public static CustomTableBuilder <? , ?> updater ( String qualifiedName , String name ) { return CustomTable . _internal () . guid ( \"-\" + ThreadLocalRandom . current (). nextLong ( 0 , Long . MAX_VALUE - 1 )) . qualifiedName ( qualifiedName ) . name ( name ); } /** * Builds the minimal object necessary to apply an update to a CustomTable, from a potentially * more-complete CustomTable object. * * @return the minimal object necessary to update the CustomTable, as a builder * @throws InvalidRequestException if any of the minimal set of required properties for CustomTable are not found in the initial object */ @Override public CustomTableBuilder <? , ?> trimToRequired () throws InvalidRequestException { validateRequired ( TYPE_NAME , Map . of ( \"qualifiedName\" , this . getQualifiedName (), \"name\" , this . getName () )); return updater ( this . getQualifiedName (), this . getName ()); } </ # macro > For asset types that have a parent asset, you should provide multiple overloaded creator methods. For example, one that takes the parent object itself (and validates the provided object has the minimal set of attributes we require on it) and one that takes only a qualifiedName of the parent asset. When implementing the method that takes a parent object, always set the relationship to the parent object explicitly and by using the trimToReference() method on the parent object. This ensures that any GUID on the parent object is used to create the reference to the parent object â€” which ensures that any negative integer present for referential integrity is preferred over a qualifiedName for the parent object. (Which further ensures that you can create both parent and child objects in the same request.) Typically the fully-parameterized creator() method (with various string parameters) will be the one you call through to from any other overloaded creator() methods, so they all share the same foundational implementation. pyatlan/generator/templates/methods/asset/custom_table.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 @overload @classmethod def creator ( cls , * , name : str , custom_dataset_qualified_name : str , ) -> CustomTable : ... @overload @classmethod def creator ( cls , * , name : str , custom_dataset_qualified_name : str , connection_qualified_name : str , ) -> CustomTable : ... @classmethod @init_guid def creator ( cls , * , name : str , custom_dataset_qualified_name : str , connection_qualified_name : Optional [ str ] = None , ) -> CustomTable : validate_required_fields ( [ \"name\" , \"custom_dataset_qualified_name\" ], [ name , custom_dataset_qualified_name ] ) attributes = CustomTable . Attributes . create ( name = name , custom_dataset_qualified_name = custom_dataset_qualified_name , connection_qualified_name = connection_qualified_name , ) return cls ( attributes = attributes ) @classmethod @init_guid def create ( cls , * , name : str , custom_dataset_qualified_name : str ) -> CustomTable : warn ( ( \"This method is deprecated, please use 'creator' \" \"instead, which offers identical functionality.\" ), DeprecationWarning , stacklevel = 2 , ) return cls . creator ( name = name , custom_dataset_qualified_name = custom_dataset_qualified_name ) pyatlan/generator/templates/methods/attribute/custom_table.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 @classmethod @init_guid def create ( cls , * , name : str , custom_dataset_qualified_name : str , connection_qualified_name : Optional [ str ] = None , ) -> CustomTable . Attributes : validate_required_fields ( [ \"name\" , \"custom_dataset_qualified_name\" ], [ name , custom_dataset_qualified_name ], ) if connection_qualified_name : connector_name = AtlanConnectorType . get_connector_name ( connection_qualified_name ) else : connection_qn , connector_name = AtlanConnectorType . get_connector_name ( custom_dataset_qualified_name , \"custom_dataset_qualified_name\" , 4 ) return CustomTable . Attributes ( name = name , custom_dataset_qualified_name = custom_dataset_qualified_name , connector_name = connector_name , connection_qualified_name = connection_qualified_name or connection_qn , qualified_name = f \" { custom_dataset_qualified_name } / { name } \" , custom_dataset = CustomDataset . ref_by_qualified_name ( custom_dataset_qualified_name ), ) CustomField template Â¶ Finally, the CustomField template will be very similar to the CustomTable template. Illustrates the case where a qualifiedName may contain forward-slashes Since qualifiedName s are typically constructed using a / as a delimiter, if the system you are representing could actually contain a / in the unique information you are placing into the qualifiedName you need to be sure you pass all information to create the qualifiedName â€” you will not be able to parse the parent's qualifiedName in these cases. Java Python sdk/src/main/resources/templates/CustomField.ftl 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 < # macro all > /** * Builds the minimal object necessary to create a CustomField. * * @param name of the CustomField * @param customTable in which the CustomField should be created, which must have at least *                a qualifiedName * @return the minimal request necessary to create the CustomField, as a builder * @throws InvalidRequestException if the CustomTable provided is without a qualifiedName */ public static CustomFieldBuilder <? , ?> creator ( String name , CustomTable customTable ) throws InvalidRequestException { // (1) Map < String , String > map = new HashMap <> (); // (2) map . put ( \"connectionQualifiedName\" , customTable . getConnectionQualifiedName ()); map . put ( \"customDatasetName\" , customTable . getCustomDatasetName ()); map . put ( \"customDatasetQualifiedName\" , customTable . getCustomDatasetQualifiedName ()); map . put ( \"name\" , customTable . getName ()); map . put ( \"qualifiedName\" , customTable . getQualifiedName ()); validateRelationship ( CustomTable . TYPE_NAME , map ); return creator ( name , customTable . getConnectionQualifiedName (), customTable . getCustomDatasetName (), customTable . getCustomDatasetQualifiedName (), customTable . getName (), customTable . getQualifiedName () ). customTable ( customTable . trimToReference ()); // (3) } /** * Builds the minimal object necessary to create a CustomField. * * @param name unique name of the CustomField * @param customTableQualifiedName unique name of the CustomTable through which the table is accessible * @return the minimal object necessary to create the CustomField, as a builder */ public static CustomFieldBuilder <? , ?> creator ( String name , String customTableQualifiedName ) { // (4) String customTableName = StringUtils . getNameFromQualifiedName ( customTableQualifiedName ); String customDatasetQualifiedName = StringUtils . getParentQualifiedNameFromQualifiedName ( customTableQualifiedName ); String customDatasetName = StringUtils . getNameFromQualifiedName ( customDatasetQualifiedName ); String connectionQualifiedName = StringUtils . getParentQualifiedNameFromQualifiedName ( customDatasetQualifiedName ); return creator ( name , connectionQualifiedName , customDatasetName , customDatasetQualifiedName , customTableName , customTableQualifiedName ); } /** * Builds the minimal object necessary to create a CustomField. * * @param name of the CustomField * @param connectionQualifiedName unique name of the connection in which to create the CustomField * @param customDatasetQualifiedName simple name of the CustomDataset in which to create the CustomField * @param customDatasetName unique name of the CustomDataset in which to create the CustomField * @param customTableName simple name of the CustomTable in which to create the CustomField * @param customTableQualifiedName unique name of the CustomTable in which to create the CustomField * @return the minimal request necessary to create the CustomField, as a builder */ public static CustomFieldBuilder <? , ?> creator ( String name , String connectionQualifiedName , String customDatasetQualifiedName , String customDatasetName , String customTableName , String customTableQualifiedName ) { AtlanConnectorType connectorType = Connection . getConnectorTypeFromQualifiedName ( connectionQualifiedName ); return CustomField . _internal () . guid ( \"-\" + ThreadLocalRandom . current (). nextLong ( 0 , Long . MAX_VALUE - 1 )) . name ( name ) . qualifiedName ( generateQualifiedName ( name , customTableQualifiedName )) . connectorType ( connectorType ) . customTableName ( customTableName ) . customTableQualifiedName ( customTableQualifiedName ) . customTable ( CustomTable . refByQualifiedName ( customTableQualifiedName )) . customDatasetName ( customDatasetName ) . customDatasetQualifiedName ( customDatasetQualifiedName ) . connectionQualifiedName ( connectionQualifiedName ); } /** * Generate a unique CustomField name. * * @param name of the CustomField * @param customTableQualifiedName unique name of the CustomTable in which this CustomField exists * @return a unique name for the CustomField */ public static String generateQualifiedName ( String name , String customTableQualifiedName ) { // (6) return customTableQualifiedName + \"/\" + name ; } /** * Builds the minimal object necessary to update a CustomField. * * @param qualifiedName of the CustomField * @param name of the CustomField * @return the minimal request necessary to update the CustomField, as a builder */ public static CustomFieldBuilder <? , ?> updater ( String qualifiedName , String name ) { return CustomField . _internal () . guid ( \"-\" + ThreadLocalRandom . current (). nextLong ( 0 , Long . MAX_VALUE - 1 )) . qualifiedName ( qualifiedName ) . name ( name ); } /** * Builds the minimal object necessary to apply an update to a CustomField, from a potentially * more-complete CustomField object. * * @return the minimal object necessary to update the CustomField, as a builder * @throws InvalidRequestException if any of the minimal set of required properties for CustomField are not found in the initial object */ @Override public CustomFieldBuilder <? , ?> trimToRequired () throws InvalidRequestException { validateRequired ( TYPE_NAME , Map . of ( \"qualifiedName\" , this . getQualifiedName (), \"name\" , this . getName () )); return updater ( this . getQualifiedName (), this . getName ()); } </ # macro > For asset types that have a parent asset, you should provide multiple overloaded creator methods. For example, one that takes the parent object itself (and validates the provided object has the minimal set of attributes we require on it) and one that takes all the qualifiedName s and de-normalized name s  of the ancestral assets. When receiving only the parent asset, you will need to validate you have all the other information required on that parent asset (in particular, the full set of de-normalized attributes needed to create this asset). When implementing the method that takes a parent object, always set the relationship to the parent object explicitly and by using the trimToReference() method on the parent object. This ensures that any GUID on the parent object is used to create the reference to the parent object â€” which ensures that any negative integer present for referential integrity is preferred over a qualifiedName for the parent object. (Which further ensures that you can create both parent and child objects in the same request.) You may still want to implement a creator() that parses details from the immediate parent's qualifiedName , if you know you will have control over when you must use the other (because some element of the qualifiedName itself has a / in it). Typically the fully-parameterized creator() method (with various string parameters) will be the one you call through to from any other overloaded creator() methods, so they all share the same foundational implementation. You may also want to define a distinct method to generate the qualifiedName for the asset based on a set of defined inputs. pyatlan/generator/templates/methods/asset/custom_field.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 @overload @classmethod def creator ( cls , * , name : str , custom_table_qualified_name : str , ) -> CustomField : ... @overload @classmethod def creator ( cls , * , name : str , custom_table_qualified_name : str , custom_table_name : str , custom_dataset_name : str , custom_dataset_qualified_name : str , connection_qualified_name : str , ) -> CustomField : ... @classmethod @init_guid def creator ( cls , * , name : str , custom_table_qualified_name : str , custom_table_name : Optional [ str ] = None , custom_dataset_name : Optional [ str ] = None , custom_dataset_qualified_name : Optional [ str ] = None , connection_qualified_name : Optional [ str ] = None , ) -> CustomField : validate_required_fields ( [ \"name\" , \"custom_table_qualified_name\" ], [ name , custom_table_qualified_name ] ) attributes = CustomField . Attributes . create ( name = name , custom_table_qualified_name = custom_table_qualified_name , custom_table_name = custom_table_name , custom_dataset_name = custom_dataset_name , custom_dataset_qualified_name = custom_dataset_qualified_name , connection_qualified_name = connection_qualified_name , ) return cls ( attributes = attributes ) @classmethod @init_guid def create ( cls , * , name : str , custom_table_qualified_name : str ) -> CustomField : warn ( ( \"This method is deprecated, please use 'creator' \" \"instead, which offers identical functionality.\" ), DeprecationWarning , stacklevel = 2 , ) return cls . creator ( name = name , custom_table_qualified_name = custom_table_qualified_name ) pyatlan/generator/templates/methods/attribute/custom_field.jinja2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 @classmethod @init_guid def create ( cls , * , name : str , custom_table_qualified_name : str , custom_table_name : Optional [ str ] = None , custom_dataset_name : Optional [ str ] = None , custom_dataset_qualified_name : Optional [ str ] = None , connection_qualified_name : Optional [ str ] = None , ) -> CustomField . Attributes : validate_required_fields ( [ \"name\" , \"custom_table_qualified_name\" ], [ name , custom_table_qualified_name ], ) if connection_qualified_name : connector_name = AtlanConnectorType . get_connector_name ( connection_qualified_name ) else : connection_qn , connector_name = AtlanConnectorType . get_connector_name ( custom_table_qualified_name , \"custom_table_qualified_name\" , 5 ) fields = custom_table_qualified_name . split ( \"/\" ) qualified_name = f \" { custom_table_qualified_name } / { name } \" connection_qualified_name = connection_qualified_name or connection_qn custom_dataset_name = custom_dataset_name or fields [ 3 ] custom_table_name = custom_table_name or fields [ 4 ] custom_dataset_qualified_name = ( custom_dataset_qualified_name or f \" { connection_qualified_name } / { custom_dataset_name } \" ) return CustomField . Attributes ( name = name , qualified_name = qualified_name , custom_table_qualified_name = custom_table_qualified_name , custom_table_name = custom_table_name , custom_dataset_name = custom_dataset_name , custom_dataset_qualified_name = custom_dataset_qualified_name , connector_name = connector_name , connection_qualified_name = connection_qualified_name , custom_table = CustomTable . ref_by_qualified_name ( custom_table_qualified_name ), ) Generate model code Â¶ Now that the creator method has been defined for each new asset type, you can regenerate the SDK's code to include these new asset types: Java Python Before running any generator scripts, make sure you have configured your environment variables (setting the ATLAN_BASE_URL and ATLAN_API_KEY environment variables is sufficient). Generate the asset model, enums, and structs in the SDK based on the typedefs present in your Atlan instance: ./gradlew genModel If you see failures The generator also generates unit tests for new asset types, which will use Java reflection to investigate objects like enums. If you see an error during the generation that a given type (struct or enum) is unknown, you may need to simply re-run the generator. The generated files will be unformatted, so we recommend running Spotless to format the code nicely: ./gradlew spotlessApply Before running any generator scripts, make sure you have configured your environment variables ( ATLAN_BASE_URL and ATLAN_API_KEY ). Retrieve the typedefs from an Atlan instance and write them to a JSON file by running the following script: python3 pyatlan/generator/create_typedefs_file.py Finally, to generate the asset model, enums, and structs modules in the SDK based on the typedefs present in your Atlan instance, run: python3 pyatlan/generator/class_generator.py The generated files will be unformatted, so we recommend running pyatlan_formatter to format the code nicely: ./pyatlan-formatter Now, you are ready to import the generated asset models into your test scripts and easily manage their objects! 2"
  }
]