[
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/how-tos/crawl-amazon-quicksight",
    "content": "Connect data BI Tools Cloud-based BI Amazon QuickSight Crawl QuickSight Assets Crawl Amazon QuickSight On this page Crawl Amazon QuickSight Once you have configured the Amazon QuickSight permissions , you can establish a connection between Atlan and Amazon QuickSight. To crawl metadata from Amazon QuickSight, review the order of operations and then complete the following steps. Select the source ​ To select Amazon QuickSight as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click QuickSight Assets . In the right panel, click Setup Workflow . Provide your credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Agent extraction, Atlan’s secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your Amazon QuickSight credentials: For Authentication, IAM User is the default authentication method. For AWS Access Key , enter the AWS access key you downloaded . For AWS Secret Key , enter the AWS secret key you downloaded . At the bottom, enter the Region and AWS Account ID of your Amazon QuickSight instance. Click the Test Authentication button to confirm connectivity to Amazon QuickSight. Once authentication is successful, navigate to the bottom of the screen and click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from Amazon QuickSight. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Amazon QuickSight data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the Amazon QuickSight connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Amazon QuickSight crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options: For Fetch all assets without folder , click Yes to fetch assets not linked to any folders, including datasets, analyses, and dashboards, or click No to only fetch assets linked to folders. To select the folders you want to include in crawling, click Include Folders . (This will default to all folders, if none are specified.) To select the folders you want to exclude from crawling, click Exclude Folders . (This will default to no folders, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Amazon QuickSight crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Amazon QuickSight Next What does Atlan crawl from Amazon QuickSight? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/how-tos/crawl-domo",
    "content": "Connect data BI Tools Cloud-based BI Domo Crawl Domo Assets Crawl Domo On this page Crawl Domo Once you have configured the Domo permissions , you can establish a connection between Atlan and Domo. To crawl metadata from Domo, review the order of operations and then complete the following steps. Select the source ​ To select Domo as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Domo Assets . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Domo credentials: For Host Name , enter the URL for your Domo instance. For Authentication , Basic is the default selection. For Client ID , enter the client ID you copied from the Domo developer portal. For Client Secret , enter the client secret you copied from the Domo developer portal. For Access Token , enter the access token you copied from your Domo instance. Click the Test Authentication button to confirm connectivity to Domo. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Domo connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Domo crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the dashboards you want to include in crawling, click Include dashboards . (This will default to all assets, if none are specified.) To select the dashboards you want to exclude from crawling, click Exclude dashboards . (This will default to no assets, if none are specified.) For DomoStats dataset ID to get cards metadata , enter the dataset ID for the dataset you created to import card metadata on the DomoStats connector. For DomoStats dataset ID to get card-dashboard relationship metadata , enter the dataset ID for the dataset you created to import card-dashboard relationship metadata on the DomoStats connector. For DomoStats dataset ID to get dataset-card relationship metadata , enter the dataset ID for the dataset you created to import dataset-card relationship metadata on the DomoStats connector. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Domo crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Domo Next What does Atlan crawl from Domo? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/crawl-ibm-cognos-analytics",
    "content": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics Crawl Cognos Analytics Assets Crawl IBM Cognos Analytics On this page Crawl IBM Cognos Analytics Once you have configured the IBM Cognos Analytics permissions , you can establish a connection between Atlan and IBM Cognos Analytics. To crawl metadata from IBM Cognos Analytics, revie w the order of operations and then complete the following steps. Select the source ​ To select IBM Cognos Analytics as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click IBM Cognos Analytics Assets . In the right panel, click Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . Direct extraction method ​ To enter your IBM Cognos Analytics credentials: For Host Name , enter the hostname of your IBM Cognos Analytics instance. For Port , enter the port number of your IBM Cognos Analytics instance. For Authentication , select the method you configured when setting up the IBM Cognos Analytics access permissions : For Basic , enter the Username and Password you configured for the new user. For API Key , enter the Username and API Key you configured for the new user. For OKTA , enter the Username and Password you configured for the new user in OKTA. For Namespace , enter the name of the namespace where you created the new user . Click the Test Authentication button to confirm connectivity to IBM Cognos Analytics. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method ​ Atlan supports the offline extraction method for fetching metadata from IBM Cognos Analytics. This method uses Atlan's cognos-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your bucket details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include output/cognos-example/contents/0/result-0.json , output/cognos-example/contents-details/0/result-0.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Configure the connection ​ To complete the IBM Cognos Analytics connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, no one can manage the connection-not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the IBM Cognos Analytics crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Folders . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Folders . (This will default to no assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the IBM Cognos Analytics crawler, after completing the steps above: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up on-premises IBM Cognos Analytics access Next Crawl on-premises IBM Cognos Analytics Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/crawl-on-premises-ibm-cognos-analytics",
    "content": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics Crawl Cognos Analytics Assets Crawl on-premises IBM Cognos Analytics On this page Crawl on-premises IBM Cognos Analytics Once you have set up the cognos-extractor tool , you can extract metadata from your on-premises IBM Cognos Analytics instances by completing the following steps. Run cognos-extractor ​ Crawl all IBM Cognos Analytics connections ​ To crawl all IBM Cognos Analytics connections using the cognos-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection ​ To crawl a specific IBM Cognos Analytics connection using the cognos-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files ​ The cognos-extractor tool will generate many folders with JSON files for each service . For example: folders explorations reports modules data sources and many others You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 ​ To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/cognos-example/contents/result-0.json , output/cognos-example/contents-details/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/cognos-example s3://my-bucket/metadata/cognos-example --recursive Crawl metadata in Atlan ​ Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl IBM Cognos Analytics Be sure you select Offline for the Extraction method . Tags: connectors data crawl Previous Crawl IBM Cognos Analytics Next What does Atlan crawl from IBM Cognos Analytics? Run cognos-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-on-premises-ibm-cognos-analytics-access",
    "content": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics Get Started Set up on-premises IBM Cognos Analytics access On this page Set up on-premises IBM Cognos Analytics access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your IBM Cognos Analytics instance details, including credentials. In some cases you will not be able to expose your IBM Cognos Analytics instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites ​ To extract metadata from your on-premises IBM Cognos Analytics instance, you will need to use Atlan's cognos-extractor tool. Did you know? Atlan uses exactly the same cognos-extractor behind the scenes when it connects to IBM Cognos Analytics in the cloud. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the cognos-extractor tool ​ To get the cognos-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl IBM Cognos Analytics: sudo docker load -i /path/to/cognos-extractor-master.tar Get the compose file ​ Atlan provides you with a Docker compose file for the cognos-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises IBM Cognos Analytics instance. The file is docker-compose.yaml . Define IBM Cognos Analytics connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your IBM Cognos Analytics connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services ​ For each on-premises IBM Cognos Analytics instance, define an entry under services in the compose file. Each entry will have the following structure: services: cognos-example: <<: *extract environment: <<: *cognos-defaults EXCLUDE_FILTER: '{}' INCLUDE_FILTER: '{}' volumes: - ./output/cognos-example:/output/process Replace cognos-example with the name of your connection. <<: *extract tells the cognos-extractor tool to run. environment contains all parameters for the tool. EXCLUDE_FILTER and INCLUDE_FILTER -  specify a regular expression to filter assets to exclude or include, respectively. For example, to exclude a folder with the ID 76471ff1e0f02c7d3349 in team_content , configure the EXCLUDE_FILTER as follows   - '{\"team_content\": {\"76471ff1e0f02c7d3349\": {}}' . volumes specifies where to store results. In this example, the extractor will store results in the ./output/cognos-example folder on the local file system. You can add as many IBM Cognos Analytics connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials ​ To define the credentials for your IBM Cognos Analytics connections, you will need to provide an IBM Cognos Analytics configuration file. The IBM Cognos Analytics configuration is a .ini file with the following format: [CognosConfig] host=http://cognos-application-host-example.us-east-2.compute.amazonaws.com port=9300 namespace=CognosEx # possible values are \"basic_auth\", \"okta_auth\" and \"api_key\" auth_type=basic_auth # Only required when auth_type = basic_auth [BasicAuth] [email protected] password=<password> # Only required when auth_type = okta_auth [OKTAAuth] [email protected] password=<password> # Only required when auth_type = api_key [APIKeyAuth] key=<yourAPIkey> Secure credentials ​ Using local files ​ danger If you decide to keep IBM Cognos Analytics credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: cognos_config: file: ./cognos.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets ​ To create and use Docker secrets: Store the IBM Cognos Analytics configuration file: sudo docker secret create cognos_config path/to/cognos.ini At the top of your compose file, add a secrets element to access your secret: secrets: cognos_config: external: true name: cognos_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local IBM Cognos Analytics configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example ​ Let's explain in detail with an example: secrets: cognos_config: external: true name: cognos_config x-templates: # ... services: cognos-example: <<: *extract environment: <<: *cognos-defaults EXCLUDE_FILTER: '{}' INCLUDE_FILTER: '{}' volumes: - ./output/cognos-example:/output/process secrets: - cognos_config In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The cognos_config refers to an external Docker secret created using the docker secret create command. The name of this service is cognos-example . You can use any meaningful name you want. The <<: *cognos-defaults sets the connection type to IBM Cognos Analytics. The EXCLUDE_FILTER and INCLUDE_FILTER tells the extractor to filter folders. The ./output/cognos-example:/output/process line tells the extractor where to store results. In this example, the extractor will store results in the ./output/cognos-example directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Set up IBM Cognos Analytics Next Crawl IBM Cognos Analytics Prerequisites Get the compose file Define IBM Cognos Analytics connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/ibm-cognos-analytics/how-tos/set-up-ibm-cognos-analytics",
    "content": "Connect data BI Tools On-premises & Enterprise BI IBM Cognos Analytics Get Started Set up IBM Cognos Analytics On this page Set up IBM Cognos Analytics Who can do this? You must be an IBM Cognos Analytics administrator to complete these steps   -  you may not have access yourself. Atlan supports the following authentication methods for fetching metadata from IBM Cognos Analytics: Basic authentication -  this method uses a username and password to fetch metadata. API authentication -  this method uses a username and an API key to fetch metadata. OKTA authentication -  this method uses a username and password of OKTA to fetch metadata. Create user ​ To create a new user for crawling IBM Cognos Analytics : Log in to your IBM Cognos Analytics instance. Expand the left menu of your homepage and then click Manage . From the corresponding menu, click People and then click Accounts . In Accounts , under Namespaces , select your Cognos namespace to open it. From the upper right of your namespace page, click the new user icon to add a new user to the selected namespace. In the New user form, enter the following details: For Given name , enter a meaningful name for the new user. For User ID , create a username for the new user. For Password , create a password for the username. For Email , you can leave this blank. Click OK to save your configuration. The new user you created is added to the list of entries in your namespace. (Optional) Create an API key ​ You can also use API authentication for integrating with Atlan. In addition to the username for the new user created in the Create user section, you need an API key for authenticating the connection. To create an API key for crawling IBM Cognos Analytics : Log in to your IBM Cognos Analytics instance as the new user created in the Create user section. In the top right of your homepage, click the personal menu icon and then click Profile and settings . In the Profile and settings tab, under Advanced options , next to My API keys , click Manage . From the upper right of the My API keys page, click the Generate API key button. In the Generate API key dialog, enter the following details: For Name , enter a meaningful name for the API key. (Optional) For Description , enter a brief description. Click Next to proceed. Once the encrypted key has appeared on the screen, copy and store the value in a secure location. danger IBM Cognos Analytics doesn't store the API key, you must copy and save it. Click Done . Your new API key appears in the list of keys on the My API keys page. If you experience any functionality issues with the newly created API key, you can renew your credentials. Navigate to the Profile and settings menu, and then next to the Credentials option, click the Renew button to refresh your credentials. (Optional) Create user in OKTA ​ If the IBM Cognos Namespace type is \"OKTA\" and OKTA is used for login, a\ncorresponding user must be created in OKTA to enable login to IBM Cognos via\nOKTA. If IBM Cognos is configured to use OKTA as the authentication provider (via the OKTA namespace type), each user must have a valid account in OKTA to successfully log in. Follow these instructions to create a new user in OKTA and assign a user type for accessing IBM Cognos Analytics: Log in to your OKTA instance with Admin credentials. From the left menu on the homepage, expand Directory and select People . Click Add Person . In the New User form, fill in the following details: Select the appropriate User Type . Enter user's personal details. Assign the user to the relevant group. Click Save to complete the process. Add user to a Cognos role ​ To add the new user to the Cognos Reader role: Log in to your IBM Cognos Analytics instance. Expand the left menu of your homepage and then click Manage . From the corresponding menu, click Administration console . From the tabs along the top of the IBM Cognos Administration page, click Security . In the Security tab, select the Cognos namespace. From the list of standard roles, navigate to Readers . In the Actions column for Readers , click More . This role allows read-only access to IBM Cognos Analytics, refer to the standard roles documentation to learn more. In the Perform an action - Readers screen, under Available actions , click Set members . In the Members tab, click Add to add a new entry to the list. In the Select entries (Navigate) - Readers screen, from the Available entries , select the namespace where you created the new user. In the corresponding screen, under Directory , click the Show users in the list checkbox and then select the new user you created . Click the right-arrow button, and when the entry you want appears in the Selected entries box, click OK . Set permissions ​ All entries such as folders, reports, modules, and more already have the Readers role assigned to them by default. You will only need to set permissions for the new user to data server connections. To set access permissions for the new user to Cognos entries: Log in to your IBM Cognos Analytics instance. Expand the left menu of your homepage and then click Data server connections . On the Data server connections page, to set permissions for each data server connection, click the vertical 3-dot icon and then click Properties . From the tabs along the top of the Properties page, click the Permissions tab. In the upper right of the Permissions page, click the + icon to add a new member. In the Add member form, select the Cognos namespace and then search for and select the Readers role. Click Add . Once you have added the role, click Save to save your configuration. Find namespace ​ You must have the name of your namespace where you created the new user for authenticating the connection in Atlan. There are several ways to find the name of your namespace, here is one such method. To find the namespace details where you created the new user: Log in to your IBM Cognos Analytics instance. Expand the left menu of your homepage and then click Manage . From the corresponding menu, click Administration console . From the tabs along the top of the IBM Cognos Administration page, click Security . In the Security tab, select the namespace where you created the new user . Make sure that the new user is listed in the selected namespace. From the top right of your namespace page, click the Set properties chart icon. In the Set properties (namespace) page, next to Location , click the View the search path, ID and URL link. In the View the search path, ID and URL form, under Search path , next to CAMID , the name of your namespace is shown enclosed within brackets   -  for example, CAMID(<YOUR NAMESPACE NAME>) . Copy the value for <YOUR NAMESPACE NAME> and store it in a secure location. Tags: connectors data crawl api authentication Previous IBM Cognos Analytics Next Set up on-premises IBM Cognos Analytics access Create user (Optional) Create an API key (Optional) Create user in OKTA Add user to a Cognos role Set permissions Find namespace"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/crawl-looker",
    "content": "Connect data BI Tools Cloud-based BI Looker Crawl Looker Assets Crawl Looker On this page Crawl Looker Once you have configured the Looker user permissions , you can establish a connection between Atlan and Looker. To crawl metadata from Looker, review the order of operations and then complete the following steps. Select the source ​ To select Looker as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Looker Assets and click on Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to Looker and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan’s secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your Looker credentials: For Host Name , enter the full URL for your Looker API host, including the https:// . For Port , keep 443 for Looker instances created after July 7, 2020, or switch to 19999 for older instances. For Client ID , enter the client ID you generated when setting up user permissions . For Client Secret , enter the client secret you generated when setting up user permissions . (Optional) For Field Level Lineage : For Private SSH Key , paste the private SSH key for the key you configured in GitHub . For Passphrase for the private key , enter the passphrase that protects the key, if any. (If the key is not protected by a passphrase, leave this blank.) For SSH Known Hosts , add any value that needs to be hardcoded in the ~/.ssh/known-hosts file before cloning your project Git repositories using SSH. (If not required, leave this blank.) At the bottom of the form, click the Test Authentication button to confirm connectivity to Looker using these details. When successful, at the bottom of the screen click the Next button. Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from Looker. This method uses Atlan's looker-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include projects.json , dashboards.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from Looker. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Looker data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the Looker connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click the Next button to proceed. Configure the crawler ​ Before running the Looker crawler, you can further configure it. (These options are only available when using the direct extraction method .) You can override the defaults for any of these options: Looker folders contain saved content, such as dashboards , looks , and tiles : To select the Looker folders you want to include in crawling, click Include Folders . (This will default to all folders, if none are specified.) To select the Looker folders you want to exclude from crawling, click Exclude Folders . (This will default to no folders, if none are specified.) Looker projects contain LookML files, such as models , views , and explores : To select the Looker projects you want to include in crawling, click Include Projects . (This will default to all projects, if none are specified.) To select the Looker projects you want to exclude from crawling, click Exclude Projects . (This will default to no projects, if none are specified.) For Use Field Level Lineage , click True to enable crawling field-level lineage for Looker or click False to disable it. Did you know? If a folder or project appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Looker crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up on-premises Looker access Next Crawl on-premises Looker Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/amazon-quicksight/how-tos/set-up-amazon-quicksight",
    "content": "Connect data BI Tools Cloud-based BI Amazon QuickSight Get Started Set up Amazon QuickSight On this page Set up Amazon QuickSight warning 🤓 Who can do this? You will probably need your Amazon QuickSight administrator to run these commands   -  you may not have access yourself. Atlan currently only supports IAM user authentication for Amazon QuickSight. Create IAM policy ​ To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"quicksight:ListAnalyses\" , \"quicksight:ListDataSets\" , \"quicksight:ListDashboards\" , \"quicksight:ListFolders\" , \"quicksight:ListDataSources\" , \"quicksight:DescribeAnalysis\" , \"quicksight:DescribeDashboard\" , \"quicksight:DescribeDataSet\" , \"quicksight:DescribeFolder\" , \"quicksight:ListFolderMembers\" ] , \"Resource\" : [ \"arn:aws:quicksight:<region>:<account_id>:*\" ] } ] } Replace <region> with the AWS region of your Amazon QuickSight instance. Replace <account_id> with your AWS account ID. Configure user-based authentication ​ Using the IAM policy created above, configure user-based authentication. To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Tags: data authentication Previous Amazon QuickSight Next Crawl Amazon QuickSight Create IAM policy Configure user-based authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/crawl-on-premises-looker",
    "content": "Connect data BI Tools Cloud-based BI Looker Crawl Looker Assets Crawl on-premises Looker On this page Crawl on-premises Looker Once you have set up the looker-extractor tool , you can extract metadata from your on-premises Looker instances using the following steps. Run looker-extractor ​ Crawl all Looker connections ​ To crawl all Looker connections using the looker-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection ​ To crawl a specific Looker connection using the looker-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <CONNECTION-NAME> (Replace <CONNECTION-NAME> with the name of the connection from the services section of the compose file.) (Optional) Review generated files ​ The looker-extractor tool will generate many JSON files for each service . For example: projects.json dashboards.json dashboard_tiles.json looks.json and many others You can inspect the metadata and make sure it is acceptable to provide the metadata to Atlan. Upload generated files to S3 ​ To provide Atlan access to the extracted metadata you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure all files for a particular database have the same prefix. For example, metadata/looker/projects.json , metadata/looker/dashboards.json , etc. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/looker-example s3://my-bucket/metadata/looker-example --recursive Crawl metadata in Atlan ​ Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl Looker Be sure you select Offline for the Extraction method . Tags: connectors data crawl Previous Crawl Looker Next What does Atlan crawl from Looker? Run looker-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/domo/how-tos/set-up-domo",
    "content": "Connect data BI Tools Cloud-based BI Domo Get Started Set up Domo On this page Set up Domo Who can do this? You will need your Domo administrator to complete these steps   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Domo. This method uses the following to fetch metadata: Client ID Client secret Access token DomoStats dataset IDs Create client ID and secret ​ danger You will need your Domo Admin to create a client ID and secret for crawling Domo datasets and dataset columns . If the user creating the client credentials does not have admin privileges, only datasets will be crawled. You will need your Domo instance name to create a client ID and secret. This will be the name preceding your domo.com URL. For example, in the case of company.domo.com , the instance name will be company . To create a client ID and secret: Log in to the Domo Developer Portal with your Domo instance name   -  for example, company . Once you have entered your instance name, enter your Domo user credentials when prompted. On the Create new client page, enter the following details: For Name , enter a meaningful name for the client application   -  for example, Atlan_connection . (Optional) For Description , enter a brief description for the client application. For Application Scope , check the following boxes to assign the minimum permissions required to crawl Domo : Data -  this scope allows Atlan to access the DataSet API . Dashboard -  this scope allows Atlan to access the Page API . Click Create to complete the client application. After you have registered the client application, you will be redirected to the Manage Clients page to view your newly provisioned client ID and secret. Copy the values for Client ID and Secret and store them in a secure location. (Optional) To access your client ID and secret at a later time, navigate to the Domo Developer Portal homepage, and then from the left menu, click Manage clients to view and manage your existing client credentials. Generate access token ​ You will need to create an access token that will allow Atlan to generate upstream lineage for Domo datasets . Only a Domo Admin default security role or a custom role with either the Manage All Company Settings or Manage All Access Tokens grant enabled can generate access tokens. If you do not have either of these privileges, request an access token from your Domo administrator. To generate an access token: Log in to your Domo instance as a Domo administrator. From the tabs along the top of your Domo homepage, click More and then click Admin . On the Admin settings page, under Authentication , click Access tokens . In the upper right of the Manage access tokens page, click the Generate access token button. To specify the token information: For Access token description , enter a meaningful name for your token   -  for example, Atlan_connection_token . For Search users , search for and select the user to assign the token. danger Access tokens are associated with specific user accounts and grant the same access as the user who generated the token. If the user's permissions change, the access token will reflect the same. For Expire after , select an expiration date for the token. Click Generate to generate the access token. From the corresponding screen, copy the access token and store it in a secure location. The token will not be displayed again after you leave the Manage access tokens page. (Optional) To revoke the access token, follow the steps in Domo documentation . Set up DomoStats connector ​ The DomoStats connector allows you to import usage metadata from your Domo instance. This connector is only available to Domo administrators. Due to limitations of the Domo APIs, you will need to set up the DomoStats connector to crawl metadata for Domo cards and catalog asset relationships between cards and dashboards as well as datasets and cards in Atlan. The DomoStats connector allows you to build datasets with the required metadata. Once you have created the datasets, Atlan will require the dataset IDs to fetch the metadata from DomoStats. All three DomoStats dataset IDs are required to crawl Domo . To set up DomoStats: Log in to your Domo instance as a Domo administrator. From the tabs along the top of your Domo homepage, click Appstore . In the search bar, search for and select DomoStats . From the right panel of the DomoStats page, click the Get the Data button. In the Create DomoStats DataSet page, you will need to create three DomoStats datasets for card, card-dashboard relationship, and dataset-card relationship metadata. Except step 1 below, the remaining steps will be the same for all three datasets. Under Details , for Report , click the dropdown and then: Click Cards to create a dataset for card metadata. Click Card Pages to create a dataset for card-dashboard relationship metadata. Click Card Datasource to create a dataset for dataset-card relationship metadata. Click Next to proceed. Under Scheduling , you can either keep the predefined update schedule or define a custom schedule, and then click Next . Under Name & Describe Your DataSet , enter the following details: For Dataset Name (Required) , enter a meaningful name for your dataset. For Dataset Description (Optional) , enter a brief description for your dataset. For Add Dataset to Cloud , keep the default Domo selection. Click Save to finish setup. Once you have completed setting up a dataset, from the corresponding page, copy the dataset ID from the URL for all three datasets and store them in a secure location. For example, if the dataset URL is company.domo.com/datasource/ae6440eb-bef6-414a-a0e5-a5b2d58d1234/details/overview , then the dataset ID will be: ae6440eb-bef6-414a-a0e5-a5b2d58d1234 . Tags: connectors data crawl authentication Previous Domo Next Crawl Domo Create client ID and secret Generate access token Set up DomoStats connector"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/how-tos/crawl-metabase",
    "content": "Connect data BI Tools Cloud-based BI Metabase Crawl Metabase Assets Crawl Metabase On this page Crawl Metabase Once you have configured the Metabase user permissions , you can establish a connection between Atlan and Metabase. To crawl metadata from Metabase, review the order of operations and then complete the following steps. Select the source ​ To select Metabase as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Metabase Assets and click on Setup Workflow . Provide credentials ​ To enter your Metabase credentials: For Host Name , enter the full URL for your Metabase instance, including the https:// . For Port , enter the port number of your Metabase instance. For Authentication , enter the Username and Password you configured. Click the Test Authentication button to confirm connectivity to Metabase using these details. Once successful, at the bottom of the screen, click Next . Configure the connection ​ To complete the Metabase connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler ​ Before running the Metabase crawler, you can further configure it. You can override the defaults for any of the remaining options: Select collections you want to include in crawling in the Include Collections field. (This will default to all collections, if none are specified.) Select collections you want to exclude from crawling in the Exclude Collections field. (This will default to no collections, if none are specified.) Did you know? If a collection appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ You can now run the Metabase crawler. To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Metabase Next What does Atlan crawl from Metabase? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/set-up-looker",
    "content": "Connect data BI Tools Cloud-based BI Looker Get Started Set up Looker On this page Set up Looker Who can do this? You will probably need your Looker administrator to run these commands   -  you may not have access yourself. Atlan supports two options for user permissions in Looker. You should choose one of these methods to set up Looker: Admin role ​ This role is required for Atlan to automatically generate lineage across Looker objects. When using this role, the crawler can access all folders in Looker including personal folders. To set up this role: Log in to your Looker instance and ensure that you are an Admin user. From the menu in the upper left, click the Admin item. Under the Users section, click the Users item. In the table, find the user you're logged in as. Click the Edit button to the right of your user's row. Next to API3 Keys , click the Edit Keys button. On the resulting Edit User API3 Keys page, click the New API3 Key button. Save the generated credentials for crawling Looker . Custom role ​ danger When using this approach, Atlan will not automatically generate lineage across Looker objects. You will need to individually allow access to each folder to be included in lineage. Create role ​ To create a custom role for Atlan to access Looker: Log in to your Looker instance. From the menu in the upper left, click the Admin item. Under the Users section, click the Roles item. At the top of the page, click the New Permission Set button. Enter a name for the new permission set. For the permissions, select the following: access_data allows access to the other permissions below. see_lookml_dashboards allows Atlan to crawl LookML dashboards. see_looks allows Atlan to crawl Looks. see_user_dashboards allows Atlan to crawl user-defined dashboards. explore allows Atlan to fetch from the Explore page. see_sql allows Atlan to fetch the SQL of a query or Look, to generate lineage. see_lookml allows Atlan to fetch model information from LookML. develop allows Atlan to fetch connection names from models, to generate lineage. see_datagroups allows Atlan to fetch all connection names, to generate lineage. At the bottom of the permissions list, click the New Permission Set button. Back on the Roles page, at the top click the New Role button. Enter a name for the new role. For Permission Set , select the permission set you created in the previous step. For Model Set , select the models that you want to give access to. At the bottom of the page click the New Role button. Create user ​ To create a user through which Atlan can access Looker: Open the Admin menu in Looker. Under the Users section, click the Users item. At the top of the page, click the Add Users button. For Email addresses enter the email address for the user. For Send setup emails uncheck the setting. For Roles check the box next to the role you created above. At the bottom of the page, click the Add Users button. On the resulting page, click the Done button. Generate API key for user ​ To generate an API key for the user: Open the Admin menu in Looker. Under the Users section, click the Users item. In the table, find the user created above. Click the Edit button to the right of that user's row. (Optional) Consider entering a First Name and Last Name for the user to make it easier to recognize and find in the future. Next to API3 Keys , click the Edit Keys button. On the resulting Edit User API3 Keys page, click the New API3 Key button. Save the generated credentials for crawling Looker . Include folders for lineage ​ To include folders when using a custom role, give permission using the following steps: From the Looker menu in the upper left, click the Admin item. Under the Users section, click the Content Access item. In the resulting page next to Folders select the folder and then click on the Manage Access... button. In the blank box at the bottom of the table, select the user created above from the list. To allow Atlan to crawl only dashboards, enable the View permission for this user. To allow Atlan to crawl tiles and queries for dashboards, enable the Manage Access, Edit permission for this user. To the right of the row for that user, click the Add button. At the lower-right of the dialog, click the Save button. danger You will need to repeat these steps for every folder you want Atlan to be able to access. SSH key for lineage ​ Who can do this? Any user with access to the Looker project files in GitHub can set up this part. You will need to share the generated private key with whoever sets up the Looker crawler in Atlan. If your organization uses single sign-on (SSO) on GitHub, you must first authorize the SSH key for use with SSO. Refer to Authorizing an SSH key for use with SAML single sign-on to complete the process. In addition to the role, you also need to set up access to your project files in GitHub for the following: Generate field-level and cross-project lineage from Looker Crawl Looker views and build upstream lineage for views and explores To configure an SSH key for access to GitHub project files: Create a new SSH key on your local computer . For example, run the following command and enter a passphrase when prompted (or leave blank for no passphrase): ssh-keygen -t ed25519 -C \" [email protected] \" -f ~/.ssh/atlan_looker_lineage Copy the generated keys from your local computer . For example: To copy the public key, run this command and copy the output: cat ~/.ssh/atlan_looker_lineage.pub To copy the private key, run this command and copy the output: cat ~/.ssh/atlan_looker_lineage In the upper-right corner of any GitHub page, click your profile photo, then click Settings . Under the Access section of the left sidebar, click SSH and GPG keys . In the upper-right, click the New SSH key button: For Title enter a descriptive label for the new key. For example, Atlan Lineage . For Key paste in the public key you copied above. At the bottom of the form, click the Add SSH key button. If prompted, enter your GitHub password and click Confirm password . Tags: crawl api Previous Looker Next Set up on-premises Looker access Admin role Custom role SSH key for lineage"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/metabase/how-tos/set-up-metabase",
    "content": "Connect data BI Tools Cloud-based BI Metabase Get Started Set up Metabase On this page Set up Metabase Who can do this? You will probably need your Metabase administrator to follow the below steps   -  you may not have access yourself. Create a user ​ To create a user for Atlan to use when integrating with Metabase: From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . At the top of the page, change to the People tab. To the upper right of the table, click the Invite someone button and enter their details: For First name enter the user's first name, for example Atlan . For Last name enter the user's last name, for example User . For Email enter the user's email address, for example a service account email address. At the bottom of the dialog, click the Create button. When prompted, click Done . Create a group ​ You can only attach Metabase permissions to groups . To create a group for Atlan to use when integrating with Metabase: From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . From the top menu bar, change to the People tab. From the left of the page, open the Groups tab. At the top right, click the Create a group button. For Group name enter Atlan . On the right of the row click the Add button. To add the user to the group: Click the Atlan group you created. To the upper right of the table, click the Add members button. Under Members start typing the name used above (for example, Atlan User ) and select it. On the right of the row, click the Add button. Set permissions ​ Did you know? We do not make any API requests or queries that will update the dashboards , collections or questions in your Metabase instance. Minimum permissions ​ To set the minimum permissions required to crawl Metabase : From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . From the top menu bar, change to the Permissions tab. From the top of the page, change to the Collection permissions tab. For each collection you want to crawl in Atlan: Under the Collections heading on the left, click the collection. Under Permissions for <collection name> , for the Atlan group, under Collection access click the No access drop-down. (Optional) To crawl sub-collections, toggle the Also change sub-collections option. Select the View permission. In the upper-right of the page, click the Save changes button. When prompted with Save permissions? click the Yes button to confirm. Partial lineage permissions ​ danger When a Metabase question uses native queries, these permissions cannot capture lineage to source tables and columns. To set the minimal permissions for extracting lineage from Metabase: From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . From the top menu bar, change to the Permissions tab. From the top of the page, change to the Data permissions tab. Below the tab, click the Groups pill. Below the pill, select the Atlan group. Under Permissions for the Atlan group , for each database: Under Data access change the drop-down value to Unrestricted . Although Atlan does not query data, this permission is necessary to enable the next option. Under Native query editing change the drop-down value to Yes . This permission is necessary for Atlan to parse the queries that power your Metabase questions, to generate lineage. In the upper-right of the page, click the Save changes button. When prompted with Save permissions? click the Yes button to confirm. Complete lineage permissions ​ To set permissions for extracting lineage from all your Metabase questions: From the upper right corner of your Metabase instance, click the gear icon and then Admin Settings . From the top menu bar, change to the People tab. On the row for the Atlan user you created above, under Groups change the drop-down value to Administrators . Did you know? Administrative access is necessary to get the default source database name used for queries. This is only available to Administrators. The unrestricted data access and native query editing permissions above are insufficient. (Optional) Allowlist the Atlan IP ​ If you are using the IP allowlist in your Metabase instance, you must add your Atlan IP to the allowlist. Please raise a support ticket to learn your Atlan IP. Tags: atlan documentation Previous Metabase Next Crawl Metabase Create a user Create a group Set permissions (Optional) Allowlist the Atlan IP"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/looker/how-tos/set-up-on-premises-looker-access",
    "content": "Connect data BI Tools Cloud-based BI Looker Get Started Set up on-premises Looker access On this page Set up on-premises Looker access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Looker access details, including credentials. In some cases you won't be able to expose your Looker instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites ​ To extract metadata from your on-premises Looker instance you will need to use Atlan's looker-extractor tool. Did you know? Atlan uses exactly the same looker-extractor behind the scenes when it connects to Looker in the cloud. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the looker-extractor tool ​ To get the looker-extractor tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl Looker: sudo docker load -i /path/to/looker-extractor-master.tar Get the compose file ​ Atlan provides you with a configuration file for the looker-extractor tool. This is a Docker compose file . To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises databases. The file is docker-compose.yaml . Define Looker connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Looker connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services ​ For each on-premises Looker instance, define an entry under services in the compose file. Each entry will have the following structure: services: CONNECTION-NAME: <<: *extract environment: <<: *looker-defaults INCLUDE_PROJECTS: \"project1,project2\" USE_FIELD_LEVEL_LINEAGE: \"true\" volumes: - ./output/looker-example:/output/process Replace CONNECTION-NAME with the name of your connection. <<: *extract tells the looker-extractor tool to run. environment contains all parameters for the tool. Replaces the values given for INCLUDE_PROJECTS with the names of your own Looker projects you want to extract. Separate each project name by a comma. volumes specifies where to store results. In this example, the extractor will store results in the ./output/looker-example folder on the local file system. You can add as many Looker connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials ​ To define the credentials for your Looker connections you will need to provide: A Looker SDK configuration file A private key to access your git repository via ssh (to extract field-level lineage) A passphrase to decipher the private key (to extract field-level lineage) The Looker metadata includes the git repo locations. The Looker SDK configuration is a .ini file with the following format: [Looker] # Base URL for your looker instance API. Do not include /api/* in the URL. base_url=https://<host>:<port> # API 3 client id client_id=YourClientID # API 3 client secret client_secret=YourClientSecret verify_ssl=True Secure credentials ​ Using local files ​ danger If you decide to keep Looker credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: looker_config: file: ./looker.ini looker_git_private_key: file: ./id_ed25519 looker_git_private_key_passphrase: file: ./passphrase.txt danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets ​ To create and use Docker secrets: Store the Looker SDK configuration file: sudo docker secret create looker_config path/to/looker.ini At the top of your compose file, add a secrets element to access your secret: secrets: looker_config: external: true name: looker_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Looker SDK configuration file. info 💪 Did you know? You can use the same steps to create Docker secrets for your git details, as well. Replace the name ( looker_config ) and path to the file, but otherwise run the same command. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example ​ Let's explain in detail with an example: secrets: looker_config: external: true name: looker_config looker_git_private_key: file: ./id_ed25519 looker_git_private_key_passphrase: external: true name: looker_git_private_key_passphrase x-templates: # ... services: my-looker: <<: *extract environment: <<: *looker-defaults INCLUDE_PROJECTS: \"project1,project2\" USE_FIELD_LEVEL_LINEAGE: \"true\" volumes: - ./output/looker-example:/output/process secrets: - looker_config - looker_git_private_key - looker_git_private_key_passphrase volumes: jars: In this example we've defined the secrets at the top of the file (you could also define them at the bottom): looker_config refers to an external Docker secret created using the docker secret create command. looker_git_private_key refers to a local file. looker_git_private_key_passphrase refers to an external Docker secret created using the docker secret create command. The name of this service is my-looker . You can use any meaningful name you want. The <<: *looker-defaults sets the connection type to Looker. INCLUDE_PROJECTS tells the extractor to only extract project1 and project2 from Looker. USE_FIELD_LEVEL_LINEAGE tells the extractor to extract field-level lineage. This means the git private key information is also required. The ./output/looker-example:/output/process line tells the extractor where to store results. In this example, the extractor will store results in the ./output/looker-example directory on the local file system. We recommend you output metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Set up Looker Next Crawl Looker Prerequisites Get the compose file Define Looker connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/crawl-microsoft-power-bi",
    "content": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI Crawl Power BI Assets Crawl Microsoft Power BI On this page Crawl Microsoft Power BI Once you have configured the Microsoft Power BI user permissions , you can establish a connection between Atlan and Microsoft Power BI. To crawl metadata from Microsoft Power BI, review the order of operations and then complete the following steps. Select the source ​ To select Microsoft Power BI as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Power BI Assets and click on Setup Workflow . Provide credentials ​ To enter your Microsoft Power BI credentials: For Authentication, choose the method you want to use to access Microsoft Power BI: For Service Principal authentication, enter the Tenant Id , Client Id , and Client Secret you configured when setting up Microsoft Power BI . Use the Enable Only Admin API Access option to control how metadata is extracted. When enabled, the crawler uses only admin APIs. If disabled, both admin and non-admin APIs are used. For Delegated User authentication, enter the Username , Password , Tenant Id , Client Id , and Client Secret you configured when setting up Microsoft Power BI . At the bottom of the form, click the Test Authentication button to confirm connectivity to Microsoft Power BI using these details. Once successful, at the bottom of the screen click the Next button. Configure connection ​ To complete the Microsoft Power BI connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, nobody can manage the connection   -  not even admins. At the bottom of the screen, click the Next button to proceed. Configure the crawler ​ Before running the Microsoft Power BI crawler, configure metadata extraction and advanced options. You can override the default settings for the following fields. Configure metadata ​ Include Workspaces : Select Microsoft Power BI workspaces to include. Defaults to all workspaces when left blank. Use Advanced Search to filter workspaces using the following options: Contains : Matches workspaces that contain the given substring. Starts with : Matches workspaces that begin with the specified text. Ends with : Matches workspaces that end with the specified text. Regex pattern : Matches workspaces based on a regular expression. All selected filters apply using an AND condition. Exclude Workspaces : Select workspaces to exclude. No workspaces are excluded by default. Advanced Search is also available for exclusion, with the same filtering options as mentioned previously. Include Dashboard and Reports Regex : Use a regular expression to include dashboards and reports based on naming patterns. Includes all by default. Exclude Dashboard and Reports Regex : Use a regular expression to exclude dashboards and reports based on naming patterns. Excludes none by default. Attach Endorsements from Power BI : Automatically certify assets endorsed in Power BI. To manually review before applying, change this setting to Send a Request . For more details, see What does Atlan crawl from Microsoft Power BI? Configure advanced settings ​ Source Connections : When your tenant has multiple connections available for the same source system that share the similar metadata, confirm the advanced options and choose the correct connections from the Source Connections list drop down to avoid creating duplicate lineage to such connections. Enable ODBC DSN Connectivity Mapping : Power BI provides multiple ways of connecting to a SQL source, including ODBC connectivity for building Reports and Dashboards. When datasets are populated using ODBC, provide a mapping of the DSN ( Data Source Name ) names to their appropriate database qualified names after enabling this toggle. Did you know? If a workspace appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Microsoft Power BI crawler: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you can see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Microsoft Power BI Next Mine Microsoft Power BI Select the source Provide credentials Configure connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/mine-microsoft-power-bi",
    "content": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI Crawl Power BI Assets Mine Microsoft Power BI On this page Mine Microsoft Power BI Once you have crawled assets from Microsoft Power BI , you can mine its activity events to generate usage metrics . To mine activity events from Microsoft Power BI, review the order of operations and then complete the following steps. Select the miner ​ To select the Microsoft Power BI miner: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Miner . From the list of packages, select Power BI Miner and then click Setup Workflow . Configure the miner ​ To configure the Microsoft Power BI miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) (Optional) For Advanced Config , keep Default for the default configuration or click Advanced to configure the miner: For Start time , choose the earliest date from which to mine activity events. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Microsoft Power BI assets. Press enter after each name to add more names. Run the miner ​ To run the Microsoft Power BI miner, after completing the steps above: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you can see usage metrics for Microsoft Power BI assets that were created in Microsoft Power BI between the start time and when the miner ran! 🎉 Tags: connectors crawl Previous Crawl Microsoft Power BI Next What does Atlan crawl from Microsoft Power BI? Select the miner Configure the miner Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microsoft-power-bi/how-tos/set-up-microsoft-power-bi",
    "content": "Connect data BI Tools On-premises & Enterprise BI Microsoft Power BI Get Started Set up Microsoft Power BI On this page Set up Microsoft Power BI Who can do this? Depending on the authentication method you choose, you may need a combination of your Cloud Application Administrator or Application Administrator for Microsoft Entra ID, Microsoft 365 administrator for Microsoft 365, and Fabric Administrator ( formerly known as Power BI Administrator ) for Microsoft Power BI to complete these tasks -> you may not have access yourself. This guide outlines how to set up Microsoft Power BI so it can connect with Atlan for metadata extraction and lineage tracking. Before you begin ​ Register application in Microsoft Entra ID ​ Who can do this? You need your Cloud Application Administrator or Application Administrator to complete these steps—> you may not have access yourself. This is required if the creation of registered applications isn't enabled for the entire organization. To register a new application in Microsoft Entra ID: Log in to the Azure portal . Search for Microsoft Entra ID and select it. Click App registrations from the left menu. Click + New registration . Enter a name for your client application and click Register . From the Overview screen, copy and securely store: Application (client) ID Directory (tenant) ID Click Certificates & secrets from the left menu. Under Client secrets , click + New client secret . Enter a description, select an expiry time, and click Add . Copy and securely store the client secret Value . Create security group in Microsoft Entra ID ​ Who can do this? You need your Cloud Application Administrator or Application Administrator to complete these steps - you may not have access yourself. To create a security group for your application: Log in to the Azure portal . Search for Microsoft Entra ID and select it. Click Groups under the Manage section. Click New group . Set the Group type to Security . Enter a Group name and optional description. Click No members selected . Add the appropriate member: For Delegated User authentication : search for the user and select it. For Service Principal authentication : search for the application registration created earlier and select it. Click Select and then Create . By the end of these steps, you have registered an application with Microsoft Entra ID and created a Security Group with the appropriate member. Configure authentication options ​ Atlan supports two authentication methods for fetching metadata from Microsoft Power BI: Service principal authentication (recommended) ​ When using Service Principal authentication, you must decide how the connector shall access metadata to catalog assets and build lineage. There are two supported options: Admin API only ​ This option grants permissions that let the service principal to access only admin-level Power BI APIs. In this mode, Atlan extracts metadata exclusively using administrative endpoints. This option is recommended for stricter access control environments. Who can do this? You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To configure admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group under Specific security groups Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply Admin and non-admin APIs ​ This option grants permissions that let the service principal to access both admin and non-admin Power BI APIs. This enables Atlan to extract richer metadata and build detailed lineage across Power BI assets. Assign security group to Power BI workspaces in PowerBI service portal ​ Who can do this? You need to be at least a member of the Microsoft Power BI workspace to which you want to add the security group to complete these steps - you may not have access yourself. Make sure that you add the security group from the homepage and not the admin portal. To assign a Microsoft Power BI workspace role to the security group: Open the Microsoft Power BI homepage . Open Workspaces and select the workspace you want to access from Atlan. Click Access . In the panel: Enter the name of your security group where it says Enter email addresses Choose one of the following roles: Viewer : For workspaces without parameters Contributor : For workspaces with semantic models containing parameters or to generate lineage for measures Member : To generate lineage for dataflows Click Add . Configure admin and non-admin API access in PowerBI Service Portal ​ Who can do this? You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks - you may not have access yourself. To enable both admin and non-admin API access: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Developer settings : Expand Service principals can use Fabric APIs and set to Enabled Add your security group under Specific security groups Click Apply Under Admin API settings : Expand Enable service principals to use read-only Power BI admin APIs and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply After making these changes, you typically need to wait 15-30 minutes for the settings to take effect across Microsoft's services. Delegated user authentication ​ info Atlan doesn't recommend using delegated user authentication as it's also not recommended by Microsoft. Fabric administrator role assignment ​ Who can do this? You need your Microsoft 365 administrator to complete these steps - you may not have access yourself. To assign the delegated user to the Fabric Administrator role: Open the Microsoft 365 admin portal . Click Users and then Active users from the left menu. Select the delegated user. Under Roles , click Manage roles . Expand Show all by category . Under Collaboration , select Fabric Administrator . Click Save changes . API permissions ​ Who can do this? You need your Cloud Application Administrator or Application Administrator to complete these steps, you may not have access yourself. danger The following permissions are only required for delegated user authentication. If using service principal authentication, you don't need to configure any delegated permissions for a service principal—it's recommended that you avoid adding these permissions. These are never used and can cause errors that may be hard to troubleshoot. To add permissions for the registered application : In your app registration, click API permissions under the Manage section. Click Add a permission . Search for and select Power BI Service . Click Delegated permissions and select: Capacity.Read.All Dataflow.Read.All Dataset.Read.All Report.Read.All Tenant.Read.All Workspace.Read.All Click Grant Admin consent (If you only see the Add permissions button, you aren't an administrator). Admin API settings configuration ​ Who can do this? You need your Fabric Administrator ( formerly known as Power BI Administrator ) to complete these tasks, you may not have access yourself. To enable the Microsoft Power BI admin API: Log in to the Power BI admin portal . Click Tenant settings under Admin portal. Under Admin API settings : Expand Enhance admin APIs responses with detailed metadata and set to Enabled Add your security group Click Apply Expand Enhance admin APIs responses with DAX and mashup expressions and set to Enabled Add your security group Click Apply . Tags: data authentication Previous Microsoft Power BI Next Crawl Microsoft Power BI Before you begin Configure authentication options"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/how-tos/crawl-microstrategy",
    "content": "Connect data BI Tools On-premises & Enterprise BI MicroStrategy Crawl MicroStrategy Assets Crawl MicroStrategy On this page Crawl MicroStrategy Once you have configured the MicroStrategy permissions , you can establish a connection between Atlan and MicroStrategy. To crawl metadata from MicroStrategy, review the order of operations and then complete the following steps. Select the source ​ To select MicroStrategy as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click MicroStrategy Assets . In the right panel, click Setup Workflow . Provide credentials ​ To enter your MicroStrategy credentials: For Host , enter the hostname of your MicroStrategy instance. For Authentication , Basic Authentication is the default selection. For Username , enter the username you created for the instance. For Password , enter the password for the username. Click the Test Authentication button to confirm connectivity to MicroStrategy. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the MicroStrategy connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the MicroStrategy crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Projects . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Projects . (This will default to no assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the MicroStrategy crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up MicroStrategy Next What does Atlan crawl from MicroStrategy? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/how-tos/crawl-mode",
    "content": "Connect data BI Tools Cloud-based BI Mode Crawl Mode Assets Crawl Mode On this page Crawl Mode Once you have configured the Mode user permissions , you can establish a connection between Atlan and Mode. To crawl metadata from Mode, review the order of operations and then complete the following steps. Select the source ​ To select Mode as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Mode Assets and click on Setup Workflow . Provide credentials ​ To enter your Mode credentials: For Authentication , Basic is the default selection. For API Key ID , enter the API key ID you copied for your API token . For API Secret , enter the API secret you copied for your API token . For Workspace , enter the name of your Mode workspace as retrieved from the API or workspace URL. For Exclude all personal collections , change to Yes to exclude your personal collections or keep the default selection No to include them. Click the Test Authentication button to confirm connectivity to Mode using these details. Once successful, at the bottom of the screen, click Next . Configure the connection ​ To complete the Mode connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler ​ danger If you're authenticating in Atlan using a member API token , Atlan currently does not distinguish between collections you have access to and ones restricted at source. This is due to limitations with the Mode API. However, Atlan will only extract basic metadata for restricted collections and no underlying assets such as reports or queries. Before running the Mode crawler, you can further configure it. You can override the defaults for any of these options: To select the collections you want to exclude from crawling, click Exclude Collections . (This will default to no collections, if none are specified.) To select the collections you want to include in crawling, click Include Collections . (This will default to all collections, if none are specified.) To disable crawling archived reports and associated queries and charts from Mode, for Crawl Archived Reports , change to No . Did you know? If a collection appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Mode crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Mode Next What does Atlan crawl from Mode? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/mode/how-tos/set-up-mode",
    "content": "Connect data BI Tools Cloud-based BI Mode Get Started Set up Mode On this page Set up Mode Who can do this? You will probably need your Mode administrator to follow the below steps   -  you may not have access yourself. The Mode administrator will also need to be a connection admin for every connection you want Atlan to be able to crawl. Invite a user ​ To invite a user for Atlan to use when integrating with Mode : From the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then click Invite to Mode... . For Email Address , enter a valid email address, for example for the service account. Click the Invite button. In your service account's email, open the email from Mode and click Accept invite . For Set up your account , enter details about the service account: For Full name , enter a name for the service account, such as Atlan Crawler . For Username , enter a username for the service account, such as atlan_crawler . For Password and Confirm password , enter the same password to use for the service account. At the bottom of the form, click the Continue button. danger If you do not see the prompts to enter details for the user above, you are probably already signed in to Mode. Sign out of Mode first, and then accept the invite in the service account email. Set permissions ​ To set the minimum permissions required to crawl Mode : Log into Mode as an administrator again. (If you just completed the steps above, you'll need to log out from the service account first.) From the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then Workspace settings . Under the People heading on the left, click Members . Next to the Search box, click the dropdown and select Current members . Confirm the user you invited is listed with Member under the Status column. If not, change the Search box dropdown to Pending members and confirm the invitation has been accepted. If yes, change the Search box dropdown to Former members & requests , click the three-dots icon to the far right of the service account's row, and then Reinvite to org . Under the Data heading on the left, click Manage Connections . From the Manage Connections table, for each connection you want to access in Atlan: Click the row for that connection. Change to the Permissions tab. At the top of the Connection access table, click the Add members button. Search for and select the service account user, and change the dropdown for access type to View . Learn more about the View permission in Mode documentation . At the bottom of the form, click the Add members button. Did you know? Atlan does not make any API requests or queries that will update the workspaces, collections, reports, charts, or queries in your Mode instance. Generate API token ​ Atlan supports the following API tokens generated in Mode for authentication in Atlan: Workspace token Member token Personal token Workspace token ​ Workspace tokens allow admin access to the workspace. You will need to be an admin user in Mode to create and manage a workspace token. To generate a workspace API token for crawling Mode : Log in to Mode as an administrator. From the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then click Workspace settings . Under the Features heading on the left, click API Keys . On the API Keys page, under Workspace API Keys , click the Create API key button. In the Create new API key dialog, enter the following details: For Display name , enter a meaningful name   -  for example, atlan-crawler . For Key expiration , keep the default selection or set a longer expiration period. Click the Create button. From the corresponding Key secret dialog, copy the values for Key ID and Secret and store them in a secure location. You will not be able to see them again in Mode after leaving this screen. Member token ​ danger Before you can create a member token, you will need your Mode administrator to enable Member API key creation . Member tokens match an individual user's permissions to access workspace resources in Mode. To generate a member API token for crawling Mode : Log in to Mode as a member. From the upper left corner of your Mode instance, click the dropdown with your workspace name and name, and then click Workspace settings . Under the Workspace heading on the left, click Personal . Under the Personal heading on the left, click My API Keys . In the upper right of the API Keys page, click the Create API key button. In the Create new API key dialog, enter the following details: For Display name , enter a meaningful name   -  for example, atlan-crawler . For Key expiration , keep the default selection or set a longer expiration period. Click the Create button. From the corresponding Key secret dialog, copy the values for Key ID and Secret and store them in a secure location. You will not be able to see them again in Mode after leaving this screen. Personal token ​ danger Mode will deprecate personal token use on February 28, 2025. You can currently continue to use existing personal API tokens , but you will not be able to generate new personal tokens. Tags: connectors crawl Previous Mode Next Crawl Mode Invite a user Set permissions Generate API token"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/crawl-qlik-sense-enterprise-on-windows",
    "content": "Connect data BI Tools On-premises & Enterprise BI Qlik Sense Enterprise on Windows Crawl Qlik Sense Enterprise Assets Crawl Qlik Sense Enterprise on Windows On this page Crawl Qlik Sense Enterprise on Windows Once you have configured the Qlik Sense Enterprise on Windows permissions , you can establish a connection between Atlan and Qlik Sense Enterprise on Windows. To crawl metadata from Qlik Sense Enterprise on Wi ndows, review the order of operations and then complete the following steps. Select the source ​ To select Qlik Sense Enterprise on Windows as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Qlik Sense Enterprise Windows Assets . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Qlik Sense Enterprise on Windows credentials: JWT authentication ​ For Host , enter the hostname for your Qlik Sense Enterprise on Windows instance. For Port , enter the port number for your Qlik Sense Enterprise on Windows instance. For Authentication , keep JWT as the authentication method. For JWT token , enter the JWT you generated . For Virtual proxy prefix , enter the prefix for your virtual proxy . Click the Test Authentication button to confirm connectivity to Qlik Sense Enterprise on Windows. Once authentication is successful, navigate to the bottom of the screen and click Next . Windows authentication ​ For Host , enter the hostname for your Qlik Sense Enterprise on Windows instance. For Port , enter the port number for your Qlik Sense Enterprise on Windows instance. For Authentication , click Windows Auth . For Username , enter the username for Qlik Sense Enterprise on Windows in the domain\\username format. For Password , enter the password for Qlik Sense Enterprise on Windows. For Virtual proxy prefix , enter the prefix for your virtual proxy . Click the Test Authentication button to confirm connectivity to Qlik Sense Enterprise on Windows. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Qlik Sense Enterprise on Windows connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Qlik Sense Enterprise on Windows crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Streams . (This will default to no assets if none are specified.) To select the assets you want to include in crawling, click Include Streams . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Qlik Sense Enterprise on Windows crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Qlik Sense Enterprise on Windows Next What does Atlan crawl from Qlik Sense Enterprise on Windows? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/crawl-qlik-sense-cloud",
    "content": "Connect data BI Tools Cloud-based BI Qlik Sense Cloud Crawl Qlik Sense Cloud Assets Crawl Qlik Sense Cloud On this page Crawl Qlik Sense Cloud Once you have configured the Qlik Sense Cloud permissions , you can establish a connection between Atlan and Qlik Sense Cloud. To crawl metadata from Qlik Sense Cloud, review th e order of operations and then complete the following steps. Select the source ​ To select Qlik Sense Cloud as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Qlik Sense Assets . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Qlik Sense Cloud credentials: For Host , enter the tenant URL for your Qlik Sense Cloud instance. For Port , enter the port number for your Qlik Sense Cloud instance. For Authentication , API key is the default option. For API Key , enter the API key you copied from your Qlik Sense Cloud instance. Click the Test Authentication button to confirm connectivity to Qlik Sense Cloud. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Qlik Sense Cloud connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Qlik Sense Cloud crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Spaces . (This will default to no assets if none are specified.) To select the assets you want to include in crawling, click Include Spaces . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Qlik Sense Cloud crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Qlik Sense Cloud Next What does Atlan crawl from Qlik Sense Cloud? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-enterprise-on-windows/how-tos/set-up-qlik-sense-enterprise-on-windows",
    "content": "Connect data BI Tools On-premises & Enterprise BI Qlik Sense Enterprise on Windows Get Started Set up Qlik Sense Enterprise on Windows On this page Set up Qlik Sense Enterprise on Windows Who can do this? You will need your Qlik Sense Enterprise on Windows administrator to complete these steps   -  you may not have access yourself. Create user in Qlik Sense Enterprise on Windows ​ Did you know? By default, your identity provider for your Qlik Sense Enterprise on Windows version will be Microsoft Windows. So, your Microsoft Windows users will be your Qlik users. To add a new user in this case, you only need to create a new Windows local account . We recommend that you create a new user Qlik Sense Enterprise on Windows for integration with Atlan. To create a new user, follow the steps in the Microsoft Windows documentation and then add the new user: Log in to your Qlik Sense Enterprise on Windows instance. Navigate to the active directory or identity provider of your Qlik Sense Enterprise on Windows version and add the new user. Allocate user access ​ Once you've created a new user, you will need to allocate user access for integration with Atlan. To allocate user access to the new user: Log in to Qlik Management Console (QMC): https://<QPS server name>/qmc . To allocate a license to the new user, in the left menu, click License management . In the right panel for License management , click Professional access allocations . At the bottom of the Professional access allocations screen, click Allocate . From the Users dialog, select the new user you created and click Allocate to complete user allocation. You can also set up roles and groups for robust access management. Set permissions ​ Did you know? Atlan does not make any API requests or queries that will update the objects in your Qlik Sense Enterprise on Windows instance. Once you've added the new user, you will need to provide the new user with Read permission to your streams, apps, sheets, charts, and connections. To set the minimum permissions required to crawl Qlik Sense Enterprise on Windows : Log in to Qlik Management Console (QMC): https://<QPS server name>/qmc . In the left menu under Manage Resources , click Security rules . At the bottom of the Security rules screen, click Create new . In the Edit security dialog, enter the following details: For Name , enter a meaningful name for your security rule. For Basic , under Actions , click Read to provide Read access. At the bottom of the dialog, click Apply to apply your security rule. danger If JWT authentication is already enabled for your Qlik Sense Enterprise on Windows instance, you can proceed to generating a JWT . If Windows authentication is already enabled for your Qlik Sense Enterprise on Windows instance, you can directly proceed to crawling Qlik Sense Enterprise on Windows . (Optional) Create a virtual proxy ​ Once you've set permissions for the new user, you can create a virtual proxy for authentication. Atlan supports the following authentication methods for Qlik Sense Enterprise on Windows: Windows authentication ​ Did you know? When Qlik Sense Enterprise on Windows is installed, it automatically creates a default virtual proxy called Central without a prefix that supports Windows authentication. If it is still available on your instance, you can skip creating a new one and simply edit it. To create a virtual proxy for Windows authentication: Log in to Qlik Management Console (QMC): https://<QPS server name>/qmc . In the left menu under Configure Systems , click Virtual proxies . At the bottom of the Virtual proxies screen, click Create new . In the Edit virtual proxy screen: For Identification , enter the following details: For Description , add a description for your virtual proxy. For Prefix , add a path name in the proxy’s URI   -  use only lowercase letters for the prefix. For Session cookie header name , add the name of the HTTP header used for the session cookie. For Authentication , enter the following details: For Authentication method , select Ticket as the authentication method. For Windows authentication pattern , select Windows . Click Apply to save your authentication details. JWT authentication ​ To create a virtual proxy for JSON Web Token (JWT) authentication: Log in to Qlik Management Console (QMC): https://<QPS server name>/qmc . In the left menu under Configure Systems , click Virtual proxies . At the bottom of the Virtual proxies screen, click Create new . In the Edit virtual proxy screen: For Identification , enter the following details: For Description , add a description for your virtual proxy. For Prefix , add a path name in the proxy’s URI   -  use only lowercase letters for the prefix. For Session cookie header name , add the name of the HTTP header used for the session cookie. For Authentication , enter the following details: For Authentication method , select JWT . For JWT certificate , you can either: To generate a key pair using openssl , open the public.key file in a text editor of your choice, copy the key, and paste it_._ To use the same certificate as your Qlik Sense Enterprise on Windows instance, you can find it in the path C:\\ProgramData\\Qlik\\Sense\\Repository\\Exported Certificates\\.Local Certificates . Open the server.pem file in a text editor of your choice, copy the content, and paste it. For JWT attribute for user ID , add the JWT attribute name for the attribute describing the user ID. For JWT attribute for user directory , add JWT attribute name for the attribute describing the user directory. (Optional) Under Advanced , for Host allow list , add the host IP addresses of your Qlik Sense Enterprise on Windows deployment. Click Apply to save your authentication details. (Optional) Generate a JWT ​ To generate a JSON Web Token (JWT) for crawling Qlik Sense Enterprise on Windows : Open the JWT website. At the top of the screen, click Debugger . For Algorithm , click the dropdown arrow and select RS256 . For Payload , add the user ID and directory for your virtual proxy. For Verify signature , paste the server_key.pem (private key) and server.pem (public key) pair from C:\\ProgramData\\Qlik\\Sense\\Repository\\Exported Certificates\\.Local Certificates in the appropriate fields. In the left Encoded field, copy the generated token and save it in a temporary location. danger To confirm that you've used the right key pair, navigate to the bottom of the screen and ensure that you can see the Signature Verified status. Tags: api rest-api graphql Previous Qlik Sense Enterprise (Windows) Next Crawl Qlik Sense Enterprise on Windows Create user in Qlik Sense Enterprise on Windows Allocate user access Set permissions (Optional) Create a virtual proxy (Optional) Generate a JWT"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/qlik-sense-cloud/how-tos/set-up-qlik-sense-cloud",
    "content": "Connect data BI Tools Cloud-based BI Qlik Sense Cloud Get Started Set up Qlik Sense Cloud On this page Set up Qlik Sense Cloud Who can do this? You will need your Qlik Sense Cloud tenant administrator to complete these steps   -  you may not have access yourself. Invite a new user ​ To invite a new user for Atlan to use when integrating with Qlik Sense Cloud: Log in to your Qlik Sense Cloud instance. From the upper right corner of your Qlik Sense Cloud instance, click the menu icon and then click Management Console . In the left menu under Governance in the Management Console , click the Users tab. From the top right of the Users page, click Invite to invite a new user. In the Invite users dialog, for Email addresses , enter the email address of the new user that you want to invite and click Invite . You can also set up roles and groups for robust access management. Set permissions ​ Did you know? Atlan does not make any API requests or queries that will update the objects in your Qlik Sense Cloud instance. Once you've invited a new user, ensure that the new user has the following minimum permissions for crawling Qlik Sense Cloud : Assign developer role to the new user to allow the creation of API keys. Ensure that the new user has List , Read , and Open permissions to spaces, apps, data, sheets, charts, and connections in Qlik Sense Cloud. In order to ensure that the new user has the above permissions to Qlik Sense Cloud objects, the new user can either be the creator of the objects, have access to the shared or managed spaces where the objects are located, or the objects need to be made public . Enable API key creation ​ Once you have created a new user and ensured the necessary permissions, you will need to enable API key creation for that user. To enable API key creation for the new user: From the upper left corner of your Qlik Sense Cloud instance, click the menu icon and then click Management Console . In the left menu under Management Console , click the Settings tab. From Settings , scroll down to API keys and enter the following details: For Enable API keys , toggle the slider on to enable the creation of API keys. For Change maximum token expiration , enter a value for allowed maximum token age. For Change maximum of active API keys per user , enter a value for the maximum number of active API keys that a user may have in the tenant. Refresh the Management Console page and navigate to the left menu to view a new tab for API keys . Generate API key ​ Once API key creation has been enabled, the new user can generate API credentials for crawling Qlik Sense Cloud. To generate an API key for crawling Qlik Sense Cloud : Log in to your Qlik Sense Cloud instance as the new user. (If you just completed the steps above, you'll need to log out from the tenant admin account first.) From the top right of your Qlik Sense Cloud instance, click your profile avatar and then click Profile settings . From the left menu under Management , click API keys . From the upper right of the API keys page, click Generate new key . In the Generate a new API key dialog, enter an API key description and select an expiration time within the allowed maximum token age. Click Generate to generate an API key. From the resulting dialog, copy the generated API key value and save it in a temporary location. danger The API key is only displayed once. You need to copy and save the value since there is no way to see that specific key again. You will have to generate a new key if you do not copy it. Tags: api rest-api graphql Previous Qlik Sense Cloud Next Crawl Qlik Sense Cloud Invite a new user Set permissions Enable API key creation Generate API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/how-tos/crawl-redash",
    "content": "Connect data BI Tools On-premises & Enterprise BI Redash Crawl Redash Assets Crawl Redash On this page Crawl Redash Once you have configured the Redash permissions , you can establish a connection between Atlan and Redash. To crawl metadata from Redash, review the order of operations and then complete the following steps. Select the source ​ To select Redash as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Redash Assets . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Redash credentials: For Host Name , enter the URL for your Redash instance. For Authentication , API Key is the default selection. For API Key , enter the API key you copied from your Redash instance. Click the Test Authentication button to confirm connectivity to Redash. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Redash connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Redash crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the queries with tags you want to include in crawling, click Include queries with tags . (This will default to all assets, if none are specified.) To select the queries with tags you want to exclude from crawling, click Exclude queries with tags . (This will default to no assets, if none are specified.) To select the dashboards with tags you want to include in crawling, click Include dashboards with tags . (This will default to all assets, if none are specified.) To select the dashboards with tags you want to exclude from crawling, click Exclude dashboards with tags . (This will default to no assets, if none are specified.) For Advanced Config , you can either keep Default to allow default options for asset inclusion or click Custom to further configure the crawler: For Include unpublished queries , click Yes to enable crawling unpublished queries or No to skip crawling them. For Include queries without tags , click Yes to enable crawling queries without tags or No to skip crawling them. For Include dashboards without tags , click Yes to enable crawling dashboards without tags or No to skip crawling them. For Alternate Host URL , enter the protocol and host name to be used for viewing your assets directly in Redash from Atlan. If added, the View in Redash option will redirect to the alternate host URL instead of the host URL used to run the crawler . Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Redash crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Redash Next What does Atlan crawl from Redash? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/how-tos/crawl-sigma",
    "content": "Connect data BI Tools Cloud-based BI Sigma Crawl Sigma Assets Crawl Sigma On this page Crawl Sigma Once you have configured the Sigma permissions , you can establish a connection between Atlan and Sigma. To crawl metadata from Sigma, review the order of operations and then complete the following steps. Select the source ​ To select Sigma as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Sigma Assets . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Sigma credentials: For Endpoint , from the dropdown, select your organization's cloud . For Authentication , API Token is the default method. For Client ID , enter the client ID associated with your API token . For API Token , enter the API token that you created. Click the Test Authentication button to confirm connectivity to Sigma. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Sigma connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Sigma crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options: To select the Sigma workbooks you want to include in crawling, click Include Workbooks . (This will default to all workbooks, if none are specified.) To select the Sigma workbooks you want to exclude from crawling, click Exclude Workbooks . (This will default to no workbooks if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Sigma crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Sigma Next What does Atlan crawl from Sigma? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/redash/how-tos/set-up-redash",
    "content": "Connect data BI Tools On-premises & Enterprise BI Redash Get Started Set up Redash On this page Set up Redash Who can do this? You will probably need your Redash administrator to complete the following steps   -  you may not have access yourself. Atlan supports the API authentication method for fetching metadata from Redash. This method uses an API key to fetch metadata. Create user in Redash ​ To create a new user for Atlan to use when integrating with Redash: Log in to your Redash instance. In the top right of your Redash instance, click your profile name, and from the dropdown, click Users . On the Settings page, under the Users tab, click the New User button. In the Create a New User dialog, enter the following details: For Name , add a meaningful name for the new user   -  for example, Atlan . For Email address , enter the email address for the new user. Click Create to create the new user. Configure new user ​ Once the new user has accepted the invitation, the new user will be added to the list of users in your Redash instance. You will need to configure the new user for integration with Atlan. To configure the new user for crawling Redash : Log in to your Redash instance. In the top right of your Redash instance, click your profile name, and from the dropdown, click Users . On the Settings page, under the Users tab, select the new user you created . From the new user screen, complete the following steps: Each new user is added to the Default group automatically in Redash. To configure group permissions , for Groups , click the dropdown and select Admin to add the new user to the admin group for full access . For API Key , click the clipboard icon to copy the API key for the new user and save it in a secure location. Tags: data api authentication Previous Redash Next Crawl Redash Create user in Redash Configure new user"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sigma/how-tos/set-up-sigma",
    "content": "Connect data BI Tools Cloud-based BI Sigma Get Started Set up Sigma On this page Set up Sigma Who can do this? You will probably need your Sigma administrator to complete these steps   -  you may not have access yourself. Identify your organization's cloud ​ You will need your organization's cloud information to determine the endpoint while authenticating in Atlan. To identify your organization's cloud: Open your Sigma account. In the top right of the screen, click your profile avatar and then click Administration to open your Account page. On the Account page, under Site in General Settings , view the cloud information. Create an API token and client ID ​ To create an API token and client ID: Open your Sigma account. In the top right of the screen, click your profile avatar and then click Administration . From the left menu of the Administration page, click Developer Access . In the top right of the Developer Access page, click the Create New button. In the Create client credentials dialog, for Select privileges , click the Rest API checkbox, and then enter the following details: For Name , enter a meaningful name. For Owner , select the user you would like to associate with the token. Click Create to finish creating the API token. Once prompted, click Copy to copy and paste your API key secret in a secure location. danger The secret cannot be retrieved once this popover is closed. Your newly created API token will be listed in the Developer Access page. Hover over the token’s Client ID and click Copy . Both the API token and the client ID are required for authentication in Atlan. Verify necessary permissions ​ Ensure that the owner associated with the API token has the following permissions: Can View permission for all the Sigma workbooks and datasets you want to crawl. Can Use permission for all the Sigma connections used in the workbooks and datasets. Grant permissions ​ To grant permissions, follow the instructions in these links: Workbooks and datasets Connections Tags: api rest-api graphql Previous Sigma Next Crawl Sigma Identify your organization's cloud Create an API token and client ID Verify necessary permissions Grant permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/how-tos/crawl-sisense",
    "content": "Connect data BI Tools Cloud-based BI Sisense Crawl Sisense Assets Crawl Sisense On this page Crawl Sisense Once you have configured the Sisense permissions , you can establish a connection between Atlan and Sisense. To crawl metadata from Sisense, review the order of operations and then complete the following steps. Select the source ​ To select Sisense as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Sisense Assets . In the right panel, click Setup Workflow . Provide credentials ​ To enter your Sisense credentials: For Host , enter the hostname of your Sisense instance. For Authentication , API Token is the default selection. For Username , enter the email address you created for the new user. For Password , enter the password you created for the username. Click the Test Authentication button to confirm connectivity to Sisense. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Sisense connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Sisense crawler, you can further configure it. Sisense allows you to organize your Sisense assets into folders and subfolders. Atlan currently supports filtering your dashboards and widgets at a folder level. However, if you select a parent folder to include or exclude, all the child folders will also be included or excluded, respectively. Folder-level filtering currently does not apply to data-models and data-model-tables , all such assets will be crawled into Atlan. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Folders . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Folders . (This will default to no assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Sisense crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Sisense Next What does Atlan crawl from Sisense? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/sisense/how-tos/set-up-sisense",
    "content": "Connect data BI Tools Cloud-based BI Sisense Get Started Set up Sisense On this page Set up Sisense Who can do this? You will need your Sisense administrator to complete these steps   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Sisense. This method uses a username and password to fetch metadata. Note that the Sisense connector does not support Sisense for Cloud Data Teams, formerly Periscope Data. Create new user in Sisense ​ Did you know? Atlan does not make any API requests or queries that will update the objects in your Sisense environment. You will need to create a new user in your Sisense instance and assign the Data Admin role to the new user for integrating with Atlan. While Atlan can crawl all other Sisense asset types with the Viewer role, the Data Admin role is required to crawl and generate lineage for data model tables . Atlan uses the Datamodels API to crawl data models and data model tables from Sisense. The Viewer role does not provide access to data models. To create a new user for crawling Sisense : Log in to your Sisense instance with the Admin role. From the tabs along the top, click Admin . On the Admin page, in the System Management box, click Users . From the top right of the Users page, click + Users to add a new user. In the Add Users dialog, enter the following details: For Email , enter an email address for the new user   -  this will be the username for authenticating the connection in Atlan . (Optional) For First Name and Last Name , enter a first and last name for the new user   -  for example, Atlan_integration . For Role , click the role dropdown and then click Data Admin to assign that role to the new user. Toggle on Define Password , and for Set Password , set a password for the new user. Confirm the password in the next step. Click Save to complete new user creation. Tags: connectors data crawl api authentication Previous Sisense Next Crawl Sisense Create new user in Sisense"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-on-premises-tableau",
    "content": "Connect data BI Tools On-premises & Enterprise BI Tableau Crawl Tableau Assets Crawl on-premises Tableau On this page Crawl on-premises Tableau Once you have set up the tableau-extractor tool , you can extract metadata from your on-premises Tableau instances by completing the following steps. Run tableau-extractor ​ Crawl all Tableau connections ​ To crawl all Tableau connections using the tableau-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection ​ To crawl a specific Tableau connection using the tableau-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files ​ The tableau-extractor tool will generate many folders with JSON files for each service . For example: calculated_fields dashboards datasources workbooks and many others You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 ​ To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/tableau-example/dashboards/result-0.json , output/tableau-example/workbooks/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/tableau-example s3://my-bucket/metadata/tableau-example --recursive Crawl metadata in Atlan ​ Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl Tableau Be sure you select Offline for the Extraction method . Tags: connectors data crawl Previous Crawl Tableau Next What does Atlan crawl from Tableau? Run tableau-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/crawl-tableau",
    "content": "Connect data BI Tools On-premises & Enterprise BI Tableau Crawl Tableau Assets Crawl Tableau On this page Crawl Tableau Once you have configured Tableau , you can establish a connection between Atlan and Tableau. (If you are also using a private network for Tableau, you will need to set that up first , too.) To crawl metadata from Tableau, review the order of operations and then complete the following steps. Select the source ​ To select Tableau as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Tableau Assets and click on Setup Workflow . Provide credentials ​ In Direct extraction, Atlan connects to Tableau and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan’s secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your Tableau credentials: For Host Name , enter the host name of your Tableau Online or Tableau Server instance (or the private DNS name if your Tableau Server instance uses an SSL certificate ). For Port , enter the port number of your Tableau instance. For Authentication , choose how you would like to connect to Tableau: For Basic authentication, enter the Username and Password you use to log in to Tableau. For Personal Access Token authentication, enter the Personal Access Token Name and Personal Access Token Value you generated _Create_a_personal_access_token). For JWT Bearer authentication, enter your Tableau Server username or Tableau Online email address for Username , and the Client ID , Secret ID , and Secret Value you copied from the connected app in Tableau. (Optional) For SSL , keep the default Enabled to use HTTPS or click Disabled to use HTTP. For Site , enter the name of the site you want to crawl. (If left blank, the default site will be used.) danger If you are using Tableau Online, the site is required for Atlan to authenticate properly. (Optional) For SSL certificate , this is only required if your Tableau Server instance uses a self-signed or an internal CA SSL certificate , paste a supported SSL certificate in the recommended format . At the bottom of the form, click the Test Authentication button to confirm connectivity to Tableau using these details. When successful, at the bottom of the screen click the Next button. Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from Tableau. This method uses Atlan's tableau-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include dashboards/result-0.json , workbooks/result-0.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from Tableau. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Tableau data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the Tableau connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. Configure the crawler ​ Before running the Tableau crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the Tableau projects you want to include in crawling, click Include Projects . (This will default to all assets, if none are specified.) To select the Tableau projects you want to exclude from crawling, click Exclude Projects . (This will default to no assets, if none are specified.) To have the crawler ignore Tableau projects based on a naming convention, specify a regular expression in the Exclude Projects Regex field. To check for any permissions or other configuration issues before running the crawler, click Preflight checks . Did you know? If a project appears in both the include and exclude filters, the exclude filter takes precedence. (The Exclude Projects Regex also takes precedence.) Configure advanced controls ​ Before running the Tableau crawler, you can also configure advanced controls for the crawler. On the Advanced page, you can override the defaults for any of these options: For Alternate Host URL , enter the protocol and host name to be used for viewing assets directly in Tableau. For Crawl Unpublished Worksheets and Dashboards , click Yes to enable crawling hidden worksheets and dashboards or No to skip crawling them. For Hidden Datasource Fields , click Yes to enable crawling hidden datasource fields or No to skip crawling them. Crawl embedded dashboards: Embedded dashboard here means linking or displaying a dashboard inside another dashboard by providing a link to the dashboard in a Web Page item of the embedding dashboard. Click Yes to enable relationships between different embedded dashboards. Click No to skip creating relationships between embedded dashboards. Run the crawler ​ To run the Tableau crawler, after completing the steps above: You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up a private network link to Tableau server Next Crawl on-premises Tableau Select the source Provide credentials Configure the connection Configure the crawler Configure advanced controls Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-a-private-network-link-to-tableau-server",
    "content": "Connect data BI Tools On-premises & Enterprise BI Tableau Get Started Set up a private network link to Tableau server On this page Set up a private network link to Tableau server AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Tableau server and Atlan, when you use our Single Tenant SaaS deployment. Who can do this? You will need your AWS administrator involved   -  you may not have access to run these tasks yourself. Prerequisites ​ You should already have the following: Tableau instance running in AWS (private EC2 instance). Atlan hosted in the same region as the Tableau instance. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Setup network to EC2 instance ​ To setup the private network of your Tableau EC2 instance, from within AWS : Copy network settings ​ To copy the network settings of your EC2 instance: Navigate to Services , then Compute , then EC2 . On the left, under Instances , click Instances . In the Instances table, click on your Tableau EC2 instance. Under the instance's Details tab: Under VPC ID copy the VPC identifier. Under Subnet ID click the subnet for the instance. In the Subnets table, copy the value under the IPv4 CIDR column. Create inbound rule ​ To create an inbound rule allowing your private subnet access to your EC2 instance: Navigate to Services , then Compute , then EC2 . On the left, under Instances , click Instances . In the Instances table, click on your Tableau EC2 instance. Under the instance's details, change to the Security tab. Under Security groups click the security group for the instance. Under the Inbound rules tab, click the Edit inbound rules button. At the bottom left of the Inbound rules table, click the Add rule button. For Type , select Custom TCP . For Port range , enter the port on which Tableau is accessible (for example, default port 80 and TLS port 443 ). For Source , choose Custom and enter the CIDR range for your Tableau instance (see Copy network settings ). Below the bottom right of the Inbound rules table, click the Save rules button. Create internal Network Load Balancer ​ Start creating NLB ​ To create an NLB, from within AWS: Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name enter a unique name. For Scheme select Internal . For IP address type select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC select the VPC where the Tableau instance is located (see Copy network settings ). For Mappings select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port enter 80 (or the non-default port value used in Created inbound rule ). For Default action click the Create target group link. This will open the target group creation in a new browser tab. Create target group ​ To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type select Instances . For Target group name enter a name. For Port enter 80 (or the non-default port value used in Create inbound rule ). For VPC select the VPC where the Tableau instance is located (see Copy network settings ). At the bottom of the form, click the Next button. From the Available instances table: Click the checkbox next to your Tableau instance. Enter the port for the instance (80 or non-default value used in steps above). Click the Include as pending below button. At the bottom right of the form, click the Create target group button. Finish creating NLB ​ Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom right of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy ​ danger As a prerequisite for TLS configuration on Tableau Server only, ensure that the health check Protocol of the target group is set to HTTPS or modify the health check settings as required. To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing , click Target Groups . From the Target groups table, click the row for the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service ​ To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name enter a meaningful name. For Load balancer type choose Network . For Available load balancers select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint enable Acceptance required . For Supported IP address types enable IPv4 . At the bottom right of the form, click the Create button. Under the Details of the endpoint service, copy the hostname under Service name . Allow Atlan account access ​ To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN enter the Atlan account ID. At the bottom right of the form, click the Allow principals button. Notify Atlan support ​ Once all the above steps are complete, provide Atlan support with the following information: The hostname for the endpoint service created above. The port number for the Tableau instance. For SSL certificates only, the private DNS name for which you have issued an SSL certificate on your Tableau Server instance. There are additional steps Atlan then needs to complete: Creating a security group. Creating an endpoint. For SSL certificates only, creating a DNS CNAME record pointing the private DNS name shared above to the VPC endpoint URL. This will allow Atlan to use your private DNS name with the SSL certificate. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request ​ To accept the consumer connection request, from within AWS: Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending . Select this row, and click the Actions button and then Accept endpoint connection request . If prompted to confirm, type accept into the field and click the Accept button. Wait for this to complete, it could take about 30 seconds. 😅 The connection is now established. You can now use the service endpoint provided by Atlan support (or the private DNS name for SSL certificates ) as the hostname to crawl Tableau in Atlan! 🎉 Tags: atlan documentation Previous Set up on-premises Tableau access Next Crawl Tableau Prerequisites Setup network to EC2 instance Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-thoughtspot",
    "content": "Connect data BI Tools Cloud-based BI ThoughtSpot Crawl ThoughtSpot Assets Crawl ThoughtSpot On this page Crawl ThoughtSpot Once you have configured the ThoughtSpot permissions , you can establish a connection between Atlan and ThoughtSpot. To crawl metadata from ThoughtSpot, review the order of operations and then complete the following steps. Select the source ​ To select ThoughtSpot as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click ThoughtSpot Assets . In the right panel, click Setup Workflow . Provide your credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan’s secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your ThoughtSpot credentials: For Extraction method , Direct is the default selection. For Hostname , enter the hostname of your ThoughtSpot cloud instance in the following format   - <company_name>.thoughtspot.cloud . For Authentication , select the method you configured when setting up the ThoughtSpot access permissions : For Basic Authentication , enter the username and password you created . For Trusted Authentication , enter the username of your ThoughtSpot instance and the secret key you created . Click the Test Authentication button to confirm connectivity to ThoughtSpot. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method ​ Atlan supports the offline extraction method for fetching metadata from ThoughtSpot. This method uses Atlan's thoughtspot-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include output/thoughtspot-example/filter/answers/result-0.json , output/thoughtspot-example/filter/liveboards/result-0.json , output/thoughtspot-example/filter/answer-sql-queries/result-0.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from ThoughtSpot. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the ThoughtSpot data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the ThoughtSpot connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the ThoughtSpot crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include tags . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude tags . (This will default to no assets, if none specified.) For Assets without tags , keep the default option Yes to skip crawling assets without tags or click No to enable crawling them. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the ThoughtSpot crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks -  currently only supported for offline extraction method . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up on-premises ThoughtSpot access Next Crawl on-premises ThoughtSpot Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-on-premises-tableau-access",
    "content": "Connect data BI Tools On-premises & Enterprise BI Tableau Get Started Set up on-premises Tableau access On this page Set up on-premises Tableau access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Tableau instance details, including credentials. In some cases you may not be able to expose your Tableau instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites ​ To extract metadata from your on-premises Tableau instance, you will need to use Atlan's tableau-extractor tool. Did you know? Atlan uses exactly the same tableau-extractor behind the scenes when it connects to Tableau in the cloud. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the tableau-extractor tool ​ To get the tableau-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl Tableau: sudo docker load -i /path/to/tableau-extractor-master.tar Get the compose file ​ Atlan provides you with a Docker compose file for the tableau-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Tableau instance. The file is docker-compose.yaml . Define Tableau connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Tableau connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services ​ For each on-premises Tableau instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *extract environment: <<: *tableau-defaults EXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\" CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\" CERT_PATH: \"\" volumes: - ./output/connection-name:/output/process Replace connection-name with the name of your connection. <<: *extract tells the tableau-extractor tool to run. environment contains all parameters for the tool. CERT_PATH -  if applicable, specify the SSL certificate path and store it as a new volume. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Tableau connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials ​ To define the credentials for your Tableau connections, you will need to provide a Tableau configuration file. The Tableau configuration is a .ini file with the following format: [TableauConfig] # Tableau instance URL. Do not include /api/* in the URL. server_url=https://:<hostname>:<port> # Tableau site name. Leaving this empty will select the default site. site_name=YourTableauSite # Tableau authentication type. Options: basic, personal_access_token. auth_type=basic # Required only if auth_type is basic. [BasicAuth] username=YourTableauUsername password=YourTableauPassword # Required only if auth_type is personal_access_token. [PersonalAccessTokenAuth] token_name=YourTableauTokenName token_value=YourTableauTokenValue danger For basic authentication, ensure that your password does not contain the special character % . If the percent sign is included in your password, add another % to escape it. Secure credentials ​ Using local files ​ danger If you decide to keep Tableau credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: tableau_config: file: ./tableau.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets ​ To create and use Docker secrets: Store the Tableau configuration file: sudo docker secret create tableau_config path/to/tableau.ini At the top of your compose file, add a secrets element to access your secret: secrets: tableau_config: external: true name: tableau_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Tableau configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example ​ Let's explain in detail with an example: secrets: tableau_config: external: true name: tableau_config x-templates: # ... services: my-tableau: <<: *extract environment: <<: *tableau-defaults EXCLUDE_PROJECTS_REGEX: \"Test1.*|Test2.*\" CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS: \"true\" CERT_PATH: \"/tmp/tab-cert.pem\" volumes: - ./output/my-tableau:/output/process - ./tab-cert.pem:/tmp/tab-cert.pem secrets: - tableau_config In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The tableau_config refers to an external Docker secret created using the docker secret create command. The name of this service is my-tableau . You can use any meaningful name you want. The <<: *tableau-defaults sets the connection type to Tableau. EXCLUDE_PROJECTS_REGEX tells the extractor to filter out all the projects whose names match the Test1.* and Test2.* regex patterns in the extracted metadata. CRAWL_UNPUBLISHED_WORKSHEETS_DASHBOARDS tells the extractor to include all hidden or unpublished worksheets and dashboards that are part of a Tableau workbook in the extracted metadata. CRAWL_EMBEDDED_DASHBOARDS tells the extractor to create relationships between Tableau dashboards used within another dashboard as a Web Page item. The CERT_PATH tells the extractor where to store the SSL certificate , if applicable. In this example, the extractor will store results in the ./tab-cert.pem directory on the local file system. If the SSL certificate is not stored in the same folder as the compose file, you will need to specify the full path. The ./output/my-tableau:/output/process line tells the extractor where to store results. In this example, the extractor will store results in the ./output/my-tableau directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Set up Tableau Next Set up a private network link to Tableau server Prerequisites Get the compose file Define Tableau connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/microstrategy/how-tos/set-up-microstrategy",
    "content": "Connect data BI Tools On-premises & Enterprise BI MicroStrategy Get Started Set up MicroStrategy On this page Set up MicroStrategy Who can do this? You will probably need your MicroStrategy administrator to complete these steps   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from MicroStrategy. This method uses a username and password to fetch metadata. Create user in MicroStrategy ​ You will need to create a new user in your MicroStrategy Workstation and assign minimum permissions for integrating with Atlan. To create a new user for crawling MicroStrategy : Open the Workstation window with the navigation pane in smart mode. From the left navigation menu, click Users and Groups . In the upper left of the Users and Groups page, click the Select an Environment dropdown and select your environment. In the left menu of your selected environment, next to All Users , click the + button to create a new user. In the Create New User dialog: For Account and Credentials , enter the following details: For Full Name , enter a meaningful name for the new user. For Email Address , enter an email address for the new user. (Optional) For Description , enter a description. For Username (Login) , enter a username for the new user. For Password , create a password for the new user and confirm it in the next step. To disallow the new user from changing the password, check the User cannot change password box. At the bottom left of the form, check the Active User box. For User Groups , all users are automatically members of the Everyone group, which typically has read permission for most objects. To assign any permissions not inherited from the default group to the new user: In the top right of User Groups , click Manage User Group to add a new user group. Click Update to confirm your selections. To assign user privileges , in the left menu, click Privileges and check the following boxes: Use Architect Editors -  for fetching attribute, fact, and table definitions Use Library Web -  for fetching project metadata Web Report SQL -  for fetching SQL statements Web use Metric Editor -  for fetching metric definitions Web run Document -  for fetching document definitions Web run Dossier -  for fetching dossier definitions Click Save to complete setup. Tags: connectors data crawl authentication Previous MicroStrategy Next Crawl MicroStrategy Create user in MicroStrategy"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/crawl-on-premises-thoughtspot",
    "content": "Connect data BI Tools Cloud-based BI ThoughtSpot Crawl ThoughtSpot Assets Crawl on-premises ThoughtSpot On this page Crawl on-premises ThoughtSpot Once you have set up the thoughtspot-extractor tool , you can extract metadata from your on-premises ThoughtSpot instances by completing the following steps. Run thoughtspot-extractor ​ Crawl all ThoughtSpot connections ​ To crawl all ThoughtSpot connections using the thoughtspot-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection ​ To crawl a specific ThoughtSpot connection using the thoughtspot-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files ​ The thoughtspot-extractor tool will generate many folders with JSON files for each service . For example: tags answers liveboards answer-sql-queries liveboard-sql-queries You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 ​ To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/thoughtspot-example/filter/answers/result-0.json , output/thoughtspot-example/filter/liveboards/result-0.json , output/thoughtspot-example/filter/answer-sql-queries/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/thoughtspot-example/filter s3://my-bucket/metadata/thoughtspot-example --recursive Crawl metadata in Atlan ​ Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl ThoughtSpot Be sure to select Offline for the Extraction method . Tags: connectors data crawl Previous Crawl ThoughtSpot Next What does Atlan crawl from ThoughtSpot? Run thoughtspot-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/tableau/how-tos/set-up-tableau",
    "content": "Connect data BI Tools On-premises & Enterprise BI Tableau Get Started Set up Tableau On this page Set up Tableau Who can do this? You will probably need your Tableau administrator to run these commands   -  you may not have access yourself. Enable the Tableau Metadata API ​ To enable the Tableau Metadata API, follow the steps in Tableau documentation . danger Atlan needs the Tableau Metadata API to crawl metadata. Please ensure you are running the latest version of Tableau Server or Tableau Online (2022.x with REST API version 3.14+). Learn more about the permissions used to access metadata through the Tableau Metadata API. Publish the worksheets you want to crawl ​ Ensure you publish the worksheets in Tableau that you want to crawl in Atlan. To publish Tableau worksheets, follow the steps in Tableau documentation . Choose authentication mechanism ​ Atlan supports the following authentication methods for fetching metadata from Tableau: Basic -  this method uses a username and password. Personal access token -  this method uses a personal access token. JWT bearer -  this method uses a username and JWT client ID, secret ID, and secret value. Basic authentication ​ Did you know? To crawl assets and extract asset lineage from Tableau, the user must have the Site Administrator Explorer role . Atlan requires the Site Administrator Explorer role in Tableau to extract data source fields and calculated fields and create field-level assets and lineage. It is not possible to fetch either with the Viewer role in the current version of the Tableau Metadata API. Add a user ​ Ensure you add a user with the role Site Administrator Explorer to the site you want to crawl. To add such a user, follow the steps in Tableau documentation . Grant user permissions ​ Ensure you grant the View capability for all the assets you want to crawl. To grant the permission, follow the steps in Tableau documentation . Personal access token authentication ​ If you want to access Tableau using an access token, you can generate a personal access token. To generate a personal access token, follow the steps in Tableau documentation . JWT bearer authentication ​ danger To access the Tableau Metadata API using JWT bearer authentication , you must have Tableau Cloud October 2023 or Tableau Server 2023.3 version. In addition, JWT authorization currently does not support all REST API capabilities . Due to these limitations at source, Atlan will not be able to crawl Tableau flows if you use the JWT bearer authentication method. Configure a connected app ​ If you want to access Tableau using a JSON web token (JWT), you can configure a Tableau connected app. There are two types of connected apps that you can configure   -  direct trust or OAuth 2.0 trust. To authenticate the Tableau connection in Atlan using this method, you will need the following: Username   -  your Tableau Server username or Tableau Online email address, the user must have a Site Administrator Explorer role Connected app ID   -  client ID generated for the connected app Secret ID   -  secret ID linked to the client ID of the connected app Secret value   -  secret value used to sign the token To configure a connected app, follow the steps in Tableau documentation: Direct trust OAuth 2.0 trust Access scopes for connected apps ​ For JWT authorization, scopes define access permissions granted to the token holder. Scopes control the specific actions that an application or user can perform in Tableau while accessing content through a connected app. The Tableau connector in Atlan uses two read scopes to extract metadata from Tableau. Note that the Tableau connector is preconfigured to use these scopes, no action required. Atlan uses the following scopes for JWT authentication: tableau:content:read -  allows read access to your assets in Tableau, including: Workbooks   -  can list, access, and retrieve metadata for workbooks. Views   -  can fetch specific views or dashboards within workbooks. Data sources   -  can access published data sources and associated metadata. Projects   -  can retrieve project metadata. Metrics   -  can read metrics associated with workbooks or dashboards. Tables and databases   -  can access metadata for tables and databases connected to Tableau. tableau:users:read -  allows read access to user details. This enables Atlan to display the source owner property for supported Tableau assets, including in the impact analysis report . Optional) tableau:workbooks:download – allows downloading a workbook ( .twb or .twbx ), enabling Atlan to display relationships for embedded Tableau dashboards. Tags: data crawl api Previous Tableau Next Set up on-premises Tableau access Enable the Tableau Metadata API Publish the worksheets you want to crawl Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-on-premises-thoughtspot-access",
    "content": "Connect data BI Tools Cloud-based BI ThoughtSpot Get Started Set up on-premises ThoughtSpot access On this page Set up on-premises ThoughtSpot access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your ThoughtSpot instance details, including credentials. In some cases you will not be able to expose your ThoughtSpot instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites ​ To extract metadata from your on-premises ThoughtSpot instance, you will need to use Atlan's thoughtspot-extractor tool. Did you know? Atlan uses exactly the same thoughtspot-extractor behind the scenes when it connects to ThoughtSpot in the cloud. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the thoughtspot-extractor tool ​ To get the thoughtspot-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl ThoughtSpot: sudo docker load -i /path/to/thoughtspot-extractor-master.tar Get the compose file ​ Atlan provides you with a Docker compose file for the thoughtspot-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises ThoughtSpot instance. The file is docker-compose.yaml . Define ThoughtSpot connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your ThoughtSpot connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services ​ For each on-premises ThoughtSpot instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *extract environment: <<: *thoughtspot-defaults EXCLUDE_TAGS_REGEX: \"Test1.*|Test2.*\" WITHOUT_TAGS: \"true\" volumes: - ./output/connection-name/filter:/output/filter Replace connection-name with the name of your connection. <<: *extract tells the thoughtspot-extractor tool to run. environment contains all parameters for the tool. EXCLUDE_TAGS_REGEX -  specify a regular expression to exclude ThoughtSpot assets based on ThoughtSpot tags. WITHOUT_TAGS -  specify a Boolean configuration to determine whether to crawl ThoughtSpot assets without any ThoughtSpot tags. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name/filter folder on the local file system. You can add as many ThoughtSpot connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials ​ To define the credentials for your ThoughtSpot connections, you will need to provide a ThoughtSpot configuration file. The ThoughtSpot configuration is a .ini file with the following format: [ThoughtSpotConfig] host=atlan.thoughtspot.cloud port=443 auth_type=basic_auth; This will use BasicAuth; auth_type=trusted_auth; This will use TruestedAuth; auth_type=oauth_access_token; This will use OAuth; [BasicAuth] username={{username}} password={{password}} [TrustedAuth] username={{username}} secret_key={{secret_key}} [OAuth] token={{oauth_access_token}} [ExtractionConfig] offset=1 limit=10 Secure credentials ​ Using local files ​ danger If you decide to keep ThoughtSpot credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: thoughtspot_config: file: ./thoughtspot.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets ​ To create and use Docker secrets: Store the ThoughtSpot configuration file: sudo docker secret create thoughtspot_config path/to/thoughtspot.ini At the top of your compose file, add a secrets element to access your secret: secrets: thoughtspot_config: external: true name: thoughtspot_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local ThoughtSpot configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example ​ Let's explain in detail with an example: secrets: thoughtspot_config: external: true name: thoughtspot_config x-templates: # ... services: thoughtspot-example: <<: *extract environment: <<: *thoughtspot-defaults EXCLUDE_TAGS_REGEX: \"Test1.*|Test2.*\" WITHOUT_TAGS: \"true\" volumes: - ./output/connection-name/filter:/output/filter In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The thoughtspot_config refers to an external Docker secret created using the docker secret create command. The name of this service is thoughtspot-example . You can use any meaningful name you want. The <<: *thoughtspot-defaults sets the connection type to ThoughtSpot. The ./output/thoughtspot_example/filter:/output/filter line tells the extractor where to store results. In this example, the extractor will store results in the ./output/thoughtspot_example/filter directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Set up ThoughtSpot Next Crawl ThoughtSpot Prerequisites Get the compose file Define ThoughtSpot connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/business-intelligence/thoughtspot/how-tos/set-up-thoughtspot",
    "content": "Connect data BI Tools Cloud-based BI ThoughtSpot Get Started Set up ThoughtSpot On this page Set up ThoughtSpot Who can do this? You will probably need your ThoughtSpot instance administrator to complete these steps   -  you may not have access yourself. Atlan supports the following authentication methods for ThoughtSpot: Basic authentication Trusted authentication Basic authentication ​ You will need to create a new user in ThoughtSpot and authenticate in Atlan with username and password. Create user in ThoughtSpot ​ To create a new user for crawling ThoughtSpot : Log in to your ThoughtSpot instance. To navigate to the admin console, in the top header, click Admin . In the top left of the Admin page, click Add User . In the Add a new user dialog, enter the following details: For Username , enter a username for the new user. For Display name , add a meaningful name for the new user   -  for example, Atlan . For Sharing visibility , keep the default selection   - SHAREABLE . danger Atlan will only crawl assets that are either created by or shared with this user. If you add the user to a group in ThoughtSpot, ensure that you share all the assets you want to crawl in Atlan with that group. For New password , enter a password for the new user and confirm it in the next step. For Email , enter an email address for the new user. Click Add to add the new user. The new user will be assigned Can upload user data and Can download data permissions by default. Trusted authentication ​ danger You will need ThoughtSpot Everywhere to use the trusted authentication option. ThoughtSpot Analytics users, however, can get ThoughtSpot Everywhere as an add-on to use trusted authentication. Learn more here . You will need to create a secret key in ThoughtSpot and authenticate in Atlan with username and secret key. Create a secret key ​ To create a secret key for crawling ThoughtSpot : Log in to your ThoughtSpot instance. To navigate to the developer console, in the top header, click Develop . In the left menu under Customizations , click Security settings . In the top right of the Security settings page, click Edit . Scroll down to Trusted authentication and turn it on. Turning on trusted authentication will generate a secret key. Click the clipboard icon to copy the secret key and store it in a secure location. Click Save Changes to save your selections. Tags: connectors crawl authentication Previous ThoughtSpot Next Set up on-premises ThoughtSpot access Basic authentication Trusted authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/crawl-salesforce",
    "content": "Connect data CRM Salesforce Crawl Salesforce Assets Crawl Salesforce On this page Crawl Salesforce Once you have configured the Salesforce user permissions , you can establish a connection between Atlan and Salesforce. To crawl metadata from Salesforce, review the order of operations and then complete the following steps. Select source ​ To select Salesforce as your source: In the top right of any screen in Atlan, navigate to New and click New Workflow . From the list of packages, click Salesforce Assets . In the right panel, click Setup Workflow . Provide credentials ​ Enter credentials for your Salesforce integration in Atlan. The required fields vary depending on the authentication flow you choose. JWT bearer flow Client credentials flow Username-password flow To enter your Salesforce credentials: For Host Name , enter the full URL for your Salesforce instance, including the https:// . For example, https://MyDomainName.my.salesforce.com . For Authentication , select OAuth 2.0 JWT Bearer . For Username , enter the integration user's email address . For Is this a Sandbox Org account? , change to Yes if your org is a copy of your production org . For Consumer Key , enter the consumer key for the connected app . For Encrypted Private Key , enter the private key from the server.key file in RSA256 format: -----BEGIN PRIVATE KEY----- MIIEvgIBADANBgkqhkiG....... -----END PRIVATE KEY----- Click Test Authentication at the bottom of the form to confirm connectivity. When successful, click Next at the bottom of the screen. To enter your Salesforce credentials: For Host Name , enter the full URL for your Salesforce instance, including the https:// . For example, https://MyDomainName.my.salesforce.com . For Authentication , select OAuth 2.0 Client Credentials . For Consumer Key (Client ID) , enter the Consumer Key from the external client app . For Consumer Secret (Client Secret) , enter the Consumer Secret from the external client app . Click Test Authentication at the bottom of the form to confirm connectivity. When successful, click Next at the bottom of the screen. To enter your Salesforce credentials: For Host Name , enter the full URL for your Salesforce instance, including the https:// . For example, https://MyDomainName.my.salesforce.com . For Authentication , keep the default option Resource Owner . For Username , enter the integration user's email address. For Password , enter the concatenation of the user's password and the personal security token . Entering either password or personal security token alone is insufficient. For example, password xyz + token 123 → enter xyz123 . For Is this a Sandbox Org account? , change to Yes if your org is a copy of your production org . For Consumer Key , enter the consumer key for the connected app . For Consumer Secret , enter the consumer secret for the connected app . Click Test Authentication at the bottom of the form to confirm connectivity. When successful, click Next at the bottom of the screen. Configure connection ​ To complete the Salesforce connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . If you don't specify any user or group, nobody can manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure crawler ​ Before running the Salesforce crawler, you can further configure it. On the Metadata page: For Extract reports , click Yes if you'd like to extract report metadata or click No . For Extract dashboards , click Yes if you'd like to extract dashboard metadata or click No . Run crawler ​ To run the Salesforce crawler, after completing the previous steps: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you can see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl salesforce setup Previous Set up username-password flow Next What does Atlan crawl from Salesforce? Select source Provide credentials Configure connection Configure crawler Run crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/oauth-client-credentials-setup",
    "content": "Connect data CRM Salesforce Get Started Set up Salesforce Set up client credentials flow On this page Set up client credentials flow Important Atlan currently supports Salesforce Sales Cloud and Financial Services Cloud (FSC). Atlan supports the Salesforce OAuth 2.0 client credentials flow for server-to-server integration. This flow enables Atlan to authenticate using a dedicated integration user and an external client app in Salesforce, providing secure, non-interactive access to Salesforce metadata and data for crawling. Prerequisites ​ Before you begin, make sure you have: Salesforce administrator access Network connectivity between Atlan and your Salesforce instance Create custom profile ​ A custom profile defines the specific permissions and access levels for your integration user. You'll create this profile with the minimum necessary permissions for Atlan to crawl your Salesforce data securely. Create a custom profile to manage permissions for the integration user: From Setup , enter profiles in the Quick Find box and select Profiles . Click New Profile . Select Standard User from the Existing Profile dropdown to clone. Enter a name, for example AtlanIntegrationProfile . Click Save . On the new profile page, click Edit . Under Connected App Access , check the External Client App you create. Under Administrative Permissions , uncheck all except: API Enabled View All Data Run Reports Under Standard Object Permissions and Custom Object Permissions , select Read and View All for all items. Click Save . Create integration user ​ The integration user acts as the identity that Atlan uses to connect to Salesforce. This dedicated user ensures secure, auditable access separate from individual user accounts. Create a dedicated Salesforce user for the external client app: From Setup , expand Administration → Users and click Users . Click New User . Enter required details: First Name , Last Name , Username , Email , Nickname . Select Salesforce for User License . Assign the custom profile created in the previous step. Click Save . Create external client app ​ The external client app provides the OAuth infrastructure for secure server-to-server authentication. This app generates the credentials that Atlan uses to authenticate without requiring user interaction. Set up the external client app for client credentials flow: From Setup , enter external client app manager in Quick Find and select External Client App Manager . Click New External Client App . Enter: External Client App Name : for example, AtlanIntegration Contact Email : your email Distribution State : Local Expand API (Enable OAuth Settings) : Check Enable OAuth Set Callback URL : https://localhost (placeholder, unused) Move the following scopes to Selected OAuth Scopes : Manage user data via APIs (api) Perform requests at any time (refresh_token, offline_access) Access Lightning applications (lightning) Under Flow Enablement , check Enable Client Credentials Flow . Enable: Require Secret for Web Server Flow Require Secret for Refresh Token Flow Optional hardening: Require Proof Key for Code Exchange (PKCE) Enable Refresh Token Rotation Issue JSON Web Token (JWT)-based access tokens Click Create . On the app details page, copy the Consumer Key (Client ID) and Consumer Secret from OAuth Settings . Store these credentials securely—they're required to configure the Atlan connection. Configure policies ​ After creating the external client app, you need to configure its security policies. These policies control which users and profiles can access the app and define the authentication flow settings. From External Client App Manager , locate your app and click Edit . Open the Policies tab. Set Start Page to None . Move the integration custom profile to Selected Profiles . If using permission sets, move relevant sets to Selected Permission Sets . In OAuth Policies , set: Permitted Users : Admin approved users are pre-authorized OAuth Start URL : leave blank unless required In OAuth Flows and External Client App Enhancements : Check Enable Client Credentials Flow Run As (Username) : enter the integration user username Set additional policies as required (IP Relaxation, Session Timeout, Refresh Token Policy) Click Save . Troubleshooting ​ If you encounter issues with Client Credentials authentication, see Troubleshooting Salesforce Connectivity . Next steps ​ Crawl Salesforce to configure the connection in Atlan. Tags: connectors salesforce authentication Previous Set up JWT bearer flow Next Set up username-password flow Prerequisites Create custom profile Create integration user Create external client app Troubleshooting Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/oauth-jwt-bearer-setup",
    "content": "Connect data CRM Salesforce Get Started Set up Salesforce Set up JWT bearer flow On this page Set up JWT bearer flow Important Atlan currently supports Salesforce Sales Cloud and Financial Services Cloud (FSC). Atlan recommends using OAuth 2.0 JWT bearer flow for secure server-to-server integration with Salesforce. This guide walks you through creating the connected app, uploading certificates, configuring policies, and preparing the integration user. Prerequisites ​ Before you begin, make sure you have: Salesforce administrator access Network connectivity between Atlan and your Salesforce instance Created a server key and certificate . Save the generated server.crt and server.key files securely. You need the server.crt file to upload to Salesforce and the server.key file to configure the connection in Atlan. Create custom profile ​ Create a custom profile with the Modify All Data permission to crawl all Salesforce objects, including custom objects. The View All Data permission isn’t sufficient because it grants read-only access and can result in missing objects. From Setup , enter Profiles in Quick Find and select Profiles . Click New Profile and clone Standard User Enter Profile Name , for example, AtlanIntegrationProfile Click Save , then click Edit Under Connected App Access , check your connected app Under Administrative Permissions , select: API Enabled View All Data Run Reports Under Standard Object Permissions and Custom Object Permissions , select Read and View All Click Save Create integration user ​ Follow these steps to create a dedicated user account for Atlan integration and assign the custom profile. From Setup , expand Users under Administration Click Users Click New User Enter required fields: First Name , Last Name , Username , Email , Nickname Set User License : Salesforce Set Profile : custom profile created in the Create custom profile section Click Save Integration user requires Salesforce license to crawl metadata in Atlan. If license is unavailable, check allowed license limit: Salesforce user licenses Create connected app ​ A connected app enables Atlan to authenticate with Salesforce using OAuth 2.0. This section guides you through creating the app and configuring OAuth settings. Log in to Salesforce. Click settings icon , then click Setup . In Setup , enter App Manager in Quick Find and select App Manager . Click New Connected App . Under Basic Information , enter: Connected App Name : AtlanConnector API Name : automatically populated Contact Email : your email Under API (Enable OAuth Settings) : Check Enable OAuth Settings Enter Callback URL : your domain. For example, https://localhost Add Selected OAuth Scopes : Access Lightning applications (lightning) Manage user data via APIs (api) Perform requests at any time (refresh_token, offline_access) Check Use digital signatures Click Choose File and upload server.crt Click Save , then Continue On connected app page, click Manage Consumer Details and copy Consumer Key ( client_id ) and Consumer Secret Before proceeding, wait approximately 10 minutes for connected app activation Edit policies ​ Configure OAuth policies to control who can access the connected app and from where. These settings provide secure access for Atlan's integration. From Setup , enter Manage Connected Apps in Quick Find and select Manage Connected Apps . Locate your connected app and click Edit Policies . Under OAuth Policies : Set Permitted Users to Admin approved users are pre-authorized Set IP Relaxation to Relax IP restrictions If needed, set Refresh Token Policy to Refresh token is valid until revoked Click Save Add server certificate ​ To add the server certificate ( server.crt ) file to the connected app: From Setup , enter app manager in the Quick Find box and select App Manager . Locate your connected app, and then click the dropdown arrow and select Edit . For API Enable OAuth Settings , check Use digital signatures . Click Choose File and upload the server.crt file. Click Save . Assign profile ​ Assign the custom profile to the connected app so the integration user has the required permissions when accessing Salesforce. Open connected app page Scroll to Manage Profile Select the custom profile created in the Create custom profile section and click Save Troubleshooting ​ If you encounter issues with JWT Bearer authentication, see Troubleshooting Salesforce Connectivity . Next steps ​ Crawl Salesforce : Configure and run your first crawl to discover Salesforce data and metadata Tags: connectors salesforce authentication Previous Set up Salesforce Next Set up client credentials flow Prerequisites Create custom profile Create integration user Create connected app Troubleshooting Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/oauth-username-password-setup",
    "content": "Connect data CRM Salesforce Get Started Set up Salesforce Set up username-password flow On this page Set up username-password flow Important Atlan currently supports Salesforce Sales Cloud and Financial Services Cloud (FSC). Atlan supports the Salesforce OAuth 2.0 username-password flow for scenarios where server-to-server integration isn't possible. This flow enables Atlan to authenticate using the integration user's credentials along with a connected app in Salesforce. Prerequisites ​ Before you begin, make sure you have: Salesforce administrator access Network connectivity between Atlan and your Salesforce instance Create connected app ​ To enable Atlan to connect to your Salesforce instance, you need to create a connected app. This app provides the OAuth credentials that Atlan uses for authentication. Log in to Salesforce. In the upper right, click the settings icon and select Setup . In Setup , enter apps in the Quick Find box and select App Manager . Click New Connected App in the upper-right corner. Under Basic Information , provide: Connected App Name : for example, AtlanConnector Contact Email : your email address Under API (Enable OAuth Settings) : Check Enable OAuth Settings Check Enable for Device Flow Set Callback URL to any domain, for example, https://localhost (unused) Add the following Selected OAuth Scopes : Access Lightning applications (lightning) Manage user data via APIs (api) Perform requests at any time (refresh_token, offline_access) Check Require Secret for Web Server Flow Check Require Secret for Refresh Token Flow Click Save . On the resulting page, click Continue . Under API (Enable OAuth Settings) , click Manage Consumer Details and copy: Consumer Key (Client ID) Consumer Secret (Client Secret) Wait approximately 10 minutes for the connected app to activate. Retrieve security token ​ Salesforce requires a security token in addition to your password for API access. You'll need to retrieve this token from your user settings and append it to your password when configuring the connection in Atlan. In Salesforce, click the integration user's user icon in the upper-right and select Settings . Expand My Personal Information and click Reset My Security Token . Click Reset Security Token . Copy the resulting security token. Important To crawl Salesforce , enter your password + security token in the Password field (for example, password xyz and token 123 → xyz123 ). Troubleshooting ​ If you encounter issues with Username-password authentication, see Troubleshooting Salesforce Connectivity . Next steps ​ Crawl Salesforce : Configure and run your first crawl to discover Salesforce data and metadata Tags: connectors salesforce authentication Previous Set up client credentials flow Next Crawl Salesforce Prerequisites Create connected app Retrieve security token Troubleshooting Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/crm/salesforce/how-tos/set-up-salesforce",
    "content": "Connect data CRM Salesforce Get Started Set up Salesforce On this page Set up Salesforce Important Atlan currently supports Salesforce Sales Cloud and Financial Services Cloud (FSC). To integrate Salesforce with Atlan, you need to set up a connection between your Salesforce account and Atlan. The steps apply to both Salesforce Cloud and Financial Services Cloud (FSC). Prerequisites ​ Make sure the following prerequisites are met, you have: Salesforce administrator access Network connectivity between Atlan and your Salesforce instance Choose authentication flow ​ Atlan supports the following authentication flows for Salesforce, select the authentication method that best fits your use case: Recommended 🔐 JWT Bearer Flow Secure server-to-server integration No password storage required Production-ready security Automated token refresh View Setup Guide 🔑 Client Credentials Flow Alternative server-to-server method Client ID/Secret based No user interaction Existing credential workflows View Setup Guide 👤 Username-Password Flow Traditional authentication method Simple setup process Widely supported No special hardware needed View Setup Guide Next steps ​ After completing your chosen authentication setup, proceed to crawl Salesforce to configure the connection in Atlan. Tags: connectors integration salesforce authentication Previous Salesforce Next Set up JWT bearer flow Prerequisites Choose authentication flow Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/mine-amazon-redshift",
    "content": "Connect data Data Warehouses Amazon Redshift Crawl Redshift Assets Mine Amazon Redshift On this page Mine Amazon Redshift Once you have crawled assets from Amazon Redshift , you can mine its query history to construct lineage and retrieve usage and popularity metrics . To mine lineage from Amazon Redshift, review the order of operations and then complete the following steps. Select the miner ​ To select the Amazon Redshift miner: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Miner . From the list of packages, select Redshift Miner and click on Setup Workflow . Configure the miner ​ To configure the Amazon Redshift miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Miner extraction method , select Query History . For Start time , choose the earliest date from which to mine query history. danger Amazon Redshift only stores query history for 2-5 days . If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. (Optional) For Advanced Config , keep Default for the default configuration or click Advanced to further configure the miner: For Cross Connection , click Yes to extract lineage across all available data source connections or click No to only extract lineage from the selected Amazon Redshift connection. For Control Config , if Atlan support has provided you with a custom control configuration, select Custom and enter the configuration into the Custom Config box. You can also: Enter {“ignore-all-case”: true} to enable crawling assets with case-sensitive identifiers. (Optional) For Enable Popularity , click Yes to retrieve usage and popularity metrics for your Amazon Redshift assets from query history: For Excluded Users , type the names of users to be excluded while calculating usage metrics for Amazon Redshift assets. Press enter after each name to add more names. Run the miner ​ To run the Amazon Redshift miner, after completing the steps above: To check for any permissions or other configuration issues before running the miner, click Preflight checks . You can either: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the miner has completed running, you will see lineage for Amazon Redshift assets that were created in Amazon Redshift between the start time and when the miner ran! 🎉 Tags: connectors data crawl Previous Crawl Amazon Redshift Next What does Atlan crawl from Amazon Redshift? Select the miner Configure the miner Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/crawl-amazon-redshift",
    "content": "Connect data Data Warehouses Amazon Redshift Crawl Redshift Assets Crawl Amazon Redshift On this page Crawl Amazon Redshift Once you have configured the Amazon Redshift access permissions , you can establish a connection between Atlan and Amazon Redshift. To crawl metadata from Amazon Redshift, review the order of operations and then complete the following steps. Select the source ​ To select Amazon Redshift as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select Redshift Assets , and click Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . Direct extraction method ​ To enter your Amazon Redshift credentials: For Host Name , enter the host name of your Amazon Redshift instance. From your Redshift cluster you can find the host name in the Configuration section as a variable called Endpoint . For Port , enter the port number for your Amazon Redshift instance. You can find this next to the host name in the Configuration section of your Redshift cluster. For Deployment Type , click Provisioned if your Amazon Redshift instance is deployed on provisioned clusters or click Serverless if deployed on a serverless workgroup . For Authentication , choose the method you configured when setting up the Amazon Redshift access permissions : For Basic authentication, enter the Username and Password you configured. For IAM User authentication, enter the AWS Access Key , AWS Secret Key , and Username you configured for the database. (Optional) This is only required if you are accessing a private cluster on provisioned deployment, using a Network Load Balancer (NLB), and connecting via IAM, for Cluster ID , enter the name of the Amazon Redshift cluster that you want to connect to. (Optional) This is only required if you are accessing a private cluster on serverless deployment, for Workgroup , enter the name of your workgroup . For IAM Role authentication, enter the Username you configured for the database only if your deployment type is Provisioned . For Serverless deployment type, you do not need to enter a username. Set the AWS Role ARN to the ARN of the role you created in your AWS account . (Optional) For Region , enter the AWS region of your Amazon Redshift instance. Offline extraction method ​ Atlan supports the offline extraction method for fetching metadata from Amazon Redshift. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. If you are reusing Atlan's S3 bucket, you can leave this blank. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Configure the connection ​ To complete the Amazon Redshift connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Amazon Redshift data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Amazon Redshift data, change Allow Data Preview to No . At the bottom of the screen, click the Next button to proceed. Configure the crawler ​ Before running the Amazon Redshift crawler, you can further configure it. You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. (Optional) For Advanced Config , keep Default for the default configuration or click Advanced to configure the crawler: For Cross Connection , click Yes to extract lineage across all available Amazon Redshift connections or click No to limit lineage extraction to the current connection. For Control Config , if Atlan support has provided you with a custom control configuration, select Custom and enter the configuration into the Custom Config box. You can also: Enter {\"ignore-all-case\": true} to enable crawling assets with case-sensitive identifiers. If you've configured a cloned schema to provide access to Atlan, add the following key-value pair to the Custom Config field: {\"clonedPgCatalogSchema\": \"cloned_schema_name\"} Replace cloned_schema_name with the name of your cloned schema. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Amazon Redshift crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up a private network link to Amazon Redshift Next Mine Amazon Redshift Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/enable-sso-for-amazon-redshift",
    "content": "Connect data Data Warehouses Amazon Redshift Get Started How to enable SSO for Amazon Redshift On this page Enable  SSO for Amazon Redshift Atlan supports SSO authentication for Amazon Redshift connections with Okta as the identity provider. Once you've configured SSO authentication for Amazon Redshift, your users can: Query data with Okta SSO credentials View sample data with Okta SSO credentials Did you know? If you have already configured Okta and AWS, skip to configure SSO authentication in Atlan . Otherwise, complete all the steps below. Create a client application in Okta ​ Who can do this? You will need your Okta administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your AWS administrator. You will need to create a client application in Okta to use for configuring the identity provider in AWS . To create a client application, within Okta: Log in to your Okta Admin Console . From the left menu of the Admin Console , click Applications . Under Applications , click the Browse App Catalog button. On the Browse App Integration Catalog page, search for and select Amazon Web Services Redshift . From the Amazon Web Services Redshift page, click the Add integration button to create an integration. For Add Amazon Web Services Redshift , enter the following details: For Application label , enter a meaningful name for your new app integration   -  for example, Atlan_SSO . Click Done to proceed. On your new app page, click the Assignments tab and then click the Assign button: Click Assign to People to select individual users to assign to the application. Click Assign to Groups to select groups to assign to the application. On your new app page, click the Sign On tab and then navigate to the SAML Signing Certificates section: Under Actions , click Actions to expand the menu, and then from the dropdown, click View IdP metadata . This will open an XML file in a new tab. Save or download this file to use for configuring the identity provider in AWS . For User Authentication , click the Edit button: From the Authentication policy dropdown, click Okta Dashboard . Click Save to save your changes. You will need the IdP metadata XML file to configure Okta as the identity provider in AWS. Configure identity provider in AWS ​ Who can do this? You will need your AWS administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Okta administrator. You will need to establish a trust relationship between Okta as the identity provider and AWS. You will also need to create a role that Okta can use to access Amazon Redshift and assign required permissions to that role. Create an identity provider ​ To create an identity provider , within AWS: Sign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console. From the left menu of your AWS Identity and Access Management (IAM) console, click Identity providers and then click the Add provider button. In the Add an Identity provider dialog, enter the following details: For Provider type , select SAML . For Provider name , enter a name for the identity provider   -  for example, Okta_AtlanSSO . Under Metadata document , click Choose file and upload the IdP metadata XML file you downloaded from Okta . At the bottom of the dialog, click Add provider to add Okta as the identity provider in AWS. Once you have configured Okta as the identity provider in AWS, you will need to create a role for Okta to access Amazon Redshift. Create a role ​ To create a role , within AWS: Sign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console. From the left menu of your AWS Identity and Access Management (IAM) console, click Roles , and then from the top right, click the Create role button. On the Create role page, enter the following details: For Select trusted entity , under Trusted entity type , click SAML 2.0 federation . Under SAML 2.0 federation , enter the following details: For SAML 2.0-based provider , select the identity provider you created in AWS -  for example, Okta_AtlanSSO . Click Allow programmatic access only . For the Attribute dropdown, select SAML:aud . For Value , enter https://signin.aws.amazon.com/saml . Click Next to continue. For Add permissions , click Next to proceed to the next step. For Name, review, and create , under Role details , enter the following details: For Role name , enter a name for the role   -  for example, Okta_AtlanSSO_role . (Optional) For Description , enter a description for the new role. Click Create role to finish role setup. This will create a new role for Okta to access Amazon Redshift. Once you have created a role for Okta to access Amazon Redshift, you will need to assign permissions to that role. Create a policy ​ You will need to create an access policy and assign the following required permissions to the newly created role: CreateClusterUser JoinGroup GetClusterCredentials To create a policy, within AWS: Sign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console. From the left menu of your AWS Identity and Access Management (IAM) console, click Roles and then search for and select the role you created in the previous step   -  for example, Okta_AtlanSSO_role . On the newly created role page, to the right of Permission policies , click Add permissions , and then from the dropdown, click Create inline policy . On the Create policy page, you will need to assign the following permissions for Redshift   - GetClusterCredentials , JoinGroup , and CreateClusterUser . Repeat the steps below to assign each permission: For Specify permissions , under Select a service , search for and select Redshift . Under Redshift , enter the following details: For Allowed actions , search for and select a permission   -  for example, GetClusterCredentials . For Resources , click All . Click Next to proceed. For Review and create , under Policy name , enter a name for the newly created policy   -  for example, Okta_AtlanSSO_rolepolicy . Retrieve identity provider and role ARN ​ Once you have configured Okta as the identity provider and created a role in AWS, you will need the identity provider ARN and role ARN for further configuration in Okta. To retrieve the identity provider and role ARN, within AWS: Sign in to the AWS Management Console and open the AWS Identity and Access Management (IAM) console. From the left menu of your AWS Identity and Access Management (IAM) console: Click Identity providers and then select the identity provider you created : On the identity provider page, under ARN , click the clipboard icon to copy the identity provider ARN value and store it in a secure location. Click Roles and then select the role you created : On the role page, under ARN , click the clipboard icon to copy the role ARN value and store it in a secure location. Configure the client application in Okta ​ Who can do this? You will need your Okta administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your AWS administrator. You will need the identity provider ARN and role ARN from AWS for further configuration in Okta. To further configure the client application in Okta: Log in to your Okta Admin Console . From the left menu of the Admin Console , click Applications . Under Applications , select the client application you created in Okta. On your new app page, click the Sign On tab. On the Sign On page, next to Settings , click Edit . Navigate to the Advanced Sign-on Settings section and enter the following details: For IdP ARN and Role ARN , enter the identity provider ARN and role ARN as comma-separated values   -  for example, arn:aws:iam::403973984390:role/oktaAtlan_SSO , arn:aws:iam::403976283490:saml-provider/oktaAtlan_SSO_role . For Allowed DB Groups (Redshift) , enter the names of the Okta groups that should be provided access to Amazon Redshift. Click Save to confirm. On your new app page, click the General tab and navigate to the App Embed Link section. Under Embed Link , copy the link   -  for example, https://**<example>.okta.com**/home/amazon_aws_redshift/**0oa78lx856GcTMDsa697/aln1dkqcfra0piaWa0g** -  and store the IdP host name and app ID in a secure location to use for configuring SSO authentication in Atlan . For example: IdP host name: <example>.okta.com App ID: 0oa78lx856GcTMDsa697/aln1dkqcfra0piaWa0g Configure SSO authentication in Atlan ​ Who can do this? You will need to be a connection admin in Atlan to complete these steps. You will also need inputs and approval from your Okta and AWS administrators. Once you have configured Okta and AWS, you can enable SSO authentication for your Amazon Redshift users to query data and view sample data in Atlan. To configure Okta SSO on a Amazon Redshift connection, from Atlan: From the left menu of any screen, click Assets . From the Assets page, click the Connector filter, and from the dropdown, select Redshift . From the pills below the search bar at the top of the screen, click Connection . From the list of results, select an Amazon Redshift connection to enable SSO authentication. From the sidebar on the right, next to Connection settings , click Edit . In the Connection settings dialog: Under Allow query , for Authentication type , click Okta authentication to enforce SSO credentials for querying data : For SSO authentication , enter the following details: For IDP host , enter the IdP host name you copied from Okta . For App ID , enter the app ID you copied from Okta . For AWS Role ARN , enter the role ARN retrieved from AWS . Under Display sample data , for Source preview , click Okta authentication to enforce SSO credentials for viewing sample data : If SSO authentication is enabled for querying data, the same connection details will be reused for viewing sample data. If a different authentication method is enabled for querying data, enter the IdP host name and app ID you copied from Okta and role ARN retrieved from AWS . (Optional) Toggle on Enable data policies created at source to apply for querying in Atlan to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. At the bottom right of the Connection settings dialog, click Update . Your users will now be able to run queries and view sample data using their Okta SSO credentials! 🎉 Tags: connectors data integration authentication Previous Set up Amazon Redshift Next Set up a private network link to Amazon Redshift Create a client application in Okta Configure identity provider in AWS Configure the client application in Okta Configure SSO authentication in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-amazon-redshift",
    "content": "Connect data Data Warehouses Amazon Redshift Get Started Set up Amazon Redshift On this page Set up Amazon Redshift Who can do this? You will need your Amazon Redshift administrator to run these commands   -  you may not have access yourself. Atlan supports fetching metadata from Amazon Redshift for the following types of deployment: Provisioned RA3 DC2 Serverless danger If you're using the DC2 node type, Redshift restricts cross-database joins and metadata access to a single database. For more information, see Considerations - Amazon Redshift . Because of this restriction, you must set up a separate workflow for each database you want to crawl. Grant permissions ​ For all supported authentication mechanisms except IAM role authentication on serverless deployment , you must first grant the following permissions on Amazon Redshift. For IAM role authentication on serverless deployment only, skip to this step . Create a group and user ​ To create a group and user, run the following commands: CREATE GROUP atlan_users ; CREATE USER atlan_user password '<pass>' IN GROUP atlan_users ; Replace <pass> with the password for the atlan_user being created. To crawl Amazon Redshift , for Username , you must enter the username you configured for the database user. For example, atlan_user . Grant required permissions to group ​ To grant the minimum required permissions, run the following commands: GRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users; GRANT SELECT ON pg_catalog.svv_table_info TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat the above commands for all the databases in your schema( <schema_name> ). The permissions are used for the following: SVV_TABLE_INFO is used to obtain information on the table ID to table/schema/database relation. External schema ​ If your Redshift instance setup external schemas, you must grant permissions for each schema. Grant USAGE permissions ​ For external schemas, use the following command to grant USAGE permission: GRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat this command for all external schemas. Did you know? If your external tables are sourced from Amazon S3 and AWS Glue Catalog, granting only the USAGE permission is sufficient, provided that the IAM role associated with the Redshift cluster has read access to the data. (Optional) Grant SELECT permissions ​ For Redshift-based external schemas, you must explicitly grant SELECT along with USAGE permissions to allow metadata crawling. Use the following command to grant this permission: GRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat the command for all the Redshift-based external schemas. Verify external schema permissions ​ Follow these steps to verify permissions granted to your external schema: Log in to the system using the IAM role created earlier. Run the following command using any database viewer tool: SELECT * FROM SVV_EXTERNAL_TABLES WHERE schema_name = '<external_schema_name>' Replace <external_schema_name> with the name of your external schema. If the tables appear in the results, the permissions are correctly configured. If you can't provide SELECT or USAGE access, create a cloned schema and grant access to the atlan_users group. For more information, see Cloned schema for restricted access section. Cloned schema for restricted access ​ If you can't grant USAGE or SELECT permissions to the atlan_users group, you must create a cloned schema containing the necessary metadata tables. Then, grant permissions to the cloned schema. Follow these steps to create a cloned schema and provide required permissions: Log in as dbadmin . Create a new schema and give it a meaningful name. For example, atlan . Clone the following views as tables from the pg_catalog schema into the cloned schema: pg_views SVV_TABLES SVV_EXTERNAL_TABLES SVV_COLUMNS Clone the following views as tables from the information_schema into the cloned schema: key_column_usage as information_schema_key_column_usage table_constraints as information_schema_table_constraints Grant USAGE and SELECT access to the atlan_users group on the cloned schema: GRANT USAGE ON SCHEMA <cloned_schema_name> TO GROUP atlan_users; GRANT SELECT ON ALL TABLES IN SCHEMA <cloned_schema_name> TO GROUP atlan_users; Replace <cloned_schema_name> with the name of your cloned schema. Since Atlan relies on these tables to crawl metadata, schedule a cron job to refresh the cloned tables periodically. Did you know? You can reach out to Atlan support if you need assistance with setting up a Cloned Schema. (Optional) Grant permissions for role-based authentication on serverless ​ For IAM role-based authentication on Amazon Redshift serverless deployment only, you must first grant the following permissions on Amazon Redshift. Create a role ​ To create a role, run the following commands: CREATE ROLE atlan_role ; Grant required permissions to role ​ To grant the minimum required permissions, run the following commands: GRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users; GRANT SELECT ON pg_catalog.svv_table_info TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat the above commands for all the databases in your schema( <schema_name> ). The permissions are used for the following: SVV_TABLE_INFO is used to obtain information on the table ID to table/schema/database relation. External schema ​ If your Redshift setup uses external schemas, you must grant permissions for each schema. You can do this in one of the following ways: Grant USAGE permissions ​ For external schemas, use the following command to grant USAGE permission: GRANT USAGE ON SCHEMA <schema_name> TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat this command for all external schemas. Did you know? If your external tables are sourced from Amazon S3 and AWS Glue Catalog, granting only the USAGE permission is sufficient, provided that the IAM role associated with the Redshift cluster has read access to the data. (Optional) Grant SELECT permissions ​ For Redshift-based external schemas, you must explicitly grant SELECT along with USAGE permissions to allow metadata crawling. Use the following command to grant this permission: GRANT SELECT ON ALL TABLES IN SCHEMA <schema_name> TO GROUP atlan_users; Replace <schema_name> with the name of your schema. Repeat the command for all the Redshift-based external schemas. Verify external schema permissions ​ Follow these steps to verify permissions granted to your external schema: Log in to the system using the IAM role created earlier. Run the following command using the Amazon Redshift Data API : SELECT * FROM SVV_EXTERNAL_TABLES WHERE schema_name = '<external_schema_name>' Replace <external_schema_name> with the name of your external schema. If the tables appear in the results, the permissions are correctly configured. If you can't provide SELECT or USAGE access, create a cloned schema and grant access to the atlan_users group. For more information, see Cloned schema for restricted access section. Cloned schema for restricted access ​ If you can't grant USAGE or SELECT permissions to the atlan_users group, you must create a cloned schema containing the necessary metadata tables. Then, grant permissions to the cloned schema. Follow these steps to create a cloned schema and provide required permissions: Log in as dbadmin . Create a new schema and give it a meaningful name. For example, atlan . Clone the following views as tables from the pg_catalog schema into the cloned schema: pg_views SVV_TABLES SVV_EXTERNAL_TABLES SVV_COLUMNS Clone the following views as tables from the information_schema into the cloned schema: key_column_usage as information_schema_key_column_usage table_constraints as information_schema_table_constraints Grant USAGE and SELECT access to the atlan_users group on the cloned schema: GRANT USAGE ON SCHEMA <cloned_schema_name> TO GROUP atlan_users; GRANT SELECT ON ALL TABLES IN SCHEMA <cloned_schema_name> TO GROUP atlan_users; Replace <cloned_schema_name> with the name of your cloned schema. Since Atlan relies on these tables to crawl metadata, schedule a cron job to refresh the cloned tables periodically. Did you know? You can reach out to Atlan support if you need assistance with setting up a Cloned Schema. Grant additional permissions for mining query history ​ Did you know? For mining query history from Redshift Serverless, permissions on STL and SVL views are not required as they do not exist in serverless deployment. To grant the additional permissions needed to mine query history, run the following commands: GRANT SELECT on pg_catalog . stl_ddltext to GROUP atlan_users ; GRANT SELECT on pg_catalog . stl_query to GROUP atlan_users ; GRANT SELECT on pg_catalog . stl_connection_log to GROUP atlan_users ; GRANT SELECT on pg_catalog . stl_undone to GROUP atlan_users ; GRANT SELECT on pg_catalog . stl_insert to GROUP atlan_users ; GRANT SELECT on pg_catalog . svl_statementtext to GROUP atlan_users ; ALTER USER atlan_user SYSLOG ACCESS UNRESTRICTED ; The additional permissions are used for the following: STL_DDLTEXT is used for DDL queries. STL_QUERY is used for DML and regular queries. STL_CONNECTION_LOG is used to obtain the session ID that a query is part of. STL_UNDONE is used to obtain information about transactions that have been undone or rolled back. STL_INSERT is used to obtain the table ID used in the insert queries. SVL_STATEMENTTEXT is used to obtain the query text for all queries. SYSLOG ACCESS UNRESTRICTED is used to access all queries performed by any user in the system tables above. To use basic authentication, your setup is now complete. To configure IAM-based authentication, you need to continue with the following steps. (Optional) Create IAM policy ​ All IAM-based authentication mechanisms require an IAM policy to be created. For all supported authentication mechanisms except IAM role authentication on serverless deployment , create the following IAM policy. For IAM role authentication on serverless deployment only, skip to this step . To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"redshift:GetClusterCredentials\" ] , \"Resource\" : [ \"arn:aws:redshift:<region>:<account_id>:dbuser:<redshift_cluster_identifier>/atlan_user\" , \"arn:aws:redshift:<region>:<account_id>:dbname:<redshift_cluster_identifier>/<database>\" ] } ] } Replace <region> with the AWS region of your Redshift instance. Replace <account_id> with your account ID. Replace <redshift_cluster_identifier> with your Redshift cluster identifier. Replace <database> with the name of the Redshift database. (Optional) Create IAM policy for role-based authentication on serverless ​ For IAM role-based authentication on Amazon Redshift serverless deployment only, create an IAM policy with the necessary permissions. Follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"redshift-serverless:GetCredentials\" ] , \"Resource\" : [ \"arn:aws:redshift-serverless:<region>:<account_id>:workgroup/<workgroup_identifier>\" , ] } ] } Replace <region> with the AWS region of your Amazon Redshift instance. Replace <account_id> with your AWS account ID. Replace <workgroup_identifier> with your Amazon Redshift serverless workgroup identifier. Configure tag for IAM role: { RedshiftDbRoles : <role> } Replace <role> with the role you created . (Optional) Choose IAM-based authentication mechanism ​ Using the policy created above, configure one of the following options for authentication. User-based authentication ​ To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role delegation-based authentication ​ To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } (Optional) To use an external ID for additional security, paste the external ID into the policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : \"<atlan_external_id>\" } } } ] } Replace <atlan_external_id> with the external ID you want to use. Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: data crawl authentication Previous Amazon Redshift Next How to enable SSO for Amazon Redshift Grant permissions (Optional) Grant permissions for role-based authentication on serverless Grant additional permissions for mining query history (Optional) Create IAM policy (Optional) Create IAM policy for role-based authentication on serverless (Optional) Choose IAM-based authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/crawl-on-premises-databricks",
    "content": "Connect data Data Warehouses Databricks On-premises Setup Crawl on-premises Databricks On this page Crawl on-premises Databricks Once you have set up the databricks-extractor tool , you can extract metadata from your on-premises Databricks instances by completing the following steps. Run databricks-extractor ​ Crawl all Databricks connections ​ To crawl all Databricks connections using the databricks-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection ​ To crawl a specific Databricks connection using the databricks-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files ​ The databricks-extractor tool will generate many folders with JSON files for each service . For example: catalogs schemas tables You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 ​ To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/databricks-example/catalogs/success/result-0.json , output/databricks-example/schemas/{{catalog_name}}/success/result-0.json , output/databricks-example/tables/{{catalog_name}}/success/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/databricks-example s3://my-bucket/metadata/databricks-example --recursive Crawl metadata in Atlan ​ Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl Databricks Be sure to select Offline for the Extraction method . Tags: connectors data crawl Previous Set up on-premises Databricks access Next Set up on-premises Databricks lineage extraction Run databricks-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/amazon-redshift/how-tos/set-up-a-private-network-link-to-amazon-redshift",
    "content": "Connect data Data Warehouses Amazon Redshift Get Started Set up a private network link to Amazon Redshift On this page Set up a private network link to Amazon Redshift Who can do this? You will need your Amazon Redshift administrator or AWS administrator involved   -  you may not have access to run these tasks. Redshift-managed VPC endpoints create a secure, private connection between services running in AWS. This document describes the steps to set this up between Amazon Redshift and Atlan, when you use our Single Tenant SaaS deployment. Prerequisites ​ Your Redshift cluster must be an RA3 node type. Your Redshift cluster must have cluster relocation turned on . Your Redshift cluster must be available through port 5439. You must have spare capacity in your VPC endpoint quota . (For all details, see Working with Redshift-managed VPC endpoints in Amazon Redshift .) Request Atlan's details ​ Before granting access to your Redshift cluster to Atlan, you will need the following: Atlan's AWS account ID Atlan's VPC ID for the connection Request these from Atlan support . Grant access to Atlan ​ Once you've received the details above, to grant Atlan access to your Redshift cluster : Sign in to the AWS Management Console and open the Amazon Redshift console . From the navigation menu, click Clusters . From the table, click the name of the cluster to which you want to grant access. Change to the Properties tab of the cluster. Under the Granted accounts section, click Grant access . In the Grantee information form: For AWS account ID , enter the Atlan AWS account ID. For VPC , choose Grant access to specific VPCs and enter the Atlan VPC ID. At the bottom right, click the Grant access button. Notify Atlan support team ​ Once you've completed the steps above, contact the Atlan support team again and provide the following details for your Redshift cluster: AWS account ID Redshift cluster identifier   -  the unique identifier of your cluster Atlan will create a Redshift-managed VPC endpoint , and then reply to you with a hostname. When you use this hostname in the configuration for crawling , Atlan will connect to Redshift over the private network. Tags: atlan documentation Previous How to enable SSO for Amazon Redshift Next Crawl Amazon Redshift Prerequisites Request Atlan's details Grant access to Atlan Notify Atlan support team"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/manage-databricks-tags",
    "content": "Connect data Data Warehouses Databricks Tag management Manage Databricks tags On this page Manage Databricks tags You must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. Atlan enables you to import your Databricks tags , update your Databricks assets with the imported tags, and push the tag updates back to Databricks: Import tags -  crawl Databricks tags from Databricks to Atlan Reverse sync -  sync Databricks tag updates from Atlan to Databricks Once you've imported your Databricks tags to Atlan: Your Databricks assets in Atlan will be automatically enriched with their Databricks tags. Imported Databricks tags will be mapped to corresponding Atlan tags through case-insensitive name match   -  multiple Databricks tags can be matched to a single tag in Atlan. You can also attach Databricks tags , including tag values, to your Databricks assets in Atlan   -  allowing you to categorize your assets at a more granular level. You can filter your assets by Databricks tags and tag values. You can enable reverse sync to push any tag updates for your Databricks assets back to Databricks   -  including tag values added to assets in Atlan. Did you know? Enabling reverse sync will only update existing tags in Databricks. It will neither create nor delete any tags in Databricks. Prerequisites ​ You must have a Unity Catalog-enabled workspace and SQL warehouse configured to import Databricks tags in Atlan. Before you can import tags from and push tag updates to Databricks using personal access token , AWS service principal , or Azure service principal authentication, you will need to do the following: Ensure that you have a Unity Catalog-enabled workspace and a SQL warehouse configured. Create tags or have existing tags in Databricks. Grant permissions to import tags from and push tag updates to Databricks. Import Databricks tags to Atlan ​ Who can do this? You will need to be an admin user in Atlan to import Databricks tags to Atlan. You will also need to work with your Databricks administrator to grant permissions to import tags from Databricks   -  you may not have access yourself. You can import your Databricks tags to Atlan through one-way tag sync. The synced Databricks tags will be matched to corresponding tags in Atlan through case-insensitive name match and your Databricks assets will be enriched with their synced tags from Databricks. To import Databricks tags to Atlan, you can either: Create a new Databricks workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Databricks workflow to change Import Tags to Yes . If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags. Once the crawler has completed running, tags imported from Databricks will be available to use for tagging assets ! 🎉 View Databricks tags in Atlan ​ Once you've imported your Databricks tags, you will be able to view and manage your Databricks tags in Atlan. To view Databricks tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Databricks to filter for tags imported from Databricks. From the left menu under Tags , select a synced tag. In the Overview section, you can view a total count of synced Databricks tags. To the right of Overview , click Synced tags to view additional details   -  including tag name, description, tag values, total count of linked assets, connection, database, and schema names, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your Databricks tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . You cannot rename tags synced from Databricks. Push tag updates to Databricks ​ Who can do this? Any admin or member user in Atlan can configure reverse sync for tag updates to Databricks. You will also need to work with your Databricks administrator to grant additional permissions to push updates   -  you may not have access yourself. You can enable reverse sync for your imported Databricks tags in Atlan and push all tag updates for your Databricks assets back to source. Once you have enabled reverse sync, any Databricks assets with tags updated in Atlan will also be updated in Databricks. To enable reverse sync for imported Databricks tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Databricks to filter for tags imported from Databricks. In the left menu under Tags , select a synced Databricks tag   -  synced tags will display the Databricks icon next to the tag name. On your selected tag page, to the right of Overview , click Synced tags . Under Synced tags , in the upper right, turn on Enable reverse sync to synchronize tag updates from Atlan to Databricks. In the corresponding confirmation dialog, click Yes, enable it to enable reverse tag sync or click Cancel . Now when you attach Databricks tags to your Databricks assets in Atlan, these tag updates will also be pushed to Databricks! 🎉 Did you know? Enabling reverse sync will not trigger any updates in Databricks until synced tags are attached to Databricks assets in Atlan. Tags: connectors data crawl Previous How to extract on-premises Databricks lineage Next What does Atlan crawl from Databricks? Prerequisites Import Databricks tags to Atlan View Databricks tags in Atlan Push tag updates to Databricks"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/extract-on-premises-databricks-lineage",
    "content": "Connect data Data Warehouses Databricks Lineage and Usage How to extract on-premises Databricks lineage On this page extract on-premises Databricks lineage Once you have set up the databricks-extractor tool , you can extract lineage from your on-premises Databricks instances by completing the following steps. Run databricks-extractor ​ To extract lineage for a specific Databricks connection using the databricks-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files ​ The databricks-extractor tool will generate many folders with JSON files for each service . For example: extracted-lineage extracted-query-history (if EXTRACT_QUERY_HISTORY is set to true) You can inspect the lineage and usage metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 ​ To provide Atlan access to the extracted lineage and usage metadata, you will need to upload the metadata to an S3 bucket. Did you know? We recommend uploading to the same S3 bucket as Atlan uses to avoid access issues. Reach out to your Data Success Manager to get the details of your Atlan bucket. To create your own bucket, refer to the Create your own S3 bucket section of the dbt documentation. (The steps will be exactly the same.) To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. For example, output/databricks-lineage-example/extracted-lineage/result-0.json , output/databricks-lineage-example/extracted-query-history/result-0.json , and so on. Upload the files to the S3 bucket using your preferred method. For example, to upload all files using the AWS CLI : aws s3 cp output/databricks-lineage-example s3://my-bucket/metadata/databricks-lineage-example --recursive Extract lineage in Atlan ​ Once you have extracted lineage on-premises and uploaded the results to S3, you can extract lineage in Atlan: How to extract lineage and usage from Databricks Be sure to select Offline for the Extraction method . Tags: connectors data Previous How to extract lineage and usage from Databricks Next Manage Databricks tags Run databricks-extractor (Optional) Review generated files Upload generated files to S3 Extract lineage in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/extract-lineage-and-usage-from-databricks",
    "content": "Connect data Data Warehouses Databricks Lineage and Usage How to extract lineage and usage from Databricks On this page extract lineage and usage from Databricks Once you have crawled assets from Databricks , you can retrieve lineage from Unity Catalog and usage and popularity metrics from query history or system tables. This is supported for all three authentication methods : personal access token, AWS service principal, and Azure service principal. Both Atlan and Databricks strongly recommend using the system tables method to extract lineage and usage and popularity metrics from Databricks. danger Usage and popularity metrics can be retrieved for all Databricks users. However, your Databricks workspace must be Unity Catalog-enabled for the retrieval of lineage and usage and popularity metrics to succeed. You may also need to upgrade existing tables and views to Unity Catalog , as well as reach out to your Databricks account executive to enable lineage in Unity Catalog. (As of publishing, the feature is still in preview from Databricks on AWS and Azure.) To retrieve lineage and usage from Databricks, rev iew the order of operations and then complete the following steps. Select the extractor ​ To select the Databricks lineage and usage extractor: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Miner . From the list of packages, select Databricks Miner and click on Setup Workflow . Configure the lineage extractor ​ Choose your lineage extraction method: In REST API , Atlan connects to your database and extracts lineage directly. In Offline , you will need to first extract lineage yourself and make it available in S3 . In System Table , Atlan connects to your database and queries system tables to extract lineage directly. REST API ​ To configure the Databricks lineage extractor: For Connection , select the connection to extract. (To select a connection, the crawler must have already run.) Click Next to proceed. Offline extraction method ​ Atlan supports the offline extraction method for extracting lineage from Databricks This method uses Atlan's databricks-extractor tool to extract lineage. You will need to first extract lineage yourself and make it available in S3 . To enter your S3 details: For Connection , select the connection to extract. (To select a connection, the crawler must have already run.) For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include extracted-lineage/result-0.json , extracted-query-history/result-0.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . System table ​ To configure the Databricks lineage extractor: For Connection , select the connection to extract. (To select a connection, the crawler must have already run.) * Extraction Catalog Type : Default : Select to fetch lineage from the system catalog and access schema. Cloned_catalog : Select to fetch lineage from a cloned catalog and schema. Before proceeding, make sure the following prerequisites are met: You have already created cloned views named column_lineage and table_lineage in your schema. If not, follow the steps in Create cloned views of system tables . The atlan-user must have SELECT permissions on both views to access lineage data. Then, provide values for the following fields: Cloned Catalog Name – Catalog containing the cloned views. Cloned Schema Name – Schema containing the cloned views. For SQL Warehouse ID , enter the ID you copied from your SQL warehouse . Click Next to proceed. (Optional) Configure the usage extractor ​ Atlan extracts usage and popularity metrics from: Query history System tables This feature is currently limited to queries on SQL warehouses   -  queries on interactive clusters are not supported. Additionally, expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the Databricks APIs . To configure the Databricks usage and popularity extractor: For Fetch Query History and Calculate Popularity , click Yes to retrieve usage and popularity metrics for your Databricks assets. For Popularity Extraction Method : Choose one of the following methods to extract usage and popularity metrics:: Click REST API to extract usage and popularity metrics from query history. Click System table to extract metrics directly from system tables: Extraction catalog type for popularity : Choose where to fetch popularity data from: Default : Uses the system catalog and query schema to fetch popularity metrics. Cloned_catalog : Select to fetch popularity from cloned views in a separate catalog and schema. Before proceeding: The query_history view must exist in the provided schema. The atlan-user must have SELECT permission on the view. Then provide: Cloned Catalog Name – The catalog that contains the query_history view. Cloned Schema Name – The schema that contains the query_history view. For more information, see Create cloned views of system tables . For SQL Warehouse ID , enter the ID you copied from your SQL warehouse . Configure the usage extractor: For Popularity Window (days) , 30 days is the maximum limit. You can set a shorter popularity window of less than 30 days. For Start time , choose the earliest date from which to mine query history. If you're using the offline extraction method to extract query history from Databricks, skip to the next step. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Databricks assets. Press enter after each name to add more names. danger If running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Run the extractor ​ To run the Databricks lineage and popularity extractor, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . This is currently only supported when using REST API and offline extraction methods. If you're using system tables, skip to step 2. You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the extractor has completed running, you will see lineage for Databricks assets! 🎉 Tags: connectors data crawl api authentication Previous Set up an Azure private network link to Databricks Next How to extract on-premises Databricks lineage Select the extractor Configure the lineage extractor (Optional) Configure the usage extractor Run the extractor"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-an-aws-private-network-link-to-databricks",
    "content": "Connect data Data Warehouses Databricks Private Network Setup Set up an AWS private network link to Databricks On this page Set up an AWS private network link to Databricks AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Databricks and Atlan. Who can do this? You will need Databricks support, and probably your Databricks administrator involved   -  you may not have access or the tools to run these tasks. Prerequisites ​ Databricks must be set up on the E2 version of the platform and Enterprise pricing tier. Your Databricks workspace must be in an AWS region that supports the E2 version of the platform, and not the us-west-1 region. Your Databricks workspace must also be hosted in the same region as Atlan. Your Databricks workspace must use customer-managed VPC. (Note that you cannot update an existing Databricks-managed VPC to a customer-managed VPC.) For all details, see Databricks documentation . Notify Atlan support ​ Once setup is completed, provide Atlan support with the following information: The AWS region of your Databricks instance. There are additional steps that Atlan will need to complete: Creating a security group Creating an endpoint Once the Atlan team has confirmed that the configuration is ready, please continue with the remaining steps. Accept the endpoint connection request ​ You can either: Accept the endpoint connection request from Atlan via API . Accept the endpoint connection request from Atlan from the Databricks console . Once the endpoint connection is accepted, Atlan support will finish the configuration on the Atlan side. When you use this endpoint in the configuration for crawling Databricks , Atlan will connect to Databricks over AWS PrivateLink. Tags: api rest-api graphql Previous Set up on-premises Databricks lineage extraction Next Set up an Azure private network link to Databricks Prerequisites Notify Atlan support Accept the endpoint connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-an-azure-private-network-link-to-databricks",
    "content": "Connect data Data Warehouses Databricks Private Network Setup Set up an Azure private network link to Databricks On this page Set up an Azure private network link to Databricks Azure Private Link creates a secure, private connection between services running in Azure. This document describes the steps to set this up between Databricks and Atlan. Who can do this? You will need Databricks support, and probably your Databricks administrator involved   -  you may not have access or the tools to run these tasks. Prerequisites ​ Your Databricks instance must be Azure-managed and created from the Azure marketplace. For all details, see Databricks documentation . Notify Atlan support ​ Provide Atlan support with the following information: Resource ID of your Azure-managed Databricks instance   -  the resource ID will be in this format: /subscriptions/<subscriptionID>/resourceGroups/azure-databricks/providers/Microsoft.Databricks/workspaces/<databricks-workspace> . There are additional steps that Atlan will need to complete. Once the Atlan team has confirmed that the configuration is ready, please continue with the remaining steps. Approve the endpoint connection request ​ To approve the endpoint connection request from Atlan: Open your Azure-managed Databricks workspace. In the left menu, click Networking and then click the Private endpoint connections tab. From the list of endpoints, search for the endpoint connection request from Atlan. In the Connection state column for the Atlan endpoint connection, click the Approve button to approve the request . Once the endpoint connection from Atlan has been approved, the status of the private endpoint in Atlan will change to Approved . When you use this endpoint in the configuration for crawling Databricks , Atlan will connect to Databricks over Azure Private Link. Tags: data configuration Previous Set up an AWS private network link to Databricks Next How to extract lineage and usage from Databricks Prerequisites Notify Atlan support Approve the endpoint connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-cross-workspace-extraction",
    "content": "Connect data Data Warehouses Databricks Cross-workspace Setup Set up cross-workspace extraction On this page Set up cross-workspace extraction Eliminate the need for separate crawler configurations by using a single service principal to crawl metadata from all workspaces within a Databricks metastore. This guide walks you through configuring the necessary permissions to enable cross-workspace extraction. Important! Cross-workspace extraction isn't supported for REST API or JDBC extraction methods. Prerequisites ​ Before you begin, make sure you have: A Unity Catalog-enabled Databricks workspace Account admin access to create and manage service principals Workspace admin access to grant permissions across all target workspaces At least one active SQL warehouse in each workspace you intend to crawl Set up Databricks authentication completed with one of the supported authentication methods System table extraction enabled for lineage and usage extraction Permissions required ​ The service principal needs the following permissions to enable cross-workspace extraction: CAN_USE on SQL warehouses in each workspace SELECT on system.access.workspace_latest table USE CATALOG , BROWSE , and SELECT on all catalogs you want to crawl Add service principal to all workspaces ​ You must use a single, common service principal that has been granted access to all Databricks workspaces you intend to crawl within the metastore. Log in to your Databricks account console as an account admin From the left menu, click Workspaces and select a workspace From the tabs along the top, click the Permissions tab In the upper right, click Add permissions In the Add permissions dialog: For User, group, or service principal , select your service principal For Permission , select workspace User Click Add Repeat steps 2-5 for each workspace you intend to crawl Grant permissions ​ Configure the necessary permissions for the service principal to access and extract metadata from all workspaces within the metastore. SQL workspace permissions: The service principal must have usage permissions on at least one active SQL warehouse within each workspace . The extractor uses the smallest available warehouse to run its discovery queries. Via SQL Via UI Connect to your Databricks workspace using a SQL client or the SQL editor Run the following command for each workspace, replacing the placeholders: GRANT CAN_USE ON WAREHOUSE < warehouse_name > TO ` <service_principal_id> ` ; Replace <warehouse_name> with your actual warehouse name Replace <service_principal_id> with your service principal's application ID Example GRANT CAN_USE ON WAREHOUSE production - warehouse TO ` 12345678-1234-1234-1234-123456789012 ` ; Log in to your Databricks workspace as a workspace admin From the left menu, click SQL Warehouses On the Compute page, for each SQL warehouse, click the 3-dot icon and then click Permissions In the Manage permissions dialog: In the Type to add multiple users or groups field, search for and select your service principal Select Can use permission Click Add to assign the permission System table permissions: Access to the system schema is essential for workspace and lineage discovery. Via SQL Via UI Connect to your Databricks workspace using a SQL client or the SQL editor Run the following command, replacing the placeholder: GRANT SELECT ON TABLE system . access . workspace_latest TO ` <service_principal_id> ` ; Replace <service_principal_id> with your service principal's application ID Example GRANT SELECT ON TABLE system . access . workspace_latest TO ` 12345678-1234-1234-1234-123456789012 ` ; Log in to your Databricks workspace as a workspace admin From the left menu, click Catalog In the Catalog Explorer , navigate to system > access Click on the workspace_latest table Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals , select your service principal Under Privileges , check SELECT Click Grant to apply the permissions Asset permissions: The service principal requires permissions to \"see\" and \"read\" the metadata for all data assets you wish to extract. These grants must be applied to all private, public, and shared catalogs that are in scope for crawling. Important! For private catalogs, grant permissions from each workspace. For public catalogs, grant from any workspace. Via SQL Via UI Connect to your Databricks workspace using a SQL client or the SQL editor Grant catalog-level permissions (required even when using BROWSE - BROWSE automatically grants access to all schemas and tables): GRANT USE CATALOG ON CATALOG < catalog_name > TO ` <service_principal_id> ` ; GRANT BROWSE ON CATALOG < catalog_name > TO ` <service_principal_id> ` ; Replace <catalog_name> with your actual catalog name Replace <service_principal_id> with your service principal's application ID If not using BROWSE, along with catalog permissions, grant additional permissions: Grant schema-level permissions: GRANT USE SCHEMA ON SCHEMA < catalog_name > . < schema_name > TO ` <service_principal_id> ` ; Replace <catalog_name> and <schema_name> with your actual values Replace <service_principal_id> with your service principal's application ID Example GRANT USE CATALOG ON CATALOG main TO ` 12345678-1234-1234-1234-123456789012 ` ; GRANT BROWSE ON CATALOG main TO ` 12345678-1234-1234-1234-123456789012 ` ; Log in to your Databricks workspace as a workspace admin From the left menu, click Catalog In the Catalog Explorer , navigate to the catalog you want to grant permissions on (for example, main ) Click the Permissions tab and then click Grant In the Grant permissions dialog: Under Principals , select your service principal Under Privileges , check the following permissions: USE CATALOG USE SCHEMA BROWSE SELECT Click Grant to apply the permissions Repeat steps 3-5 for each catalog you want to crawl in Atlan Need help? ​ Check Troubleshooting Databricks connectivity for common issues Contact Atlan support for help with setup or integration Next steps ​ Crawl Databricks Tags: databricks setup cross-workspace-extraction Previous Set up Databricks Next Crawl Databricks Prerequisites Permissions required Add service principal to all workspaces Grant permissions Need help? Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-databricks",
    "content": "Connect data Data Warehouses Databricks Get Started Set up Databricks On this page Set up Databricks Atlan supports three authentication methods for fetching metadata from Databricks. You can set up any of the following authentication methods: Personal access token authentication AWS service principal authentication Azure service principal authentication Personal access token authentication ​ Who can do this? Check that you have Admin and Databricks SQL access for the Databricks workspace. This is required for both cluster options described below. If you don't have this access, contact your Databricks administrator. Grant user access to workspace ​ To grant workspace access to the user creating a personal access token: From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the user. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the user to grant access. For Permission , click the dropdown and select workspace User. Generate a personal access token ​ You can generate a personal access token in your Databricks workspace to the authenticate the integration in Atlan . To generate a personal access token: From the top right of your Databricks workspace, click your Databricks username, and then from the dropdown, click User Settings . Under the Settings menu, click Developer . On the Developer page, next to Access tokens , click Manage . On the Access tokens page, click the Generate new token button. In the Generate new token dialog: For Comment , enter a description of the token's intended use - for example, Atlan crawler . For Lifetime (days) , consider removing the number. This enables the token to be used indefinitely - it won't need to be refreshed. Important! If you do enter a number, remember that you need to periodically regenerate it and update Atlan's crawler configuration with the new token each time. At the bottom of the dialog, click Generate . Copy and save the generated token in a secure location, and then click Done . Select a cluster ​ Did you know? Atlan recommends using serverless SQL warehouses for instant compute availability. To enable serverless SQL warehouses, refer to Databricks documentation for AWS Databricks workspaces or Microsoft documentation for Azure Databricks workspaces. You can set up personal access token authentication for your Databricks instance using one of the following cluster options: Interactive cluster SQL warehouse (formerly SQL endpoint) Interactive cluster ​ To confirm an all-purpose interactive cluster is configured: From the left menu of any page of your Databricks instance, click Compute . Under the All-purpose clusters tab, verify you have a cluster defined. Click the link under the Name column of the table to open your cluster. Under the Configuration tab, verify the Autopilot options to Terminate after ... minutes is enabled. At the bottom of the Configuration tab, expand the Advanced options expandable. Under the Advanced options expandable, open the JDBC/ODBC tab. Confirm that all of the fields in this tab are populated, and copy them for use in crawling: Server Hostname , Port , and HTTP Path . SQL warehouse (formerly SQL endpoint) ​ To confirm a SQL warehouse is configured: From the left menu of any page of your Databricks instance, open the dropdown just below the databricks logo and change to SQL . From the refreshed left menu, click SQL Warehouses . Click the link under the Name column of the table to open your SQL warehouse. Under the Connection details tab, confirm that all of the fields are populated and copy them for use in crawling: Server hostname , Port , and HTTP path . AWS service principal authentication ​ Who can do this? You need your AWS Databricks account admin to create a service principal and manage OAuth credentials for the service principal and your AWS Databricks workspace admin to add the service principal to your AWS Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID Client secret Create a service principal ​ You can create a service principal directly in your Databricks account or from a Databricks workspace. Identity federation enabled on your workspaces: Databricks recommends creating the service principal in the account and assigning it to workspaces. Identity federation disabled on your workspaces: Databricks recommends that you create your service principal from a workspace. Identity federation enabled ​ To create a service principal from your Databricks account, with identify federation enabled: Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal and then click Add . Once the service principal has been created, you can assign it to your identity federated workspace. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown and select workspace User. Identity federation disabled ​ To create a service principal from a Databricks workspace, with identity federation disabled: Log in to your AWS Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , enter a name for the service principal and then click Add . Create an OAuth secret for the service principal ​ You need to create an OAuth secret to authenticate to Databricks REST APIs. To create an OAuth secret for the service principal : Log in to your Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, select the service principal you created . On the service principal page, under OAuth secrets , click Generate secret . From the Generate secret dialog, copy the Secret and Client ID and store it in a secure location. danger Note that this secret is only revealed once during creation. The client ID is the same as the application ID of the service principal. Once you've copied the client ID and secret, click Done . Azure service principal authentication ​ Who can do this? You need your Azure Databricks account admin to create a service principal and your Azure Databricks workspace admin to add the service principal to your Azure Databricks workspace - you may not have access yourself. You need the following to authenticate the connection in Atlan: Client ID (application ID) Client secret Tenant ID (directory ID) Create a service principal ​ To use service principals on Azure Databricks , an admin user must create a new Microsoft Entra ID (formerly Azure Active Directory) application and then add it to the Azure Databricks workspace to use as a service principal. To create a service principal: Sign in to the Azure portal . If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to create the service principal. In_Search resources, services, and docs_, search for and select Microsoft Entra ID . Click**+ Add and select App registration**. For_Name_, enter a name for the application. In the_Supported account types_section, select Accounts in this organizational directory only (Single tenant) and then click Register . On the application page's_Overview_page, in the_Essentials_section, copy and store the following values in a secure location: Application (client) ID Directory (tenant) ID To generate a client secret, within_Manage_, click Certificates & secrets . On the_Client secrets_tab, click New client secret . In the_Add a client secret_dialog, enter the following details: For Description , enter a description for the client secret. For_Expires_, select an expiry time period for the client secret and then click Add . Copy and store the client secret's_Value_in a secure place. Add a service principal to your account ​ To add a service principal to your Azure Databricks account: Log in to your Azure Databricks account console as an account admin. From the left menu of the account console, click User management . From the tabs along the top of the User management page, click the Service principals tab. In the upper right of the Service principals page, click Add service principal . On the Add service principal page, enter a name for the service principal. Under UUID , paste the Application (client) ID for the service principal. Click Add . Assign a service principal to a workspace ​ To add users to a workspace using the account console, the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. Identity federation enabled ​ To assign a service principal to your Azure Databricks account: Log in to your Databricks account console as an account admin. From the left menu of the account console, click Workspaces and then select a workspace to which you want to add the service principal. From the tabs along the top of your workspace page, click the Permissions tab. In the upper right of the Permissions page, click Add permissions . In the Add permissions dialog, enter the following details: For User, group, or service principal , select the service principal you created. For Permission , click the dropdown to select workspace User . Identity federation disabled ​ To assign a service principal to your Azure Databricks workspace: Log in to your Azure Databricks workspace as a workspace admin. From the top right of your workspace, click your username, and then from the dropdown, click Admin Settings . In the left menu of the Settings page, under the Workspace admin subheading, click Identity and access . On the Identity and access page, under Management and permissions , next to Service principals , click Manage . In the upper right of the Service principals page, click Add service principal . In the Add service principal dialog, click the Add new button. For New service principal display name , paste the Application (client) ID for the service principal , enter a display name, and then click Add . Grant permissions to crawl metadata ​ You must have a Unity Catalog-enabled Databricks workspace to crawl metadata in Atlan. To extract metadata, you can grant the BROWSE privilege , currently in public preview. You no longer require the Data Reader preset that granted the following privileges on objects in the catalog - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . To grant permissions to a user or service principal: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to crawl in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privileges , check the BROWSE privilege. At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to crawl in Atlan. System tables extraction method ​ To crawl metadata via system tables, you must have a Unity Catalog-enabled workspace and a configured SQL warehouse. Follow these steps to extract metadata using system tables: Create one of the following authentication methods: Personal access token AWS service principal Azure service principal Grant the following privileges to the identity you created: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalogs system.information_schema.schemata system.information_schema.tables system.information_schema.columns system.information_schema.key_column_usage system.information_schema.table_constraints Cross-workspace extraction ​ To crawl metadata from all workspaces within a Databricks metastore using a single connection, see Set up cross-workspace extraction for instructions. (Optional) Grant permissions to query and preview data ​ danger Atlan currently only supports querying data and viewing sample data preview for the personal access token authentication method. To grant permissions to query data and preview example data: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the left menu of the Catalog Explorer page, select the catalog you want to query and preview data from in Atlan. From the tabs along the top of your workspace page, click the Permissions tab and then click the Grant button. In the Grant on (workspace name) dialog, configure the following: Under Principals , click the dropdown and then select the user or service principal. Under Privilege presets , click the dropdown and then click Data Reader to enable read-only access to the catalog. Doing so automatically selects the following privileges - USE CATALOG , USE SCHEMA , EXECUTE , READ VOLUME , and SELECT . At the bottom of the dialog, click Grant . (Optional) Repeat steps 3-5 for each catalog you want to query and preview data from in Atlan. (Optional) Grant permissions to import and update tags ​ To import Databricks tags , you must have a Unity Catalog-enabled workspace and a SQL warehouse configured. Atlan supports importing Databricks tags using system tables for all three authentication methods. Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following privileges: CAN_USE on a SQL warehouse USE CATALOG on system catalog USE SCHEMA on system.information_schema SELECT on the following tables: system.information_schema.catalog_tags system.information_schema.schema_tags system.information_schema.table_tags system.information_schema.column_tags To push tags updated for assets in Atlan to Databricks, you need to grant the following privileges : APPLY TAG on the object USE CATALOG on the object's parent catalog USE SCHEMA on the object's parent schema (Optional) Grant permissions to extract lineage and usage from system tables ​ You must have a Unity Catalog-enabled workspace to use system tables. Atlan supports extracting the following for your Databricks assets using system tables : lineage usage and popularity metrics Enable system.access schema ​ You need your account admin to enable the system.access schema using the SystemSchemas API . This enables Atlan to extract lineage using system tables. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore in your Databricks environment for which you want to extract lineage. To verify that system schemas are enabled for each schema, follow the steps in Databricks documentation : List system schemas using the SystemSchemas API to check the status. If enabled for any given schema, the state is EnableCompleted . This confirms that the schema has been enabled for that specific metastore. Atlan can only extract lineage using system tables when the state is marked as EnableCompleted . (Optional) enable system.information_schema.table ​ To generate lineage with the target type set as PATH for a table, Atlan uses metadata from system.information_schema.table to resolve table paths and dependencies. To enable this, you must grant the following permissions on the relevant catalog, schema, and tables. Grant permissions ​ Who can do this? You must be a metastore admin, have the MANAGE privilege on the object, or be the owner of the catalog, schema, or table to grant these permissions. In Atlan, one Databricks connection corresponds to one metastore. Repeat the following process for each metastore from which you want to extract lineage. Open Catalog Explorer in your Databricks workspace. Navigate to the catalog (for example, main ) and then to the appropriate schema (for example, sales ). Click the Permissions tab. Click Grant . Enter the user or group name (principal). Assign the following permissions: USAGE on the catalog USAGE on the schema SELECT on each relevant table Click Grant to apply the changes. These privileges enable Atlan to read table definitions and other metadata from the metastore. (Optional) enable system.query schema ​ This is only required if you also want to extract usage and popularity metrics from Databricks. You need your account admin to enable the system.query schema using the SystemSchemas API . This enables Atlan to mine query history using system tables for usage and popularity metrics. To verify that system schemas is enabled for each schema, follow the steps in Databricks documentation . If enabled for any given schema, the state is EnableCompleted . info 💪 Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . Grant permissions ​ Atlan supports extracting Databricks lineage and usage and popularity metrics using system tables for all three authentication methods . Once you have created a personal access token , an AWS service principal , or an Azure service principal , you will need to grant the following permissions: CAN_USE on a SQL warehouse USE_CATALOG on system catalog USE SCHEMA on system.access schema USE SCHEMA on system.query schema (tomine query history for usage and popularity metrics) SELECT on the following tables: system.query.history (to mine query history for usage and popularity metrics) system.access.table_lineage system.access.column_lineage You need to create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. info 💪 Did you know? Can't grant SELECT permissions on the system tables in system.access and system.query ? Skip the previous steps and create cloned views in a separate catalog and schema. See Create cloned views of system tables . (Optional) Create cloned views of system tables ​ When you don't want to grant access to system tables directly, you can create cloned views to expose lineage and popularity metrics through a separate schema. Follow these steps to set up cloned views: Create a catalog and schema to store cloned views. Use meaningful and unique names—for example, atlan_cloned_catalog and atlan_cloned_schema . Create cloned views for the following system tables: Lineage tables CREATE OR REPLACE VIEW < cloned - catalog - name > . < cloned - schema - name > . column_lineage AS SELECT * FROM system . access . column_lineage ; CREATE OR REPLACE VIEW < cloned - catalog - name > . < cloned - schema - name > . table_lineage AS SELECT * FROM system . access . table_lineage ; Replace <cloned-catalog-name> and <cloned-schema-name> with the catalog and schema names used in your environment. Popularity metrics CREATE OR REPLACE VIEW < cloned - catalog - name > . < cloned - schema - name > . query_history AS SELECT * FROM system . query . history ; Replace <cloned-catalog-name> and <cloned-schema-name> with the catalog and schema names used in your environment. Grant permissions ​ Grant the following permissions to enable access to the cloned views: CAN_USE on a SQL warehouse USE CATALOG on the catalog (for example, <cloned-catalog-name> ) USE SCHEMA and SELECT on the schema (for example, <cloned-catalog-name>.<cloned-schema-name> ) You must create a Databricks connection in Atlan for each metastore. You can use the hostname of your Unity Catalog-enabled workspace as the Host for the connection. Locate warehouse ID ​ To extract lineage and usage and popularity metrics using system tables, you will also need the warehouse ID of your SQL warehouse . To locate the warehouse ID: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, select the warehouse you want to use. From the Overview tab of your warehouse page, next to the Name of your warehouse, copy the value for your SQL warehouse ID . For example, example-warehouse (ID: 123ab4c5def67890) , copy the value 123ab4c5def67890 and store it in a secure location. (Optional) Grant view permissions to access Databricks entities via APIs ​ Atlan uses Databricks REST APIs to extract metadata for Notebooks, Queries, Jobs, and Pipelines. This information helps to understand which Databricks enitity was used to create a lineage between assets. Use the steps below for each object type to grant CAN VIEW permission to the Databricks user or service principal configured in your integration: Notebook API ( /api/2.0/workspace/list ): Grant CAN VIEW permission on individual notebooks, or on the workspace folder containing the notebooks, or on the entire workspace. For more information, see Manage Access Control Lists with Folders . Queries API ( /api/2.0/sql/queries ): Grant CAN VIEW permission on individual queries, or on the workspace folder containing the queries, or on the entire workspace. For more information, see View Queries . Job API ( /api/2.2/jobs/list ): Grant CAN VIEW permission on each job object directly. Databricks Jobs are distinct from notebooks or files and require permission set directly on the job object. For more information, see Control Access to a Job . Pipeline API ( /api/2.0/pipelines ): Grant CAN VIEW permission on each Delta Live Tables (DLT) pipeline object directly. For more information, see Configure Pipeline Permissions . (Optional) Grant permissions for views and materialized views ​ Atlan requires the following permissions to to extract view definitions from and generate lineagefor views and materialized views: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click Catalog . In the Catalog Explorer , select the catalog you want to extract view definitions from and generate lineage for in Atlan. From the tabs at the top, click the Permissions tab, and then click Grant . In the Grant on (workspace name) dialog, configure the following: Select the user or service principal under Principals . Select the following privileges under Privilege presets : USE CATALOG USE SCHEMA SELECT Click Grant to apply the permissions. Repeat steps 3–6 for each catalog you want to crawl in Atlan. Did you know? SELECT permission is required to extract the definitions of views and materialized views. If you prefer not to grant SELECT at the catalog level, you can grant it on individual views and materialized views instead. (Optional) Grant permissions to mine query history ​ To mine query history using REST API, you will need to assign the CAN MANAGE permission on your SQL warehouses to the user or service principal. To grant permissions to mine query history: Log in to your Databricks workspace as a workspace admin. From the left menu of your workspace, click SQL Warehouses . On the Compute page, for each SQL warehouse you want to mine query history, click the 3-dot icon and then click Permissions . In the Manage permissions dialog, configure the following: In the Type to add multiple users or groups field, search for and select a user or service principal. Expand the Can use permissions dropdown and then select Can manage . This permission enables the service principal to view all queries for the warehouse . Click Add to assign the CAN MANAGE permission to the service principal. Tags: data authentication Previous Databricks Next Set up cross-workspace extraction Personal access token authentication AWS service principal authentication Azure service principal authentication Grant permissions to crawl metadata (Optional) Grant permissions to query and preview data (Optional) Grant permissions to import and update tags (Optional) Grant permissions to extract lineage and usage from system tables (Optional) Grant view permissions to access Databricks entities via APIs (Optional) Grant permissions for views and materialized views (Optional) Grant permissions to mine query history"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/crawl-databricks",
    "content": "Connect data Data Warehouses Databricks Crawl Databricks Assets Crawl Databricks On this page Crawl Databricks Once you have configured the Databricks access permissions , you can establish a connection between Atlan and your Databricks instance. (If you are also using AWS PrivateLink or Azure Private Link for Databricks, you will need to set that up first, too.) To crawl metadata from your Databricks instance, review the order of operations and then complete the following steps. Select the source ​ To select Databricks as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select Databricks Assets , and click Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. Next, select an authentication method: In JDBC , you will need a personal access token and HTTP path for authentication . In AWS Service , you will need a client ID and client secret for AWS service principal authentication . In Azure Service , you will need a tenant ID, client ID, and client secret for Azure service principal authentication . In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ JDBC ​ To enter your Databricks credentials: For Host , enter the hostname, AWS PrivateLink endpoint , or Azure Private Link endpoint for your Databricks instance. For Port , enter the port number of your Databricks instance. For Personal Access Token , enter the access token you generated when setting up access . For HTTP Path , enter one of the following: A path starting with /sql/1.0/warehouses to use the Databricks SQL warehouse . A path starting with sql/protocolv1/o to use the Databricks interactive cluster . Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next . danger Make sure your Databricks instance (SQL warehouse or interactive cluster) is up and running, otherwise the Test Authentication step times out. AWS service principal ​ To enter your Databricks credentials: For Host , enter the hostname or AWS PrivateLink endpoint for your Databricks instance. For Port , enter the port number of your Databricks instance. For Client ID , enter the client ID for your AWS service principal . For Client Secret , enter the client secret for your AWS service principal . Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next . Azure service principal ​ To enter your Databricks credentials: For Host , enter the hostname or Azure Private Link endpoint for your Databricks instance. For Port , enter the port number of your Databricks instance. For Client ID , enter the application (client) ID for your Azure service principal . For Client Secret , enter the client secret for your Azure service principal . For Tenant ID , enter the directory (tenant) ID for your Azure service principal . Click Test Authentication to confirm connectivity to Databricks using these details. Once successful, at the bottom of the screen click Next . Offline extraction method ​ Atlan supports the offline extraction method for fetching metadata from Databricks. This method uses Atlan's databricks-extractor tool to fetch metadata. You need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include output/databricks-example/catalogs/success/result-0.json , output/databricks-example/schemas/{{catalog_name}}/success/result-0.json , output/databricks-example/tables/{{catalog_name}}/success/result-0.json , and similar files. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from Databricks. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Databricks data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the Databricks connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, nobody can manage the connection - not even admins. (Optional) To prevent users from querying any Databricks data, change Enable SQL Query to No . (Optional) To prevent users from previewing any Databricks data, change Enable Data Preview to No . (Optional) To prevent users from running large queries, change Max Row Limit or keep the default selection. At the bottom of the screen, click the Next button to proceed. Configure the crawler ​ Before running the Databricks crawler, you can further configure it. System tables extraction method ​ The system metadata extraction method is only available for Unity Catalog-enabled workspaces . It provides access to detailed metadata from system tables and supports all three authentication types. You can extract metadata from your Databricks workspace using this method. Follow these steps: Set up authentication using one of the following: Personal access token AWS service principal Azure service principal The default options can work as is. You may choose to override the defaults for any of the remaining options: For Asset selection , select a filtering option: For SQL warehouse , click the dropdown to select the SQL warehouse you want to configure. To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This defaults to all assets, if none are specified.) To have the crawler include Databases , Schemas , or Tables & Views based on a naming convention, click Include by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_DB.* for Databases includes all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This defaults to no assets, if none are specified.) To have the crawler ignore Databases , Schemas , or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression - for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views excludes all the matching tables and views. Click + to add more filters. If you add multiple filters, assets are crawled based on matching all the filtering conditions you have set. To import tags from Databricks to Atlan , change Import Tags to Yes . Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Incremental extraction Public preview ​ Toggle incremental extraction, for a faster and more efficient metadata extraction. JDBC extraction method ​ The JDBC extraction method uses JDBC queries to extract metadata from your Databricks instance. This was the original extraction method provided by Databricks. This extraction method is only supported for personal access token authentication . You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For View Definition Lineage , keep the default Yes to generate upstream lineage for views based on the tables referenced in the views or click No to exclude from crawling. For Advanced Config , keep Default for the default configuration or click Advanced to further configure the crawler: To enable or disable schema-level filtering at source, click Enable Source Level Filtering and select True to enable it or False to disable it. REST API extraction method ​ The REST API extraction method uses Unity Catalog to extract metadata from your Databricks instance. This extraction method is supported for all three authentication options: personal access token , AWS service principal , and Azure service principal . This method is only supported by Unity Catalog-enabled workspaces. If you enable an existing workspace, you also need to upgrade your tables and views to Unity Catalog . While REST APIs are used to extract metadata, JDBC queries are still used for querying purposes. You can override the defaults for any of these options: Change the extraction method under Extraction method to REST API . To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To import tags from Databricks to Atlan , change Import Tags to Yes . Note that you must have a Unity Catalog-enabled workspace to import Databricks tags in Atlan. For SQL warehouse , click the dropdown to select the SQL warehouse you have configured. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ Follow these steps to run the Databricks crawler: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up cross-workspace extraction Next Set up on-premises Databricks access Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-access",
    "content": "Connect data Data Warehouses Databricks On-premises Setup Set up on-premises Databricks access On this page Set up on-premises Databricks access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Databricks instance details, including credentials. In some cases you will not be able to expose your Databricks instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites ​ To extract metadata from your on-premises Databricks instance, you will need to use Atlan's databricks-extractor tool. Did you know? Atlan uses exactly the same databricks-extractor behind the scenes when it connects to Databricks in the cloud. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the databricks-extractor tool ​ To get the databricks-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl Databricks: sudo docker load -i /path/to/databricks-extractor-master.tar Get the compose file ​ Atlan provides you with a Docker compose file for the databricks-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Databricks instance. The file is docker-compose.yaml . Define Databricks connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Databricks connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services ​ For each on-premises Databricks instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *extract environment: <<: *databricks-defaults INCLUDE_FILTER: '{\"DB_1\": [], \"DB_2\": [\"SCHEMA_1\", \"SCHEMA_2\"]}' EXCLUDE_FILTER: '{\"DB_1\": [\"SCHEMA_1\", \"SCHEMA_2\"]}' TEMP_TABLE_REGEX: '.*temp.*|.*tmp.*|.*TEMP.*|.*TMP.*' SYSTEM_SCHEMA_REGEX: '^information_schema$' volumes: - ./output/connection-name:/output Replace connection-name with the name of your connection. <<: *extract tells the databricks-extractor tool to run. environment contains all parameters for the tool. INCLUDE_FILTER -  specify the databases and schemas from which you want to extract metadata. Remove this line if you want to extract metadata from all databases and schemas. EXCLUDE_FILTER -  specify the databases and schemas you want to exclude from metadata extraction. This will take precedence over INCLUDE_FILTER . Remove this line if you do not want to exclude any databases or schemas. TEMP_TABLE_REGEX -  specify a regular expression for excluding temporary tables. Remove this line if you do not want to exclude any temporary tables. SYSTEM_SCHEMA_REGEX -  specify a regular expression for excluding system schemas. If unspecified, INFORMATION_SCHEMA will be excluded from the extracted metadata by default. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Databricks connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials ​ To define the credentials for your Databricks connections, you will need to provide a Databricks configuration file. The Databricks configuration is a .ini file with the following format: [DatabricksConfig] host = <host> port = <port> # seconds to wait for a response from the server timeout = 300 # Databricks authentication type. Options: personal_access_token, aws_service_principal auth_type = personal_access_token # Required only if auth_type is personal_access_token. [PersonalAccessTokenAuth] personal_access_token = <personal_access_token> # Required only if auth_type is aws_service_principal. [AWSServicePrincipalAuth] client_id = <client_id> client_secret = <client_secret> Secure credentials ​ Using local files ​ danger If you decide to keep Databricks credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: databricks_config: file: ./databricks.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets ​ To create and use Docker secrets: Store the Databricks configuration file: sudo docker secret create databricks_config path/to/databricks.ini At the top of your compose file, add a secrets element to access your secret: secrets: databricks_config: external: true name: databricks_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Databricks configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example ​ Let's explain in detail with an example: secrets: databricks_config: external: true name: databricks_config x-templates: # ... services: databricks-example: <<: *extract environment: <<: *databricks-defaults INCLUDE_FILTER: '{\"DB_1\": [], \"DB_2\": [\"SCHEMA_1\", \"SCHEMA_2\"]}' EXCLUDE_FILTER: '{\"DB_1\": [\"SCHEMA_1\", \"SCHEMA_2\"]}' TEMP_TABLE_REGEX: '.*temp.*|.*tmp.*|.*TEMP.*|.*TMP.*' SYSTEM_SCHEMA_REGEX: '^information_schema$' volumes: - ./output/databricks-example:/output secrets: - databricks_config In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The databricks_config refers to an external Docker secret created using the docker secret create command. The name of this service is databricks-example . You can use any meaningful name you want. The <<: *databricks-defaults sets the connection type to Databricks. The ./output/databricks-example:/output line tells the extractor where to store results. In this example, the extractor will store results in the ./output/databricks-example directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Crawl Databricks Next Crawl on-premises Databricks Prerequisites Get the compose file Define Databricks connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/databricks/how-tos/set-up-on-premises-databricks-lineage-extraction",
    "content": "Connect data Data Warehouses Databricks On-premises Setup Set up on-premises Databricks lineage extraction On this page Set up on-premises Databricks lineage extraction Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Databricks instance details, including credentials. In some cases you will not be able to expose your Databricks instance for Atlan to extract and ingest lineage. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of lineage from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites ​ To extract lineage from your on-premises Databricks instance, you will need to use Atlan's databricks-extractor tool. Did you know? Atlan uses exactly the same databricks-extractor behind the scenes when it connects to Databricks in the cloud. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the databricks-extractor tool ​ To get the databricks-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to extract lineage from Databricks: sudo docker load -i /path/to/databricks-extractor-master.tar Get the compose file ​ Atlan provides you with a Docker compose file for the databricks-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Databricks instance. The file is docker-compose.yaml . Define Databricks connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Databricks connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services ​ For each on-premises Databricks instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *extract-lineage environment: <<: *databricks-defaults EXTRACT_QUERY_HISTORY: true QUERY_HISTORY_START_TIME_MS: 0 volumes: - ./output/connection-name:/output Replace connection-name with the name of your connection. <<: *extract-lineage tells the databricks-extractor tool to run. environment contains all parameters for the tool. EXTRACT_QUERY_HISTORY -  specifies whether to extract query history for the Databricks connection, in addition to lineage. The query history output can then be used to calculate usage and popularity metrics . QUERY_HISTORY_START_TIME_MS -  specifies the time in epoch milliseconds from when to extract query history. If unspecified, the extractor will extract queries for the past 30 days by default. In Databricks, the query history retains query data for the past 30 days. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Databricks connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials ​ To define the credentials for your Databricks connections, you will need to provide a Databricks configuration file. The Databricks configuration is a .ini file with the following format: [DatabricksConfig] host = <host> port = <port> # seconds to wait for a response from the server timeout = 300 # Databricks authentication type. Options: personal_access_token, aws_service_principal auth_type = personal_access_token # Required only if auth_type is personal_access_token. [PersonalAccessTokenAuth] personal_access_token = <personal_access_token> # Required only if auth_type is aws_service_principal. [AWSServicePrincipalAuth] client_id = <client_id> client_secret = <client_secret> Secure credentials ​ Using local files ​ danger If you decide to keep Databricks credentials in plaintext files, we recommend you restrict access to the directory and the compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets: databricks_config: file: ./databricks.ini danger This secrets section is at the same top-level as the services section described earlier. It is not a subsection of the services section. Using Docker secrets ​ To create and use Docker secrets: Store the Databricks configuration file: sudo docker secret create databricks_config path/to/databricks.ini At the top of your compose file, add a secrets element to access your secret: secrets: databricks_config: external: true name: databricks_config The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Databricks configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example ​ Let's explain in detail with an example: secrets: databricks_config: external: true name: databricks_config x-templates: # ... services: databricks-lineage-example: <<: *extract-lineage environment: <<: *databricks-defaults EXTRACT_QUERY_HISTORY: true QUERY_HISTORY_START_TIME_MS: 0 volumes: - ./output/databricks-lineage-example:/output secrets: - databricks_config In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The databricks_config refers to an external Docker secret created using the docker secret create command. The name of this service is databricks-lineage-example . You can use any meaningful name you want. The <<: *databricks-defaults sets the connection type to Databricks. The ./output/databricks-lineage-example:/output line tells the extractor where to store results. In this example, the extractor will store results in the ./output/databricks-lineage-example directory on the local file system. We recommend you output the extracted lineage for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: lineage data-lineage impact-analysis integration connectors security access-control permissions Previous Crawl on-premises Databricks Next Set up an AWS private network link to Databricks Prerequisites Get the compose file Define Databricks connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/enable-sso-for-google-bigquery",
    "content": "Connect data Data Warehouses Google BigQuery Get Started How to enable SSO for Google BigQuery On this page Enable  SSO for Google BigQuery Atlan supports SSO authentication for Google BigQuery connections. Once you've configured SSO authentication for Google BigQuery, your users can: Query data with SSO credentials View sample data with SSO credentials Did you know? When using OAuth 2.0 for authorization, Google displays a consent screen to the user that includes a summary of your project, policies, and scopes. If you have not configured the consent screen, complete the steps in configure OAuth consent screen . Otherwise, skip to create access credentials . (Optional) Configure OAuth consent screen in Google BigQuery ​ Who can do this? You will need your Google BigQuery administrator to complete these steps   -  you may not have access yourself. To configure the OAuth consent screen , from Google BigQuery: Open the Google Cloud console . In the left menu of the Google Cloud console, under APIs & Services , click OAuth consent screen . On the OAuth consent screen page, under User Type , select a preferred user type and then click Create . In the corresponding Edit app registration page, enter the following details: For App name , enter a meaningful name   -  for example, Atlan_SSO . For User support email , enter a support email for your users to troubleshoot. For Developer contact information , enter an email address where Google can notify you about any changes to your project. Click Save and continue to proceed to the next step. On the Scopes page, complete the following steps: Click Add or remove scopes to add a new scope. In the Update selected scopes dialog, click BigQuery API to add the /auth/bigquery scope and then click Update . Click Save and continue to finish setup. Once the OAuth consent screen configuration is successful, click Go back to dashboard . Create access credentials in Google BigQuery ​ Who can do this? You will need your Google BigQuery administrator to complete these steps   -  you may not have access yourself. Credentials are used to obtain an access token from Google's authorization servers for authentication in Atlan. To create access credentials , from Google BigQuery: Open the Google Cloud console . In the left menu of the Google Cloud console, under APIs & Services , click Credentials . From the upper right of the Credentials page, click Create credentials , and from the dropdown, click OAuth client ID . In the OAuth client ID screen, enter the following details: For Application type , click Web application . For Name , enter a meaningful name   -  for example, Atlan_client . Under Authorized JavaScript origins , click Add URI and enter your Atlan instance   -  for example, https://<company-name>.atlan.com . Under Authorized redirect URIs , click Add URI and enter your Atlan endpoint URI   -  for example, https://<company-name>.atlan.com/api/service/oauth . Click Create to finish setup. From the corresponding OAuth client created dialog, copy the Client ID and Client secret and store it in a secure location. Configure SSO authentication in Atlan ​ Who can do this? You will need to be a connection admin in Atlan to complete these steps. You will also need inputs and approval from your Google BigQuery administrator. Once you have configured access credentials in Google BigQuery , you can enable SSO authentication for your users to query data and view sample data in Atlan. To configure SSO on a Google BigQuery connection, from Atlan: From the left menu of any screen, click Assets . From the Assets page, click the Connector filter, and from the dropdown, select BigQuery . From the pills below the search bar at the top of the screen, click Connection . From the list of results, select a Google BigQuery connection to enable SSO authentication. From the sidebar on the right, next to Connection settings , click Edit . In the Connection settings dialog: Under Allow query , for Authentication type , click SSO authentication to enforce SSO credentials for querying data : For SSO authentication , enter the following details: For Client ID , enter the client ID you copied from Google BigQuery. For Client secret , enter the client secret you copied from Google BigQuery. Under Display sample data , for Source preview , click SSO authentication to enforce SSO credentials for viewing sample data : If SSO authentication is enabled for querying data, the same connection details will be reused for viewing sample data. If a different authentication method is enabled for querying data, enter the client ID and client secret you copied from Google BigQuery. (Optional) Toggle on Enable data policies created at source to apply for querying in Atlan to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. At the bottom right of the Connection settings dialog, click Update . Your users will now be able to run queries and view sample data using their SSO credentials! 🎉 Tags: connectors data integration authentication Previous Set up Google BigQuery Next Crawl Google BigQuery (Optional) Configure OAuth consent screen in Google BigQuery Create access credentials in Google BigQuery Configure SSO authentication in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/manage-google-bigquery-tags",
    "content": "Connect data Data Warehouses Google BigQuery Manage BigQuery in Atlan Manage Google BigQuery tags On this page Manage Google BigQuery tags Atlan imports your Google BigQuery tags and allows you to update your Google BigQuery assets with the imported tags. Note that object tagging in Google BigQuery currently requires Enterprise edition or higher . Once you've crawled Google BigQuery : Your Google BigQuery assets in Atlan will be automatically enriched with their Google BigQuery tags. Imported Google BigQuery tags will be mapped to corresponding Atlan tags through case-insensitive name match   -  multiple Google BigQuery tags can be matched to a single tag in Atlan. You can also attach Google BigQuery tags , including tag values and hierarchies, to your Google BigQuery assets in Atlan   -  allowing you to categorize your assets at a more granular level. Atlan supports: Tags -  enrich your Google BigQuery tables, views, and materialized views with tags and tag values. Policy tags -  enrich your Google BigQuery columns with policy tags and tag hierarchies. You can filter your assets by Google BigQuery tags. Atlan currently does not support crawling Dataplex tag templates . Prerequisites ​ Before you can import tags from Google BigQuery, you will need to do the following: Create tags or have existing tags in Google BigQuery. Grant permissions to import tags from Google BigQuery. Import Google BigQuery tags to Atlan ​ Who can do this? You will need to be an admin user in Atlan to import Google BigQuery tags. You will also need to work with your Google BigQuery administrator for additional inputs and approval. Atlan imports existing Google BigQuery tags through one-way tag sync. The imported Google BigQuery tags are matched to corresponding tags in Atlan through case-insensitive name match and your Google BigQuery assets enriched with the tags synced from Google BigQuery. To import Google BigQuery tags to Atlan, you can either: Create a new Google BigQuery workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Google BigQuery workflow to change Import Tags to Yes . If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags. Once the crawler has completed running, tags synced from Google BigQuery will be available to use for tagging assets ! 🎉 View Google BigQuery tags in Atlan ​ Once you've crawled Google BigQuery , you will be able to view and manage your Google BigQuery tags in Atlan. To view synced Google BigQuery tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click BigQuery to filter for tags imported from Google BigQuery. In the Overview section, you can view a total count of synced Google BigQuery tags. To the right of Overview , click Synced tags to view additional details   -  including tag name, type, and values, description, total count of linked assets, connection name, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your Google BigQuery tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . Tags synced from Google BigQuery cannot be renamed. You can now attach Google BigQuery tags to your Google BigQuery assets in Atlan! 🎉 Tags: connectors data crawl Previous Mine Google BigQuery Next What does Atlan crawl from Google BigQuery? Prerequisites Import Google BigQuery tags to Atlan View Google BigQuery tags in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/set-up-google-bigquery",
    "content": "Connect data Data Warehouses Google BigQuery Get Started Set up Google BigQuery On this page Set up Google BigQuery Who can do this? You must be a Google BigQuery administrator to run these commands. For more information, see Google Cloud's Granting, changing, and revoking access to resources . Atlan extracts metadata from Google BigQuery through read-only access. Once you have crawled metadata for your Google BigQuery assets, you can mine query history to construct lineage. If you have enabled sample data preview or querying, asset previews and queries will be cost-optimized for tables only. For views and materialized views , Atlan will send you a cost nudge before you run the preview or query the data   -  learn more here . You must create a service account to enable Atlan to extract metadata from Google BigQuery . To create a service account, you can either use: Google Cloud console Google Cloud CLI Permissions ​ Atlan requires the following permissions to extract metadata from Google BigQuery. (Required) For metadata crawling ​ To configure permissions for crawling metadata, add the following permissions to the custom role: bigquery.datasets.get enables Atlan to retrieve metadata about a dataset. bigquery.datasets.getIamPolicy enables Atlan to read a dataset's IAM permissions. bigquery.jobs.create enables Atlan to run jobs (including queries) within the project. danger Without this, Atlan can't query the source. bigquery.routines.get enables Atlan to retrieve routine definitions and metadata. bigquery.routines.list enables Atlan to list routines and metadata on routines. bigquery.tables.get enables Atlan to retrieve table metadata. bigquery.tables.getIamPolicy enables Atlan to read a table's IAM policy. bigquery.tables.list enables Atlan to list tables and metadata on tables. bigquery.readsessions.create enables Atlan to create a session to stream large results. bigquery.readsessions.getData enables Atlan to retrieve data from the session. bigquery.readsessions.update enables Atlan to cancel the session. resourcemanager.projects.get enables Atlan to retrieve project names and metadata. (Optional) To add data preview and querying ​ To configure permissions for previewing and querying data, add the following permissions to the custom role: bigquery.tables.getData enables Atlan to retrieve table data. danger This permission is also required for retrieving metadata such as the row count and update time of a table. bigquery.jobs.get enables Atlan to retrieve data and metadata on any job, including queries. bigquery.jobs.listAll enables Atlan to list all jobs and retrieve metadata on any job submitted by any user. bigquery.jobs.update enables Atlan to cancel any job, including a running query. (Optional) To add query history mining ​ To configure permissions for mining query history, add the following permissions to the custom role: bigquery.jobs.listAll enables Atlan to fetch all queries for a project. bigquery.jobs.get enables Atlan to access query text for queries. (Optional) To crawl tags ​ To configure permissions for crawling Google BigQuery tags and policy tags , add the following permissions to the custom role: resourcemanager.tagKeys.list enables Atlan to fetch all tag keys. resourcemanager.tagValues.list enables Atlan to fetch all tag values for tag keys. datacatalog.taxonomies.list enables Atlan to fetch all policy tag taxonomies. datacatalog.taxonomies.get enables Atlan to fetch all policy tag taxonomies. Google Cloud console ​ Create a custom role ​ You will need to create a custom role in the Google Cloud console for integration with Atlan. To create a custom role: Open the Google Cloud console . From the left menu under IAM and admin , click Roles . Using the dropdown list at the top of the page, select the project in which you want to create a role. From the upper left of the Roles page, click Create Role . In the Create role page, enter the following details: For Title , enter a meaningful name for the custom role   -  for example, Atlan User Role . (Optional) For Description , enter a description for the custom role. For ID , the Google Cloud console generates a custom role ID based on the custom role name. Edit the ID if necessary   -  the ID cannot be changed later. (Optional) For Role launch stage , assign a stage for the custom role   -  for example, Alpha , Beta , or General Availability . Click Add permissions to select the permissions you want to include in the custom role. In the Add permissions dialog, click the Enter property name or value filter and add the required and any optional permissions. Click Create to finish custom role setup. Once you have created a custom role, you will need to create a service account and add your custom role to it. To create a service account: Open the Google Cloud console . From the left menu under IAM and admin , click Service accounts . Select a Google Cloud project. From the upper left of the Service accounts page, click Create Service Account . For Service account details , enter the following details: For Service account name , enter a service account name to display in the Google Cloud console. For Service account ID , the Google Cloud console generates a service account ID based on this name. Edit the ID if necessary   -  the ID cannot be changed later. (Optional) For Service account description , enter a description for the service account. Click Create and continue to proceed to the next step. For Grant this service account access to the project , enter the following details: Click the Select a role dropdown and then select the custom role you created in the previous step   -  for example, Atlan User Role . Click Continue to proceed to the next step. Click Done to finish the service account setup. Create service account key ​ Once you have created a service account, you will need to create a service account key for crawling Google BigQuery . To create a service account key: Open the Google Cloud console . From the left menu under IAM and admin , click Service accounts . Select the Google Cloud project for which you created the service account. On the Service accounts page, click the email address of the service account that you want to create a key for. From the upper left of your service account page, click the Keys tab. On the Keys page, click the Add Key dropdown and then click Create new key . In the Create private key dialog, for Key type , click JSON and then click Create . This will create a service account key file. Download the key file and store it in a secure location   -  you will not be able to download it again. Google Cloud CLI ​ Prerequisites ​ You will need to set up the Google Cloud CLI in any one of the following development environments: Cloud Shell -  to use an online terminal with the gcloud CLI already set up, activate Cloud Shell: To launch a Cloud Shell session from the Google Cloud console, open the Google Cloud console , and from the top right, click the Activate Cloud Shell icon. A Cloud Shell session will start and display a command-line prompt. It can take a few seconds for the session to initialize. Local shell -  to use a local development environment, install and initialize the gcloud CLI. Create a custom role ​ To create a custom role with the requisite and any optional permissions, run the following command: gcloud iam roles create atlanUserRole --project=<project_id> \\ --title=\"Atlan User Role\" --description=\"Atlan User Role to extract metadata\" \\ --permissions=\"bigquery.datasets.get,bigquery.datasets.getIamPolicy,bigquery.jobs.create,bigquery.readsessions.create,bigquery.readsessions.getData,bigquery.readsessions.update,bigquery.routines.get,bigquery.routines.list,bigquery.tables.get,bigquery.tables.getIamPolicy,bigquery.tables.list,resourcemanager.projects.get\" \\ --stage=ALPHA Replace <project_id> with the project ID of your Google Cloud project. Create a service account ​ To create a service account, run the following command: gcloud iam service-accounts create atlanUser \\ --description=\"Atlan Service Account to extract metadata\" \\ --display-name=\"Atlan User\" To add your custom role to your service account, run the following command: gcloud projects add-iam-policy-binding <project_id> \\ --member=\"serviceAccount:atlanUser@<project_id>.iam.gserviceaccount.com\" \\ --role=\"atlanUserRole\" Replace <project_id> with the project ID of your Google Cloud project. Create a service account key ​ To create a service account key, run the following command: gcloud iam service-accounts keys create  <key_file_path> \\ --iam-account=atlanUser@<project_id>.iam.gserviceaccount.com\" Replace <key_file_path> with path to a new output file for the private key   -  for example, ~/atlanUser-private-key.json . Replace <project_id> with the project ID of your Google Cloud project. danger Due to limitations at source, Atlan currently does not support generating lineage using the bq cp commands   -  for example, bq cp <source-table> <destination-table> . Tags: connectors data crawl Previous Google BigQuery Next How to enable SSO for Google BigQuery Permissions Google Cloud console Google Cloud CLI"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/mine-google-bigquery",
    "content": "Connect data Data Warehouses Google BigQuery Crawl BigQuery Assets Mine Google BigQuery On this page Mine Google BigQuery Once you have crawled assets from Google BigQuery , you can mine its query history to construct lineage. To mine lineage from Google BigQuery, review the order of operations and then complete the following steps. Select the miner ​ To select the Google BigQuery miner: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Miner . From the list of packages, select BigQuery Miner and then click Setup Workflow . Configure the miner ​ To configure the Google BigQuery miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Miner Extraction Method , select Query History . For Start time , choose the earliest date from which to mine query history. info 💪 Did you know? The miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. (Optional) By default, the miner fetches data from the US region. To fetch data from another region , for Region , select Custom and then enter the region where your INFORMATION_SCHEMA is hosted under Custom BigQuery Region . Enter the region in the following format region-<REGION> , replacing <REGION> with your specific region   -  for example, europe-north1 . To check for any permissions or other configuration issues before running the miner, click Preflight checks . At the bottom of the screen, click Next to proceed. danger If running the miner for the first time, Atlan recommends setting a start date roughly three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Configure the miner behavior ​ To configure the Google BigQuery miner behavior: (Optional) For Calculate popularity , change to True to retrieve usage and popularity metrics for your Google BigQuery assets from query history: To select a pricing model for running queries , for Pricing Model , click On Demand to be charged for the number of bytes processed or Flat Rate for the number of slots purchased. For Popularity Window (days) , 30 days is the maximum limit. You can set a shorter popularity window of less than 30 days. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Google BigQuery assets. Press enter after each name to add more names. (Optional) For Control Config , click Custom to configure the following: For Fetch excluded project's QUERY_HISTORY , click Yes to mine query history from databases or projects excluded while crawling metadata from Google BigQuery . If Atlan support has provided you with a custom control configuration, enter the configuration into the Custom Config box. You can also: (Optional) Enter {“ignore-all-case”: true} to enable crawling assets with case-sensitive identifiers. Run the miner ​ To run the Google BigQuery miner, after completing the steps above: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you will see lineage for Google BigQuery assets that were created in Google BigQuery between the start time and when the miner ran! 🎉 Tags: connectors data crawl setup Previous Crawl Google BigQuery Next Manage Google BigQuery tags Select the miner Configure the miner Configure the miner behavior Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/google-bigquery/how-tos/crawl-google-bigquery",
    "content": "Connect data Data Warehouses Google BigQuery Crawl BigQuery Assets Crawl Google BigQuery On this page Crawl Google BigQuery Once you have configured the Google BigQuery user permissions , you can establish a connection between Atlan and Google BigQuery. To crawl metadata from Google BigQuery, review the order of operations and then complete the following steps. Select the source ​ To select Google BigQuery as your source: In the top right corner of any screen, click New and then click New Workflow . From the list of packages, select BigQuery Assets and click Setup Workflow . Provide credentials ​ To enter your Google BigQuery credentials: For Authentication , Service Account is the default selection. For Connectivity , choose how you want Atlan to connect to Google BigQuery: To connect using a public endpoint from Google, click Public Network . To connect through a private endpoint, click Private Network Link . Next, contact Atlan support to request the DNS name of the Private Service Connect endpoint that Atlan created for the integration: For Host , enter the DNS name of the Private Service Connect endpoint received from Atlan in the following format   - https://bigquery-<privateserver>.p.googleapis.com . Replace <privateserver> with the DNS name. For Port , 443 is the default selection. For Project Id , enter the value of project_id from the JSON for the service account you created . This project ID is only used to authenticate the connection. You can configure the crawler to extract more than just the specified project. For Service Account Json , paste in the entire JSON for the service account you created . For Service Account Email , enter the value of client_email from the JSON for the service account you created . At the bottom of the form, click the Test Authentication button to confirm connectivity to Google BigQuery using these details. When successful, at the bottom of the screen click the Next button. Configure the connection ​ To complete the Google BigQuery connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Google BigQuery data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Google BigQuery data, change Allow Data Preview to No . At the bottom of the screen, click the Next button to proceed. Configure the crawler ​ Before running the Google BigQuery crawler, you can further configure it. You can override the defaults for any of these options: For Filter Sharded Tables , keep No for the default configuration or click Yes to enable Atlan to catalog and display sharded tables with the same naming prefix as a single table in asset discovery and the lineage graph. Select assets you want to include in crawling in the Include Metadata field. (This will default to all assets, if none are specified.) Select assets you want to exclude from crawling in the Exclude Metadata field. (This will default to no assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. To import existing tags from Google BigQuery to Atlan , for Import Tags , click Yes . For Advanced Config , keep Default for the default configuration or click Custom if Atlan support has provided you with a custom control configuration. Enter the configuration into the Custom Config box. You can also enter {“ignore-all-case”: true} to enable crawling assets with case-sensitive identifiers. For Hidden Assets , keep No for the default configuration or click Yes to crawl metadata from your hidden datasets in Google BigQuery. Did you know? If a folder or project appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Google BigQuery crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous How to enable SSO for Google BigQuery Next Mine Google BigQuery Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/crawl-microsoft-azure-synapse-analytics",
    "content": "Connect data Data Warehouses Microsoft Azure Synapse Analytics Crawl Synapse Assets Crawl Microsoft Azure Synapse Analytics On this page Crawl Microsoft Azure Synapse Analytics Once you have configured the Microsoft Azure Synapse Analytics permissions , you can establish a connection between Atlan and Microsoft Azure Synapse Analytics. To crawl metadata from Microsoft Azure Synapse Ana lytics, review the order of operations and then complete the following steps. Select the source ​ To select Microsoft Azure Synapse Analytics as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Synapse Assets . In the right panel, click Setup Workflow . Provide your credentials ​ Choose your authentication method: In Basic authentication, you will need a username , password , and database name. In Service principal authentication, you will need a client ID, client secret, tenant ID , and database name. Basic authentication ​ To enter your Microsoft Azure Synapse Analytics credentials: For Host , enter the server name of your SQL pool . For Port , enter the port number where your SQL pool is available. For Username , enter the username you created when setting up user permissions. For Password , enter the password you created when setting up user permissions. For Database , enter the name of the database you want to crawl. Click the Test Authentication button to confirm connectivity to Microsoft Azure Synapse Analytics. Once authentication is successful, navigate to the bottom of the screen and click Next . Service principal authentication ​ Did you know? For service principal authentication, Atlan fetches Synapse pipeline metadata from the Azure Synapse Analytics REST API to generate lineage. Refer to What lineage does Atlan extract from Microsoft Azure Synapse Analytics? to learn more. To enter your Microsoft Azure Synapse Analytics credentials: For Host , enter the server name of your SQL pool . For Port , enter the port number where your SQL pool is available. For Client ID , enter the application (client) ID you copied for your service principal. For Client Secret , enter the client secret you copied for your service principal. For Tenant ID , enter the directory (tenant) ID you copied for your service principal. For Database , enter the name of the database you want to crawl. Click the Test Authentication button to confirm connectivity to Microsoft Azure Synapse Analytics. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Microsoft Azure Synapse Analytics connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Microsoft Azure Synapse Analytics crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Microsoft Azure Synapse Analytics crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up on-premises Microsoft Azure Synapse Analytics miner access Next What does Atlan crawl from Microsoft Azure Synapse Analytics? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/mine-microsoft-azure-synapse-analytics",
    "content": "Connect data Data Warehouses Microsoft Azure Synapse Analytics Get Started Mine Microsoft Azure Synapse Analytics On this page Mine Microsoft Azure Synapse Analytics danger Atlan currently only supports mining query history for dedicated SQL pools with the Microsoft Azure Synapse Analytics miner. Mining query history for serverless SQL pools is currently not supported. Once you have crawled assets from Microsoft Azure Synapse Analytics , you can mine query history to construct lineage. To mine lineage from Microsoft Azure Synapse Analy tics, review the order of operations and then complete the following steps. Select the miner ​ To select the Microsoft Azure Synapse Analytics miner: In the top right of any screen, navigate to + New and then click New workflow . Under Marketplace , from the filters along the top, click Miner . From the list of packages, select Synapse Miner and then click Setup Workflow . Configure the miner ​ To configure the Microsoft Azure Synapse Analytics miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run .) For Miner Extraction Method , choose your extraction method: In Query History , Atlan connects to your database and mines query history directly. In Offline , you will need to first mine query history yourself and make it available in S3 . Run the miner ​ To run the Microsoft Azure Synapse Analytics miner, after completing the steps above: To run the miner once, immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you will see lineage for Microsoft Azure Synapse Analytics assets that were created in Microsoft Azure Synapse Analytics! 🎉 Tags: connectors data crawl Previous Set up Microsoft Azure Synapse Analytics Next Set up on-premises Microsoft Azure Synapse Analytics miner access Select the miner Configure the miner Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-microsoft-azure-synapse-analytics",
    "content": "Connect data Data Warehouses Microsoft Azure Synapse Analytics Get Started Set up Microsoft Azure Synapse Analytics On this page Set up Microsoft Azure Synapse Analytics Atlan supports crawling the following with the Microsoft Azure Synapse Analytics package: Dedicated SQL pools (formerly SQL DW) Serverless SQL pools Atlan supports the following authentication methods for fetching metadata from Microsoft Azure Synapse Analytics: Basic authentication -  this method uses a username and password to fetch metadata. Service principal authentication -  this method requires a client ID, client secret, and tenant ID to fetch metadata. Basic authentication ​ Who can do this? You will need your Microsoft Azure Synapse Analytics administrator to run these commands   -  you may not have access yourself. Create a login ​ You must create a login within the master database for the new user. To create a login for the new user: CREATE LOGIN < login_name > WITH PASSWORD = '<password>' ; Replace <login_name> with the name of the login. Replace <password> with the password for the login. Create a user ​ You will need to create a new user for integrating with Atlan . To create a user for the newly created login : CREATE USER < username > FOR LOGIN < login_name > ; Replace <username> with the username to use when integrating Atlan. Replace <login_name> with the name of the login used in the previous step. Crawl assets and mine view lineage ​ You will need to connect to the target database that you want to crawl in Atlan . The following grant crawls all your Microsoft Azure Synapse Analytics assets and mines lineage for views. To grant the minimum permissions required to crawl assets and mine view lineage from a SQL pool: GRANT VIEW DEFINITION ON DATABASE :: < database_name > TO < username > ; Replace <database_name> with the name of the database. You must grant these permissions to all the databases you want to crawl in Atlan. Replace <username> with the username created above . Service principal authentication ​ Register app with Microsoft Entra ID ​ Who can do this? You will need your Cloud Application Administrator or Application Administrator to complete these steps   -  you may not have access yourself. This will be required if the creation of registered applications is not enabled for the entire organization. You will need to register your service principal application with Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret. To register your app with Microsoft Entra ID: Log in to the Azure portal . In the search bar, search for Microsoft Entra ID , and select it from the dropdown list. From the left menu of the Microsoft Entra ID page, click App registrations . From the toolbar on the App registrations page, click + New registration . On the Register an application page, for Name , enter a name for your service principal application and then click Register . On the homepage of your newly created application, from the Overview screen, copy the values for the following fields and store them in a secure location: Application (client) ID Directory (tenant) ID From the left menu of your newly created application page, click Certificates & secrets . On the Certificates & secrets page, under Client secrets , click + New client secret . In the Add a client secret screen, enter the following details: For Description , enter a description for your client secret. For Expiry , select when the client secret will expire. Click Add . On the Certificates & secrets page, under Client secrets , for the newly created client secret, click the clipboard icon to copy the Value and store it in a secure location. Create a service principal user ​ To create a service principal user: CREATE USER < service_principal_display_name > FROM EXTERNAL PROVIDER ; Replace <service_principal_display_name> with the name of the service principal you created in the previous step. Grant SQL permissions ​ To grant SQL permissions to the service principal : GRANT VIEW DEFINITION ON DATABASE :: < database_name > TO < service_principal_display_name > ; Replace <database_name> with the name of the database. Replace <service_principal_display_name> with the name of the service principal you created . Assign Synapse RBAC role ​ Who can do this? You will need your Synapse Administrator to complete these steps   -  you may not have access yourself. To assign a Synapse role-based access control (RBAC) role to the service principal: Open Synapse Studio and log in to your Synapse workspace. From the left menu of your Synapse workspace, click the Manage tab. Then from under Security , click Access control . From the options along the top of the Access control page, click + Add . In the Add role assignment tab, enter the following details: For Scope , select Workspace as the scope. For Role , select Synapse Artifact User as the Synapse RBAC role to assign. The Synapse Artifact User role provides read access to published code artifacts and their outputs. Although it can create new artifacts, it can neither publish changes nor run code without additional permissions. For Select user , search for and select the service principal you created . Click Apply to assign the Synapse RBAC role to the service principal. Mine query history ​ danger Atlan currently only supports mining query history for dedicated SQL pools with the Microsoft Azure Synapse Analytics miner . Mining query history for serverless SQL pools is currently not supported. To mine query history from Microsoft Azure Synapse Analytics, complete these steps. Enable query store ​ The Query Store is disabled by default for new Microsoft Azure Synapse Analytics databases. It stores 7 days of query history by default, which can be extended to 30 days. To enable the Query Store for mining query history in Atlan , run the following T-SQL command: ALTER DATABASE < database_name > SET QUERY_STORE = ON ; Replace <database_name> with the name of the database. Grant permissions ​ To mine query history, grant the following permissions: Basic authentication : GRANT VIEW DATABASE STATE TO < username > Replace <username> with the username you created for basic authentication. Service principal authentication : GRANT VIEW DATABASE STATE TO < service_principal_display_name > Replace <service_principal_display_name> with the name of the service principal you created for service principal authentication. Find your SQL pool server ​ To find the server name of your SQL pool for crawling Microsoft Azure Synapse Analytics : Open Synapse Studio . On the login page, select Synapse Workspace . From the left menu of your Synapse workspace, click the Manage tab. Then from under Analytics pools , click SQL pools . On the SQL pools page, under Name , select your SQL pool. In the Properties form, navigate to Workspace SQL endpoint and copy the server name of your SQL pool and save it in a temporary location. Tags: connectors data crawl authentication Previous Microsoft Azure Synapse Analytics Next Mine Microsoft Azure Synapse Analytics Basic authentication Service principal authentication Mine query history Find your SQL pool server"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/configure-snowflake-data-metric-functions",
    "content": "Connect data Data Warehouses Snowflake Manage Snowflake in Atlan Configure Snowflake data metric functions On this page Configure Snowflake data metric functions Private Preview To use system data metric functions (DMFs) from Snowflake, you need to configure your Snowflake setup. Prerequisites ​ Before proceeding with the configuration, make sure the following prerequisites are met: Snowflake editions -  Atlan data quality for Snowflake is only supported for Enterprise and Business Critical editions of Snowflake. Administrative access   -  the user configuring Snowflake must have an ACCOUNTADMIN role or equivalent administrative privileges. Dedicated warehouse   -  your organization must have a dedicated Snowflake warehouse for running data quality-related queries. Create roles in Snowflake ​ You will need to create the following two roles in Snowflake for the Atlan data quality integration: Data quality admin role ( dq_admin )   -  a high-privilege role responsible for managing DMF associations on tables and views. This role is used to create and manage the database objects required for data quality operations. Atlan data quality service role ( atlan_dq_service_role )   -  a service role that Atlan will use to interact with Snowflake objects related to data quality operations. This role will be assigned to the Atlan service user. Create data quality admin role ​ Run the following commands to create the dq_admin role and grant access to the Snowflake warehouse: CREATE OR REPLACE ROLE dq_admin ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE dq_admin ; Replace <warehouse-name> with the name of your dedicated Snowflake warehouse for running data quality-related queries. Create Atlan data quality service role ​ Run the following commands to create the atlan_dq_service_role and grant access to the Snowflake warehouse: CREATE OR REPLACE ROLE atlan_dq_service_role ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE atlan_dq_service_role ; Replace <warehouse-name> with the name of your dedicated Snowflake warehouse for running data quality-related queries. Create a user in Snowflake ​ A dedicated Snowflake user is required for Atlan to connect to your Snowflake instance. You will need to create this integration user and assign the Atlan data quality service role to this user. Refer to the Create a user documentation to create the new user. After creating the user, grant the Atlan data quality service role to the new user you created in Snowflake: GRANT ROLE atlan_dq_service_role TO USER < atlan_dq_user > ; Grant privileges ​ The following privileges must be granted to the roles created in Snowflake for the Atlan data quality integration: Privileges for data quality admin ​ Grant the dq_admin role the ability to create databases and access system DMFs in Snowflake: GRANT CREATE DATABASE ON ACCOUNT TO ROLE dq_admin ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE dq_admin ; Privileges for table owners ​ For each role that owns tables in your Snowflake environment, grant the following privileges: GRANT ROLE < table_owner > TO ROLE dq_admin ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE < table_owner > ; GRANT EXECUTE DATA METRIC FUNCTION ON ACCOUNT TO ROLE < table_owner > ; Replace <table_owner> with the role that owns Snowflake tables. Privileges for Atlan data quality service role ​ Grant the following privileges to the atlan_dq_service_role : GRANT APPLICATION ROLE SNOWFLAKE . DATA_QUALITY_MONITORING_VIEWER TO ROLE atlan_dq_service_role ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE atlan_dq_service_role ; GRANT EXECUTE TASK ON ACCOUNT TO ROLE atlan_dq_service_role ; GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE atlan_dq_service_role ; Set up required objects ​ Once you have created roles and granted the required privileges, you will need to create the necessary objects such as a dedicated database, schema, and stored procedure to be used for managing DMF operations. Change to the dq_admin role: USE ROLE dq_admin ; Create the database and schema in Snowflake for the Atlan data quality integration: CREATE DATABASE ATLAN_DQ ; CREATE SCHEMA ATLAN_DQ . SHARED ; The ATLAN_DQ database serves as a container for all objects related to the Atlan data quality integration. The ATLAN_DQ.SHARED schema provides a separate namespace for shared procedures and functions. Create the store procedure in Snowflake to manage DMFs: /** * Manages Data Metric Functions (DMF) attachment operations for Snowflake tabular entities. Runs with the privileges of the procedure owner. * @param {string} ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE) * @param {string} ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE) * @param {string} ENTITY_NAME - Fully qualified name of the entity (database.schema.name) * @param {string} [DMF_NAME=null] - Fully qualified name of the DMF (database.schema.name) * @param {string} [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations * @param {string} [SCHEDULE_TYPE=null] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED) * @param {string} [SCHEDULE_VALUE=null] - Schedule value based on type * @returns {string} - JSON string with operation status and result message */ CREATE OR REPLACE SECURE PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( ACTION STRING , ENTITY_TYPE STRING , ENTITY_NAME STRING , DMF_NAME STRING DEFAULT NULL , DMF_ARGUMENTS_JSON STRING DEFAULT '[]' , SCHEDULE_TYPE STRING DEFAULT NULL , SCHEDULE_VALUE STRING DEFAULT NULL ) RETURNS STRING LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS $$ // -----------------------------------------------------UTILITY FUNCTIONS----------------------------------------------------- /** * Executes a SQL query with parameters * @param {string} sqlText - SQL statement to execute * @param {Array} [binds=[]] - Array of bind parameters for the query * @param {boolean} [returnFirstRow=false] - Whether to return only the first row * @returns {Object} Object containing execution result or error information */ function executeQuery ( sqlText , binds = [ ] , returnFirstRow = false ) { try { if ( ! sqlText ) return { isErrored: true , message: \"SQL Text is required\" , result: null , } ; const statement = snowflake . createStatement ( { sqlText , binds } ) ; const result = statement . execute ( ) ; const response = { isErrored: false , message: \"\" , result: null , } ; if ( returnFirstRow ) { response . result = result . next ( ) ? result : null ; return response ; } response . result = result ; return response ; } catch ( err ) { return { isErrored: true , message: ` ${err.code} - ${err.message} - ${sqlText} with binds: ${binds.join(\", \")} ` , result: null , } ; } } /** * Safely parses a JSON string * @param {string} jsonString - JSON string to parse * @returns {Object} Parsed JSON object or null if invalid */ function safelyParseJSON ( jsonString ) { try { return JSON . parse ( jsonString ) ; } catch ( err ) { return null ; } } /** * Validates a number within a range * @param {string} value - Number to validate * @param {number} min - Minimum value * @param {number} max - Maximum value * @returns {boolean} True if number is valid * @returns {boolean} False if number is invalid */ function isNumberValid ( value , min , max ) { const num = parseInt ( value , 10 ) ; return ! isNaN ( num ) && num = min && num & le ; max ; = \"max;\" } = \"}\" * * = \"**\" * = \"*\" escapes = \"Escapes\" and = \"and\" quotes = \"quotes\" a = \"a\" snowflake = \"Snowflake\" identifier = \"identifier\" @param = \"@param\" {string} = \"{string}\" - = \"-\" raw = \"Raw\" to = \"to\" normalize = \"normalize\" @returns = \"@returns\" properly = \"Properly\" quoted = \"quoted\" safe = \"safe\" for = \"for\" sql = \"SQL\" function = \"FUNCTION\" normalizeidentifier ( identifier ) = \"normalizeIdentifier(identifier)\" { = \"{\" return = \"return\" ` =\" ` \" ${identifier . replace ( = \"${identifier.replace(\" g , = \"g,\" ) } = \")}\" ` ;=\" ` ; \" retrieves=\" Retrieves \" all=\" all \" columns=\" columns \" given=\" given \" entity.=\" entity . \" validates=\" Validates \" that=\" that \" the=\" the \" entityexists=\" entityexists \" procedure=\" procedure \" owner=\" owner \" has=\" has \" access=\" access \" it.=\" it . \" entityname=\" entityName \" fully=\" Fully \" qualified=\" qualified \" entity=\" entity \" name=\" name \" {array}=\" {Array} \" array=\" Array \" of=\" of \" column=\" column \" objects=\" objects \" with=\" with \" datatype=\" dataType \" properties=\" properties \" @throws=\" @throws \" {error} = \"{Error}\" if = \"if\" doesn = \"doesn\" t = \"t\" exist = \"exist\" or = \"or\" is = \"is\" inaccessible = \"inaccessible\" getallcolumnsforentity ( entityname ) = \"getAllColumnsForEntity(entityName)\" const = \"const\" sqltext = \"SHOW COLUMNS IN IDENTIFIER(?)\" ; = \";\" binds = \"[entityName];\" result , = \"result,\" iserrored , = \"isErrored,\" message = \"message\" binds ) ; = \"binds);\" ( iserrored ) = \"(isErrored)\" exists = \"exists\" it = \"it\" throw = \"throw\" new = \"new\" error ( message ) ; = \"Error(message);\" while = \"while\" ( result . next ( ) ) = \"(result.next())\" name: = \"name:\" result . getcolumnvalue ( = \"result.getColumnValue(\" column_name = \"column_name\" ) , = \"),\" datatype: = \"dataType:\" json . parse ( result . getcolumnvalue ( = \"JSON.parse(result.getColumnValue(\" data_type = \"data_type\" ) ) . type , = \")).type,\" } ; = \"};\" ( column . datatype = \"==\" fixed = \"FIXED\" ) = \")\" column . datatype = \"NUMBER\" columns . push ( column ) ; = \"columns.push(column);\" columns ; = \"columns;\" dmf = \"DMF\" valid = \"valid\" dmfname = \"dmfName\" dmfarguments = \"dmfArguments\" arguments = \"arguments\" { boolean } = \"{boolean}\" whether = \"Whether\" invalid = \"invalid\" isdmfvalid ( dmfname , = \"isDMFValid(dmfName,\" dmfarguments ) = \"dmfArguments)\" identifier ( ? ) ( ${dmfarguments} ) ` ,=\"IDENTIFIER(?)(${dmfArguments}) ` , \" [dmfname],=\" [ dmfName ] , \" true);=\" true ) ; \" true;=\" true ; \" checks=\" Checks \" timezone=\" Timezone \" validate=\" validate \" true=\" True \" false=\" False \" istimezonevalid(timezone)=\" isTimezoneValid ( timezone ) \" result=\" executeQuery ( ` SELECT\" convert_timezone(?,=\"CONVERT_TIMEZONE(?,\" current_timestamp()) ` , = \"CURRENT_TIMESTAMP())`,\" [ timezone ] , = \"[timezone],\" ! result . iserrored ; = \"!result.isErrored;\" generates = \"Generates\" type = \"type\" signature = \"signature\" based = \"based\" on = \"on\" {object} = \"{Object}\" entitycolumnsmap = \"entityColumnsMap\" map = \"Map\" names = \"names\" in = \"in\" format = \"format\" > : [ { name: , dataType:  } ] } * @param {string} baseEntityName - Name of the base entity * @returns {string} DMF type signature * @throws {Error} If entity not found in the cache * / function generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) { if ( ! dmfArguments || ! dmfArguments . length ) return \"\" ; const baseEntityColumns = entityColumnsMap [ baseEntityName ] ; if ( ! baseEntityColumns ) { throw new Error ( ` Entity ${baseEntityName} not found in the cache ` ) ; } const baseEntityColumnArguments = dmfArguments . filter ( param = param . type = = = \"COLUMN\" ) . map ( param = { const column = baseEntityColumns . find ( col = col . name = = = param . name ) ; return column ? column . dataType : null ; } ) . join ( \", \" ) ; const baseEntityArguments = ` TABLE(${baseEntityColumnArguments}) ` ; const referencedEntityArguments = dmfArguments . filter ( param = param . type = = = \"TABLE\" ) . map ( entityParam = { const entityName = entityParam . name ; const entityColumns = entityColumnsMap [ entityName ] ; if ( ! entityColumns ) { throw new Error ( ` Entity ${entityName} not found in the cache ` ) ; } const columnTypes = entityParam . nested . map ( nestedParam = { const column = entityColumns . find ( col = col . name = = = nestedParam . name ) ; return column ? column . dataType : null ; } ) . filter ( Boolean ) . join ( \", \" ) ; return ` TABLE(${columnTypes}) ` ; } ) ; const arguments = [ baseEntityArguments , . . . referencedEntityArguments ] . join ( \", \" ) ; return arguments ; } /** * Generates DMF arguments for SQL statements * @param {string} dmfArguments - Array of DMF arguments * @returns {string} Formatted DMF arguments for SQL statements */ function generateDMFColumnArguments ( dmfArguments ) { return dmfArguments . map ( param = { if ( param . type = = = \"COLUMN\" ) { return normalizeIdentifier ( param . name ) ; } // Handle TABLE type with nested columns return ` ${normalizeIdentifier(param.name)}(${ param.nested .map(nested = normalizeIdentifier(nested.name)) .join(\", \") }) ` ; } ) . join ( \", \" ) ; } // -----------------------------------------------------VALIDATION FUNCTIONS----------------------------------------------------- /** * Validates that mandatory arguments are provided and valid * @throws {Error} If any mandatory argument is missing or invalid */ function validateMandatoryArguments ( ) { const VALID_ACTIONS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" , \"UPDATE_SCHEDULE\" ] ) ; const VALID_ENTITY_TYPES = new Set ( [ \"TABLE\" , \"VIEW\" , \"MATERIALIZED VIEW\" , \"EXTERNAL TABLE\" , \"ICEBERG TABLE\" ] ) ; const DMF_OPS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] ) ; const VALID_SCHEDULE_TYPES = new Set ( [ \"MINUTES\" , \"CRON\" , \"ON_DATA_CHANGE\" , \"NOT_SCHEDULED\" ] ) ; const SCHEDULE_TYPES_THAT_REQUIRE_VALUE = new Set ( [ \"MINUTES\" , \"CRON\" ] ) ; if ( ! VALID_ACTIONS . has ( ACTION ) ) throw new Error ( ` Invalid ACTION: \"${ACTION}\". Valid options are ${Array.from(VALID_ACTIONS).join(\", \")} ` ) ; if ( ! VALID_ENTITY_TYPES . has ( ENTITY_TYPE ) ) throw new Error ( ` Invalid ENTITY_TYPE: \"${ENTITY_TYPE}\". Valid options are ${Array.from(VALID_ENTITY_TYPES).join(\", \")} ` ) ; if ( DMF_OPS . has ( ACTION ) && ! DMF_NAME ) throw new Error ( \"DMF_NAME is required for DMF related actions\" ) ; if ( ACTION = = = \"UPDATE_SCHEDULE\" ) { if ( ! SCHEDULE_TYPE ) throw new Error ( \"SCHEDULE_TYPE is required for SCHEDULE action\" ) ; if ( ! VALID_SCHEDULE_TYPES . has ( SCHEDULE_TYPE ) ) throw new Error ( ` Invalid schedule type: \"${SCHEDULE_TYPE}\". Valid options are ${Array.from(VALID_SCHEDULE_TYPES).join(\", \")} ` ) ; if ( SCHEDULE_TYPES_THAT_REQUIRE_VALUE . has ( SCHEDULE_TYPE ) && ! SCHEDULE_VALUE ) throw new Error ( \"SCHEDULE_VALUE is required for SCHEDULE action\" ) ; } } /** * Parses a fully qualified name into its components * @param {string} fullyQualifiedName - Fully qualified name to parse * @returns {Object} Object with database, schema, and name properties * @throws {Error} If invalid fully qualified name */ function validateFullyQualifiedName ( fullyQualifiedName ) { const parts = fullyQualifiedName . split ( \".\" ) . map ( part = part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length != = 3 ) throw new Error ( ` Invalid fully qualified name: ${fullyQualifiedName}. Expected format: database.schema.name ` ) ; } /** * Validates the structure of DMF arguments JSON * @param {string} rawDMFArguments - Raw JSON string of DMF arguments * @throws {Error} If DMF arguments structure is invalid */ function validateDMFArgumentsStructure ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) = param . type = = = \"TABLE\" ) ; if ( referencedEntities . length 1 ) throw new Error ( \"Only one referenced entity is allowed\" ) ; const validationFunctions = { arrayItem: ( param ) = [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) && param . name , nestedItem: ( param ) = [ \"COLUMN\" ] . includes ( param . type ) && param . name , } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; if ( referencedEntities . length 0 ) { for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } } /** * Validates that all specified columns exist in an entity * @param {Array} columnsToCheck - Array of column names to validate * @param {Array} entityColumns - Array of column metadata from the entity * @param {string} entityName - Name of the entity for error message * @throws {Error} If any column doesn't exist in the entity */ function validateColumnsExistInEntity ( entityName , allColumnsInEntity , columnsToCheck ) { for ( const column of columnsToCheck ) { if ( ! allColumnsInEntity . some ( col = col . name = = = column ) ) throw new Error ( ` Column ${column} not found in entity ${entityName} ` ) ; } } /** * Validates that all provided identifiers exist and are accessible * Checks entity names, column names, and DMF compatibility * @param {string} entityName - Fully qualified name of the entity * @param {string} dmfName - Fully qualified name of the DMF * @param {Array} dmfArguments - Array of DMF arguments * @throws {Error} If any identifier doesn't exist or is inaccessible */ function validateProvidedIdentifiers ( entityName , dmfName = \"\" , dmfArguments = [ ] ) { validateFullyQualifiedName ( entityName ) ; // Validate the provided entity names and store all the columns for each entity in a map const baseEntityName = entityName ; const baseEntityAllColumns = getAllColumnsForEntity ( entityName ) ; const entityColumnsMap = { [ baseEntityName ] : baseEntityAllColumns } ; const allReferencedEntities = dmfArguments . filter ( param = param . type = = = \"TABLE\" ) ; for ( const referencedEntity of allReferencedEntities ) { const columns = getAllColumnsForEntity ( referencedEntity . name ) ; entityColumnsMap [ referencedEntity . name ] = columns ; } // Valite all of the provided columns are valid and exist in their respective entities const allBaseEntityColumnsInArguments = dmfArguments . filter ( param = param . type = = = \"COLUMN\" ) . map ( param = param . name ) ; validateColumnsExistInEntity ( baseEntityName , baseEntityAllColumns , allBaseEntityColumnsInArguments ) ; for ( const referencedEntity of allReferencedEntities ) { const columnsInArguments = referencedEntity . nested . map ( nestedParam = nestedParam . name ) ; validateColumnsExistInEntity ( referencedEntity . name , entityColumnsMap [ referencedEntity . name ] , columnsInArguments ) ; } if ( dmfName ) { // Validate that the DMF is valid and exists const generatedTypeSignature = generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) ; isDMFValid ( dmfName , generatedTypeSignature ) ; } // All provided identifiers are valid, actually exist and are accessible to the procedure owner } /** * Validates CRON expression syntax * Performs detailed validation of all CRON components and timezones to protect against SQL injection * @param {string} cronExpression - CRON expression to validate * @throws {Error} If CRON expression is invalid */ function validateCronExpression ( cronExpression ) { if ( cronExpression . length 100 ) throw new Error ( \"Cron expression is too long\" ) ; const cronFields = cronExpression . trim ( ) . split ( / \\s + / ) ; if ( cronFields . length != = 6 ) throw new Error ( \"Invalid cron expression. Expected 6 fields\" ) ; const [ minute , hour , dayOfMonth , month , dayOfWeek , timezone ] = cronFields ; const isTimezoneValidResult = isTimezoneValid ( timezone ) ; if ( ! isTimezoneValidResult ) throw new Error ( \"Invalid timezone provided in the cron expression\" ) ; const regexPatterns = { minute : / ^ ( \\ * | \\d + | \\ * \\ / \\d + | \\d + \\ - \\d + | \\d + ( , \\d + ) * ) $ / , hour : / ^ ( \\ * | \\d + | \\ * \\ / \\d + | \\d + \\ - \\d + | \\d + ( , \\d + ) * ) $ / , dayOfMonth: / ^ ( \\ * | L | \\d + | \\ * \\ / \\d + | \\d + \\ - \\d + | \\d + ( , \\d + ) * ) $ / , month : / ^ ( \\ * | \\d + | JAN | FEB | MAR | APR | MAY | JUN | JUL | AUG | SEP | OCT | NOV | DEC | \\ * \\ / \\d + | \\d + \\ - \\d + | [ A - Z ] { 3 }\\ - [ A - Z ] { 3 } | \\d + ( , \\d + ) * | ( [ A - Z ] { 3 } ( , [ A - Z ] { 3 } ) * ) ) $ / i , dayOfWeek: / ^ ( \\ * | \\d + | SUN | MON | TUE | WED | THU | FRI | SAT | \\d + L | [ A - Z ] { 3 }L | \\ * \\ / \\d + | \\d + \\ - \\d + | [ A - Z ] { 3 }\\ - [ A - Z ] { 3 } | \\d + ( , \\d + ) * | ( [ A - Z ] { 3 } ( , [ A - Z ] { 3 } ) * ) ) $ / i , } ; if ( minute . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( minute , 0 , 59 ) ) throw new Error ( \"Invalid minute value\" ) ; if ( hour . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( hour , 0 , 23 ) ) throw new Error ( \"Invalid hour value\" ) ; if ( dayOfMonth . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfMonth , 1 , 31 ) ) throw new Error ( \"Invalid day of month value\" ) ; if ( month . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( month , 1 , 12 ) ) throw new Error ( \"Invalid month value\" ) ; if ( dayOfWeek . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfWeek , 0 , 6 ) ) throw new Error ( \"Invalid day of week value\" ) ; if ( ! regexPatterns . minute . test ( minute ) || ! regexPatterns . hour . test ( hour ) || ! regexPatterns . dayOfMonth . test ( dayOfMonth ) || ! regexPatterns . month . test ( month ) || ! regexPatterns . dayOfWeek . test ( dayOfWeek ) ) throw new Error ( \"Invalid cron expression\" ) ; } /** * Validates schedule-specific arguments * Ensures schedule type and value are compatible and valid * @throws {Error} If schedule configuration is invalid */ function validateProvidedArgumentsForSchedule ( ) { const VALID_MINUTES = new Set ( [ \"5\" , \"15\" , \"30\" , \"60\" , \"720\" , \"1440\" ] ) ; if ( SCHEDULE_TYPE = = = \"MINUTES\" && ! VALID_MINUTES . has ( SCHEDULE_VALUE ) ) throw new Error ( ` Invalid SCHEDULE_VALUE for MINUTES. Valid options are ${Array.from(VALID_MINUTES).join(\", \")} ` ) ; if ( SCHEDULE_TYPE = = = \"CRON\" ) validateCronExpression ( SCHEDULE_VALUE ) ; // SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE } /** * Validates all provided arguments * Performs comprehensive validation on input parameters * @throws {Error} If any validation fails */ function validateAllArguments ( ) { validateMandatoryArguments ( ) ; // Validates all mandatory arguments are provided in the correct format if ( ACTION = = = \"UPDATE_SCHEDULE\" ) validateProvidedArgumentsForSchedule ( ) ; // Validates the provided schedule type and value else { validateDMFArgumentsStructure ( DMF_ARGUMENTS_JSON ) ; } validateProvidedIdentifiers ( ENTITY_NAME , DMF_NAME , safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; // All provided arguments are valid and legal } // -----------------------------------------------------MAIN FUNCTION----------------------------------------------------- /** * Main function to manage DMF operations * Validates all arguments and executes the main logic * @returns {string} JSON string with operation status and result message */ function main ( ) { validateAllArguments ( ) ; // Validate all provided arguments // If the provided arguments are valid, proceed with the main logic const dmfArguments = generateDMFColumnArguments ( safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; const SQL_TEMPLATES = { ATTACH_DMF: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} ADD DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) ` , DETACH_DMF: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} DROP DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) ` , SUSPEND_DMF: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} MODIFY DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) SUSPEND ` , RESUME_DMF: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} MODIFY DATA METRIC FUNCTION ${DMF_NAME} ON (${dmfArguments}) RESUME ` , UPDATE_SCHEDULE: { MINUTES: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = '${SCHEDULE_VALUE} MINUTE' ` , CRON: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = 'USING CRON ${SCHEDULE_VALUE}' ` , ON_DATA_CHANGE: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES' ` , NOT_SCHEDULED: ` ALTER ${ENTITY_TYPE} ${ENTITY_NAME} UNSET DATA_METRIC_SCHEDULE ` , } , } ; let sqlText = \"\" ; let returnMessage = \"\" ; if ( ACTION = = = \"UPDATE_SCHEDULE\" ) { sqlText = SQL_TEMPLATES [ ACTION ] [ SCHEDULE_TYPE ] ; returnMessage = ` Successfully updated schedule for ${ENTITY_NAME} to ${SCHEDULE_TYPE} ${SCHEDULE_VALUE} ` ; } else { sqlText = SQL_TEMPLATES [ ACTION ] ; returnMessage = ` ACTION: ${ACTION} performed successfully on ${ENTITY_NAME} with DMF: ${DMF_NAME} and DMF Arguments: ${dmfArguments} ` ; } const result = executeQuery ( sqlText ) ; return JSON . stringify ( { isSuccessful: ! result . isErrored , message: result . isErrored ? result . message : returnMessage , } ) ; } // Execute the main function and return the result try { return main ( ) ; } catch ( err ) { return JSON . stringify ( { isSuccessful: false , message: err . message , } ) ; } $$ ; Grant access to Atlan data quality service role ​ Finally, grant permissions to the Atlan data quality service role to access the database, schema, and stored procedure you created in Snowflake: USE ROLE dq_admin ; GRANT USAGE ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ . SHARED TO ROLE atlan_dq_service_role ; GRANT USAGE ON PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING ) TO ROLE atlan_dq_service_role ; GRANT CREATE SCHEMA ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role ; Tags: data integration setup configuration Previous Manage Snowflake tags Next Multiple tag values and concatenation Prerequisites Create roles in Snowflake Create a user in Snowflake Grant privileges Set up required objects Grant access to Atlan data quality service role"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/microsoft-azure-synapse-analytics/how-tos/set-up-on-premises-microsoft-azure-synapse-analytics-miner-access",
    "content": "Connect data Data Warehouses Microsoft Azure Synapse Analytics Get Started Set up on-premises Microsoft Azure Synapse Analytics miner access On this page Set up on-premises Microsoft Azure Synapse Analytics miner access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Microsoft Azure Synapse Analytics instance details, including credentials. In some cases you will not be able to expose your Microsoft Azure Synapse Analytics instance for Atlan to mine query history from the Query Store . For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the mining of query history from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Once you have mined query history on-premises and uploaded the results to S3 , you can mine query history in Atlan: How to mine Microsoft Azure Synapse Analytics Prerequisites ​ To mine query history from your on-premises Microsoft Azure Synapse Analytics instance, you will need to use Atlan's synapse-miner tool. Did you know? Atlan uses exactly the same synapse-miner behind the scenes when it connects to Microsoft Azure Synapse Analytics in the cloud. danger If you have already installed Docker Compose, ensure that the version is 1.17.0 or higher. It is good practice to upgrade the tool to the latest available version. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the synapse-miner tool ​ To get the synapse-miner tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to mine Microsoft Azure Synapse Analytics: sudo docker load -i /path/to/synapse-miner-master.tar Get the compose file ​ Atlan provides you with a configuration file for the synapse-miner tool. This is a Docker compose file . To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Microsoft Azure Synapse Analytics instance. The file is docker-compose.yml . Define database connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Microsoft Azure Synapse Analytics connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services ​ For each on-premises Microsoft Azure Synapse Analytics instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *mine environment: <<: *synapsedb USERNAME: <USERNAME> PASSWORD: <PASSWORD> HOST: <HOST> PORT: <PORT> DATABASE: <DATABASE> volumes: - ./output/connection-name:/output Replace connection-name with the name of your connection. <<: *mine tells the synapse-miner tool to run. environment contains all parameters for the tool: USERNAME -  specify the database username. PASSWORD -  specify the database password. HOST -  specify the database host. PORT -  specify the database port. DATABASE -  specify the database name. volumes specifies where to store results. In this example, the miner will store results in the ./output/connection-name folder on the local file system. You can add as many Microsoft Azure Synapse Analytics connections as you want. Did you know? Docker's documentation describes the services format in more detail. Secure credentials ​ Using local files ​ danger If you decide to keep Microsoft Azure Synapse Analytics credentials in plaintext files, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. Using Docker secrets ​ To create and use Docker secrets: Create a new Docker secret: printf \"This is a secret password\" | docker secret create my_database_password - At the top of your compose file, add a secrets element to access your secret: secrets: my-database-password: external: true Within the service section of the compose file, add a new secrets element and specify PASSWORD_SECRET_PATH to use it as a password. Example ​ Let's explain in detail with an example: secrets: my-database-password: external: true x-templates: # ... services: my-database: <<: *mine environment: <<: *synapsedb USERNAME: <USERNAME> PASSWORD_SECRET_PATH: \"/run/secrets/my_database_password\" # ... volumes: # ... secrets: - my-database-password volumes: jars: Tags: connectors data Previous Mine Microsoft Azure Synapse Analytics Next Crawl Microsoft Azure Synapse Analytics Prerequisites Get the compose file Define database connections Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-an-aws-private-network-link-to-snowflake",
    "content": "Connect data Data Warehouses Snowflake Get Started Set up an AWS private network link to Snowflake On this page Set up an AWS private network link to Snowflake AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Snowflake and Atlan, when you use our Single Tenant SaaS deployment. Who can do this? You will need Snowflake Support, and probably your Snowflake administrator involved   -  you may not have access or the tools to run these tasks. Prerequisites ​ Snowflake must be setup with Business Critical Edition (or higher). Open a ticket with Snowflake Support to enable PrivateLink for your Snowflake account. Snowflake support will take 1-2 days to review and enable PrivateLink. If you are using IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support request to do so. (For all details, see the Snowflake documentation .) Fetch PrivateLink information ​ Log in to snowCLI using the ACCOUNTADMIN account, and run the following commands: use role accountadmin; select system$get_privatelink_config(); This will produce output like the following (formatted here for readability): { \"privatelink-account-name\":\"abc123.ap-south-1.privatelink\", \"privatelink-vpce-id\":\"com.amazonaws.vpce.ap-south-1.vpce-svc-257a4d536bd8e3594\", \"privatelink-account-url\":\"abc123.ap-south-1.privatelink.snowflakecomputing.com\", \"regionless-privatelink-account-url\":\"xyz789-abc123.privatelink.snowflakecomputing.com\", \"privatelink_ocsp-url\":\"ocsp.abc123.ap-south-1.privatelink.snowflakecomputing.com\", \"privatelink-connection-urls\":\"[]\" } Share details with Atlan support team ​ Share the following values with the Atlan support team: privatelink-account-name privatelink-vpce-id privatelink-account-url privatelink_ocsp-url Atlan support will finish the configuration on the Atlan side using these values. Support will then provide the Snowflake PrivateLink endpoint back to you. When you use this endpoint in the configuration for crawling and mining , Atlan will connect to Snowflake over the PrivateLink. Tags: atlan documentation Previous Set up Snowflake Next Set up an Azure private network link to Snowflake Prerequisites Fetch PrivateLink information Share details with Atlan support team"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/crawl-snowflake",
    "content": "Connect data Data Warehouses Snowflake Crawl Snowflake Assets Crawl Snowflake On this page Crawl Snowflake Once you have configured the Snowflake user permissions , you can establish a connection between Atlan and Snowflake. (If you are also using AWS PrivateLink or Azure Private Link for Snowflake, you will need to set that up first, too.) To crawl metadata from Snowflake, review the order of operations and then complete the following steps. Select the source ​ To select Snowflake as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Snowflake Assets and click on Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . This is currently only supported when using the information schema extraction method to fetch metadata with basic authentication . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your Snowflake credentials: For Account Identifiers (Host) , enter the hostname, AWS PrivateLink endpoint , or Azure Private Link endpoint for your Snowflake instance. For Authentication , choose the method you configured when setting up the Snowflake user : For Basic authentication, enter the Username and Password you configured in either Snowflake or the identity provider. info 💪 Did you know? Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. For Keypair authentication, enter the Username , Encrypted Private Key , and Private Key Password you configured. Atlan only supports encrypted private keys with a non-empty passphrase   -  generally recommended as more secure. An empty passphrase will result in workflow failures. To generate an encrypted private key, refer to Snowflake documentation . For Okta SSO authentication, enter the Username , Password , and Authenticator you configured. The Authenticator will be the Okta URL endpoint of your Okta account , typically in the form of https://<okta_account_name>.okta.com . For Role , select the Snowflake role through which the crawler should run. For Warehouse , select the Snowflake warehouse in which the crawler should run. Click Test Authentication to confirm connectivity to Snowflake using these details. Once successful, at the bottom of the screen, click Next . Offline extraction method ​ Atlan supports the offline extraction method for fetching metadata from Snowflake. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. If you are reusing Atlan's S3 bucket, you can leave this blank. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Configure the connection ​ To complete the Snowflake connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Snowflake data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Snowflake data, change Allow Data Preview to No . At the bottom of the screen, click Next to proceed. Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from Snowflake. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the crawler ​ danger When modifying an existing Snowflake connection, switching to a different extraction method will delete and recreate all assets in the existing connection. If you'd like to change the extraction method, contact Atlan support for assistance. Before running the Snowflake crawler, you can further configure it. You must select the Extraction method you configured when you set up Snowflake : For Information Schema method , keep the default selection. Change to Account Usage method and specify the following: Database Name of the copied Snowflake database Schema Name of the copied ACCOUNT_USAGE schema Incremental extraction Public preview - Toggle incremental extraction for faster and more efficient metadata extraction. You can override the defaults for any of the remaining options: For Asset selection , select a filtering option: To select the assets you want to include in crawling, click Include by hierarchy and filter for assets down to the database or schema level. (This will default to all assets, if none are specified.) To have the crawler include Databases , Schemas , or Tables & Views based on a naming convention, click Include by regex and specify a regular expression   -  for example, specifying ATLAN_EXAMPLE_DB.* for Databases will include all the matching databases and their child assets. To select the assets you want to exclude from crawling, click Exclude by hierarchy and filter for assets down to the database or schema level. (This will default to no assets, if none are specified.) To have the crawler ignore Databases , Schemas , or Tables & Views based on a naming convention, click Exclude by regex and specify a regular expression   -  for example, specifying ATLAN_EXAMPLE_TABLES.* for Tables & Views will exclude all the matching tables and views. Click + to add more filters. If you add multiple filters, assets will be crawled based on matching all the filtering conditions you have set. To exclude lineage for views in Snowflake, change View Definition Lineage to No . To import tags from Snowflake to Atlan , change Import Tags to Yes . Note the following: If using the Account Usage extraction method, grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. If using the Information Schema extraction method, note that Snowflake stores all tag objects in the ACCOUNT_USAGE schema. You will need to grant permissions on the account usage schema instead to import tags from Snowflake. danger Object tagging in Snowflake currently requires Enterprise Edition or higher . If your organization does not have Enterprise Edition or higher and you try to import Snowflake tags to Atlan, the Snowflake connection will fail with an error   -  unable to retrieve tags. For Control Config , keep Default for the default configuration or click Custom to further configure the crawler: If you have received a custom crawler configuration from Atlan support, for Custom Config , enter the value provided. You can also: Enter {\"ignore-all-case\": true} to enable crawling assets with case-sensitive identifiers. For Enable Source Level Filtering , click True to enable schema-level filtering at source or keep False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. For Exclude tables with empty data , change to Yes to exclude any tables and corresponding columns without any data. For Exclude views , change to Yes to exclude all views from crawling. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Snowflake crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Note that the Atlan crawler will currently skip any unsupported data types to ensure a successful workflow run. Tags: connectors data crawl Previous How to enable Snowflake OAuth Next Mine Snowflake Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-an-azure-private-network-link-to-snowflake",
    "content": "Connect data Data Warehouses Snowflake Get Started Set up an Azure private network link to Snowflake On this page Set up an Azure private network link to Snowflake Azure Private Link creates a secure, private connection between services running in Azure. This document describes the steps to set this up between Snowflake and Atlan. Who can do this? You will need Snowflake Support, and probably your Snowflake administrator involved   -  you may not have access or the tools to run these tasks. Prerequisites ​ Snowflake must be set up with Business Critical Edition (or higher). Open a ticket with Snowflake Support to enable Azure Private Link for your Snowflake account. Snowflake support will take 1-2 days to review and enable Azure Private Link. If you are using IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support request to do so. (For all details, see the Snowflake documentation .) Fetch Private Link information ​ Log in to snowCLI using the ACCOUNTADMIN account, and run the following commands: use role accountadmin; select system$get_privatelink_config(); This will produce an output like the following (formatted here for readability): { \"regionless-snowsight-privatelink-url\": \"abc123.privatelink.snowflakecomputing.com\", \"privatelink-account-name\": \"abc123.west-europe.privatelink\", \"snowsight-privatelink-url\": \"abc123.west-europe.privatelink.snowflakecomputing.com\", \"privatelink-account-url\": \"abc123.west-europe.privatelink.snowflakecomputing.com\", \"privatelink-connection-ocsp-urls\": \"[]\", \"privatelink-pls-id\": \"abc123.westeurope.azure.privatelinkservice\", \"regionless-privatelink-account-url\": \"abc123.privatelink.snowflakecomputing.com\", \"privatelink_ocsp-url\": \"ocsp.abc123.west-europe.privatelink.snowflakecomputing.com\", \"privatelink-connection-urls\": \"[]\" } Share details with Atlan support team ​ Share the following values with the Atlan support team : regionless-snowsight-privatelink-url privatelink-account-name snowsight-privatelink-url privatelink-account-url privatelink-connection-ocsp-urls privatelink-pls-id regionless-privatelink-account-url privatelink_ocsp-url privatelink-connection-urls Atlan support will finish the configuration on the Atlan side using these values. Support will then provide you with the Snowflake private endpoint resource ID and Azure token for you to approve the request. Approve the endpoint connection request ​ Log in to snowCLI using the ACCOUNTADMIN account, and run the following commands: use role accountadmin; SELECT SYSTEM$AUTHORIZE_PRIVATELINK ( '/subscriptions/26d.../resourcegroups/sf-1/providers/microsoft.network/privateendpoints/test-self-service', 'eyJ...' ); Snowflake will return an Account is authorized for PrivateLink. message to confirm successful authorization. The status of the private endpoint in Atlan will then change to Approved . When you use this endpoint in the configuration for crawling and mining Snowflake, Atlan will connect to Snowflake over the Private Link. (Optional) Configure private endpoint for internal stages ​ This is only required if you're using Snowflake internal stages. To enable Atlan to securely access your Snowflake internal stages, Atlan will require a private endpoint to your Azure storage account. Refer to Snowflake documentation to learn more. To configure an Azure private endpoint to access Snowflake internal stages: Open the Azure portal and navigate to your Azure Storage account. On the Storage accounts page, select the storage account to connect. From the storage account menu, click Overview . In the Resource JSON form, for Resource ID , click the clipboard icon to copy the value and contact Atlan support to share the value . (Atlan support will finish the configuration on the Atlan side using the Resource ID value and contact you to confirm endpoint creation.) From the storage account menu, click Security + networking and then click Networking . On the Networking page, change to the Private endpoint connections tab and then approve the endpoint connection request from Atlan. Tags: atlan documentation Previous Set up an AWS private network link to Snowflake Next How to enable Snowflake OAuth Prerequisites Fetch Private Link information Share details with Atlan support team Approve the endpoint connection request (Optional) Configure private endpoint for internal stages"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/enable-snowflake-oauth",
    "content": "Connect data Data Warehouses Snowflake Get Started How to enable Snowflake OAuth On this page Enable  Snowflake OAuth Atlan supports Snowflake OAuth-based authentication for Snowflake connections. Once the integration has been completed, Atlan will generate a trusted secure token with Snowflake. This will allow Atlan to authenticate users with Snowflake on their behalf to: Query data with Snowflake OAuth credentials View sample data with Snowflake OAuth credentials Configure Snowflake OAuth in Atlan ​ Who can do this? You will need to be a connection admin in Atlan to complete these steps. You will also need inputs and approval from your Snowflake account administrator . To configure Snowflake OAuth on a Snowflake connection, from Atlan: From the left menu of any screen, click Assets . From the Assets page, click the Connector filter, and from the dropdown, click Snowflake . From the pills below the search bar at the top of the screen, click Connection . From the list of results, select a Snowflake connection to enable Snowflake OAuth-based authentication. From the sidebar on the right, next to Connection settings , click Edit . In the Connection settings dialog: Under Allow query , for Authentication type , click Snowflake OAuth to enforce Snowflake OAuth credentials for querying data : For Authentication Required , click Copy Code to copy a security authorization code to execute it in Snowflake . Under Display sample data , for Source preview , click Snowflake OAuth to enforce Snowflake OAuth credentials for viewing sample data : If Snowflake OAuth-based authentication is enabled for querying data, the same connection details will be reused for viewing sample data. If a different authentication method is enabled for querying data, click Copy Code to copy a security authorization code to execute it in Snowflake . (Optional) Toggle on Enable data policies created at source to apply for querying in Atlan to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. At the bottom right of the Connection settings dialog, click Update . Did you know? The refresh token does not expire by default. Create a security integration in Snowflake ​ Who can do this? You will need your Snowflake account administrator to run these commands. You will also need to have an existing Snowflake connection in Atlan. To create a security integration in Snowflake: Log in to your Snowflake instance. From the top right of your Snowflake instance, click the + button, and then from the dropdown, click SQL Worksheet to open a new worksheet. In the query editor of your Snowflake SQL worksheet, paste the security authorization code you copied in Atlan . See a representative example below: CREATE SECURITY INTEGRATION < name > TYPE = EXTERNAL_OAUTH ENABLED = TRUE EXTERNAL_OAUTH_TYPE = OKTA EXTERNAL_OAUTH_ISSUER = 'https://<COMPANY>.okta.com/oauth2/<ID>' EXTERNAL_OAUTH_JWS_KEYS_URL = 'https://<COMPANY>.okta.com/oauth2/<ID>/v1/keys' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '<snowflake_account_url' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_ANY_ROLE_MODE = 'ENABLE' ; EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'EMAIL_ADDRESS' Run the security integration in Snowflake. (Optional) To allow the ACCOUNTADMIN , ORGADMIN , or SECURITYADMIN role to query with Snowflake OAuth-based authentication, add and run the following command to set account-level permissions: ALTER ACCOUNT SET EXTERNAL_OAUTH_ADD_PRIVILEGED_ROLES_TO_BLOCKED_LIST = FALSE ; Your users will now be able to run queries and view sample data using their Snowflake OAuth credentials! 🎉 Did you know? You can refer to troubleshooting connector-specific SSO authentication to troubleshoot any errors. Tags: connectors data integration authentication Previous Set up an Azure private network link to Snowflake Next Crawl Snowflake Configure Snowflake OAuth in Atlan Create a security integration in Snowflake"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/mine-snowflake",
    "content": "Connect data Data Warehouses Snowflake Crawl Snowflake Assets Mine Snowflake On this page Mine Snowflake Once you have crawled assets from Snowflake , you can mine its query history to construct lineage. To mine lineage from Snowflake, review the order of operations and then complete the following steps. Select the miner ​ To select the Snowflake miner: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Miner . From the list of packages, select Snowflake Miner and then click Setup Workflow . Configure the miner ​ To configure the Snowflake miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Miner Extraction Method , select Source , Agent , or see the separate instructions for the S3 miner . For Snowflake Database : If the connection is configured with access to the snowflake database , choose Default . If the connection can only access a separate cloned database , choose Cloned Database . If you are using a cloned database, enter the name of the cloned database in Database Name and the name of the cloned schema in Schema Name . For Start time , choose the earliest date from which to mine query history. info 💪 Did you know? The miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. To check for any permissions or other configuration issues before running the miner, click Preflight checks . At the bottom of the screen, click Next to proceed. Agent extraction method ​ Atlan supports using a Secure Agent for mining query history from Snowflake. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Snowflake data source by adding the secret keys for your secret store. For details on the required fields, refer to the connection configuration used when crawling Snowflake . Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. danger If running the miner for the first time, Atlan recommends setting a start date around three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. For all subsequent runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Configure the miner behavior ​ To configure the Snowflake miner behavior: (Optional) For Calculate popularity , keep True to retrieve usage and popularity metrics for your Snowflake assets from query history. For Excluded Users , type the names of users to be excluded while calculating usage metrics for Snowflake assets. Press Enter after each name to add more names. (Optional) For Advanced Config , keep Default for the default configuration or click Custom to configure the miner: If Atlan support has provided you with a custom control configuration, enter the configuration into the Custom Config box. You can also enter {“ignore-all-case”: true} to enable crawling assets with case-sensitive identifiers. For Popularity Window (days) , 90 days is the maximum limit. You can set a shorter popularity window of less than 90 days. Run the miner ​ To run the Snowflake miner, after completing the steps above: To run the miner once immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you will see lineage for Snowflake assets that were created in Snowflake between the start time and when the miner ran! 🎉 Tags: connectors data crawl setup Previous Crawl Snowflake Next Manage Snowflake tags Select the miner Configure the miner Configure the miner behavior Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/manage-snowflake-tags",
    "content": "Connect data Data Warehouses Snowflake Manage Snowflake in Atlan Manage Snowflake tags On this page Manage Snowflake tags Note that object tagging in Snowflake currently requires Enterprise Edition or higher . Atlan enables you to import your Snowflake tags , update your Snowflake assets with the imported tags, and push the tag updates back to Snowflake: Import tags   -  crawl Snowflake tags from Snowflake to Atlan Reverse sync   -  sync Snowflake tag updates from Atlan to Snowflake Once you've imported your Snowflake tags to Atlan: Your Snowflake assets in Atlan are automatically enriched with their Snowflake tags. Imported Snowflake tags are mapped to corresponding Atlan tags through case-insensitive name match   -  multiple Snowflake tags can be matched to a single tag in Atlan. You can also attach Snowflake tags , including tag values, to your Snowflake assets in Atlan   -  allowing you to categorize your assets at a more granular level. Atlan supports: Allowed values : attach an allowed value from a predefined list of values imported from Snowflake. Tag values: enter any value in Atlan while attaching or editing imported Snowflake tags on an asset. You can enable reverse sync to push any tag updates for your Snowflake assets back to Snowflake   -  including allowed and tag values added to assets in Atlan. You can filter your assets by Snowflake tags and tag and allowed values. Did you know? Enabling reverse sync only updates existing tags in Snowflake. It neither creates nor deletes any tags in Snowflake. Prerequisites ​ Did you know? Additional privileges are only required when using the information schema method for fetching metadata. This is because Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , any permissions required are already set. Account usage method ​ Before you can import tags from Snowflake, you need to do the following: Create tags or have existing tags in Snowflake. Grant the same permissions as required for crawling Snowflake assets to import tags and push updated tags to Snowflake. Information schema method ​ Before you can import tags from Snowflake, you need to do the following: Create tags or have existing tags in Snowflake. Grant additional permissions to import tags from Snowflake. Grant additional permissions to push updated tags to Snowflake. Import Snowflake tags to Atlan ​ Who can do this? You need to be an admin user in Atlan to import Snowflake tags to Atlan. You also need to work with your Snowflake administrator to grant additional permissions to import tags from Snowflake   -  you may not have access yourself. You can import your Snowflake tags to Atlan through one-way tag sync. The synced Snowflake tags are matched to corresponding tags in Atlan through case-insensitive name match and your Snowflake assets are enriched with their synced tags from Snowflake. To import Snowflake tags to Atlan, you can either: Create a new Snowflake workflow and configure the crawler to import tags. Modify the crawler's configuration for an existing Snowflake workflow to change Import Tags to Yes . If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan preserves those tags. Once the crawler has completed running, tags imported from Snowflake are available to use for tagging assets! 🎉 View Snowflake tags in Atlan ​ Once you've imported your Snowflake tags, you can view and manage your Snowflake tags in Atlan. To view Snowflake tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Snowflake to filter for tags imported from Snowflake. From the left menu under Tags , select a synced tag   -  synced tags display the Snowflake ❄️ icon next to the tag name. In the Overview section, you can view a total count of synced Snowflake tags. To the right of Overview , click Synced tags to view additional details   -  including tag name, description, tag values, total count of linked assets, connection, database, and schema names, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your Snowflake tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . You can't rename tags synced from Snowflake. Push tag updates to Snowflake ​ Who can do this? Any admin or member user in Atlan can configure reverse sync for tag updates to Snowflake. You also need to work with your Snowflake administrator to grant additional permissions to push updates -  you may not have access yourself. Did you know? Reverse sync is currently only available for imported Snowflake tags in Atlan. The imported tags display a Snowflake ❄️ icon next to the tag name. If using the account usage method , expect a data latency of up to 3 hours for reverse tag sync to be successful. You can enable reverse sync for your imported Snowflake tags in Atlan and push all tag updates for your Snowflake assets back to source. Once you have enabled reverse sync, any Snowflake assets with tags updated in Atlan are also updated in Snowflake. To enable reverse sync for imported Snowflake tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click Snowflake to filter for tags imported from Snowflake. In the left menu under Tags , select a synced Snowflake tag   -  synced tags display the Snowflake ❄️ icon next to the tag name. On your selected tag page, to the right of Overview , click Synced tags . Under Synced tags , in the upper right, turn on Enable reverse sync to synchronize tag updates from Atlan to Snowflake. In the advanced settings, you can also enable concatenation to support multiple tag values for a single column. For detailed information about multiple tag values and concatenation, see Multiple tag values and concatenation . In the corresponding confirmation dialog, click Yes, enable it to enable reverse tag sync or click Cancel . Now when you attach Snowflake tags to your Snowflake assets in Atlan, these tag updates are also pushed to Snowflake! 🎉 Did you know? Enabling reverse sync won't trigger any updates in Snowflake until synced tags are attached to Snowflake assets in Atlan. For any questions about managing Snowflake tags, head over here . Tags: connectors data crawl Previous Mine Snowflake Next Configure Snowflake data metric functions Prerequisites Import Snowflake tags to Atlan View Snowflake tags in Atlan Push tag updates to Snowflake"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/data-warehouses/snowflake/how-tos/set-up-snowflake",
    "content": "Connect data Data Warehouses Snowflake Get Started Set up Snowflake On this page Set up Snowflake Who can do this? You need your Snowflake administrator to run these commands   -  you may not have access yourself. Create user and role in Snowflake ​ Create a role and user in Snowflake using the following commands: Create role ​ Create a role in Snowflake using the following commands: CREATE OR REPLACE ROLE atlan_user_role ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE atlan_user_role ; Replace <warehouse-name> with the default warehouse to use when running the Snowflake crawler . Atlan requires the following privileges to: OPERATE enables Atlan to start the virtual warehouse to fetch metadata if the warehouse has stopped. USAGE enables Atlan to show or list metadata from Snowflake. This in turn enables the Snowflake crawler to run the SHOW query. Create a user ​ Create a separate user to integrate into Atlan, using one of the following 3 options: With a public key in Snowflake ​ See Snowflake's official guide for details on generating an RSA key-pair . To create a user with a key-pair, replace the value for rsa_public_key with the public key and run the following: CREATE USER atlan_user rsa_public_key = 'MIIBIjANBgkqh...' default_role = atlan_user_role default_warehouse = '<warehouse-name>' display_name = 'Atlan' TYPE = 'SERVICE' Learn more about the SERVICE type property in Snowflake documentation . Did you know? Atlan only supports encrypted private keys with a non-empty passphrase   -  generally recommended as more secure. An empty passphrase results in workflow failures. To generate an encrypted private key, omit the -nocrypt option. Refer to Snowflake documentation to learn more. With a password in Snowflake ​ Did you know? Snowflake recommends transitioning away from basic authentication using username and password. Change to key-pair authentication for enhanced security. For any existing Snowflake workflows, you can modify the crawler configuration to update the authentication method. To create a user with a password, replace <password> and run the following: CREATE USER atlan_user password = '<password>' default_role = atlan_user_role default_warehouse = '<warehouse-name>' display_name = 'Atlan' TYPE = 'LEGACY_SERVICE' Learn more about the LEGACY_SERVICE type property in Snowflake documentation . Managed through your identity provider (IdP) Private preview ​ This method is currently only available if Okta is your IdP (Snowflake supports) authenticating natively through Okta : Create a user in your identity provider (IdP) and use federated authentication in Snowflake . The password for this user must be maintained solely in the IdP and multi-factor authentication (MFA) must be disabled. Grant role to user ​ To grant the atlan_user_role to the new user: GRANT ROLE atlan_user_role TO USER atlan_user ; Configure OAuth (client credentials flow) with Microsoft Entra ID ​ To configure OAuth authentication using Microsoft Entra ID (formerly Azure AD) with the client credentials flow: Follow Snowflake's documentation to: Register a new application in Microsoft Entra ID Collect the client ID , tenant ID , and client secret Add the required API permissions In Snowflake, create a security integration using the following: CREATE SECURITY INTEGRATION external_oauth_azure_ad TYPE = external_oauth ENABLED = true EXTERNAL_OAUTH_TYPE = azure EXTERNAL_OAUTH_ISSUER = '\\<AZURE_AD_ISSUER\\>' EXTERNAL_OAUTH_JWS_KEYS_URL = '\\<AZURE_AD_JWS_KEY_ENDPOINT\\>' EXTERNAL_OAUTH_AUDIENCE_LIST = ( '\\<SNOWFLAKE_APPLICATION_ID_URI\\>' ) EXTERNAL_OAUTH_TOKEN_USER_MAPPING_CLAIM = 'sub' EXTERNAL_OAUTH_SNOWFLAKE_USER_MAPPING_ATTRIBUTE = 'login_name' ; Replace the placeholders with actual values from your Azure AD app: <AZURE_AD_ISSUER> → Your tenant's OAuth 2.0 issuer URL <AZURE_AD_JWS_KEY_ENDPOINT> → Azure JWKs URI <SNOWFLAKE_APPLICATION_ID_URI> → Application ID URI of the Azure app Create a Snowflake user with a login name that exactly matches the Azure AD client object ID: CREATE USER oauth_svc_user WITH LOGIN_NAME = '\\<AZURE_AD_CLIENT_OBJECT_ID\\>' -- Use Azure client OBJECT ID DEFAULT_ROLE = \\ < ROLE\\ > DEFAULT_WAREHOUSE = \\ < WAREHOUSE\\ > ; Grant the configured role to this user: GRANT ROLE \\ < ROLE\\ > TO USER oauth_svc_user ; Choose metadata fetching method ​ Atlan supports two methods for fetching metadata from Snowflake   -  account usage and information schema. You should choose one of these two methods to set up Snowflake: Account usage Information schema Overview Simplified grants but some limitations in functionality Most comprehensive approach, more grant management required Method Views in the SNOWFLAKE database that display object metadata and usage metrics for your account System-defined views and table functions that provide extensive metadata for objects created in your account Permissions User role and account, single grant for SNOWFLAKE database User role and account, multiple grants per database Data latency 45 minutes to 3 hours (varies by view) None Historical data retention 1 year 7 days to 6 months (varies by view or table function) Asset extraction ACCOUNT_USAGE schema INFORMATION_SCHEMA schema View lineage ACCOUNT_USAGE schema INFORMATION_SCHEMA schema Table lineage ACCOUNT_USAGE schema ACCOUNT_USAGE schema Tag import ACCOUNT_USAGE schema ACCOUNT_USAGE schema Usage and popularity ACCOUNT_USAGE schema ACCOUNT_USAGE schema Metadata extraction time Varies by warehouse size. For example, 8 minutes for 10 million assets (recommended for extracting a large number of assets) Varies by warehouse size. For example, 2+ hours for 10 million assets Extraction limitations External table location data, procedures, and primary and foreign keys None Grant permissions for account usage method ​ danger If you want to set warehouse timeouts when using this method, set a large value initially for the workflow to succeed. Once the workflow has succeeded, adjust the value to be twice the extraction time. This method uses the views in SNOWFLAKE.ACCOUNT_USAGE (or a copied version of this schema) to fetch the metadata from Snowflake into Atlan. You can be more granular with permissions using this method, but there are limitations with this approach . To crawl assets, generate lineage, and import tags ​ If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Snowflake stores all tag objects in the ACCOUNT_USAGE schema. If you're using the account usage method to crawl metadata in Atlan or you have configured the Snowflake miner , you need to grant the same permissions to import tags as required for crawling Snowflake assets. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: USE ROLE ACCOUNTADMIN ; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role ; The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: GRANT USAGE ON DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT USAGE ON SCHEMA \"<copied-schema>\" IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; Replace <copied-database> with the copied Snowflake database name. Replace <copied-schema> with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. To crawl streams ​ To crawl streams, provide the following permissions: To crawl current streams: GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the Snowflake database name. To crawl future streams: GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the Snowflake database name. (Optional) To preview and query existing assets ​ To query and preview data within assets that already exist in Snowflake, add these permissions: GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets ​ To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. Grant permissions for information schema method ​ This method uses views in the INFORMATION_SCHEMA schema in Snowflake databases to fetch metadata. You still need to grant specific permissions to enable Atlan to crawl metadata, preview data, and query data with this method. To crawl existing assets ​ Grant these permissions to crawl assets that already exist in Snowflake. If you also want to be able to preview and query the data, you can use the preview and query existing assets permissions instead. Grant permissions to crawl existing assets: GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) Grant permissions to crawl functions: GRANT USAGE ON ALL FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be available in Atlan. (Repeat the statements for every database you wish to integrate into Atlan.) For secure user-defined functions (UDFs), grant OWNERSHIP permissions to retrieve metadata: GRANT OWNERSHIP ON FUNCTION < schema_name > . < udf_name > TO ROLE < role_name > ; Replace the placeholders with the appropriate values: <schema_name> : The name of the schema that contains the user-defined function (UDF). <udf_name> : The name of the secure UDF that requires ownership permissions. <role_name> : The role that gets assigned ownership of the secure UDF. Did you know? The statements given on this page apply to all schemas, tables, and views in a database in Snowflake. If you want to limit access to only certain objects, you can instead specify the exact objects individually as well. To crawl future assets ​ To crawl assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. To grant permissions at a database level: GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT USAGE ON FUTURE FUNCTIONS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to crawl in Atlan. (Repeat the statements for every database you want to integrate into Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: GRANT REFERENCES ON FUTURE TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE EXTERNAL TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON FUTURE MATERIALIZED VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database and <schema-name> with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) To mine query history for lineage ​ To also mine Snowflake's query history (for lineage), add these permissions. You can use either option: To mine query history direct from Snowflake's internal tables: USE ROLE ACCOUNTADMIN ; GRANT IMPORTED PRIVILEGES ON DATABASE snowflake TO ROLE atlan_user_role ; To mine query history from a cloned or copied set of tables, where you can also remove any sensitive data: GRANT USAGE ON DATABASE \"<cloned-database>\" TO ROLE atlan_user_role ; GRANT USAGE ON SCHEMA \"<cloned-database>\" . \"<cloned-account-usage-schema>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL TABLES IN SCHEMA \"<cloned-database>\" . \"<cloned-account-usage-schema>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL VIEWS IN SCHEMA \"<cloned-database>\" . \"<cloned-account-usage-schema>\" TO ROLE atlan_user_role ; Replace <cloned-database> with the name of the cloned database, and <cloned-account-usage-schema> with the name of the cloned schema containing account usage details. When using a cloned or copied version, verify that the table or view definition remains unchanged as in your SNOWFLAKE database. If the format is different. For example, a column is missing and it no longer qualifies as a clone. (Optional) To preview and query existing assets ​ To query and preview data within assets that already exist in Snowflake, add these permissions: GRANT USAGE ON DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT USAGE ON ALL SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON ALL STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON PIPE \"<pipe-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you wish to preview and query in Atlan.) (Optional) To preview and query future assets ​ To query and preview data within assets that may be created in the future in Snowflake, add these permissions. Again, if you want to also be able to preview and query the data for future assets, you can add the preview and query future assets permissions instead. GRANT USAGE ON FUTURE SCHEMAS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE EXTERNAL TABLES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN DATABASE \"<database-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database you want to be able to preview and query in Atlan. (Repeat the statements for every database you want to preview and query in Atlan.) danger For any future grants defined at a schema level to a different role, the schema-level grants take precedence over the database-level grants and any future grants defined at a database level to the Atlan role get ignored. To learn more, refer to Snowflake documentation . To grant permissions at a schema level: GRANT SELECT ON FUTURE TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE EXTERNAL TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE MATERIALIZED VIEWS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT SELECT ON FUTURE STREAMS IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; GRANT MONITOR ON FUTURE PIPES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database and <schema-name> with the schema you want to be able to preview and query in Atlan. (Repeat the statements for every database and schema you want to preview and query in Atlan.) danger Verify that all the assets you'd like to crawl are present in these grants by checking the grants on the user role defined for the crawler. (Optional) To import Snowflake tags ​ Snowflake stores all tag objects in the ACCOUNT_USAGE schema. Note that object tagging in Snowflake currently requires Enterprise Edition or higher . To import tags from Snowflake , grant these permissions: To use the default SNOWFLAKE database and ACCOUNT_USAGE schema and also mine Snowflake's query history (for lineage), grant these permissions: USE ROLE ACCOUNTADMIN ; GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE atlan_user_role ; The ACCOUNTADMIN role is required to grant privileges on the SNOWFLAKE database due to the following reasons: By default, only the ACCOUNTADMIN role can access the SNOWFLAKE database. To enable other roles to access the database and schemas and query the views, a user with the ACCOUNTADMIN role needs to grant IMPORTED PRIVILEGES on the SNOWFLAKE database to the desired roles. To use a copied or cloned version of this default schema, where you can also remove any sensitive data for security purposes, grant these permissions: GRANT USAGE ON DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT USAGE ON SCHEMA \"<copied-schema>\" IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE \"<copied-database>\" TO ROLE atlan_user_role ; Replace <copied-database> with the copied Snowflake database name. Replace <copied-schema> with the copied Snowflake ACCOUNT_USAGE schema name. The grants for the copied version can't be used on the original SNOWFLAKE database. This is because Snowflake produces an error that granular grants can't be given to imported databases. (Optional) To push updated tags to Snowflake ​ To push tags updated for assets in Atlan to Snowflake , grant these permissions: GRANT APPLY TAG ON ACCOUNT TO ROLE < role - name > ; You can learn more about tag privileges from Snowflake documentation . (Optional) To crawl dynamic tables ​ Atlan currently supports fetching metadata for dynamic tables using the MONITOR privilege. Refer to Snowflake documentation to learn more. To crawl existing dynamic tables from Snowflake: Grant permissions at a database level: GRANT MONITOR ON ALL DYNAMIC TABLES IN DATABASE \"<DATABASE_NAME>\" TO ROLE atlan_user_role ; Grant permissions at a schema level: GRANT MONITOR ON ALL DYNAMIC TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; To crawl future dynamic tables from Snowflake: Grant permissions at a database level: GRANT MONITOR ON FUTURE DYNAMIC TABLES IN DATABASE \"<DATABASE_NAME>\" TO ROLE atlan_user_role ; Grant permissions at a schema level: GRANT MONITOR ON FUTURE DYNAMIC TABLES IN SCHEMA \"<database-name>.<schema-name>\" TO ROLE atlan_user_role ; Replace <database-name> with the database and <schema-name> with the schema you want to crawl in Atlan. (Repeat the statements for every database and schema you want to integrate into Atlan.) (Optional) To crawl Iceberg tables ​ Atlan currently supports fetching metadata for Iceberg tables only for the information schema extraction method. To crawl Iceberg tables from Snowflake, grant the following permissions: To crawl existing Iceberg tables in Snowflake: GRANT REFERENCES ON ALL ICEBERG TABLES IN DATABASE < database - name > TO ROLE atlan_user_role ; To crawl future Iceberg tables in Snowflake: GRANT REFERENCES ON FUTURE ICEBERG TABLES IN DATABASE < database - name > TO ROLE atlan_user_role ; To crawl Iceberg catalog metadata for Iceberg tables in Snowflake: GRANT USAGE ON INTEGRATION < integration - name > TO ROLE atlan_user_role ; danger You must first grant permissions to crawl existing Iceberg tables for this permission to work on catalogs. You must also grant permissions to all the catalogs you want to crawl in Atlan individually. (Optional) To crawl Snowflake stages ​ Atlan supports crawling metadata for Snowflake stages using the USAGE and READ privileges. For more information, see the Snowflake documentation for INFORMATION_SCHEMA.STAGES. To crawl stages from Snowflake: Grant USAGE and READ privileges on all existing stages at the database level: GRANT USAGE ON ALL STAGES IN DATABASE < database_name > TO ROLE atlan_user_role ; GRANT READ ON ALL STAGES IN DATABASE < database_name > TO ROLE atlan_user_role ; Replace <database_name> with the name of your Snowflake database Replace <atlan_user_role> with the role you've granted Atlan to use for crawling. Grant USAGE and READ privileges on all future stages at the database level: GRANT USAGE ON FUTURE STAGES IN DATABASE < database_name > TO ROLE atlan_user_role ; GRANT READ ON FUTURE STAGES IN DATABASE < database_name > TO ROLE atlan_user_role ; Replace <database_name> with the name of your Snowflake database Replace <atlan_user_role> with the role you've granted Atlan to use for crawling. Allowlist the Atlan IP ​ If you are using the IP allowlist in your Snowflake instance, you must add the Atlan IP to the allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you aren't using the IP allowlist in your Snowflake instance, you can skip this step.) Tags: connectors data crawl Previous Snowflake Next Set up an AWS private network link to Snowflake Create user and role in Snowflake Choose metadata fetching method Grant permissions for account usage method Grant permissions for information schema method Allowlist the Atlan IP"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/how-tos/crawl-amazon-athena",
    "content": "Connect data Databases Query Engines Amazon Athena Crawl Athena Assets Crawl Amazon Athena On this page Crawl Amazon Athena Once you have configured the Amazon Athena access permissions , you can establish a connection between Atlan and Amazon Athena. (If you are also using a private network for Amazon Athena, you will need to set that up first , too.) To crawl metadata from Amazon Athena, review the order of operations and then complete the following steps. Select the source ​ To select Amazon Athena as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select Athena Assets , and click Setup Workflow . Provide credentials ​ To enter your Amazon Athena credentials: For Host enter the host name (or PrivateLink endpoint ) for your Amazon Athena instance. For Authentication choose the method you configured when setting up the Amazon Athena access permissions : At the bottom, enter the AWS Role ARN and S3 Output Location you configured. The S3 Output Location is where you store temporary Athena query results. For IAM User authentication, enter the AWS Access Key and AWS Secret Key you configured. For IAM Role authentication, enter the following: Set the AWS Role ARN to the ARN of the role you created in your AWS account . (Optional) Under External ID , click the Generate button. Click the button to the right of this field to copy the generated ID and use it in setting up your trust policy . (Optional) For Workgroup , you can override the default primary workgroup for tracking compute costs, granular permission controls, and more. Click Test Authentication to confirm connectivity to Amazon Athena. Once successful, at the bottom of the screen, click Next . Configure the connection ​ To complete the Amazon Athena connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Amazon Athena data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Amazon Athena data, change Allow Data Preview to No . At the bottom of the screen, click the Next button to proceed. Configure the crawler ​ Before running the Amazon Athena crawler, you can further configure it. You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a Java regular expression in the Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Amazon Athena crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen click the Run button. To schedule the crawler to run hourly, daily, weekly or monthly, at the bottom of the screen click the Schedule & Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up a private network link to Amazon Athena Next What does Atlan crawl from Amazon Athena? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/how-tos/set-up-a-private-network-link-to-amazon-athena",
    "content": "Connect data Databases Query Engines Amazon Athena Get Started Set up a private network link to Amazon Athena On this page Set up a private network link to Amazon Athena Who can do this? You will need your Amazon Athena or AWS administrator involved   -  you may not have access yourself to complete these steps. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Amazon Athena and Atlan. Request Atlan's details ​ Before configuring the connection, you will need the following: VPC endpoint ID of the Atlan VPC endpoint in the following format   - vpce-0d90d77d1be568544 . This will be required to create the IAM policy. To enter a hostname for crawling Amazon Athena : If private DNS hostnames are enabled , enter the default Athena endpoint in the following format   - https://athena.<region>.amazonaws.com -  and it will resolve to your VPC endpoint. If private DNS hostnames are not enabled, enter the primary DNS name of the Atlan VPC endpoint in the following format   - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com -  as retrieved from Atlan support. Request it from Atlan support . Create IAM policy ​ To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAthenaListDataCatalog\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"athena:ListDataCatalogs\" ] , \"Resource\" : \"*\" , \"Condition\" : { \"StringEquals\" : { \"aws:SourceVpce\" : [ \"<vpce-endpoint-id>\" ] } } } , { \"Sid\" : \"AllowAthenaActions\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"athena:StartQueryExecution\" , \"athena:GetQueryResults\" , \"athena:DeleteNamedQuery\" , \"athena:GetNamedQuery\" , \"athena:ListQueryExecutions\" , \"athena:StopQueryExecution\" , \"athena:GetQueryResultsStream\" , \"athena:ListNamedQueries\" , \"athena:CreateNamedQuery\" , \"athena:GetQueryExecution\" , \"athena:BatchGetNamedQuery\" , \"athena:BatchGetQueryExecution\" , \"athena:GetWorkGroup\" , \"athena:GetTableMetadata\" , \"athena:GetDatabase\" , \"athena:GetDataCatalog\" , \"athena:ListDatabases\" , \"athena:ListTableMetadata\" ] , \"Resource\" : [ \"arn:aws:athena:us-east-2:666568140392:datacatalog/*\" , \"arn:aws:athena:us-east-2:666568140392:workgroup/*\" ] , \"Condition\" : { \"StringEquals\" : { \"aws:SourceVpce\" : [ \"<vpce-endpoint-id>\" ] } } } , { \"Sid\" : \"AllowGlueActionsViaAthena\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"glue:GetDatabase\" , \"glue:GetDatabases\" , \"glue:CreateDatabase\" , \"glue:GetTables\" , \"glue:GetTable\" , \"glue:SearchTables\" , \"glue:GetTableVersions\" , \"glue:GetTableVersion\" , \"glue:GetPartition\" , \"glue:GetPartitions\" , \"glue:GetUserDefinedFunctions\" , \"glue:GetUserDefinedFunction\" ] , \"Resource\" : [ \"arn:aws:glue:us-east-2:666568140392:tableVersion/*/*/*\" , \"arn:aws:glue:us-east-2:666568140392:catalog\" , \"arn:aws:glue:us-east-2:666568140392:table/*/*\" , \"arn:aws:glue:us-east-2:666568140392:database/*\" ] , \"Condition\" : { \"ForAnyValue:StringEquals\" : { \"aws:CalledVia\" : [ \"athena.amazonaws.com\" ] } } } , { \"Sid\" : \"AllowS3ActionsOnDataViaAthena\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:GetObject\" ] , \"Resource\" : [ \"arn:aws:s3:::demo-wide-world-importers\" , \"arn:aws:s3:::demo-wide-world-importers/*\" ] , \"Condition\" : { \"ForAnyValue:StringEquals\" : { \"aws:CalledVia\" : [ \"athena.amazonaws.com\" ] } } } , { \"Sid\" : \"AllowS3ActionsOnMetadataViaAthena\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:GetObject\" , \"s3:ListBucket\" , \"s3:ListBucketMultipartUploads\" , \"s3:ListMultipartUploadParts\" , \"s3:AbortMultipartUpload\" , \"s3:CreateBucket\" , \"s3:PutObject\" ] , \"Resource\" : [ \"arn:aws:s3:::source-curation-athena-metadata\" , \"arn:aws:s3:::source-curation-athena-metadata/*\" ] , \"Condition\" : { \"ForAnyValue:StringEquals\" : { \"aws:CalledVia\" : [ \"athena.amazonaws.com\" ] } } } ] } Replace <vpce-endpoint-id> with the VPC endpoint ID received from Atlan support . Attach this policy to the IAM user or role used for authentication. For more information, see Choose authentication mechanism or create a new IAM user by following the steps in the Create an IAM user section. Create an IAM user ​ Create an AWS IAM user and attach the policy created above to this user. To create an AWS IAM user: Follow the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Refer to managing access keys for IAM users to create an access key for the new user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl Amazon Athena in Atlan! 🎉 Tags: connectors data crawl Previous Set up Amazon Athena Next Crawl Amazon Athena Request Atlan's details Create IAM policy Create an IAM user"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/how-tos/set-up-cloudera-impala",
    "content": "Connect data Databases Query Engines Cloudera Impala Get Started Set up Cloudera Impala On this page Set up Cloudera Impala Who can do this? You will probably need your Cloudera Impala instance administrator to complete these steps — you may not have access yourself. This guide provides step-by-step instructions to configure user access and grant the required permissions in Cloudera Impala so that Atlan can crawl metadata. Create user ​ Create a user in your LDAP system for Atlan to authenticate with Impala. You can use identity providers like OpenLDAP, Active Directory, or any other service your organization uses to create this user. Based on the authorization service your organization uses with Impala, sync the created user with either Ranger or Sentry. For Ranger, follow the Ranger Authentication and User Sync documentation . For Sentry, refer to the Sentry Overview documentation . Connect to Impala using the admin user from either Ranger or Sentry to manage permissions. Grant permissions to assets ​ There are three ways in which you can grant permissions to assets, depending on your requirements for crawling assets. Who can do this? The Impala or Ranger administrator likely needs to complete these steps, as you may not have the required access. Grant permission to crawl schema ​ To provide the SELECT privilege for the entire schema, run the following command: GRANT SELECT ON SCHEMA <schema_name> TO USER <atlan-user>; Repeat the above command for each schema you want to crawl. Grant permission to crawl specific tables ​ To grant access to a specific table, run the following command: GRANT SELECT ON TABLE <table_name> TO USER <atlan-user>; Replace <table_name> with the name of the table. Grant permission to crawl specific columns ​ To grant column-level access, use the following command: GRANT SELECT(column1, column2) ON TABLE <table_name> TO USER <atlan-user>; Replace column1 , column2 with the relevant column names. Replace <table_name> the relevant table name. (Optional) Grant permission to calculate specific attributes ​ Run the following SQL commands: GRANT ALTER ON TABLE <table_name> TO USER <atlan-user>; GRANT SELECT ON TABLE <table_name> TO USER <atlan-user>; Replace <table_name> with the name of the table. These permissions are needed to calculate attributes like rowCount and sizeBytes for the tables. Tags: atlan documentation Previous Cloudera Impala Next Crawl Cloudera Impala Create user Grant permissions to assets (Optional) Grant permission to calculate specific attributes"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/how-tos/crawl-amazon-dynamodb",
    "content": "Connect data Databases NoSQL Databases Amazon DynamoDB Crawl DynamoDB Assets Crawl Amazon DynamoDB On this page Crawl Amazon DynamoDB Once you have configured the Amazon DynamoDB permissions , you can establish a connection between Atlan and Amazon DynamoDB. To crawl metadata from Amazon DynamoDB, review the order of operations and then complete the following steps. Select the source ​ To select Amazon DynamoDB as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Amazon DynamoDB Assets . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Amazon DynamoDB credentials: For Extraction method , Direct is the default extraction method. For Authentication, choose the method you configured when setting up the Amazon DynamoDB access permissions : For IAM User authentication, enter the AWS Access Key and AWS Secret Key you downloaded . For IAM Role authentication, enter the AWS Role ARN you configured . (Optional) Enter the AWS External ID only if you have configured an external ID in the role definition. For AWS Region , enter the AWS region of your Amazon DynamoDB instance. Click the Test Authentication button to confirm connectivity to Amazon DynamoDB. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Amazon DynamoDB connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Amazon DynamoDB crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To have the crawler ignore tables based on a naming convention, specify a regular expression in the Exclude tables regex field. To have the crawler include tables based on a naming convention, specify a regular expression in the Include tables regex field. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Amazon DynamoDB crawler, after completing the steps above: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Amazon DynamoDB Next What does Atlan crawl from Amazon DynamoDB? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-dynamodb/how-tos/set-up-amazon-dynamodb",
    "content": "Connect data Databases NoSQL Databases Amazon DynamoDB Get Started Set up Amazon DynamoDB On this page Set up Amazon DynamoDB warning 🤓 Who can do this? You will probably need your Amazon DynamoDB administrator to run these commands   -  you may not have access yourself. Atlan supports the following authentication methods for fetching metadata from Amazon DynamoDB: IAM user authentication -  this method uses an AWS access key, secret key, and region to fetch metadata. IAM role authentication -  this method uses an AWS role ARN and region to fetch metadata. Create IAM policy ​ To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:ListTables\" ] , \"Resource\" : \"*\" } , { \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:DescribeTable\" ] , \"Resource\" : \"arn:aws:dynamodb:<region>:<account_id>:table/*\" } ] } Replace <region> with the AWS region of your Amazon DynamoDB instance. Replace <account_id> with your AWS account ID. IAM permissions ​ Atlan requires the following permissions: dynamodb:ListTables : Fetches a list of your Amazon DynamoDB tables. This permission is used during the metadata extraction process to dynamically determine a list of tables. Note that this action does not support resource-level permissions and requires you to choose all resources, hence * for Resource . dynamodb:DescribeTable : Fetches metadata for extracted tables. This action supports resource-level permissions, so for Resource , you can either: Grant permission to all tables in the region for which you want to extract metadata: arn:aws:dynamodb:<region>:<account_id>:table/* Specify the table names for which you want to extract metadata: arn:aws:dynamodb:<region>:<account_id>:table/table_name_1 , arn:aws:dynamodb:<region>:<account_id>:table/table_name_2 Choose authentication mechanism ​ Using the policy created above , configure one of the following options for authentication. User-based authentication ​ To configure IAM user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Refer to managing access keys for IAM users to create an access key for the new user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role delegation-based authentication ​ To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } (Optional) To use an external ID for additional security, paste the external ID into the policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : \"<atlan_external_id>\" } } } ] } Replace <atlan_external_id> with the external ID you want to use. Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: connectors data authentication Previous Amazon DynamoDB Next Crawl Amazon DynamoDB Create IAM policy IAM permissions Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/amazon-athena/how-tos/set-up-amazon-athena",
    "content": "Connect data Databases Query Engines Amazon Athena Get Started Set up Amazon Athena On this page Set up Amazon Athena warning 🤓 Who can do this? You will probably need your Amazon Athena administrator to run these commands   -  you may not have access yourself. Did you know? Prefixing all resources created for Atlan with atlan- will help you better identify them. You should also add AWS tags and descriptions to these resources for later reference. Create IAM policy ​ To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:GetObject\" , \"glue:GetTables\" , \"glue:GetDatabases\" , \"glue:GetTable\" , \"glue:GetDatabase\" , \"glue:SearchTables\" , \"glue:GetTableVersions\" , \"glue:GetTableVersion\" , \"glue:GetPartition\" , \"glue:GetPartitions\" , \"glue:GetUserDefinedFunctions\" , \"glue:GetUserDefinedFunction\" , \"athena:GetTableMetadata\" , \"athena:StartQueryExecution\" , \"athena:GetQueryResults\" , \"athena:GetDatabase\" , \"athena:GetDataCatalog\" , \"athena:ListQueryExecutions\" , \"athena:GetWorkGroup\" , \"athena:StopQueryExecution\" , \"athena:GetQueryResultsStream\" , \"athena:ListDatabases\" , \"athena:GetQueryExecution\" , \"athena:ListTableMetadata\" , \"athena:BatchGetQueryExecution\" ] , \"Resource\" : [ \"arn:aws:glue:<region>:<account_id>:tableVersion/*/*/*\" , \"arn:aws:glue:<region>:<account_id>:table/*/*\" , \"arn:aws:glue:<region>:<account_id>:catalog\" , \"arn:aws:glue:<region>:<account_id>:database/*\" , \"arn:aws:athena:<region>:<account_id>:datacatalog/*\" , \"arn:aws:athena:<region>:<account_id>:workgroup/*\" , \"arn:aws:s3:::<data_bucket>\" , \"arn:aws:s3:::<data_bucket>/*\" ] } , { \"Sid\" : \"VisualEditor1\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"s3:GetObject\" , \"s3:ListBucketMultipartUploads\" , \"s3:AbortMultipartUpload\" , \"s3:ListBucket\" , \"s3:GetBucketLocation\" , \"s3:ListMultipartUploadParts\" ] , \"Resource\" : [ \"arn:aws:s3:::<s3_bucket>/*\" , \"arn:aws:s3:::<s3_bucket>\" ] } , { \"Sid\" : \"VisualEditor2\" , \"Effect\" : \"Allow\" , \"Action\" : \"athena:ListDataCatalogs\" , \"Resource\" : \"*\" } ] } Replace <region> with the AWS region of your Athena instance. Replace <account_id> with your account ID. Replace <data_bucket> with the S3 bucket where your actual data resides, such as your Glue tables. Replace <s3_bucket> with the S3 bucket where Athena can store temporary Athena query results. info 💪 Did you know? We recommend using Atlan's deployment bucket to store these results. This ensures all Atlan assets are managed in a single bucket. If you have an external Hive metastore connected to Athena, also add these policies: { \"Sid\" : \"VisualEditor3\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:InvokeFunction\" , \"lambda:GetFunction\" ] , \"Resource\" : [ \"arn:aws:lambda:<region>:<account_id>:function:<lambda_fn_name>\" ] } Replace <region> with the AWS region of your Athena instance. Replace <account_id> with your account ID. Replace <lambda_fn_name> with the name of the Lambda function used to configure the catalog. These allow Atlan to trigger the Lambda function. danger If you're using AWS Lake Formation to manage access to your AWS resources, you will need to grant permissions in AWS Lake Formation as well as to the objects you want to crawl. Choose authentication mechanism ​ Using the policy created above, configure one of the following options for authentication. User-based authentication ​ To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role delegation-based authentication ​ To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } (Optional) To use an external ID for additional security: Generate the external ID within Atlan . Paste the external ID into the policy (replace <atlan_generated_external_id> with it): { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : \"<atlan_generated_external_id>\" } } } ] } Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: atlan documentation Previous Amazon Athena Next Set up a private network link to Amazon Athena Create IAM policy Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cloudera-impala/how-tos/crawl-cloudera-impala",
    "content": "Connect data Databases Query Engines Cloudera Impala Crawl Cloudera Impala Assets Crawl Cloudera Impala On this page Crawl Cloudera Impala Once you have configured the Cloudera Impala user permissions , you can establish a connection between Atlan and Cloudera Impala. To crawl metadata from Cloudera Impala, review the order of operations and then complete the following steps. Select the source ​ To select Cloudera Impala as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Cloudera Impala Assets . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Cloudera Impala credentials: For Extraction method , Direct is the default selection. For Hostname , enter the host name of your Cloudera Impala coordinator or load balancer. For Authentication , select LDAP as the authentication method. For Username , enter the LDAP username that has access to Cloudera Impala. For Password , enter the password associated with the LDAP username. For SSL , keep Enabled to connect via a Secure Sockets Layer (SSL) channel or click Disabled . Click the Test Authentication button to confirm connectivity to Cloudera Impala. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Cloudera Impala connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . Careful If you do not specify any user or group, no one will be able to manage the connection — not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Cloudera Impala crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options: To include specific assets in crawling, click Include Metadata , and select the assets you want. If you don't select any, all assets will be included by default. To exclude specific assets from crawling, click Exclude Metadata , and choose the assets you want to omit. If you don't select any, no assets will be excluded. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Cloudera Impala crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: cloudera impala connectivity crawling Previous Set up Cloudera Impala Next What does Atlan crawl from Cloudera Impala? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/how-tos/set-up-cratedb",
    "content": "Connect data Databases SQL Databases CrateDB Get Started Set up CrateDB On this page Set up CrateDB This guide provides steps to set up a dedicated database user and configure permissions to enable Atlan to securely connect to your CrateDB cluster and extract metadata for data discovery and governance. Prerequisites ​ Before you begin, make sure you have: Administrative access to your CrateDB cluster Network connectivity between Atlan and your CrateDB instance Your CrateDB cluster's HTTP endpoint and port information Permission required ​ Before setting up the CrateDB connector, you need: Administrative access to your CrateDB cluster to create users and grant permissions To run the setup statements for each schema you want to crawl Create database user ​ Create a dedicated database user with the necessary permissions for Atlan to access your CrateDB cluster securely. Create a dedicated user for Atlan with basic authentication: CREATE USER atlan_user WITH ( password = '<password>' ) ; Replace <password> with a secure password for the atlan_user . Grant DQL permissions on the target schema: GRANT DQL ON SCHEMA < schema > TO atlan_user ; Replace <schema> with the schema to which the user needs access. Grant DQL permissions on system schemas for metadata access: GRANT DQL ON SCHEMA information_schema TO atlan_user ; GRANT DQL ON SCHEMA sys TO atlan_user ; Configure authentication ​ Choose the authentication method that best fits your security requirements and infrastructure setup. Basic authentication Certificate-based authentication To create a username and password for basic authentication: CREATE USER atlan_user WITH ( password = '<password>' ) ; GRANT DQL ON SCHEMA < schema > TO atlan_user ; Replace <password> with the password for the atlan_user user you are creating. Certificate-based authentication is available for Enterprise customers. Grant permissions ​ Grant the necessary permissions to enable Atlan to access metadata and data from your CrateDB cluster. Important DQL permissions grant both metadata access and data read access. CrateDB doesn't support metadata-only access. System schema permissions : Grant DQL permissions on system schemas for metadata access: GRANT DQL ON SCHEMA information_schema TO atlan_user ; GRANT DQL ON SCHEMA sys TO atlan_user ; These system schemas contain CrateDB metadata: information_schema : Standard SQL metadata tables sys : CrateDB-specific system tables for cluster and table information Data access permissions : Grant DQL permissions on user schemas for data preview and querying: GRANT DQL ON TABLE < schema > . * TO atlan_user ; Replace <schema> with the name of the schema you want Atlan to access. Configure connection ​ To connect Atlan to your CrateDB cluster, you’ll need to provide the following details during setup: Host : The HTTP endpoint of your CrateDB cluster. For example, https://your-cluster.crate.io Username : The database username created in the Create database user section. Password : The secure password set for the database user created in the Create database user section. Database : The name of the CrateDB database to crawl. Enter the specific database for metadata extraction. Troubleshooting ​ If you encounter connection or authentication issues, see Connection issues . Next steps ​ Crawl CrateDB - Extract metadata from your CrateDB instance Tags: connectors cratedb database setup Previous CrateDB Next Crawl CrateDB Prerequisites Permission required Create database user Configure authentication Grant permissions Configure connection Troubleshooting Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/how-tos/crawl-datastax-enterprise",
    "content": "Connect data Databases NoSQL Databases Datastax Enterprise Crawl Datastax Enterprise Assets Crawl DataStax Enterprise On this page Crawl DataStax Enterprise Once you have configured DataStax Enterprise , you can establish a connection between Atlan and DataStax Enterprise. To crawl metadata from DataStax Enterprise, review the order of operations and then complete the following steps. Select the source ​ To select DataStax Enterprise as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select DataStax Enterprise Assets and click the Setup Workflow button. Provide credentials ​ In Direct extraction, Atlan connects to DataStax Enterprise and crawls metadata directly. In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your DataStax Enterprise credentials: For Host Name , enter the host name of your DataStax Enterprise instance. For Port , enter the port number of your DataStax Enterprise instance. For Authentication , Basic authentication, enter the Username and Password you use to log in to DataStax Enterprise. (Optional) For SSL , keep the default Enabled to use HTTPS or click Disabled to use HTTP. (Optional) For SSL certificate , this is only required if your DataStax Enterprise instance uses a self-signed or an internal CA SSL certificate , paste a supported SSL certificate in the recommended format . At the bottom of the form, click the Test Authentication button to confirm connectivity to DataStax Enterprise using these details. When successful, at the bottom of the screen click the Next button. Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from DataStax Enterprise. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the DataStax Enterprise data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the DataStax Enterprise connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. Configure the crawler ​ Before running the DataStax Enterprise crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the DataStax Enterprise keyspaces you want to include in crawling, click Include Keyspaces . (This will default to all assets, if none are specified.) To select the DataStax Enterprise keyspaces you want to exclude from crawling, click Exclude Keyspaces . (This will default to no assets, if none are specified.) To have the crawler ignore DataStax Enterprise keyspaces based on a naming convention, specify a regular expression in the Exclude Keyspaces Regex field. To check for any permissions or other configuration issues before running the crawler, click Preflight checks . Did you know? If a keyspace appears in both the include and exclude filters, the exclude filter takes precedence. (The Exclude Keyspace Regex also takes precedence.) Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up DataStax Enterprise Next What does Atlan crawl from DataStax Enterprise? Select the source Provide credentials Configure the connection Configure the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/cratedb/how-tos/crawl-cratedb",
    "content": "Connect data Databases SQL Databases CrateDB Crawl CrateDB Assets Crawl CrateDB On this page Crawl CrateDB Extract metadata from your CrateDB database and make it available in Atlan for data discovery, governance, and lineage tracking. This guide walks you through setting up authentication and running your first crawl. Prerequisites ​ Before you begin, make sure you have: Set up CrateDB with proper user permissions Network connectivity between Atlan and your CrateDB instance Your CrateDB cluster HTTP endpoint and port information Set up workflow ​ Create a new CrateDB Assets workflow to extract metadata from your database. Select New > New Workflow . From the list of packages, select CrateDB Assets . Click Setup Workflow . Configure extraction method ​ Choose how to connect to your CrateDB environment: Direct extraction Agent extraction Select Direct for the extraction method. Enter your CrateDB connection details: Host : Your CrateDB cluster HTTP endpoint (for example, https://your-cluster.crate.io ) Port : The port number of your CrateDB instance Authentication : Choose Basic authentication Username : Enter the username you configured in CrateDB Password : Enter the password you configured in CrateDB Database : Enter the name of the database to crawl Click Test Authentication to confirm connectivity to CrateDB using these details. When successful, click Next . Select Agent for the extraction method. Add the secret keys for your secret store configuration. Follow the Secure Agent configuration guide . Click Next . Configure connection details ​ Enter a Connection Name to identify your CrateDB environment. For example, production-cratedb , analytics-db , data-warehouse . Assign Connection Admins to manage access. At least one admin is required. Configure crawler settings ​ Before running the CrateDB crawler, you can configure additional settings: Exclude Metadata : Select assets you want to exclude from crawling Include Metadata : Select assets you want to include in crawling Exclude regex for tables & views : Specify a regular expression to ignore tables and views based on naming conventions Advanced Config : Enable Source Level Filtering : Enable schema-level filtering at source Use JDBC Internal Methods : Enable JDBC internal methods for data extraction Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run crawler ​ You can now start extracting metadata from your CrateDB database: Run now : Click Run to start a one-time crawl. Schedule runs : Click Schedule Run to automate recurring crawls (hourly, daily, weekly, or monthly). Monitor crawl progress in the activity log. Once complete, your CrateDB assets appear in Atlan. Troubleshooting ​ If you encounter connection or authentication issues, see Connection issues . See also ​ What does Atlan crawl from CrateDB? Preflight checks for CrateDB Tags: connectors data crawl Previous Set up CrateDB Next What does Atlan crawl from CrateDB? Prerequisites Set up workflow Troubleshooting See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/datastax-enterprise/how-tos/set-up-datastax-enterprise",
    "content": "Connect data Databases NoSQL Databases Datastax Enterprise Get Started Set up DataStax Enterprise On this page Set up DataStax Enterprise 🤓 Who can do this? You need your DataStax Enterprise administrator or a user who has create role permissions to run these commands; you may not have access yourself. For more details, refer to the Apache Cassandra documentation on role-based access control . This guide outlines the steps to configure your DataStax Enterprise instance so Atlan can crawl its metadata. Prerequisites ​ Before you begin, ensure you have the following: Access to a DataStax Enterprise instance - You need the necessary credentials to connect to your Cassandra instance. cqlsh installed - The Cassandra Query Language Shell (cqlsh) is required to execute commands for user and permission management. If cqlsh is not installed, refer to the Apache Cassandra documentation for cqlsh . Create an Atlan user role ​ This section guides you through creating a dedicated role in DataStax Enterprise for Atlan. Connect to the DataStax Enterprise instance : Use cqlsh to connect to your DataStax Enterprise instance. Create a dedicated role for Atlan : Run the following command in cqlsh to create a role. CREATE ROLE <username> WITH SUPERUSER = false AND LOGIN = true AND PASSWORD = '<password>'; Replace the placeholders <username> : The username for the Atlan role. <password> : A strong and unique password. Grant permissions to the Atlan user role ​ Atlan needs specific permissions to access metadata in DataStax Enterprise Follow these steps to grant the required permissions: Grant DESCRIBE on all keyspaces with: GRANT DESCRIBE ON ALL KEYSPACES TO atlan-admin; You may also choose to grant DESCRIBE permission on specific keyspaces by running the following command: GRANT DESCRIBE ON KEYSPACE <keyspace_name> TO <username>; Replace placeholders: <keyspace_name> : The name of the keyspace. <username> : The username for the Atlan role. Run this command for each keyspace you want Atlan to access. Tags: data crawl Previous DataStax Enterprise Next Crawl DataStax Enterprise Prerequisites Create an Atlan user role Grant permissions to the Atlan user role"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/how-tos/crawl-hive",
    "content": "Connect data Databases Query Engines Hive Crawl Hive Assets Crawl Hive On this page Crawl Hive Once you have configured the Hive permissions , you can establish a connection between Atlan and Hive. (If you are also using a private network for Hive, you will need to set that up first , too.) To crawl metadata from Hive, review the order of operations and then complete the following steps. Select the source ​ To select Hive as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Hive Assets . In the right panel, click Setup Workflow . Provide your credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . Direct extraction method ​ To enter your Hive credentials: For Host Name , enter the host name (or PrivateLink endpoint ) for your Hive instance. For Port , enter the port number for your Hive instance. For Username , enter the username you created for that instance. For Password , enter the password for the username. For Default Schema , enter the default schema name for your Hive instance. Click the Test Authentication button to confirm connectivity to Hive. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from Hive. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your bucket details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen click Next . Configure the connection ​ To complete the Hive connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Hive crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the Hive assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the Hive assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Hive crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Did you know? Once you have crawled assets from Hive, you can run the Hive Miner to mine query history through S3 . Tags: connectors data crawl Previous Set up a private network link to Hive Next What does Atlan crawl from Hive? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/how-tos/set-up-a-private-network-link-to-hive",
    "content": "Connect data Databases Query Engines Hive Get Started Set up a private network link to Hive On this page Set up a private network link to Hive AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Hive and Atlan. Who can do this? You will need your AWS administrator involved   -  you may not have access to run these tasks yourself. Prerequisites ​ You should already have the following: Hive instance running in AWS (private EMR instance). Atlan hosted in the same region as the Hive instance. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Set up network to EMR instance ​ To set up the private network of your Hive EMR instance, from within AWS : Copy network settings ​ To copy the network settings of your Hive EMR instance: From the left menu, under EMR on EC2 , click Clusters . In the Clusters table, click on your Hive EMR cluster. From the cluster's Network and security tab, under Network , for Virtual Private Cloud (VPC) , click on your VPC to view more details. Under your VPC's Details tab, copy and save the value under the IPv4 CIDR column. Create inbound rule ​ To create an inbound rule allowing your VPC access to your Hive EMR instance: From the left menu, under EMR on EC2 , click Clusters . In the Clusters table, click on your Hive EMR cluster. From the cluster's Network and security tab, click the downward arrow for EC2 security groups (firewall) to expand this section. Under EC2 security groups (firewall) , click on a security group for the cluster. Under the Inbound rules tab, click the Edit inbound rules button. At the bottom left of the Inbound rules table, click the Add rule button. For Type , select All traffic . For Port , enter the port on which Hive is accessible. For Source , choose Custom and enter the CIDR range for your Hive instance (see Copy network settings ). Below the bottom right of the Inbound rules table, click the Save rules button. Repeat steps 4 to 7 for each security group in the cluster. Create internal Network Load Balancer ​ Start creating NLB ​ To create an NLB, from within AWS: Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name, enter a unique name. For Scheme , select Internal . For IP address type , select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC , select the VPC where the Hive instance is located (see Copy network settings ). For Mappings , select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port , enter the port value used in Created inbound rule . For Default action , click the Create target group link. This will open the target group creation in a new browser tab. Create target group ​ To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type , select Instances . For Target group name , enter a name. For Port , enter the port value used in Create inbound rule . For VPC , select the VPC where the Hive instance is located (see Copy network settings ). At the bottom of the form, click the Next button. From the Available instances table: Click the checkbox next to your Hive instance. Enter the port for the port value used in steps above. Click the Include as pending below button. At the bottom right of the form, click the Create target group button. Finish creating NLB ​ Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom right of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy ​ To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing click Target Groups . From the Target groups table, click the row for the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service ​ To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name , enter a meaningful name. For Load balancer type , choose Network . For Available load balancers , select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint , enable Acceptance required . For Supported IP address types , enable IPv4 . At the bottom right of the form, click the Create button. Under the Details of the endpoint service, copy the hostname under Service name . Allow Atlan account access ​ To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN , enter the Atlan account ID. At the bottom right of the form, click the Allow principals button. Notify Atlan support ​ Once all the above steps are complete, provide Atlan support with the following information: The hostname for the endpoint service created above. The port number for your Hive instance. There are additional steps Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request ​ To accept the consumer connection request, from within AWS: Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending . Select this row, and click the Actions button and then Accept endpoint connection request . If prompted to confirm, type accept into the field and click the Accept button. Wait for this to complete, it could take about 30 seconds. 😅 The connection is now established. You can now use the service endpoint provided by Atlan support as the hostname to crawl Hive in Atlan! 🎉 Tags: integration connectors Previous Set up Hive Next Crawl Hive Prerequisites Set up network to EMR instance Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/hive/how-tos/set-up-hive",
    "content": "Connect data Databases Query Engines Hive Get Started Set up Hive On this page Set up Hive Who can do this? You will need your Hadoop administrator to run these commands   -  you may not have access yourself. Currently, we only support basic username and password authentication for Hive. Complete the following steps to configure it. Set the right permissions ​ To configure basic authentication for Hive, enter the following details: For Host Name , enter the Atlan-accessible Hive instance URL. For Port , enter the port number where your Hive instance is accessible. For Default Schema , enter the default schema name in your Hive instance for connection. Atlan will crawl other schemas too   -  not just the default one. Grant read permission on objects ​ Grant read permission on objects with the following commands: GRANT SELECT ON DATABASE < database_name > TO USER < username > ; Atlan requires read permission for all the objects you want to crawl in Hive. Did you know? Available users and access control may also be controlled or affected by HDFS ACL, LDAP, and any other policy engine that is in effect. Overall, Atlan requires the authenticating user to have read permission at a minimum. Tags: data crawl authentication Previous Hive Next Set up a private network link to Hive Set the right permissions Grant read permission on objects"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/crawl-microsoft-azure-cosmos-db",
    "content": "Connect data Databases NoSQL Databases Microsoft Azure Cosmos DB Crawl Cosmos DB Assets Crawl Microsoft Azure Cosmos DB On this page Crawl Microsoft Azure Cosmos DB Once you have configured the Microsoft Azure Cosmos DB permissions , you can establish a connection between Atlan and Microsoft Azure Cosmos DB. To crawl metadata from Microsoft Azure Cosmos DB, review the order of operations and then complete the following steps. Select the source ​ To select Microsoft Azure Cosmos DB as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Cosmos DB Assets . In the right panel, click Setup Workflow . Provide your credentials ​ Choose your deployment method: In vCore deployment, you will need the primary connection string(s) of your vCore-based Microsoft Azure Cosmos DB account(s). In RU deployment, you will need the client ID, client secret, and tenant ID of the service principal you created for your RU-based Microsoft Azure Cosmos DB account. In vCore and RU deployment, you will need the primary connection string(s) of your vCore-based account(s) and client ID, client secret, and tenant ID of the service principal you created for your RU-based account. vCore deployment ​ To enter your Microsoft Azure Cosmos DB credentials: For Database API , MongoDB is the default selection. For Extraction method , Direct is the default selection. For Select the deployment types to crawl , click vCore . For Connection Strings , enter the primary connection string(s) you copied from your Microsoft Azure Cosmos DB account(s). Click the Test Authentication button to confirm connectivity to Microsoft Azure Cosmos DB. Once authentication is successful, navigate to the bottom of the screen and click Next . RU deployment ​ To enter your Microsoft Azure Cosmos DB credentials: For Database API , MongoDB is the default selection. For Extraction method , Direct is the default selection. For Select the deployment types to crawl , click RU . For Client ID , enter the application (client) ID you copied for your service principal. For Client Secret , enter the client secret you copied for your service principal. For Tenant ID , enter the directory (tenant) ID you copied for your service principal. Click the Test Authentication button to confirm connectivity to Microsoft Azure Cosmos DB. Once authentication is successful, navigate to the bottom of the screen and click Next . vCore and RU deployment ​ To enter your Microsoft Azure Cosmos DB credentials: For Database API , MongoDB is the default selection. For Extraction method , Direct is the default selection. For Select the deployment types to crawl , click vCore and RU . For Client ID , enter the application (client) ID you copied of the service principal for your RU-based account. For Client Secret , enter the client secret you copied of the service principal for your RU-based account. For Tenant ID , enter the directory (tenant) ID you copied of the service principal for your RU-based account. For Connection Strings , enter the primary connection string(s) you copied from your vCore-based account(s). Click the Test Authentication button to confirm connectivity to Microsoft Azure Cosmos DB. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Microsoft Azure Cosmos DB connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Microsoft Azure Cosmos DB crawler, you can further configure it. On the Metadata page, you can override the defaults for the following: For Extract Collection Schemas , change to Yes to enable Atlan to extract collection schemas by reading a subset of the documents in the collection and map them to column assets . For Schema extraction sample size , you can set a custom value of up to 1,000 for documents to be read for schema analysis. Run the crawler ​ To run the Microsoft Azure Cosmos DB crawler, after completing the steps above: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Microsoft Azure Cosmos DB Next What does Atlan crawl from Microsoft Azure Cosmos DB? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/crawl-microsoft-sql-server",
    "content": "Connect data Databases SQL Databases Microsoft SQL Server Crawl SQL Server Assets Crawl Microsoft SQL Server On this page Crawl Microsoft SQL Server Once you have configured the Microsoft SQL Server user permissions , you can establish a connection between Atlan and Microsoft SQL Server. (If you are also using a private network for Microsoft SQL Server, you will need to set that up first, too, for your Microsoft SQL Server on Amazon RDS or Amazon EC2 instance.) To crawl metadata from Microsoft SQL Server, revie w the order of operations and then complete the following steps. Select the source ​ Select Microsoft SQL Server as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select SQL Server Assets and click on Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan’s secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your Microsoft SQL Server credentials: For Host , enter the availability group listener name (or PrivateLink endpoint for your Microsoft SQL Server on Amazon RDS or Amazon EC2 instance). For Port , enter the port on which Microsoft SQL Server is available (default is 1433). For Username , enter the username created when setting up user permissions. For Password , enter the password created when setting up user permissions. For Database , enter the name of the database. Click Test Authentication to confirm connectivity to Microsoft SQL Server using these details. When successful, at the bottom of the screen click Next . Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from Microsoft SQL Server. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include database.json , columns-<database>.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. Once completed, navigate to the bottom of the screen and click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from Microsoft SQL Server. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Microsoft SQL Server data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ Complete the Microsoft SQL Server connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Microsoft SQL Server data, change Allow SQL Query to No . (Optional) To prevent users from previewing any Microsoft SQL Server data, change Allow Data Preview to No . At the bottom of the screen, click Next to proceed. Configure the crawler ​ Before running the Microsoft SQL Server crawler, you can further configure it. To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. You can also specify the following system tables to exclude from crawling   - sys* , MSmerge* , dbo.sys* , MSrepl* , IH* , MSpeer* , cdc.* , MS*history , MS*agent , MSdist* , MSpub* , MSsubcri* , MSdbms* , MSdynamic* , and MSagent* . Note that this is not an exhaustive list, for more information refer to source documentation . For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Microsoft SQL Server crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's assets page! 🎉 Did you know? Once you have crawled assets from Microsoft SQL Server, you can run the SQL Server Miner to mine query history through S3 . Tags: connectors data crawl Previous Set up Microsoft SQL Server Next Set up a private network link to Microsoft SQL Server on Amazon EC2 Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-azure-cosmos-db/how-tos/set-up-microsoft-azure-cosmos-db",
    "content": "Connect data Databases NoSQL Databases Microsoft Azure Cosmos DB Get Started Set up Microsoft Azure Cosmos DB On this page Set up Microsoft Azure Cosmos DB Did you know? Atlan currently only supports crawling Microsoft Azure Cosmos DB for MongoDB with the Microsoft Azure Cosmos DB connector. Atlan supports the following deployment types for fetching metadata from Microsoft Azure Cosmos DB: vCore-based deployment   -  you can use SCRAM-SHA authentication for vCore-based accounts. You will need to authenticate the connection in Atlan with a primary connection string to fetch metadata from vCore-based accounts. Atlan provides multi-account support for fetching metadata. RU-based deployment   -  you can use service principal authentication for request unit (RU)-based accounts. You will need to authenticate the connection in Atlan with a client ID, client secret, and tenant ID to fetch metadata from RU-based accounts. Atlan provides multi-account support for fetching metadata. If your Microsoft Azure Cosmos DB deployment includes a mix of vCore- and RU-based accounts, you must configure both to fetch metadata. You can then use the vCore and RU deployment option to crawl your Microsoft Azure Cosmos DB assets . vCore deployment ​ Who can do this? You will need your Microsoft Azure Cosmos DB administrator to complete these steps   -  you may not have access yourself. For vCore-based accounts, you will need the primary connection string of your Microsoft Azure Cosmos DB deployment to use SCRAM-SHA authentication for integrating with Atlan . To retrieve the primary connection string for vCore-based accounts: Log in to the Azure portal as an admin. In the portal, search for and select Azure Cosmos DB . On the Azure Cosmos DB page, select your Azure Cosmos DB for MongoDB (vCore) account. From the Overview page, copy the value of the Admin username . For password, you will need the password that was set up during your Microsoft Azure Cosmos DB deployment. In the left menu of the account page, under Settings , click Connection strings . Copy the value of the Primary Connection String and store it in a secure location. You will need to add the values of the admin username and password to the placeholder values in the primary connection string you copied. Repeat steps 1 to 6 for all the vCore-based accounts you want to crawl in Atlan. RU-based deployment ​ For request Unit (RU)-based accounts, you will need a client ID, client secret, and tenant ID for service principal authentication. Microsoft Azure Cosmos DB for MongoDB deployment currently does not support service principal authentication for vCore-based accounts. Register app with Microsoft Entra ID ​ Who can do this? You will need your Cloud Application Administrator or Application Administrator to complete these steps   -  you may not have access yourself. This will be required if the creation of registered applications is not enabled for the entire organization. You will need to register your service principal application with Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret. To register your app with Microsoft Entra ID: Log in to the Azure portal . In the search bar, search for Microsoft Entra ID , and select it from the dropdown list. From the left menu of the Microsoft Entra ID page, click App registrations . From the toolbar on the App registrations page, click + New registration . On the Register an application page, for Name , enter a name for your service principal application and then click Register . On the homepage of your newly created application, from the Overview screen, copy the values for the following fields and store them in a secure location: Application (client) ID Directory (tenant) ID From the left menu of your newly created application page, click Certificates & secrets . On the Certificates & secrets page, under Client secrets , click + New client secret . In the Add a client secret screen, enter the following details: For Description , enter a description for your client secret. For Expiry , select when the client secret will expire. Click Add . On the Certificates & secrets page, under Client secrets , for the newly created client secret, click the clipboard icon to copy the Value and store it in a secure location. Set permissions ​ Who can do this? You will need your Microsoft Azure Cosmos DB administrator to complete these steps   -  you may not have access yourself. You will need to add the service principal to the Cosmos DB Account Reader Role . This will allow the service principal read-only access to your Azure Cosmos DB account data. To add the service principal to the Cosmos DB Account Reader Role : Log in to the Azure portal . Open the menu and search for or select Azure Cosmos DB . On the Azure Cosmos DB page, select your Azure Cosmos DB for MongoDB (RU) account. From the left menu of your Azure Cosmos DB for MongoDB (RU) account page, click Access control (IAM) . From the tabs along the top of the Access control (IAM) page, click Add and then click Add role assignment . On the Add role assignment page, configure the following: In the Roles tab, from the list of roles under Job function roles , select Cosmos DB Account Reader Role -  this allows read-only access to Azure Cosmos DB account data -  and then click Next . You will need to assign this role to all the RU-based accounts you want to crawl in Atlan. In the Members tab, enter the following details: For Assign access to , click User, group, or service principal . For Members , click + Select members and then select the service principal you created. Click Next to proceed to the next step. In the Review + assign tab, click Review + assign to add role assignment. (Optional) Whitelist Atlan IP range ​ You may need to whitelist Atlan's IP range to allow Atlan to crawl Microsoft Azure Cosmos DB . To whitelist the Atlan IP range: Log in to the Azure portal . Open the menu and search for or select Azure Cosmos DB . On the Azure Cosmos DB page, select your Azure Cosmos DB for MongoDB account. From the left menu of your Azure Cosmos DB for MongoDB account page, click Networking . On the Networking page, under Public network access , check the following: If All networks is enabled, no further action required. If Select networks is enabled, raise an Atlan support request to obtain Atlan's IP range. Once received from Atlan support, for IP (Single IPv4 or CIDR range) , enter Atlan's IP range and click the Save button. Tags: data crawl authentication Previous Microsoft Azure Cosmos DB Next Crawl Microsoft Azure Cosmos DB vCore deployment RU-based deployment (Optional) Whitelist Atlan IP range"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/set-up-a-private-network-link-to-microsoft-sql-server-on-amazon-rds",
    "content": "Connect data Databases SQL Databases Microsoft SQL Server Private Network Set up a private network link to Microsoft SQL Server on Amazon RDS On this page Set up a private network link to Microsoft SQL Server on Amazon RDS Who can do this? You will need your AWS administrator to complete these tasks   -  you may not have access yourself. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Microsoft SQL Server on Amazon RDS and Atlan. Prerequisites ​ You should already have the following: Your own non-default VPC configured in AWS. A Microsoft SQL Server on Amazon RDS instance running in AWS, linked to the non-default VPC. Private subnets defined within the non-default VPC sufficient for availability. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from Atlan support . Set up network to RDS (in AWS) ​ To set up the private network of your Microsoft SQL Server instance, from within AWS : Copy network settings ​ Navigate to Services , then Database , and then RDS . On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, copy the following values: Endpoint and Port values VPC value Subnet group value On the left, click Subnet groups . From the table, click the row whose Name matches the subnet group copied above. From the Subnets table, copy each value under the CIDR block column for private subnets. Create inbound rule ​ To create an inbound rule allowing your private subnet access to your RDS instance: On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, under the Security column and the VPC security groups heading, click the link to your security group. At the bottom of the screen, change to the Inbound rules tab and then click the Edit inbound rules button. At the bottom of the table, click the Add rule button and create the following rule: For Type , use SQL Server if you are using the default port (1433), or use Custom and enter your port under Port range . For Source , use Custom and enter your CIDR range (see Copy network settings ). Repeat these sub-steps for each of your CIDR ranges. Below the table, click the Save rules button. (Optional) Create RDS proxy ​ Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This method uses a username and password to connect to the RDS database. To create an RDS proxy for your RDS instance: On the left, under Amazon RDS , click on Proxies . In the upper right of the Proxies table, click the Create proxy button. Under Proxy configuration , enter the following details: For Engine family , select SQL Server . For Proxy identifier , enter a meaningful name for your proxy. Under Target group configuration for Database , choose your RDS instance. Under Authentication for the Secrets Manager secrets : If you have an existing secret for your RDS instance's database credentials, select it from the dropdown. If not, click the Create a new secret link and enter these details in the new tab: For Secret type , select Credentials for Amazon RDS database . For Credentials , enter the Username and Password of the database user. Under Database , select your RDS instance. At the bottom of the form, click the Next button. For Secret name , enter a name for the secret. At the bottom of the form, click the Next button. Leave the automatic secret rotation off and click the Next button. Review the secret definition and click the Store button. Return to the tab where you started creating the RDS proxy. Under Authentication for IAM authentication : If IAM authentication is set to Required , Atlan will use an IAM role to connect to the RDS proxy. If IAM authentication is set to Not Allowed , basic authentication will be enabled. Atlan will use a username and password to connect to the RDS proxy. Under Connectivity , expand the Additional connectivity configuration : For VPC security group , select Choose existing . For Existing VPC security groups , select the security group you edited with the inbound rules above. At the bottom right of the form, click the Create proxy button. From the Proxies table, click the link for the proxy you just created. Under Proxy endpoints section, copy the hostname in the Endpoint column. Create internal Network Load Balancer ​ Retrieve IP address of the RDS ​ From an EC2 instance in your AWS account, run the following command: nslookup <endpoint> Replace <endpoint> with the fully-qualified endpoint hostname copied from the RDS endpoint or RDS proxy created above. Copy the IP address that comes back from the command, under Non-authoritative answer and to the right of Address . Start creating NLB ​ To create an NLB, from within AWS : Navigate to Services , then Compute , and then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name , enter a unique name. For Scheme , select Internal . For IP address type , select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC , select the VPC where the RDS instance is located (see Copy network settings ). For Mappings , select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port , enter 1433 (or the non-default port value from Copy network settings ). For Default action , click the Create target group link. This will open the target group creation in a new browser tab. Create target group ​ To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type , select IP addresses . For Target group name , enter a name. For Port , enter 1433 (or the non-default port value from Copy network settings ). For IP address type , select IPv4 . For VPC , select the VPC where the RDS instance is located (see Copy network settings ). At the bottom of the form, click the Next button. Enter the following IP addresses settings for the target group: For Network , select the VPC where the RDS instance is located (see Copy network settings ). For IPv4 address , enter the IP address returned by the nslookup command (see Retrieve IP address of the RDS ). For Ports , enter 1433 (or the non-default port value from Copy network settings ). At the bottom of the IP addresses section, click the Include as pending below button. Confirm the following Review targets settings for the target group: Confirm IP address matches the IP address returned by the nslookup command. Confirm Port is 1433 (or the non-default port value used by your RDS instance). At the bottom of the form, click the Create target group button. Finish creating NLB ​ Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action dropdown box. Select the target group you created above in the Default action drop-down. At the bottom of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy ​ To verify that the target group is healthy: From the EC2 menu on the left, under Load Balancing , click Target Groups . From the Target groups table, click the link to the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service ​ To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , and then VPC . From the menu on the left, under Virtual private cloud , click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name , enter a meaningful name. For Load balancer type , choose Network . For Available load balancers , select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint , enable Acceptance required . For Supported IP address types , enable IPv4 . At the bottom of the form, click the Create button. Allow Atlan account access ​ To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN , enter the Atlan account ID and root principal -  for example, arn:aws:iam::<account_id>:root . At the bottom of the form, click the Allow principals button. Notify Atlan support ​ Once all of the above steps are complete, contact Atlan support . You will need to provide Atlan support: The RDS proxy or RDS endpoint DNS -  if IAM authentication is enabled on your RDS proxy or RDS database, respectively. Once this is done, there are additional steps that Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request ​ To accept the consumer connection request, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending acceptance . Select this row, and click the Actions button and then Accept endpoint connection request . Wait for this to complete, it could take about 30 seconds. Request DNS name from Atlan ​ Contact Atlan support to request the regional DNS name of the VPC endpoint that Atlan created in the following format - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com . This is the hostname you will need to use to connect to your Microsoft SQL Server on Amazon RDS instance from within Atlan. 😅 The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl Microsoft SQL Server in Atlan! 🎉 Tags: atlan documentation Previous Set up a private network link to Microsoft SQL Server on Amazon EC2 Next What does Atlan crawl from Microsoft SQL Server? Prerequisites Set up network to RDS (in AWS) (Optional) Create RDS proxy Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request Request DNS name from Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/set-up-microsoft-sql-server",
    "content": "Connect data Databases SQL Databases Microsoft SQL Server Get Started Set up Microsoft SQL Server On this page Set up Microsoft SQL Server Who can do this? You will probably need your Microsoft SQL Server administrator to run these commands   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Microsoft SQL Server. This method uses a username and password to fetch metadata. Create a login ​ To create a login with a specific password to integrate into Atlan: CREATE LOGIN < login_name > WITH PASSWORD = '<password>' ; Replace <login_name> with the name of the login. Replace <password> with the password for the login. Create a user ​ To create a user for that login: CREATE USER < username > FOR LOGIN < login_name > ; Replace <username> with the username to use when integrating Atlan. Replace <login_name> with the name of the login used in the previous step. Grant permissions ​ Crawl assets and mine view lineage ​ The following grant crawls all your Microsoft SQL Server assets and mines lineage for views. As the option of least privilege access, it avoids accessing any raw data. To grant the minimum permissions required to crawl assets and mine view lineage from Microsoft SQL Server: GRANT VIEW DEFINITION ON DATABASE :: < database_name > TO < username > ; Replace <database_name> with the name of the database. Replace <username> with the username created above. (Optional) Preview and query assets ​ To grant the minimum permissions required to also preview sample data and query assets from Microsoft SQL Server: GRANT SELECT ON DATABASE :: < database_name > TO < username > ; Replace <database_name> with the name of the database. Replace <username> with the username created above. danger You must grant permissions to the user for all the databases you want to crawl in Atlan except the system databases ( master , tempdb , msdb , model ). The Microsoft SQL Server crawler will only fetch database and schema names without these permissions and no other metadata for other asset types. Tags: data crawl authentication Previous Microsoft SQL Server Next Crawl Microsoft SQL Server Create a login Create a user Grant permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/microsoft-sql-server/how-tos/set-up-a-private-network-link-to-microsoft-sql-server-on-amazon-ec2",
    "content": "Connect data Databases SQL Databases Microsoft SQL Server Private Network Set up a private network link to Microsoft SQL Server on Amazon EC2 On this page Set up a private network link to Microsoft SQL Server on Amazon EC2 Who can do this? You will need your AWS administrator to complete these tasks   -  you may not have access yourself. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Microsoft SQL Server on Amazon EC2 and Atlan. Prerequisites ​ You should already have the following: Your own non-default VPC configured in AWS. A Microsoft SQL Server on Amazon EC2 instance running in AWS, linked to the non-default VPC. Private subnets defined within the non-default VPC sufficient for availability. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from Atlan support . Create security group ​ You will need to create a security group for the following: Microsoft SQL Server on Amazon EC2 instance Network Load Balancer (NLB) Microsoft SQL Server on Amazon EC2 instance ​ You can either create a new security group or add the following rule to an existing security group already attached to your Microsoft SQL Server on Amazon EC2 instance. To create a security group for your Microsoft SQL Server on Amazon EC2 instance: Open the Amazon VPC console . From the left menu, under Security , click Security Groups . Click the Create security group button. Enter a name and description for the new security group. From the VPC list, select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located. For Inbound rules , leave this blank until after you have created a security group for the Network Load Balancer . Return to this step once you have created it, click the Add rule button, and then add the following rule: For Type , use MSSQL if you are using the default port (1433), or use Custom and enter your port under Port range . For Destination , add the security group you created for the NLB . Click Create security group to finish setup. Network Load Balancer ​ To create a security group for the Network Load Balancer: Open the Amazon VPC console . From the left menu, under Security , click Security Groups . Click the Create security group button. Enter a name and description for the new security group. From the VPC list, select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located. For Outbound rules , click the Add rule button and then add the following rule: For Type , use MSSQL if you are using the default port (1433), or use Custom and enter your port under Port range . For Destination , add the security group you created for your Microsoft SQL Server on Amazon EC2 instance . Click Save . Click Create security group to finish setup. Create a target group ​ To create a target group for the NLB: Open the Amazon EC2 console . From the left menu, under Load Balancing , click Target Groups . Click Create target group . For Basic configuration , enter the following details: For Choose a target type , keep Instances . For Target group name , enter a unique name for the new target group. For Protocol , select TCP . For Port , enter 1433 . For IP address type , select IPv4 . For VPC , select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located. In the Health checks section, change the protocol to TCP and keep Advanced health check settings as the default. Click Next to proceed. To register your Amazon EC2 instance, on the Register targets page: For Available instances , select your Amazon EC2 instance running Microsoft SQL Server. Keep the default port 1433 and then choose Include as pending below . At the bottom of the form, click the Create target group button. Create internal Network Load Balancer ​ To create an NLB: Open the Amazon EC2 console . From the left menu, under Load Balancing , click Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name , enter a unique name. For Scheme , select Internal . For IP address type , select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC , select the VPC where your Microsoft SQL Server on Amazon EC2 instance is located. For Mappings , select the availability zones with private subnets. For Security groups , select the security group you created for the Network Load Balancer. info 💪 Did you know? The Enforce inbound rules on PrivateLink traffic setting is turned on by default and cannot be modified until after the load balancer has been created. If this setting is left on, you will need to contact Atlan support and request the CIDR range of Atlan's cluster to add as an inbound rule on the NLB security group . To turn it off, follow these instructions . Enter the following Listeners and routing settings for the load balancer: For Protocol , select TCP . For Port , enter 1433 . For Target group , select the target group you created. Review your configuration, and click Create load balancer . Verify target group is healthy ​ To verify that the target group is healthy: From the EC2 menu on the left, under Load Balancing , click Target Groups . From the Target groups table, click the link to the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . (Note: This number could be more than 1 if you have a multi-node deployment.) Create endpoint service ​ To create an endpoint service: Open the Amazon VPC console . From the left menu, under Virtual private cloud , click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name , enter a meaningful name. For Load balancer type , choose Network . For Available load balancers , select the load balancer you created above. Enter the following Additional settings : For Require acceptance for endpoint , enable Acceptance required to require manual acceptance of connection requests to your endpoint service. Otherwise, these requests will be accepted automatically. For Enable private DNS name , leave unchecked. For Supported IP address types , enable IPv4 . At the bottom of the form, click the Create button. Once the endpoint service has been created, navigate to the Details page. From the Details page: Under Service Name , copy the value to send to Atlan. Under Availability Zones , copy the zones to send to Atlan. Allow Atlan account access ​ To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN , enter the Atlan account ID and root principal   -  for example, arn:aws:iam::<account_id>:root . At the bottom of the form, click the Allow principals button. Notify Atlan support ​ Once all of the above steps have been completed, contact Atlan support and provide the following details: Service name of the endpoint service Availability zones for the endpoint service There are additional steps Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request ​ To accept the consumer connection request, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud , click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending acceptance . Select this row, and click the Actions button and then Accept endpoint connection request . Wait for this to complete, it could take about 30 seconds. Request DNS name from Atlan ​ Contact Atlan support to request the regional DNS name of the VPC endpoint that Atlan created in the following format   - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com . This is the hostname you will need to use to connect to your Microsoft SQL Server on Amazon EC2 instance from within Atlan. 😅 The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl Microsoft SQL Server in Atlan! 🎉 Tags: atlan documentation Previous Crawl Microsoft SQL Server Next Set up a private network link to Microsoft SQL Server on Amazon RDS Prerequisites Create security group Create a target group Create internal Network Load Balancer Verify target group is healthy Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request Request DNS name from Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-a-private-network-link-to-mysql",
    "content": "Connect data Databases SQL Databases MySQL Get Started Set up a private network link to MySQL On this page Set up a private network link to MySQL Who can do this? You will need your AWS administrator to complete these tasks   -  you may not have access yourself. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between MySQL (RDS) and Atlan. Prerequisites ​ You should already have the following: Your own non-default VPC configured in AWS. A MySQL RDS instance running in AWS, linked to the non-default VPC. Private subnets defined within the non-default VPC sufficient for availability. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Setup network to RDS (in AWS) ​ To setup the private network of your MySQL instance, from within AWS : Copy network settings ​ Navigate to Services , then Database , then RDS . On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, copy the following values: Endpoint and Port values VPC value Subnet group value On the left, click Subnet groups . From the table, click the row whose Name matches the subnet group copied above. From the Subnets table, copy each value under the CIDR block column for private subnets. Create inbound rule ​ To create an inbound rule allowing your private subnet access to your RDS instance: On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, under the Security column and the VPC security groups heading click the link to your security group. At the bottom of the screen, change to the Inbound rules tab and click the Edit inbound rules button. At the bottom of the table, click the Add rule button and create the following rule: For Type use MySQL/Aurora if you are using the default port (3306), or use Custom and enter your port under Port range . For Source use Custom and enter your CIDR range (see Copy network settings ). Repeat these sub-steps for each of your CIDR ranges. Below the table, click the Save rules button. (Optional) Create RDS proxy ​ Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This method uses a username and password to connect to the RDS database. To create an RDS proxy for your RDS instance: On the left, under Amazon RDS , click on Proxies . In the upper right of the Proxies table, click the Create proxy button. Under Proxy configuration enter the following details: For Engine family select MySQL . For Proxy identifier enter a meaningful name for your proxy. Under Target group configuration for Database choose your RDS instance. Under Authentication for the Secrets Manager secrets : If you have an existing secret for your RDS instance's database credentials, select it from the drop-down. If not, click the Create a new secret link and enter these details in the new tab: For Secret type select Credentials for Amazon RDS database . For Credentials enter the Username and Password of the database user. Under Database select your RDS instance. At the bottom of the form, click the Next button. For Secret name enter a name for the secret. At the bottom of the form, click the Next button. Leave the automatic secret rotation off and click the Next button. Review the secret definition and click the Store button. Return to the tab where you started creating the RDS proxy. Under Authentication for IAM authentication : If IAM authentication is set to Required , Atlan will use an IAM role to connect to the RDS proxy. If IAM authentication is set to Not Allowed , basic authentication will be enabled. Atlan will use a username and password to connect to the RDS proxy. Under Connectivity expand the Additional connectivity configuration : For VPC security group select Choose existing . For Existing VPC security groups select the security group you edited with the inbound rules above. At the bottom right of the form, click the Create proxy button. From the Proxies table, click the link for the proxy you just created. Under Proxy endpoints section, copy the hostname in the Endpoint column. Create internal Network Load Balancer ​ Retrieve IP address of the RDS ​ From an EC2 instance in your AWS account, run the following command: nslookup <endpoint> Replace <endpoint> with the fully-qualified endpoint hostname copied from the RDS endpoint or RDS proxy created above. Copy the IP address that comes back from the command, under Non-authoritative answer and to the right of Address . Start creating NLB ​ To create an NLB, from within AWS : Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name enter a unique name. For Scheme select Internal . For IP address type select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC select the VPC where the RDS instance is located (see Copy network settings ). For Mappings select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port enter 3306 (or the non-default port value from Copy network settings ). For Default action click the Create target group link. This will open the target group creation in a new browser tab. Create target group ​ To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type select IP addresses . For Target group name enter a name. For Port enter 3306 (or the non-default port value from Copy network settings ). For IP address type select IPv4 . For VPC select the VPC where the RDS instance is located (see Copy network settings ). At the bottom of the form, click the Next button. Enter the following IP addresses settings for the target group: For Network select the VPC where the RDS instance is located (see Copy network settings ). For IPv4 address enter the IP address returned by the nslookup command (see Retrieve IP address of the RDS). For Ports enter 3306 (or the non-default port value from Copy network settings ). At the bottom of the IP addresses section, click the Include as pending below button. Confirm the following Review targets settings for the target group: Confirm IP address matches the IP address returned by the nslookup command. Confirm Port is 3306 (or the non-default port value used by your RDS instance). At the bottom of the form, click the Create target group button. Finish creating NLB ​ Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy ​ To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing click Target Groups . From the Target groups table, click the link to the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service ​ To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name enter a meaningful name. For Load balancer type choose Network . For Available load balancers select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint enable Acceptance required . For Supported IP address types enable IPv4 . At the bottom of the form, click the Create button. Did you know? Under the Details of the endpoint service, enter the DNS name of the Atlan VPC endpoint in the following format   - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com . This is the hostname you will need to use to connect to the RDS instance from within Atlan. Allow Atlan account access ​ To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN enter the Atlan account ID and specified principal. At the bottom of the form, click the Allow principals button. Notify Atlan support ​ Once all of the above steps are complete, contact Atlan support . You will need to provide Atlan support: The RDS proxy or RDS endpoint DNS   -  if IAM authentication is enabled on your RDS proxy or RDS database, respectively. Once this is done, there are additional steps that Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request ​ To accept the consumer connection request, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending acceptance . Select this row, and click the Actions button and then Accept endpoint connection request . Wait for this to complete, it could take about 30 seconds. 😅 The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl MySQL in Atlan! 🎉 Tags: atlan documentation Previous Set up MySQL Next Crawl MySQL Prerequisites Setup network to RDS (in AWS) (Optional) Create RDS proxy Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/crawl-mysql",
    "content": "Connect data Databases SQL Databases MySQL Crawl MySQL Assets Crawl MySQL On this page Crawl MySQL Once you have configured the MySQL user permissions , you can establish a connection between Atlan and MySQL. (If you are also using a private network for MySQL, you will need to set that up first , too.) To crawl metadata from MySQL, review the order of operations and then complete the following steps. Select the source ​ To select MySQL as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select MySQL Assets and click on Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your MySQL credentials: For Host Name enter the host for your MySQL instance. For Port enter the port number of your MySQL instance. For Authentication choose the method you configured when setting up the MySQL user : For Basic authentication, enter the Username and Password you configured in MySQL. For IAM User authentication, enter the AWS Access Key , AWS Secret Key , and database Username you configured. For IAM Role authentication, enter the AWS Role ARN of the new role you created and database Username you configured. (Optional) Enter the AWS External ID only if you have not configured an external ID in the role definition. Click Test Authentication to confirm connectivity to MySQL using these details. info 💪 Did you know? If you get an Error: 1129: Host ... is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts' , ask your database admin to run the FLUSH HOSTS; command in the RDS instance, and then try again. When successful, at the bottom of the screen click Next . Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from MySQL. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from MySQL. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the MySQL data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the MySQL connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any MySQL data, change Allow SQL Query to No . (Optional) To prevent users from previewing any MySQL data, change Allow Data Preview to No . At the bottom of the screen, click Next to proceed. Configure the crawler ​ Before running the MySQL crawler, you can further configure it. (Some of the options may only be available when using the direct extraction method .) You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the MySQL crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up a private network link to MySQL Next What does Atlan crawl from MySQL? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/how-tos/crawl-mongodb",
    "content": "Connect data Databases NoSQL Databases MongoDB Crawl MongoDB Assets Crawl MongoDB On this page Crawl MongoDB Once you have configured the MongoDB permissions , you can establish a connection between Atlan and MongoDB. To crawl metadata from MongoDB, review the order of operations and then complete the following steps. Select the source ​ To select MongoDB as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click MongoDB Assets . In the right panel, click Setup Workflow . Provide your credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Agent extraction, Atlan’s secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ For Extraction method , Direct is the default selection. To enter your MongoDB credentials: For SQL interface host name , enter the host name of the SQL (or JDBC) endpoint you copied from your MongoDB database. For Authentication , Basic is the default method. For Username , enter the username you created in your MongoDB database. For Password , enter the password you created for the username . For MongoDB native host , enter the host name of your MongoDB database you copied. For Default database , enter the name of the default database you copied from your MongoDB database. For Authentication database , enter the name of the authentication database you copied from your MongoDB database. admin is the default selection   -  learn more about authentication databases in MongoDB . For SSL , keep Yes to connect via a Secure Sockets Layer (SSL) channel or click No . Click the Test Authentication button to confirm connectivity to MongoDB. Once authentication is successful, navigate to the bottom of the screen and click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from MongoDB. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the MongoDB data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the MongoDB connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the MongoDB crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets, if none specified.) To have the crawler ignore collections based on a naming convention, specify a regular expression in the Exclude regex for collections field. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the MongoDB crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up MongoDB Next What does Atlan crawl from MongoDB? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mysql/how-tos/set-up-mysql",
    "content": "Connect data Databases SQL Databases MySQL Get Started Set up MySQL On this page Set up MySQL Who can do this? You will probably need your MySQL administrator to run these commands   -  you may not have access yourself. Did you know? Atlan supports both of the following AWS database engines   -  RDS MySQL and Aurora MySQL. Currently we support the following authentication mechanisms. You will need to choose one and configure it according to the steps below. Basic authentication Identity and Access Management (IAM) authentication Basic authentication ​ To configure basic authentication for MySQL, run the following commands: CREATE USER '{{db-username}}' @'%' IDENTIFIED BY '{{password}}' ; GRANT SELECT , SHOW VIEW , EXECUTE ON * . * TO '{{db-username}}' @'%' ; FLUSH PRIVILEGES ; Replace {{db-username}} with the username you want to create. Replace {{password}} with the password to be used for that username. Atlan requires the following privileges to: SELECT : Fetch the technical metadata persisted in the INFORMATION_SCHEMA . *.* is required because INFORMATION_SCHEMA tables cannot be granted access directly. Metadata is inferred from the access that the querying user has on the underlying tables. Enable users to preview or query the underlying tables and views   -  this functionality can also be turned off. SHOW VIEW enables the use of the SHOW CREATE VIEW statement to fetch view definitions for generating lineage. EXECUTE is only required if using MySQL 5.7 and any earlier versions. Identity and Access Management (IAM) authentication ​ To configure IAM authentication for MySQL follow each of these steps. Enable IAM authentication ​ To enable IAM authentication for your database instance: For Amazon RDS, follow the steps in the Amazon RDS documentation . For Aurora, follow the steps in the User Guide for Aurora documentation . When given the option, apply the changes immediately and wait until they are complete. Create database user ​ To create a database user with the necessary permissions run the following commands: CREATE USER '{{db-username}}' @'%' IDENTIFIED WITH AWSAuthenticationPlugin as 'RDS' ; GRANT SELECT , SHOW VIEW , EXECUTE ON * . * TO '{{db-username}}' @'%' ; FLUSH PRIVILEGES ; Replace {{db-username}} with the username you want to create. These permissions will allow you to crawl metadata, preview and query data from within Atlan. Create IAM policy ​ To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"rds-db:connect\" ] , \"Resource\" : [ \"arn:aws:rds-db:{{aws-region}}:{{account-id}}:dbuser:{{resource-id}}/{{db-username}}\" ] } ] } Replace {{aws-region}} with the AWS region of your database instance. Replace {{account-id}} with your account ID. Replace {{resource-id}} with the resource ID. Replace {{db-username}} with the username created in the previous step. Attach IAM policy ​ To attach the IAM policy for Atlan's use, you have two options: IAM role : Attach the policy created in the previous step to the EC2 role that Atlan uses for its EC2 instances in the EKS cluster. Please raise a support ticket to use this option. IAM user : Create an AWS IAM user and attach the policy to this user. To create an AWS IAM user: Follow the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Tags: data authentication Previous MySQL Next Set up a private network link to MySQL Basic authentication Identity and Access Management (IAM) authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/mongodb/how-tos/set-up-mongodb",
    "content": "Connect data Databases NoSQL Databases MongoDB Get Started Set up MongoDB On this page Set up MongoDB Who can do this? Atlan currently only supports integration with MongoDB Atlas . You will need your MongoDB Organization Owner or Project Owner to complete these steps   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from MongoDB. This method uses a username and password to fetch metadata. You will also need the following connection details from your MongoDB database deployment for integrating with Atlan : Host name of your MongoDB database Host name of the SQL (or JDBC) endpoint of your MongoDB database obtained via Data Federation Name of the default database Name of the authentication database Create database user in MongoDB ​ You will need to create a database user in MongoDB to allow Atlan to crawl MongoDB . A database user's access is determined by the role assigned to that user. You can either: Create a database user with a built-in role -  provides read-only access to all databases. Create a database user with a custom role -  provides restricted access to selected databases and requires allowed actions. Create database user with built-in role ​ To add a database user with a built-in role for crawling MongoDB : Sign in to your MongoDB database. From the left menu of the Data Services page, under the Security heading, click Database Access . In the upper right of the Database Access page, click Add New Database User . In the Add New Database User dialog, enter the following details: For Authentication Method , keep the default Password . For Password Authentication , there are two text fields: Enter a username for the new database user in the top text field   -  for example, atlan_user . Enter a password in the lower text field or click the Autogenerate Secure Password button to copy and use an auto-generated password. To assign database privileges to the new user, for Database Privileges , under Built-in Role , click the Add Built-in Role dropdown to select a built-in role : From the Select role dropdown, click Only read any database to assign read-only access to your MongoDB database(s). (Optional) By default, users can access all the clusters and federated database instances in the project. To restrict access to specific clusters and federated database instances: Toggle on Restrict Access to Specific Clusters/Federated Database Instances . For Grant Access To , check the boxes next to the clusters and federated database instances to which you want to grant access to the new database user. At the bottom of the dialog, click Add User to finish setup. Create database user with custom role ​ If you have a large number of databases, you can programmatically create a custom role in MongoDB using Atlas API instead   -  refer to MongoDB documentation to learn more. To add a database user with a custom role for crawling MongoDB : Sign in to your MongoDB database. From the left menu of the Data Services page, under the Security heading, click Database Access . In the Database Access page, change to the Custom Roles tab. In the upper right of the Custom Roles page, click Add New Custom Role . In the Add Custom Role dialog, for Custom Role Name , enter a meaningful name   -  for example, atlan_integration . For Action or Role , click Select Actions or Roles and grant the following privileges to the custom role: listDatabases , listed under Global Actions and Roles -  to list all existing databases in the cluster. sqlGetSchema , listed under Global Actions and Roles -  to retrieve collection schema generated by MongoDB Atlas Data Federation without read or find permission on the database or collection. listCollections , listed under Database Actions and Roles -  to list collections in a database. For Database , specify all the databases you want to crawl in Atlan. For Collection , you can either specify collections within selected databases or leave blank to include all. collStats , listed under Collection Actions -  to retrieve collection metadata such as average document size, document count, and more. For Database , specify all the databases you want to crawl in Atlan. For Collection , you can either specify collections within selected databases or leave blank to include all. find , listed under Collection Actions -  this action provides read permission on the data. Atlan requires this action for the MongoDB JDBC driver to validate Atlan's connection to the database. For Database , specify all the databases you want to crawl in Atlan. For Collection , you can either specify collections within selected databases, leave blank to include all, or restrict read access by specifying a nonexistent collection such as na , none , or - against a selected database. Click Add Custom Role to complete setup. In the Database Access page, change to the Database Users tab. In the upper right of the Database Access page, click Add New Database User . In the Add New Database User dialog, enter the following details: For Authentication Method , keep the default Password . For Password Authentication , there are two text fields: Enter a username for the new database user in the top text field   -  for example, atlan_user . Enter a password in the lower text field or click the Autogenerate Secure Password button to copy and use an auto-generated password. To assign database privileges to the new user, for Database Privileges , under Custom Roles , click the Add Custom Role dropdown. From the Select role dropdown, select the custom role you created previously. (Optional) By default, users can access all the clusters and federated database instances in the project. To restrict access to specific clusters and federated database instances: Toggle on Restrict Access to Specific Clusters/Federated Database Instances . For Grant Access To , check the boxes next to the clusters and federated database instances to which you want to grant access to the new database user. At the bottom of the dialog, click Add User to finish setup. Data Federation enables a SQL-like interface for Atlan to interact with MongoDB. It also provides schema access to collections that are either generated automatically through sampling or manual updates. This allows Atlan to fetch metadata without read access to databases or collections through the sqlGetSchema permission. Retrieve connection details ​ To retrieve connection details for crawling MongoDB : Sign in to your MongoDB database. From the left menu of the Data Services page, under the Overview heading, click Database . On the Database Deployment page, navigate to the database deployment you want to crawl in Atlan and click Connect . From the corresponding page, under Connect to your application : Click Drivers , and then navigate to the Add your connection string into your application code section: Copy the host name of your MongoDB database from the code snippet and store it in a secure location. For example, in mongodb://myDBReader:D1fficultP% [email protected] :27017/?authSource=admin , mongodb0.example.com will be the MongoDB native host . Close the dialog box and return to the Connect to your application page. Click Atlas SQL , and then navigate to the Select your driver heading: From the driver dropdown, click JDBC Driver . Navigate to the Get Connection String heading, and then for URL , copy the following connection details and store them in a secure location. As an example, jdbc:mongodb://atlas-sql-64c0b504b658f37cd67dc406-xtapf.a.query.mongodb.net/atlan_db?ssl=trueauth&Source=admin : Copy the host name of the SQL (or JDBC) endpoint of your MongoDB database atlas-sql-64c0b504b658f37cd67dc406-xtapf.a.query.mongodb.net to enter as the SQL interface host name . Copy the name of the default database atlan_db to enter as the Default database . Copy the name of the authentication database admin to enter as the Authentication database . Tags: connectors data integration crawl authentication Previous MongoDB Next Crawl MongoDB Create database user in MongoDB Retrieve connection details"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/how-tos/crawl-oracle",
    "content": "Connect data Databases SQL Databases Oracle Crawl Oracle Assets Crawl Oracle On this page Crawl Oracle Once you have configured the Oracle user permissions , you can establish a connection between Atlan and Oracle. To crawl metadata from Oracle, review the order of operations and then complete the following steps. Select the source ​ To select Oracle as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Oracle Assets and click on Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your Oracle credentials: For Host Name , enter the host for your Oracle instance. For Port , enter the port number of your Oracle instance. For Username and Password , enter the credentials you created when configuring the permissions . For SID , enter the Oracle system identifier for your database. For Default Database Name, enter the database name (usually the same as the SID). Click the Test Authentication button to confirm connectivity to Oracle using these details. Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from Oracle. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from Oracle. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Oracle data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the Oracle connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. (Optional) To prevent users from querying any Oracle data, change Allow SQL Query to No . (Note: This option has no effect when using the S3 extraction method.) (Optional) To prevent users from previewing any Oracle data, change Allow Data Preview to No . (Note: This option has no effect when using the S3 extraction method.) At the bottom of the screen, click Next to proceed. Configure the crawler ​ Before running the Oracle crawler, you can further configure it. (These options are only available when using the direct extraction method .) You can override the defaults for any of these options: To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Oracle crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . (This option is only available when using the S3 extraction method.) You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Oracle Next What does Atlan crawl from Oracle? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/how-tos/connect-on-premises-databases-to-kubernetes",
    "content": "On this page Connect on-premises databases to Kubernetes Who can do this? You will need access to a machine that can run Kubernetes on-premises. You will also need your database access details, including credentials. You can configure and use Atlan's metadata-extractor tool to extract metadata from on-premises databases with Kubernetes deployment architecture, as an alternative to using Docker Compose. Get the metadata-extractor tool ​ To get the metadata-extractor tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. Load image to Kubernetes cluster ​ You cannot upload the extractor image directly to a Kubernetes cluster. You must upload the extractor image to a container registry that your Kubernetes cluster can access. This ensures that Kubernetes can readily deploy pods with the metadata-extractor tool. Apply configuration maps ​ ConfigMaps contain essential settings that enable the metadata-extractor tool to connect to your database, including connection details and extraction parameters. This can help you customize the extraction process to fit your database environment. Deploy configurations to your specific database setup: kubectl apply -f config-maps.yml Create a config-maps.yml containing your database settings, for example: apiVersion: v1 kind: ConfigMap metadata: name: atlan-extractor-config-mysql data: DOWNLOAD_JDBC: \"true\" DOWNLOAD_JDBC_URL: \"https://example.com/path/to/jdbc-driver.tar.gz\" DRIVER: \"com.example.jdbc.Driver\" # Add other necessary configurations as key-value pairs Replace example values with details of your database connection and the JDBC driver. Deploy extraction job ​ Set up the CronJob for metadata extraction from the database: kubectl apply -f job.yml Example ​ Create a job.yml for the extraction job with details like the following: apiVersion: batch/v1 kind: CronJob metadata: name: atlan-extractor-cron-job spec: schedule: \"@weekly\" jobTemplate: spec: template: spec: containers: - name: crawler image: your-registry/path-to-extractor-image:latest # Define environment variables and volume mounts as required (Optional) Configure CronJob schedule ​ The CronJob is configured to execute weekly by default. To configure the CronJob schedule: Open the job.yml file. In the spec section, for schedule: , replace the \"@weekly\" cron expression with your preferred schedule. For example, use \"@daily\" for daily executions or provide a custom cron schedule . For more information on CronJob schedules, refer to Kubernetes documentation . (Optional) Trigger the job manually ​ To trigger an immediate metadata extraction, execute the CronJob manually: kubectl create job --from=cronjob/atlan-extractor-cron-job crawl-mysql-$(date '+%Y-%m-%d-%H-%M-%S') Request files from Atlan ​ To get started, contact Atlan support to request sample ConfigMap and CronJob files for supported SQL connectors: Microsoft SQL Server MySQL Oracle PostgreSQL Tags: connectors data Get the metadata-extractor tool Load image to Kubernetes cluster Apply configuration maps Deploy extraction job (Optional) Trigger the job manually Request files from Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/how-tos/set-up-on-premises-database-access",
    "content": "Connect data Databases On-premises On-premises Databases Get Started Set up on-premises database access On this page Set up on-premises database access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your database access details, including credentials. In some cases you won't be able to expose your databases for Atlan to crawl and ingest metadata. This may happen for various reasons: Transactional databases may have high-load mechanisms. That could make direct connection problematic. Security requirements may restrict accessing sensitive, mission critical transactional databases from outside. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites ​ To extract metadata from your on-premises databases you will need to use Atlan's metadata-extractor tool. Did you know? Atlan uses exactly the same metadata-extractor behind the scenes when it connects to cloud databases. danger If you have already installed Docker Compose, ensure that the version is 1.17.0 or higher. It is good practice to upgrade the tool to the latest available version. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the metadata-extractor tool ​ To get the metadata-extractor tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. To load the image: For Docker Image, load the image to the server you'll use to crawl databases: sudo docker load -i /path/to/jdbc-metadata-extractor-master.tar For OCI Image: Docker: Install Skopeo . Load the image to the server you'll use to crawl databases: skopeo copy oci-archive:/path/to/jdbc-metadata-extractor-master-oci.tar docker-daemon:jdbc-metadata-extractor-master:latest Podman: Load the image to the server you'll use to crawl databases: podman load -i /path/to/jdbc-metadata-extractor-master-oci.tar podman tag <loaded image hash> jdbc-metadata-extractor-master:latest Get the compose file ​ Atlan provides you a configuration file for the metadata-extractor tool. This is a Docker compose file . To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises databases. The file is docker-compose.yml . Define database connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your database connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services ​ For each on-premises database, define an entry under services in the compose file. Each entry will have the following structure: services: CONNECTION-NAME: <<: *extract environment: <<: *CONNECTION-TYPE # Credentials # Database address volumes: # Output folder Replace CONNECTION-NAME with the name of your connection. <<: *extract tells the metadata-extractor tool to run. environment contains all parameters for the tool. <<: *CONNECTION-TYPE applies default arguments for the corresponding connection type. Refer to Supported connections for on-premises databases for full details of each connection type. Example ​ Let's explain in detail using an example: services: inventory:                        # 1. Call this connection \"inventory\" <<: *extract environment: <<: *psql                     # 2. Connect to PostgreSQL using basic authentication USERNAME: some-username       # 3. Credentials PASSWORD: some-password HOST: inventory.local         # 4. Database address PORT: 5432 DATABASE: inventory volumes: - *shared-jdbc-drivers - ./output/inventory:/output  # 5. Store results in ./output/inventory The name of this service is inventory . You can use any meaningful name you want. In this example, we are using the same name as the database we're going to crawl. The <<: *psql sets the connection type to Postgres using basic authentication. USERNAME and PASSWORD specify the credentials required for the psql connection. HOST , PORT and DATABASE specify the database address. The PORT is 5432 by default, so you can omit it most of the time. The ./output/inventory:/output line specifies where to store results. You will need to replace inventory with the name of your connection. We recommend you to output metadata for different databases in separate folders. You can add as many database connections as you want. Did you know? Docker's documentation describes the services format in more detail. Secure credentials ​ danger If you decide to keep database credentials in the compose file, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To create and use Docker secrets: Create a JSON file and add the credentials that you want to use in Docker secrets. For example: { \"USERNAME\" : \"my-secret-user\" , \"PASSWORD\" : \"my-secret-password\" } info 💪 Did you know? The keys here will be the environment variable names, hence consider migrating them from the compose file to secrets. Once set to secrets, the environment variables in secrets will take precedence over the ones in the compose file. If not provided in secrets, the values will be parsed from the compose file instead. Create a new Docker secret: docker secret create my_database_credentials credentials.json At the top of your compose file, add a secrets element to access your secret: secrets : my_database_credentials : external : true Within the service section of the compose file, add a new secrets element and specify CREDENTIAL_SECRET_PATH to use it as credentials. danger If you have added database credentials directly to the compose file, Atlan recommends that you leave CREDENTIAL_SECRET_PATH as blank. For example, your compose file would now look something like this: secrets : my_database_password : external : true x-templates : # ... services : my-database : << : *extract environment : << : *psql CREDENTIAL_SECRET_PATH : \"/run/secrets/my_database_credentials\" # ... volumes : # ... secrets : - my_database_password volumes : jars : Troubleshooting secure credentials ​ Atlan recommends the following troubleshooting measures: If you're unable to create Docker secrets, ensure that Swarm mode is enabled. Secrets are encrypted during transit and at rest in a Docker swarm. Run the following command to enable Swarm mode: docker swarm init If running the compose file after providing the credentials secret results in Unsupported external secret <secret_name> , complete the following steps: Modify the compose file as follows: secrets : my_database_password : external : true x-templates : # ... services : my-database : << : *extract environment : << : *psql CREDENTIAL_SECRET_PATH : \"/run/secrets/my_database_credentials\" # ... volumes : # ... secrets : - my_database_password deploy : replicas : 1 restart_policy : condition : none volumes : jars : Run the compose file using the following command: docker stack deploy -c docker-compose.yml <stack_name> Replace the <stack_name> with the name you provided while deploying the stack. Verify deployment status using the following command: docker stack ps --no-trunc <stack_name> Replace the <stack_name> with the name you provided while deploying the stack. If stack deployment has been successfully completed, monitor the docker service logs using the following command: docker service logs <stack_name>_<service_name> --follow Replace the <stack_name> with the name you provided while deploying the stack. Replace the <service_name> with the service name in Docker. danger The docker stack deploy command will run all the services in the docker-compose.yml file, so ensure that the docker-compose.yml only contains the service you intend to run. Tags: data crawl Previous On-Premises Databases Next Crawl on-premises databases Prerequisites Get the compose file Define database connections Secure credentials"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/crawl-postgresql",
    "content": "Connect data Databases SQL Databases PostgreSQL Crawl PostgreSQL Assets Crawl PostgreSQL On this page Crawl PostgreSQL Once you have configured the PostgreSQL user permissions , you can establish a connection between Atlan and PostgreSQL. (If you are using a private network for PostgreSQL, you will need to set that up first , too.) To crawl metadata from PostgreSQL, review the order of operations and then complete the following steps. Select the source ​ To select PostgreSQL as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Postgres Assets and click on Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan's secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your PostgreSQL credentials: For Host enter the host for your PostgreSQL instance. For Port enter the port number of your PostgreSQL instance. For Authentication choose the method you configured when setting up the PostgreSQL user : For Basic authentication, enter the Username and Password you configured in PostgreSQL. For IAM User authentication, enter the AWS Access Key , AWS Secret Key , and database Username you configured. For IAM Role authentication, enter the AWS Role ARN of the new role you created and database Username you configured. (Optional) Enter the AWS External ID only if you have not configured an external ID in the role definition. For Database enter the name of the database to crawl. Click Test Authentication to confirm connectivity to PostgreSQL using these details. When successful, at the bottom of the screen click Next . Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from PostgreSQL. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include database.json , columns-<database>.json , and so on. When complete, at the bottom of the screen click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from PostgreSQL. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the PostgreSQL data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ To complete the PostgreSQL connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler ​ Before running the PostgreSQL crawler, you can further configure it. (These options are only available when using the direct extraction method.) You can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the PostgreSQL crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up PostgreSQL Next What does Atlan crawl from PostgreSQL? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/on-premises-databases/how-tos/crawl-on-premises-databases",
    "content": "Connect data Databases On-premises On-premises Databases Crawl On-premises Assets Crawl on-premises databases On this page Crawl on-premises databases Once you have set up the metadata-extractor tool , you can extract metadata from your on-premises databases using the following steps. Run metadata-extractor ​ Crawl all databases ​ To crawl all databases using the metadata-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up . Crawl a specific database ​ To crawl a specific database using the metadata-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Save the compose file and use the command sudo docker-compose up <CONNECTION-NAME> within the local folder where the compose file is stored. (Replace <CONNECTION-NAME> with the name of the connection from the services section of the compose file.) (Optional) Review generated files ​ The metadata-extractor tool will generate the following JSON files for each service : columns-<DATABASE>.json databases.json extras-procedures-<DATABASE>.json procedures-<DATABASE>.json schemas-<DATABASE>.json table-<DATABASE>.json view-<DATABASE>.json You can inspect the metadata and make sure it is acceptable to provide the metadata to Atlan. Upload generated files to S3 ​ To provide Atlan access to the extracted metadata, you need to upload the metadata to an S3 bucket. Did you know? To avoid access issues, upload to the same S3 bucket that Atlan uses. Raise a support request to get your Atlan bucket details, and include the ARN of the IAM user or IAM role so access can be provisioned. To create a separate bucket, see Option 1: Use your own bucket in the dbt documentation (the steps are the same). To upload the metadata to S3: Confirm that all the files for a particular service have the same prefix. For example, metadata/inventory/columns-inventory.json , metadata/inventory/databases.json , etc. Upload the files to the S3 bucket using your preferred method. For example, to upload all the files using the AWS CLI : aws s3 cp output/inventory s3://my-bucket/metadata/inventory --recursive Crawl metadata in Atlan ​ Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: Amazon Redshift Hive Microsoft SQL Server MySQL Oracle PostgreSQL SAP HANA Snowflake Teradata For all of the above cases, select Offline for the extraction method. Tags: connectors data crawl Previous Set up on-premises database access Next Supported connections for on-premises databases Run metadata-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/oracle/how-tos/set-up-oracle",
    "content": "Connect data Databases SQL Databases Oracle Get Started Set up Oracle On this page Set up Oracle Who can do this? You need your Oracle database administrator or a similar role to run these commands  - you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Oracle. This method uses a username and password to fetch metadata. Create user in Oracle ​ To create a username and password for basic authentication for Oracle, run the following commands: CREATE USER <username> IDENTIFIED BY <password>; GRANT CREATE SESSION TO <username>; Replace <username> with the username you want to create. Replace <password> with the password to use for that username. Grant permissions ​ Atlan requires specific privileges to crawl assets and fetch technical metadata from Oracle. Grant permissions for metadata extraction ​ Run the following commands to grant permissions for metadata extraction: GRANT SELECT_CATALOG_ROLE TO <username>; GRANT SELECT ON DBA_TABLES TO <username>; GRANT SELECT ON DBA_VIEWS TO <username>; GRANT SELECT ON DBA_TAB_COLUMNS TO <username>; GRANT SELECT ON DBA_SYNONYMS TO <username>; Replace <username> with the username you created. If these permissions aren’t sufficient in your environment, use the optional approach below. Before proceeding, revoke the previously granted DBA permissions. (Optional) Grant permissions to query and preview data ​ Grant permissions on specific tables ​ To grant permissions to query and preview data for specific tables, run the following command for each table you want to provide access to. GRANT SELECT ON <schema_name>.<table_name> TO <username>; Replace <schema_name> with the name of the schema you want to crawl. Replace <table_name> with the name of the table (or view) you want to crawl. Replace <username> with the username you created. Grant permissions on any table ​ To grant permissions on specific tables, run the following command for each table you want to provide access to. GRANT SELECT ANY TABLE TO <username>; Replace <username> with the username you created. This permission allows the new user to query tables or views in any schema except SYS and AUDSYS . danger Oracle recommends granting ANY privileges only to trusted users. These permissions allow you to crawl metadata, preview data, and run queries in Atlan, depending on the privileges granted. Tags: data crawl authentication Previous Oracle Next Crawl Oracle Create user in Oracle Grant permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-a-private-network-link-to-postgresql",
    "content": "On this page Set up a private network link to PostgreSQL AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between PostgreSQL (RDS) and Atlan, when you use our Single Tenant SaaS deployment. Who can do this? You will need your AWS administrator involved   -  you may not have access to run these tasks yourself. Prerequisites ​ You should already have the following: Your own non-default VPC configured in AWS. A PostgreSQL RDS instance running in AWS, linked to the non-default VPC. Private subnets defined within the non-default VPC sufficient for availability. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Setup network to RDS (in AWS) ​ To setup the private network of your PostgreSQL instance, from within AWS : Copy network settings ​ Navigate to Services , then Database , then RDS . On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, copy the following values: Endpoint and Port values VPC value Subnet group value On the left, click Subnet groups . From the table, click the row whose Name matches the subnet group copied above. From the Subnets table, copy each value under the CIDR block column for private subnets. Create inbound rule ​ To create an inbound rule allowing your private subnet access to your RDS instance: On the left, under Amazon RDS , click on Databases . From the Databases table, click your instance's name under the DB identifier column. Under the Connectivity & security tab, under the Security column and the VPC security groups heading click the link to your security group. At the bottom of the screen, change to the Inbound rules tab and click the Edit inbound rules button. At the bottom of the table, click the Add rule button and create the following rule: For Type use PostgreSQL if you are using the default port (5432), or use Custom and enter your port under Port range . For Source use Custom and enter your CIDR range (see Copy network settings ). Repeat these sub-steps for each of your CIDR ranges. Below the table, click the Save rules button. (Optional) Create RDS proxy ​ Before you create an RDS proxy, ensure that the user created in the RDS database is enabled with basic authentication. This method uses a username and password to connect to the RDS database. To create an RDS proxy for your RDS instance: On the left, under Amazon RDS , click on Proxies . In the upper right of the Proxies table, click the Create proxy button. Under Proxy configuration enter the following details: For Engine family select PostgreSQL . For Proxy identifier enter a meaningful name for your proxy. Under Target group configuration for Database choose your RDS instance. Under Authentication for the Secrets Manager secrets : If you have an existing secret for your RDS instance's database credentials, select it from the drop-down. If not, click the Create a new secret link and enter these details in the new tab: For Secret type select Credentials for Amazon RDS database . For Credentials enter the Username and Password of the database user. Under Database select your RDS instance. At the bottom of the form, click the Next button. For Secret name enter a name for the secret. At the bottom of the form, click the Next button. Leave the automatic secret rotation off and click the Next button. Review the secret definition and click the Store button. Return to the tab where you started creating the RDS proxy. Under Authentication for IAM authentication : If IAM authentication is set to Required , Atlan will use an IAM role to connect to the RDS proxy. If IAM authentication is set to Not Allowed , basic authentication will be enabled. Atlan will use a username and password to connect to the RDS proxy. Under Connectivity expand the Additional connectivity configuration : For VPC security group select Choose existing . For Existing VPC security groups select the security group you edited with the inbound rules above. At the bottom right of the form, click the Create proxy button. From the Proxies table, click the link for the proxy you just created. Under Proxy endpoints section, copy the hostname in the Endpoint column. Create internal Network Load Balancer ​ Retrieve IP address of the RDS ​ From an EC2 instance in your AWS account, run the following command: nslookup <endpoint> Replace <endpoint> with the fully-qualified endpoint hostname copied from the RDS proxy created above. Copy the IP address that comes back from the command, under Non-authoritative answer and to the right of Address . Start creating NLB ​ To create an NLB, from within AWS : Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name enter a unique name. For Scheme select Internal . For IP address type select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC select the VPC where the RDS instance is located (see Copy network settings ). For Mappings select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port enter 5432 (or the non-default port value from Copy network settings ). For Default action click the Create target group link. This will open the target group creation in a new browser tab. Create target group ​ To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type select IP addresses . For Target group name enter a name. For Port enter 5432 (or the non-default port value from Copy network settings ). For IP address type select IPv4 . For VPC select the VPC where the RDS instance is located (see Copy network settings ). At the bottom of the form, click the Next button. Enter the following IP addresses settings for the target group: For Network select the VPC where the RDS instance is located (see Copy network settings ). For IPv4 address enter the IP address returned by the nslookup command (see Retrieve IP address of the RDS ). For Ports enter 5432 (or the non-default port value from Copy network settings ). At the bottom of the IP addresses section, click the Include as pending below button. Confirm the following Review targets settings for the target group: Confirm IP address matches the IP address returned by the nslookup command. Confirm Port is 5432 (or the non-default port value used by your RDS instance). At the bottom of the form, click the Create target group button. Finish creating NLB ​ Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy ​ To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing click Target Groups . From the Target groups table, click the link to the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service ​ To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name enter a meaningful name. For Load balancer type choose Network . For Available load balancers select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint enable Acceptance required . For Supported IP address types enable IPv4 . At the bottom of the form, click the Create button. Did you know? Under the Details of the endpoint service, enter the DNS name of the Atlan VPC endpoint in the following format   - vpce-<hash>-<hash.>vpce-svc-<hash>.<region>.vpce.amazonaws.com . This is the hostname you will need to use to connect to the RDS instance from within Atlan. Allow Atlan account access ​ To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN enter the Atlan account ID. At the bottom of the form, click the Allow principals button. Notify Atlan support ​ Once all of the above steps are complete, contact Atlan support . You will need to provide Atlan support: The name of the endpoint service created in the previous step . Go to Endpoint Services and copy the \" Service Name \". The RDS proxy or RDS endpoint DNS   -  if IAM authentication is enabled on your RDS proxy or RDS database, respectively. Once this is done, there are additional steps that Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request ​ To accept the consumer connection request, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending acceptance . Select this row, and click the Actions button and then Accept endpoint connection request . Wait for this to complete, it could take about 30 seconds. 😅 The connection is now established. You can now use the DNS name of the Atlan VPC endpoint as the hostname to crawl PostgreSQL in Atlan! 🎉 Tags: atlan documentation Prerequisites Setup network to RDS (in AWS) (Optional) Create RDS proxy Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/how-tos/crawl-sap-hana",
    "content": "Connect data Databases SQL Databases SAP HANA Crawl SAP HANA Assets Crawl SAP HANA On this page Crawl SAP HANA Once you have configured the SAP HANA permissions , you can establish a connection between Atlan and SAP HANA. To crawl metadata from SAP HANA, review the order of operations and then complete the following steps. Select the source ​ To select SAP HANA as your source: In the top right of any screen in Atlan, navigate to New and then click New workflow . From the Marketplace page, click SAP HANA Assets . In the right panel, click Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you will need to first extract metadata yourself and make it available in S3 . Direct extraction method ​ To enter your SAP HANA credentials: For Host , enter the host name for your SAP HANA instance. For Port , enter the port number for your SAP HANA instance. For Username , enter the username you created for the instance. For Password , enter the password for the username. Click the Test Authentication button to confirm connectivity to SAP HANA. Once authentication is successful, navigate to the bottom of the screen and then click Next . Offline extraction method ​ Atlan supports the offline extraction method for fetching metadata from SAP HANA. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include databases.json , columns-<database>.json , and so on. For Bucket region , enter the name of the S3 region. When complete, at the bottom of the screen, click Next . Configure the connection ​ To complete the SAP HANA connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the SAP HANA crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the SAP HANA assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To select the SAP HANA assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the SAP HANA crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up SAP HANA Next What does Atlan crawl from SAP HANA? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/how-tos/set-up-prestosql",
    "content": "Connect data Databases Query Engines PrestoSQL Get Started Set up PrestoSQL On this page Set up PrestoSQL danger For Starburst Presto, we recommend using the Trino connector because the official Starburst documentation recommends using the Trino JDBC driver . Who can do this? You will probably need your Presto administrator to run these commands   -  you may not have access yourself. Atlan only supports PrestoSQL until version 349   -  PrestoDB is not supported at present. Currently, we only support basic (username and password) authentication for PrestoSQL. We recommend creating a separate user for Atlan with read-only access. Please ensure you are using frontend password authentication over HTTPS for clients. Create user in PrestoSQL ​ To create a new user with password file authentication follow the steps in the official Presto documentation . Grant read-only access ​ To grant read-only access to the user created above follow the steps in the official Presto documentation . This includes adding a list of catalogs you wish to crawl to your rules.json file, for example: { \"catalogs\" : [ { \"user\" : \"atlan\" , \"catalog\" : \"postgresql\" , \"allow\" : \"read-only\" } , { \"user\" : \"atlan\" , \"catalog\" : \"mysql\" , \"allow\" : \"read-only\" } , ... ] } Tags: connectors data authentication Previous PrestoSQL Next Crawl PrestoSQL Create user in PrestoSQL Grant read-only access"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/postgresql/how-tos/set-up-postgresql",
    "content": "Connect data Databases SQL Databases PostgreSQL Get Started Set up PostgreSQL On this page Set up PostgreSQL Who can do this? You will probably need your PostgreSQL administrator to run these commands   -  you may not have access yourself. Create a database role ​ To configure a database role for PostgreSQL, run the following commands: CREATE role atlan_user_role ; GRANT USAGE ON SCHEMA < schema > TO atlan_user_role ; Replace <schema> with the schema to which the user should have access. danger You (or your administrator) will need to run these statements for each database and schema you want to crawl. Atlan requires the following privileges: USAGE : Access a schema and fetch metadata. By default, users cannot access any objects in schemas that they do not own. The owner of a schema must grant the USAGE privilege on the schema to allow access. Fetch the technical metadata persisted in the INFORMATION_SCHEMA . These permissions enables Atlan to crawl metadat from PostgreSQL. (Optional) Grant permissions to query and preview data ​ To grant permissions to query data and preview sample data: GRANT SELECT , REFERENCES ON ALL TABLES IN SCHEMA schema_name TO atlan_user_role ; Replace schema_name : Name of the schema you want Atlan to access. Replace atlan_user_role : Role assigned to Atlan in your database. The SELECT privilege is required to preview and query data from within Atlan. Choose authentication mechanism ​ Atlan currently supports the following authentication mechanisms. You will need to choose one and configure it according to the steps below. Basic authentication Identity and Access Management (IAM) authentication Basic authentication ​ To create a username and password for basic authentication for PostgreSQL run the following commands: CREATE USER atlan_user password '<pass>' ; GRANT atlan_user_role TO atlan_user ; Replace <pass> with the password for the atlan_user user you are creating. Identity and Access Management (IAM) authentication ​ To configure IAM authentication for PostgreSQL follow each of these steps. Enable IAM authentication ​ To enable IAM authentication for your database instance follow the steps in the Amazon RDS documentation . When given the option, apply the changes immediately and wait until they are complete. Create database user ​ To create a database user with the necessary permissions run the following commands: Connect to the database: psql -h {{endpoint}} -U {{username}} -d {{database}} Replace {{endpoint}} with the database or cluster endpoint. Replace {{username}} with the master username (admin account) for the database. Replace {{database}} with the name of the database. Create a database user: CREATE USER {{db - username}} WITH LOGIN ; GRANT atlan_user_role , rds_iam TO {{db - username}} ; Replace {{db-username}} with the name for the database user to create. Create IAM policy ​ To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"rds-db:connect\" ] , \"Resource\" : [ \"arn:aws:rds-db:{{aws-region}}:{{account-id}}:dbuser:{{resource-id}}/{{db-username}}\" ] } ] } Replace {{aws-region}} with the AWS region of your database instance. Replace {{account-id}} with your account ID. Replace {{resource-id}} with the resource ID. Replace {{db-username}} with the username created in the previous step. Attach IAM policy ​ To attach the IAM policy for Atlan's use, you have two options: IAM role : Create a new role in your AWS account and attach the policy to this role. To create an AWS IAM role: Follow the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. Raise a support ticket to provide the AWS IAM role ARN to Atlan and get the ARN of the Node Instance Role for your Atlan EKS cluster from Atlan. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , } ] } IAM user : Create an AWS IAM user and attach the policy to this user. To create an AWS IAM user: Follow the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Tags: data crawl Previous PostgreSQL Next Crawl PostgreSQL Create a database role (Optional) Grant permissions to query and preview data Choose authentication mechanism Basic authentication Identity and Access Management (IAM) authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/prestosql/how-tos/crawl-prestosql",
    "content": "Connect data Databases Query Engines PrestoSQL Crawl PrestoSQL Assets Crawl PrestoSQL On this page Crawl PrestoSQL Once you have configured the PrestoSQL user permissions , you can establish a connection between Atlan and PrestoSQL. Did you know? Atlan currently only supports PrestoSQL until version 349. PrestoDB is not supported at present. To crawl metadata from PrestoSQL, review the order of operations and then complete the following steps. Select the source ​ To select PrestoSQL as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select PrestoSQL Assets and click on Setup Workflow . Provide credentials ​ To enter your PrestoSQL credentials: For Host enter the host for your PrestoSQL instance. For Port enter the port of your PrestoSQL instance. For Username enter the name of the user you created . For Password enter the password for the user you created . Click Test Authentication to confirm connectivity to PrestoSQL using these details. When successful, at the bottom of the screen click Next . Configure the connection ​ To complete the PrestoSQL connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler ​ Before running the PrestoSQL crawler, you can further configure it. You can override the defaults for any of these options: To select the PrestoSQL assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the PrestoSQL assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To enable or disable schema-level filtering at source, click Enable Source Level Filtering and select the relevant option. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the PrestoSQL crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up PrestoSQL Next What does Atlan crawl from PrestoSQL? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/crawl-teradata",
    "content": "Connect data Databases SQL Databases Teradata Crawl Teradata Assets Crawl Teradata On this page Crawl Teradata Once you have configured the Teradata user permissions , you can establish a connection between Atlan and Teradata. To crawl metadata from Teradata, review the order of operations and then complete the following steps. Select the source ​ To select Teradata as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Teradata Assets . In the right panel, click Setup Workflow . Provide credentials ​ Choose your extraction method: In Direct extraction, Atlan connects to your database and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . In Agent extraction, Atlan’s secure agent executes metadata extraction within the organization's environment. Direct extraction method ​ To enter your Teradata credentials: For Host , enter hostname of your Teradata instance. For Port , enter the port number of your Teradata instance. For Authentication , Basic is the default method. For Username , enter the username created when setting up user permissions. For Password , enter the password created when setting up user permissions. Click the Test Authentication button to confirm connectivity to Teradata. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from Teradata. This method uses Atlan's metadata-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket or Atlan's bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include database.json , columns-<database>.json , and so on. (Optional) For Bucket region , enter the name of the S3 region. Once completed, navigate to the bottom of the screen and click Next . Agent extraction method ​ Atlan supports using a Secure Agent for fetching metadata from Teradata. To use a Secure Agent, follow these steps: Select the Agent tab. Configure the Teradata data source by adding the secret keys for your secret store. For details on the required fields, refer to the Direct extraction section. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. Click Next after completing the configuration. Configure the connection ​ Complete the Teradata connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler ​ Before running the Teradata crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets, if none are specified.) To select the assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) To have the crawler ignore tables and views based on a naming convention, specify a regular expression in the Exclude regex for tables & views field. For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Teradata crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's assets page! 🎉 Tags: connectors data crawl setup Previous Set up Teradata Next Mine Teradata Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/sap-hana/how-tos/set-up-sap-hana",
    "content": "Connect data Databases SQL Databases SAP HANA Get Started Set up SAP HANA On this page Set up SAP HANA Who can do this? You will probably need your SAP HANA administrator to run these commands   -  you may not have access yourself. Did you know? This connector supports both SAP HANA on-premise as well as SAP HANA Cloud and SAP HANA Platform database deployments. Atlan currently only supports basic username and password authentication for fetching metadata from SAP HANA. Complete the following steps to configure it: Create a database user ​ Create a database user with the following commands: CREATE USER < username > PASSWORD < password > NO FORCE_FIRST_PASSWORD_CHANGE ; Replace <username> with the username you want to create. Replace <password> with the password for that username. Grant read permission on schema ​ Grant read permission on schema with the following commands. To crawl metadata as well as preview and query data in Atlan: GRANT SELECT , SELECT METADATA ON SCHEMA < schema > TO < username > ; To only crawl metadata in Atlan: GRANT SELECT METADATA ON SCHEMA < schema > TO < username > ; Replace <schema> with the name of the schema you want to crawl. To crawl calculation views in Atlan: GRANT SELECT ON _SYS_REPO . ACTIVE_OBJECT TO < username > ; GRANT SELECT ON _SYS_BI . BIMC_PROPERTIES TO < username > ; danger Your SAP HANA administrator will need to run these statements for each schema you want to crawl. Tags: data crawl authentication Previous SAP HANA Next Crawl SAP HANA Create a database user Grant read permission on schema"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/mine-teradata",
    "content": "Connect data Databases SQL Databases Teradata Mine Mine Teradata On this page Mine Teradata Once you have crawled assets from Teradata , you can mine its query history to construct lineage. To mine lineage from Teradata, review the order of operations and then complete the following steps. Select the miner ​ To select the Teradata miner: In the top right of any screen, navigate to +New and then click New workflow . Under Marketplace , from the filters along the top, click Miner . From the list of packages, select Teradata Miner and then click Setup Workflow . Configure the miner ​ To configure the Teradata miner: For Connection , select the connection to mine. (To select a connection, the crawler must have already run.) For Miner Extraction Method , choose your extraction method: In Query History , Atlan connects to your database and mines query history directly. In Offline , you will need to first mine query history yourself and make it available in S3 . This method uses Atlan's teradata-miner tool to mine query history. For Start date , choose the earliest date from which to mine query history. info 💪 Did you know? The miner restricts you to only querying the past two weeks of query history. If you need to query more history, for example in an initial load, consider using the S3 miner first. After the initial load, you can modify the miner's configuration to use query history extraction. (Optional) For Advanced Config , keep Default for the default configuration or click Advanced to configure the miner: For Cross Connection , click Yes to extract lineage across all available data source connections or click No to only extract lineage from the selected Teradata connection. For Control Config , if Atlan support has provided you with a custom control configuration, select Custom and enter the configuration into the Custom Config box. You can also: Enter {“ignore-all-case”: true} to enable crawling assets with case-sensitive identifiers. danger If running the miner for the first time, Atlan recommends setting a start date roughly three days prior to the current date and then scheduling it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause delays. Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Run the miner ​ To run the Teradata miner, after completing the steps above: To run the miner once, immediately, at the bottom of the screen, click the Run button. To schedule the miner to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the miner has completed running, you will see lineage for Teradata assets that were created in Teradata between the start date and when the miner ran! 🎉 Tags: connectors data crawl setup Previous Crawl Teradata Next Set up on-premises Teradata miner access Select the miner Configure the miner Run the miner"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/how-tos/crawl-trino",
    "content": "Connect data Databases Query Engines Trino Crawl Trino Assets Crawl Trino On this page Crawl Trino Once you have configured the Trino user permissions , you can establish a connection between Atlan and Trino. (If you are also using a private network for Trino, you will need to set that up first , too.) To crawl metadata from Trino, review the order of operations and then complete the following steps. Select the source ​ To select Trino as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Trino Assets and click on Setup Workflow . Provide credentials ​ To enter your Trino credentials: For Host , enter the hostname (or PrivateLink endpoint ) for your Trino instance. For Port , enter the port of your Trino instance. For Username , enter the username you created . For Password , enter the password for the user you created . For Enable TLS/HTTPS , change to True to only allow TLS or HTTPS connections or keep the default False . For Disable SSL verification , change to True to disable SSL verification for self-signed certificates or keep the default False. Click Test Authentication to confirm connectivity to Trino using these details. Once successful, at the bottom of the screen click Next . Configure the connection ​ To complete the Trino connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click Next to proceed. Configure the crawler ​ Before running the Trino crawler, you can further configure it. You can override the defaults for any of these options: To select the Trino assets you want to exclude from crawling, click Exclude Metadata . (This will default to no assets if none are specified.) To select the Trino assets you want to include in crawling, click Include Metadata . (This will default to all assets, if none are specified.) For Advanced Config , keep Default for the default configuration or click Custom to configure the crawler: For Enable Source Level Filtering , click True to enable schema-level filtering at source or click False to disable it. For Use JDBC Internal Methods , click True to enable JDBC internal methods for data extraction or click False to disable it. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Trino crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Trino Next Set up a private network link to Trino Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/how-tos/set-up-a-private-network-link-to-trino",
    "content": "Connect data Databases Query Engines Trino Private Network Set up a private network link to Trino On this page Set up a private network link to Trino Who can do this? You will need your AWS administrator involved   -  you may not have access to run these tasks yourself. AWS PrivateLink creates a secure, private connection between services running in AWS. This document describes the steps to set this up between Trino and Atlan. Prerequisites ​ You should already have the following: Trino instance running in AWS (private EC2 instance). Atlan hosted in the same region as the Trino instance. Did you know? You will also need Atlan's AWS account ID later in this process. If you do not already have this, request it now from support . Set up network to EC2 instance ​ To set up the private network of your Trino EC2 instance, from within AWS : Copy network settings ​ To copy the network settings of your EC2 instance: Navigate to Services , then Compute , then EC2 . On the left, under Instances , click Instances . In the Instances table, click your Trino EC2 instance. Under the instance's Details tab: Under VPC ID , copy the VPC identifier. Under Subnet ID , click the subnet for the instance. In the Subnets table, copy the value under the IPv4 CIDR column. Create inbound rule ​ To create an inbound rule allowing your private subnet access to your EC2 instance: Navigate to Services , then Compute , then EC2 . On the left, under Instances , click Instances . In the Instances table, click your Trino EC2 instance. Under the instance's details, change to the Security tab. Under Security groups , click the security group for the instance. Under the Inbound rules tab, click the Edit inbound rules button. At the bottom left of the Inbound rules table, click the Add rule button. For Type , select Custom TCP . For Port range , enter the port on which Trino is accessible (for example, 80 ). For Source , choose Custom and enter the CIDR range for your Trino instance (see Copy network settings ). Below the bottom right of the Inbound rules table, click the Save rules button. Create internal Network Load Balancer ​ Start creating NLB ​ To create an NLB, from within AWS: Navigate to Services , then Compute , then EC2 . On the left, under Load Balancing , click on Load Balancers . At the top of the screen, click the Create Load Balancer button. Under the Network Load Balancer option, click the Create button. Enter the following Basic configuration settings for the load balancer: For Load balancer name , enter a unique name. For Scheme , select Internal . For IP address type , select IPv4 . Enter the following Network mapping settings for the load balancer: For VPC , select the VPC where the Trino instance is located (see Copy network settings ). For Mappings , select the availability zones with private subnets. Enter the following Listeners and routing settings for the load balancer: For Port , enter 80 (or the non-default port value used in Created inbound rule ). For Default action , click the Create target group link. This will open the target group creation in a new browser tab. Create target group ​ To create a target group for the NLB: Enter the following Basic configuration settings for the target group: For Choose target type , select Instances . For Target group name , enter a name. For Port , enter 80 (or the non-default port value used in Create inbound rule ). For VPC , select the VPC where the Trino instance is located (see Copy network settings ). At the bottom of the form, click the Next button. From the Available instances table: Click the checkbox next to your Trino instance. Enter the port for the instance (80 or non-default value used in steps above). Click the Include as pending below button. At the bottom right of the form, click the Create target group button. Finish creating NLB ​ Return to the browser tab where you started the NLB creation, and continue: Under Listeners and routing , click the refresh arrow to the far right of the Default action drop-down box. Select the target group you created above in the Default action drop-down. At the bottom right of the form click the Create load balancer button. In the resulting screen, click the View load balancer button. Verify target group is healthy ​ To verify the target group is healthy: From the EC2 menu on the left, under Load Balancing click Target Groups . From the Target groups table, click the row for the target group you created above. At the bottom of the screen, under the Details tab, check that there is a 1 under both Total targets and Healthy . Create endpoint service ​ To create an endpoint service, from within AWS : Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud click Endpoint services . At the top of the page, click the Create endpoint service button. Enter the following Endpoint service settings : For Name enter a meaningful name. For Load balancer type choose Network . For Available load balancers select the load balancer you created above in Create internal Network Load Balancer . Enter the following Additional settings : For Require acceptance for endpoint enable Acceptance required . For Supported IP address types enable IPv4 . At the bottom right of the form, click the Create button. Under the Details of the endpoint service, copy the hostname under Service name . Allow Atlan account access ​ To allow Atlan's account access to the service, from within the endpoint service screen: At the bottom of the screen, change to the Allow principals tab. At the top of the Allow principals table, click the Allow principals button. Under Principals to add and ARN enter the Atlan account ID. At the bottom right of the form, click the Allow principals button. Notify Atlan support ​ Once all the above steps are complete, provide Atlan support with the following information: The hostname for the endpoint service created above. The port number for the Trino instance. There are additional steps Atlan then needs to complete: Creating a security group. Creating an endpoint. Once the Atlan team has confirmed the configuration is ready, please continue with the remaining steps. Accept the consumer connection request ​ To accept the consumer connection request, from within AWS: Navigate to Services , then Networking & Content Delivery , then VPC . From the menu on the left, under Virtual private cloud , click Endpoint services . From the Endpoint services table, select the endpoint service you created in Create endpoint service . At the bottom of the screen, change to the Endpoint connections tab. You should see a row in the Endpoint connections table with a State of Pending . Select this row, and click the Actions button and then Accept endpoint connection request . If prompted to confirm, type accept into the field and click the Accept button. Wait for this to complete, it could take about 30 seconds. 😅 The connection is now established. You can now use the service endpoint provided by Atlan support as the hostname to crawl Trino in Atlan! 🎉 Tags: integration connectors Previous Crawl Trino Next What does Atlan crawl from Trino? Prerequisites Set up network to EC2 instance Create internal Network Load Balancer Create endpoint service Allow Atlan account access Notify Atlan support Accept the consumer connection request"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/set-up-teradata",
    "content": "Connect data Databases SQL Databases Teradata Get Started Set up Teradata On this page Set up Teradata Who can do this? You will probably need your Teradata administrator to run these commands   -  you may not have access yourself. Atlan supports the basic authentication method for fetching metadata from Teradata. This method uses a username and password to fetch metadata. To create a username and password for basic authentication for Teradata, run the following commands: Create role in Teradata ​ Create a role in Teradata using the following commands: CREATE role atlan_user_role Create user in Teradata ​ Create a new user for integrating with Atlan using the following commands: CREATE USER atlan_user FROM [ database ] AS PASSWORD = [ password ] PERM = 20000000 ; Grant access to the role or directly to the user with the following commands: GRANT SELECT ON dbc . databases TO atlan_user_role ; GRANT SELECT ON dbc . tables TO atlan_user_role ; GRANT SELECT ON dbc . TablesV TO atlan_user_role ; GRANT SELECT ON DBC . TableStatsV TO atlan_user_role ; GRANT SELECT ON dbc . columns TO atlan_user_role ; GRANT SELECT ON dbc . TableTextV TO atlan_user_role ; GRANT SELECT ON dbc . TableSizeV TO atlan_user_role ; GRANT SELECT ON DBC . ColumnsV TO atlan_user_role ; GRANT SELECT ON DBC . IndicesV TO atlan_user_role ; GRANT SELECT ON DBC . All_RI_ChildrenV TO atlan_user_role ; Grant additional permissions to mine query history with the following commands: GRANT SELECT ON dbc . dbqlogtbl TO atlan_user_role ; Grant role to user ​ To grant the atlan_user_role to the new user: GRANT atlan_user_role TO atlan_user ; Tags: connectors data crawl authentication Previous Teradata Next Crawl Teradata Create role in Teradata Create user in Teradata Grant role to user"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/teradata/how-tos/set-up-on-premises-teradata-miner-access",
    "content": "Connect data Databases SQL Databases Teradata Mine Set up on-premises Teradata miner access On this page Set up on-premises Teradata miner access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Teradata instance details, including credentials. In some cases you will not be able to expose your Teradata instance for Atlan to mine query history. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the mining of query history from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Once you have mined query history on-premises and uploaded the results to S3 , you can mine query history in Atlan: How to mine Teradata Prerequisites ​ To mine query history from your on-premises Teradata instance, you will need to use Atlan's teradata-miner tool. Did you know? Atlan uses exactly the same teradata-miner behind the scenes when it connects to Teradata in the cloud. danger If you have already installed Docker Compose, ensure that the version is 1.17.0 or higher. It is good practice to upgrade the tool to the latest available version. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. But you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the teradata-miner tool ​ To get the teradata-miner tool: Raise a support ticket to get a link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to mine Teradata: sudo docker load -i /path/to/teradata-miner-master.tar Get the compose file ​ Atlan provides you with a configuration file for the teradata-miner tool. This is a Docker compose file . To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Teradata instance. The file is docker-compose.yml . Define database connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. You should ignore this section   -  do not make any changes to it. services is where you will define your Teradata connections. volumes contains mount information. You should ignore this section as well   -  do not make any changes to it. Define services ​ For each on-premises Teradata instance, define an entry under services in the compose file. Each entry will have the following structure: services: connection-name: <<: *mine environment: <<: *teradatadb USERNAME: <USERNAME> PASSWORD: <PASSWORD> HOST: <HOST> MARKER: \"0\" volumes: - ./output/connection-name:/output Replace connection-name with the name of your connection. <<: *mine tells the teradata-miner tool to run. environment contains all parameters for the tool: USERNAME -  specify the database username. PASSWORD -  specify the database password. HOST -  specify the database host. MARKER -  specify the timestamp from when queries should be mined. volumes specifies where to store results. In this example, the miner will store results in the ./output/connection-name folder on the local file system. You can add as many Teradata connections as you want. Did you know? Docker's documentation describes the services format in more detail. Secure credentials ​ Using local files ​ danger If you decide to keep Teradata credentials in plaintext files, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. Using Docker secrets ​ To create and use Docker secrets: Create a new Docker secret: printf \"This is a secret password\" | docker secret create my_database_password - At the top of your compose file, add a secrets element to access your secret: secrets: my-database-password: external: true Within the service section of the compose file, add a new secrets element and specify PASSWORD_SECRET_PATH to use it as a password. Example ​ Let's explain in detail with an example: secrets: my-database-password: external: true x-templates: # ... services: my-database: <<: *mine environment: <<: *teradatadb USERNAME: <USERNAME> PASSWORD_SECRET_PATH: \"/run/secrets/my_database_password\" # ... volumes: # ... secrets: - my-database-password volumes: jars: Tags: connectors data Previous Mine Teradata Next What does Atlan crawl from Teradata? Prerequisites Get the compose file Define database connections Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc/how-tos/crawl-sap-ecc",
    "content": "Connect data ERP SAP ECC Crawl SAP ECC Assets Crawl SAP ECC On this page Crawl SAP ECC Once you have configured the SAP ECC access permissions , you can establish a connection between Atlan and your SAP ECC system. To crawl metadata from your SAP ECC system, review the order of operations and then complete the following steps. Select the source ​ To select SAP ECC as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select SAP ECC Assets , and click Setup Workflow . Provide credentials ​ Atlan supports using a Secure Agent for fetching metadata from SAP ECC. To use a Secure Agent, follow these steps: Select the Agent tab. Add secret keys for your SAP ECC credentials in the linked secret store. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. The SAP ECC agent configuration requires the following parameters: SAP Host Name : The hostname or IP address of your SAP ECC system SAP User Name : The dedicated user account for metadata extraction SAP Password : The password for the user account Connection Type : Choose between Application Server or Message Server. If you have chosen Message server, provide below details: Port : Port number (required if using Message Server) Group : Group number (required if using Message Server) SAP System Number : Two-digit system identifier (00-99) SAP Client Number : Three-digit client identifier (001-999) Use SAP Router : Choose Yes , if using a SAP Router, and provide the SAP Router String . Click Next after completing the configuration. Configure the connection ​ To complete the SAP ECC connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, nobody can manage the connection - not even admins. Use Secure Network Connection : Choose Yes to use secure communication. When using a secure connection, prvoide below details: SNC Name : Your SNC name SNC Partner : Partner SNC name SNC Security Level : Security level SNC Library Path : Path to SNC library At the bottom of the screen, click the Next button to proceed. Configure the crawler ​ Before running the SAP ECC crawler, you can configure which components to include or exclude: To select the components you want to include in crawling, click Include Components and use the dropdown to select specific SAP components and their subcomponents. You can search for components using the search bar or use the advanced search option. To select the components you want to exclude from crawling, click Exclude Components and use the dropdown to select specific SAP components and their subcomponents to exclude from the crawl. The component selection supports hierarchical browsing, allowing you to expand and collapse component categories to select specific subcomponents. You may choose to configure the crawler to use advance settings by providing below details: SAP Language : Language code (default: EN) SAP Codepage : Character encoding (default: 0) SAP Unicode System : Choose Yes to use unicode Unicode Support : Choose Yes to enable Unicode support SAP Trace : Choose Yes to enable trace logging for debugging Connection Pool Size : Maximum number of connections (default: 5) Run the crawler ​ Follow these steps to run the SAP ECC crawler: You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you see the assets in Atlan's asset page! 🎉 Tags: erp crawl setup Previous Set up SAP ECC Next What does Atlan crawl from SAP ECC? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana/how-tos/crawl-sap-s4hana",
    "content": "Connect data ERP SAP S/4HANA Crawl SAP S/4HANA Assets Crawl SAP S/4HANA On this page Crawl SAP S/4HANA Once you have configured the SAP S/4HANA access permissions , you can establish a connection between Atlan and your SAP S/4HANA system. To crawl metadata from your SAP S/4HANA system, review the order of operations and then complete the following steps. Select the source ​ To select SAP S/4HANA as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select SAP S/4HANA Assets , and click Setup Workflow . Provide credentials ​ Atlan supports using a Secure Agent for fetching metadata from SAP S/4HANA. To use a Secure Agent, follow these steps: Select the Agent tab. Add secret keys for your SAP S/4HANA credentials in the linked secret store. Complete the Secure Agent configuration by following the instructions in the How to configure Secure Agent for workflow execution guide. The SAP S/4HANA agent configuration requires the following parameters: SAP Host Name : The hostname or IP address of your SAP S/4HANA system SAP User Name : The dedicated user account for metadata extraction SAP Password : The password for the user account Connection Type : Choose between Application Server or Message Server. If you have chosen Message server, provide below details: Port : Port number (required if using Message Server) Group : Group number (required if using Message Server) SAP System Number : Two-digit system identifier (00-99) SAP Client Number : Three-digit client identifier (001-999) Use SAP Router : Choose Yes , if using a SAP Router, and provide the SAP Router String . Click Next after completing the configuration. Configure the connection ​ To complete the SAP S/4HANA connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, nobody can manage the connection - not even admins. Use Secure Network Connection : Choose Yes to use secure communication. When using a secure connection, prvoide below details: SNC Name : Your SNC name SNC Partner : Partner SNC name SNC Security Level : Security level SNC Library Path : Path to SNC library At the bottom of the screen, click the Next button to proceed. Configure the crawler ​ Before running the SAP S/4HANA crawler, you can configure which components to include or exclude: To select the components you want to include in crawling, click Include Components and use the dropdown to select specific SAP components and their subcomponents. You can search for components using the search bar or use the advanced search option. To select the components you want to exclude from crawling, click Exclude Components and use the dropdown to select specific SAP components and their subcomponents to exclude from the crawl. The component selection supports hierarchical browsing, allowing you to expand and collapse component categories to select specific subcomponents. You may choose to configure the crawler to use advance settings by providing below details: SAP Language : Language code (default: EN) SAP Codepage : Character encoding (default: 0) SAP Unicode System : Choose Yes to use unicode Unicode Support : Choose Yes to enable Unicode support SAP Trace : Choose Yes to enable trace logging for debugging Connection Pool Size : Maximum number of connections (default: 5) Run the crawler ​ Follow these steps to run the SAP S/4HANA crawler: You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you see the assets in Atlan's asset page! 🎉 Tags: erp crawl setup Previous Set up SAP S/4HANA Next What does Atlan crawl from SAP S/4HANA? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-ecc/how-tos/set-up-sap-ecc",
    "content": "Connect data ERP SAP ECC Get Started Set up SAP ECC On this page Set up SAP ECC This guide explains how to create a dedicated service user in SAP ECC and grant the necessary permissions for Atlan to extract metadata. Prerequisites ​ Before you begin, make sure you have: Administrative access to SAP ECC. SAP system details, including: Host System number Client number Create communication user for metadata extraction ​ In the SAP GUI command field, enter SU01 and press Enter to open User Maintenance . In the User field, enter a name for the new service user and click Create . On the Address tab, provide the required contact information. Open the Logon Data tab and: Set an initial password (enter it twice). Set User Type to C (Communications Data) . Switch to the Roles tab and assign roles that enable: Remote function call (RFC) execution Table-level read access for metadata tables Access to system-level information Verify that the assigned roles enable execution of these RFC modules: STFC_CONNECTION : Verifies connectivity between Atlan and SAP ECC. RFC_SYSTEM_INFO : Retrieves system metadata such as SYSID, operating system, and release version. RFC_READ_TABLE : Enables table-level reads for metadata extraction. Click Save to confirm the changes. note The user must change the password on first login. Next steps ​ Crawl SAP ECC : Follow the instructions to extract metadata from SAP ECC using the configured service user. Tags: erp setup permissions sap-ecc Previous SAP ECC Next Crawl SAP ECC Prerequisites Create communication user for metadata extraction Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/alteryx/how-tos/integrate-alteryx",
    "content": "Connect data ETL Tools Alteryx Get Started Set up Alteryx On this page Set up Alteryx Private preview Set up real-time integration between Alteryx and Atlan using OpenLineage. This integration automatically catalogs assets and creates lineage in Atlan whenever workflows run in Alteryx, providing immediate visibility into your ETL processes. Prerequisites ​ Before you begin, make sure you have: You have Admin or Workflow Admin permissions to create workflows and generate API tokens. You have Administrator access to configure OpenLineage integration. An API token for authentication. Create workflow in Atlan ​ Follow these steps to create an Alteryx listener workflow that receives OpenLineage events: Navigate to the Workflow section. In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Alteryx and then click Create Listener . Configure the connection ​ Important A single connection (namespace) must be used for only one Alteryx instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You only need to configure the connection once to enable Atlan to receive incoming OpenLineage events. After setup, Atlan automatically processes these events as Alteryx workflows run, enabling seamless cataloging of Alteryx assets. To configure the Alteryx connection in Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . Important If no user or group is specified, the connection can't be managed, not even by admins. Click Create connection at the bottom of the screen to save and activate it. Configure the integration in Alteryx ​ Did you know? You need the Atlan API token and connection name to configure the integration in Alteryx. Contact your account manager to enable Alteryx data lineage on the Alteryx side. This enables Alteryx to connect with the OpenLineage API and send events to Atlan. An Alteryx administrator configures the OpenLineage integration using: Navigate to the OpenLineage configuration settings in your Alteryx environment. Paste the API token you generated into the API Token field. Enter the connection name you used when creating the connection in Atlan (for example, production , analytics ). Add your Atlan Hostname in the format https://your-workspace.atlan.com and click Save . Verify the connection ​ To verify that the integration is working correctly: Run a workflow in your Alteryx instance to trigger OpenLineage events. Open Atlan and check for newly created Alteryx assets under the configured connection. Confirm that lineage information from the workflow appears in the asset view. Open the Event Logs in Atlan to review the OpenLineage events and verify successful ingestion. Atlan validates the existence of the corresponding Alteryx connection by checking that the connection name matches and the API token is valid. Once the workflows finish running in Alteryx, Alteryx workflows and lineage generated from OpenLineage events appear automatically in Atlan. See also ​ API authentication Tags: connectors etl-tools alteryx workflow Previous Alteryx Next What does Atlan crawl from Alteryx? Prerequisites Create workflow in Atlan Configure the connection Configure the integration in Alteryx Verify the connection See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/database/trino/how-tos/set-up-trino",
    "content": "Connect data Databases Query Engines Trino Get Started Set up Trino On this page Set up Trino Who can do this? You will probably need your Trino administrator to run these commands   -  you may not have access yourself. Currently we only support basic (username and password) authentication for Trino. We recommend creating a separate user for Atlan with read-only access. Please ensure you are using frontend password authentication over HTTPS for clients. Create user in Trino ​ To create a new user with password file authentication follow the steps in the official Trino documentation . Grant read-only access ​ To grant read-only access to the user created above follow the steps in the official Trino documentation . This includes adding a list of catalogs you wish to crawl to your rules.json file, for example: { \"catalogs\" : [ { \"user\" : \"atlan\" , \"catalog\" : \"postgresql\" , \"allow\" : \"read-only\" } , { \"user\" : \"atlan\" , \"catalog\" : \"mysql\" , \"allow\" : \"read-only\" } , ... ] } Tags: crawl authentication Previous Trino Next Crawl Trino Create user in Trino Grant read-only access"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/erp/sap-s4hana/how-tos/set-up-sap-s4hana",
    "content": "Connect data ERP SAP S/4HANA Get Started Set up SAP S/4HANA On this page Set up SAP S/4HANA This guide explains how to create a dedicated service user in SAP S/4HANA and grant the necessary permissions for Atlan to extract metadata. Prerequisites ​ Before you begin, make sure you have: Administrative access to SAP S/4HANA. SAP system details, including: Host System number Client number Create communication user for metadata extraction ​ In the SAP GUI command field, enter SU01 and press Enter to open User Maintenance . In the User field, enter a name for the new service user and click Create . On the Address tab, provide the required contact information. Open the Logon Data tab and: Set an initial password (enter it twice). Set User Type to C (Communications Data) . Switch to the Roles tab and assign roles that enable: Remote function call (RFC) execution Table-level read access for metadata tables Access to system-level information Verify that the assigned roles enable execution of these RFC modules: STFC_CONNECTION : Verifies connectivity between Atlan and SAP S/4HANA. RFC_SYSTEM_INFO : Retrieves system metadata such as SYSID, operating system, and release version. RFC_READ_TABLE : Enables table-level reads for metadata extraction. DD_DDL_DEPENDENCY_GET : Fetches dependencies used in CDS views and table lineage. Click Save to confirm the changes. note The user must change the password on first login. Next steps ​ Crawl SAP S/4HANA : Follow the instructions to extract metadata using the configured service user. Tags: erp setup permissions sap-s4hana Previous SAP S/4HANA Next Crawl SAP S/4HANA Prerequisites Create communication user for metadata extraction Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/how-tos/set-up-aws-glue",
    "content": "Connect data ETL Tools AWS Glue Get Started Set up AWS Glue On this page Set up AWS Glue warning 🤓 Who can do this? You will need your AWS Glue Data Catalog administrator to run these commands   -  you may not have access yourself. Did you know? Prefixing all resources created for Atlan with atlan- will help you better identify them. You should also add AWS tags and descriptions to these resources for later reference. Atlan supports fetching metadata from AWS Glue Data Catalog . If you also want to be able to preview and query the data, you can set up an Amazon Athena connection instead. Create IAM policy ​ To create an IAM policy with the necessary permissions follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"glue:GetTables\" , \"glue:GetDatabases\" , \"glue:GetTable\" , \"glue:GetDatabase\" , \"glue:SearchTables\" , \"glue:GetTableVersions\" , \"glue:GetTableVersion\" , \"glue:GetPartition\" , \"glue:GetPartitions\" , \"glue:GetUserDefinedFunctions\" , \"glue:GetUserDefinedFunction\" ] , \"Resource\" : [ \"arn:aws:glue:<region>:<account_id>:tableVersion/*/*/*\" , \"arn:aws:glue:<region>:<account_id>:table/*/*\" , \"arn:aws:glue:<region>:<account_id>:catalog\" , \"arn:aws:glue:<region>:<account_id>:database/*\" ] } ] } Replace <region> with the AWS region of your Glue instance. Replace <account_id> with your account ID. danger If you're using AWS Lake Formation to manage access to your AWS resources, you will need to grant permissions in AWS Lake Formation as well as to the objects you want to crawl. Choose authentication mechanism ​ Using the policy created above, configure one of the following options for authentication. User-based authentication ​ To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role delegation-based authentication ​ To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } (Optional) To use an external ID for additional security: Generate the external ID within Atlan . Paste the external ID into the policy (replace <atlan_generated_external_id> with it): { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { \"StringEquals\" : { \"sts:ExternalId\" : \"<atlan_generated_external_id>\" } } } ] } Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: connectors data crawl Previous AWS Glue Next Crawl AWS Glue Create IAM policy Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/aws-glue/how-tos/crawl-aws-glue",
    "content": "Connect data ETL Tools AWS Glue Crawl AWS Glue Assets Crawl AWS Glue On this page Crawl AWS Glue Once you have configured the AWS Glue access permissions , you can establish a connection between Atlan and AWS Glue. To crawl metadata from AWS Glue, review the order of operations and then complete the following steps. Select the source ​ To select AWS Glue as your source: In the top right corner of any screen, navigate to New and then click New Workflow . From the list of packages, select Glue Assets , and click Setup Workflow . Provide credentials ​ To enter your AWS Glue credentials: For Authentication , choose the method you configured when setting up the AWS Glue access permissions : At the bottom, enter the Region of your AWS Glue deployment. For IAM User authentication, enter the AWS Access Key and AWS Secret Key you configured. For IAM Role authentication, enter the following: Set the AWS Role ARN to the ARN of the role you created in your AWS account . (Optional) Under External ID , click the Generate button. Click the button to the right of this to copy the generated ID and use this in setting up your trust policy . Click Test Authentication to confirm connectivity to AWS Glue. Once successful, at the bottom of the screen, click Next . Configure the connection ​ To complete the AWS Glue connection configuration: Provide a Connection Name that represents your source environment. For example, you might want to use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. At the bottom of the screen, click the Next button to proceed. Configure the crawler ​ Before running the AWS Glue crawler, you can further configure it. You can override the defaults for any of these options: Select assets you want to include in crawling in the Include Metadata field. (This will default to all assets, if none are specified.) Select assets you want to exclude from crawling in the Exclude Metadata field. (This will default to no assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the AWS Glue crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen click the Run button. To schedule the crawler to run hourly, daily, weekly or monthly, at the bottom of the screen click the Schedule & Run button. Once the crawler has completed running, you will see the assets in Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up AWS Glue Next What does Atlan crawl from AWS Glue? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/add-impact-analysis-in-gitlab",
    "content": "Connect data ETL Tools dbt Impact Analysis Add impact analysis in GitLab On this page Add impact analysis in GitLab danger For existing users, the dbt-action is no longer maintained and will be deprecated eventually. Atlan strongly recommends migrating to the atlan-action . Refer to How to migrate from dbt to Atlan action to learn more and complete the migration. If you have ever changed a dbt model only to find out later that it broke a downstream table or dashboard, Atlan provides a GitLab CI/CD pipeline to help you out. This pipeline places Atlan's impact analysis right into your merge request. So, you can view the potential downstream impact of your changes before merging the request. Prerequisites ​ Before running the action, you will need to create an Atlan API token . Assign a persona to the API token and add a metadata policy that provides requisite permissions on assets for the Atlan dbt action to work. For example, you can add the following permissions: dbt   - Read and Update Materialized layer, such as Snowflake   - Read and Update Any downstream connections, such as Microsoft Power BI   - Read only When a merge request with changes to one or more dbt models is merged, the Atlan dbt action will link the merge request as a resource to the assets in Atlan. To ensure that the merge request is linked as a resource, you will need to assign the right persona with requisite permissions to the API token . Configure the action ​ To set up the Atlan dbt action in GitLab: Define CI/CD variables in your repository: ATLAN_INSTANCE_URL with the URL of your Atlan instance. ATLAN_API_TOKEN with the value of the API token . GITLAB_TOKEN with the value of the project access token. Click the checkboxes for Mask variable and Expand variable only. Leave the Protect variable checkbox unchecked   -  merge request pipelines do not have access to protected variables . Add the GitLab pipeline to your workflow: Create a workflow file in the root directory of your repository   - .gitlab-ci.yml . Add the following code to your workflow file: stages : - get - downstream - impact get-downstream-impact-open : stage : get - downstream - impact image : node : 20 script : - git clone - branch v1 https : //github.com/atlanhq/atlan - action.git - cd atlan - action - npm install - npm run build - node ./adapters/index.js environment : name : get - downstream - impact rules : - if : '$CI_PIPELINE_SOURCE == \"merge_request_event\"' - if : '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS' when : never - if : '$CI_COMMIT_BRANCH' Test the action ​ After you've completed the configuration above, create a merge request with a changed dbt model file to test the action. You should see the Atlan GitLab CI/CD pipeline running and adding comments in your merge request: The GitLab CI/CD pipeline will add and update a single comment for every file change. The impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type. The comment will include some metadata for your impacted assets   -  such as descriptions, owners, and linked glossary terms. View the impacted assets in Atlan or open the source URL   -  for example, view an impacted Looker dashboard directly in Looker. Once you have merged the merge request, it will be added as a resource to the dbt model and its materialized assets. You can view the linked merge request from the Resources tab of the asset sidebar. For example: Inputs ​ Name Description Required GITLAB_TOKEN For writing comments on PRs to print downstream assets true ATLAN_INSTANCE_URL For making API requests to the user's tenant true ATLAN_API_TOKEN For authenticating API requests to the user's tenant true DBT_ENVIRONMENT_BRANCH_MAP For mapping the GitLab branch with a specific dbt environment false IGNORE_MODEL_ALIAS_MATCHING For turning off matching aliases using this variable false Troubleshooting the action ​ Why does the action fetch a model from an incorrect environment? ​ If there are multiple dbt models with the same name but across different environments in your Atlan instance, the action may fetch an incorrect model. In order to ensure that the action fetches a model from the right environment, you can map the GitLab branch with a specific dbt environment. This will allow the Atlan GitLab CI/CD pipeline to parse lineage for that specific environment. For example, you can provide the mapping in this format   - branch name : dbt environment name stages: - get-downstream-impact get-downstream-impact-open: stage: get-downstream-impact image: node:20 variables: +    DBT_ENVIRONMENT_BRANCH_MAP: | +      main: [Enter Your Branch name] script: - git clone   - branch v1 https://github.com/atlanhq/atlan-action.git - cd atlan-action - npm install - npm run build - node ./adapters/index.js environment: name: get-downstream-impact rules: - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"' - if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS' when: never - if: '$CI_COMMIT_BRANCH' Why does the action fetch a model by its alias and not model name? ​ By default, the action checks if there is an alias defined for a dbt model in the code and looks for the relevant asset in Atlan using that alias. To turn off matching aliases for your dbt models, you can set the IGNORE_MODEL_ALIAS_MATCHING input to true. For example: stages: - get-downstream-impact get-downstream-impact-open: stage: get-downstream-impact image: node:20 variables: +    IGNORE_MODEL_ALIAS_MATCHING: \"true\" script: - git clone   - branch v1 https://github.com/atlanhq/atlan-action.git - cd atlan-action - npm install - npm run build - node ./adapters/index.js environment: name: get-downstream-impact rules: - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"' - if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS' when: never - if: '$CI_COMMIT_BRANCH' Tags: connectors data api authentication model Previous Add impact analysis in GitHub Next What does Atlan crawl from dbt Cloud? Prerequisites Configure the action Test the action Inputs Troubleshooting the action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/add-impact-analysis-in-github",
    "content": "Connect data ETL Tools dbt Impact Analysis Add impact analysis in GitHub On this page Add impact analysis in GitHub danger For existing users, the dbt-action is no longer maintained and will be deprecated eventually. Atlan strongly recommends migrating to the atlan-action . Refer to How to migrate from dbt to Atlan action to learn more and complete the migration. If you have ever changed a dbt model only to find out later that it broke a downstream table or dashboard, Atlan provides a GitHub Action to help you out. This action places Atlan's impact analysis right into your pull request. So, you can view the potential downstream impact of your changes before merging the pull request. Prerequisites ​ Before running the action, you will need to create an Atlan API token . You will also need to assign a persona to the API token and add a metadata policy that provides the requisite permissions on assets for the Atlan dbt action to work. For example, you can add the following permissions: dbt   - Read and Update Materialized layer, such as Snowflake   - Read and Update Any downstream connections, such as Microsoft Power BI   - Read only When a pull request with changes to one or more dbt models is merged, the Atlan dbt action will link the pull request as a resource to the assets in Atlan. To ensure that the pull request is linked as a resource, you will need to assign the right persona with requisite permissions to the API token. You will need to configure the default GITHUB_TOKEN permissions. Grant Read and write permissions to the GITHUB_TOKEN in your repository to allow the atlan-action to seamlessly add or update comments on pull requests. Refer to GitHub documentation to learn more. Configure the action ​ To set up the Atlan action in GitHub: Create repository secrets in your repository: ATLAN_INSTANCE_URL with the URL of your Atlan instance. ATLAN_API_TOKEN with the value of the API token. Add the GitHub Action to your workflow: Create a workflow file in your repository   - .github/workflows/atlan-dbt.yml . Add the following code to your workflow file: name : Atlan action on : pull_request : types : [ opened , edited , synchronize , reopened , closed ] jobs : get-downstream-impact : name : Get Downstream Assets runs-on : ubuntu - latest steps : - name : Run Action uses : atlanhq/atlan - action@v1 with : GITHUB_TOKEN : $ { { secrets.GITHUB_TOKEN } } ATLAN_INSTANCE_URL : $ { { secrets.ATLAN_INSTANCE_URL } } ATLAN_API_TOKEN : $ { { secrets.ATLAN_API_TOKEN } } Test the action ​ After you've completed the configuration above, create a pull request with a changed dbt model file to test the action. You should see the Atlan GitHub action running and then adding comments in your pull request: The GitHub workflow will add and update a single comment for every file change. The impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type. The comment will include some metadata for your impacted assets   -  such as descriptions, owners, and linked glossary terms. View the impacted assets in Atlan or open the source URL   -  for example, view an impacted Looker dashboard directly in Looker. Once you have merged the pull request, it will be added as a resource to the dbt model and its materialized assets. You can view the linked pull request from the Resources tab of the asset sidebar. For example: Inputs ​ Name Description Required GITHUB_TOKEN For writing comments on PRs to print downstream assets true ATLAN_INSTANCE_URL For making API requests to the user's tenant true ATLAN_API_TOKEN For authenticating API requests to the user's tenant true DBT_ENVIRONMENT_BRANCH_MAP For mapping the GitHub branch with a specific dbt environment false IGNORE_MODEL_ALIAS_MATCHING For turning off matching aliases using this variable false Troubleshooting the action ​ Why does the action fetch a model from an incorrect environment? ​ If there are multiple dbt models with the same name but across different environments in your Atlan instance, the action may fetch an incorrect model. In order to ensure that the action fetches a model from the right environment, you can map the GitHub branch with a specific dbt environment. This will allow the Atlan GitHub action to parse lineage for that specific environment. For example, you can provide the mapping in this format   - branch name : dbt environment name jobs: get-downstream-impact: name: Get Downstream Assets runs-on: ubuntu-latest steps: - name: Run Action uses: atlanhq/atlan-action@v1 with: GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}} ATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}} ATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}} +         DBT_ENVIRONMENT_BRANCH_MAP: | +           main: dbt-prod +           beta: dbt-test Why does the action fetch a model by its alias and not model name? ​ By default, the action checks if there is an alias defined for a dbt model in the code and looks for the relevant asset in Atlan using that alias. To turn off matching aliases for your dbt models, you can set the IGNORE_MODEL_ALIAS_MATCHING input to true. For example: jobs: get-downstream-impact: name: Get Downstream Assets runs-on: ubuntu-latest steps: - name: Run Action uses: atlanhq/atlan-action@v1 with: GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}} ATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}} ATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}} +         IGNORE_MODEL_ALIAS_MATCHING: true Tags: connectors api authentication model Previous Migrate from dbt to Atlan action Next Add impact analysis in GitLab Prerequisites Configure the action Test the action Inputs Troubleshooting the action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/migrate-from-dbt-to-atlan-action",
    "content": "Connect data ETL Tools dbt Manage dbt in Atlan Migrate from dbt to Atlan action On this page Migrate from dbt to Atlan action The dbt-action is a custom action designed to perform impact analysis on changes to your dbt models in a GitHub or GitLab repository. Atlan plans to enhance this custom action to provide additional capabilities, such as impact analysis for data contracts ) and more. Instead of creating separate custom actions for each new capability, Atlan has renamed the dbt-action to atlan-action to better reflect the multiple capabilities on offer and will eventually deprecate the dbt-action . If you're currently using the dbt-action , Atlan strongly recommends migrating to the atlan-action . Migration notice and timeline ​ Atlan is providing you with a window of over six months to complete the migration, with a deadline set for June 2025. However, rest assured that Atlan will not archive the dbt-action until every organization has successfully transitioned to the atlan-action . If you choose not to migrate, please be aware that the dbt-action will no longer receive any updates. This means no new fixes or features will be implemented. Impact of migration ​ You can expect a seamless transition   -  there will be no changes in terms of functionality. Your workflows will continue to operate as usual post-migration. Migrate to Atlan action ​ GitHub ​ To migrate to the atlan-action : Open your GitHub workflow file that currently uses the dbt-action . Replace the dbt-action@v1 with atlan-action@v1 as follows: name: Atlan action on: pull_request: types: [opened, edited, synchronize, reopened, closed] jobs: get-downstream-impact: name: Get Downstream Assets runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 - name: Run Action -       uses: atlanhq/dbt-action@v1 +        uses: atlanhq/atlan-action@v1 with: GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}} ATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}} ATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}} GitLab ​ To migrate to the atlan-action : Open your GitLab workflow file .gitlab-ci.yml that currently uses the dbt-action . Clone v1 tag of atlan-action instead of the main branch of dbt-action : stages: - get-downstream-impact get-downstream-impact-open: stage: get-downstream-impact image: node:20 script: -  - git clone https://github.com/atlanhq/dbt-action.git +   - git clone --branch v1 https://github.com/atlanhq/atlan-action.git -  - cd dbt-action +   - cd atlan-action - npm install - npm run build - node ./adapters/index.js environment: name: get-downstream-impact rules: - if: '$CI_PIPELINE_SOURCE == \"merge_request_event\"' - if: '$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS' when: never - if: '$CI_COMMIT_BRANCH' Tags: connectors data model Previous Enrich Atlan through dbt Next Add impact analysis in GitHub Migration notice and timeline Impact of migration Migrate to Atlan action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud",
    "content": "Connect data ETL Tools dbt Get Started Set up dbt Cloud On this page Set up dbt Cloud Who can do this? You will probably need your dbt Cloud administrator to complete these steps   -  you may not have access yourself. If you have a dbt Cloud account, Atlan can help enrich your assets with dbt metadata. danger To enable Atlan to fetch metadata for dbt models defined in your project, you must add the dbt docs generate command to the list of commands in the job run steps. This will produce a catalog.json file containing all the relevant metadata. Alternatively, you can select the Generate docs on run checkbox to automatically generate updated project docs each time a job runs. Refer to dbt documentation to learn more. Create a token ​ Be sure to copy the generated token for crawling dbt . Service account token ​ Only dbt Cloud administrators can generate service account tokens. This is required for authenticating as a service account user and to set up granular access permissions. To generate a service account token, follow the steps in dbt documentation and configure the following permissions: Team plans: add Read-only access to all projects you want to integrate into Atlan. This permission is required to authorize requests to both the dbt Cloud Administrative API and dbt Cloud Discovery API . Enterprise plans: add Job Viewer access to all projects you want to integrate into Atlan. This will provide read-only access to your dbt account, project, environment, job, and run metadata. Learn more about dbt Cloud Enterprise permissions . Personal access token ​ danger User API tokens will be deprecated and replaced with account-scoped personal access tokens by October 22, 2024. If you have configured any dbt crawler workflows in Atlan with user API tokens, you may encounter errors. You must modify the configuration for any existing workflows with updated credentials   -  either a service account or personal access token. You can also create an account-scoped personal access token for crawling dbt . To generate a personal access token, follow the steps in dbt documentation and note the following: The user creating the token must have Job Viewer access to all projects you want to integrate into Atlan. This will provide read-only access to your dbt account, project, environment, job, and run metadata. Tags: connectors data crawl model Previous dbt Next Set up dbt Core Create a token"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/how-tos/crawl-fivetran",
    "content": "Connect data ETL Tools Fivetran Crawl Fivetran Assets Crawl Fivetran On this page Crawl Fivetran Once you have configured the Fivetran permissions , you can establish a connection between Atlan and Fivetran. To enrich Atlan with metadata from Fivetran, review the order of operations and then complete the following steps. Select the source ​ To select Fivetran as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select Fivetran Enrichment and then click Setup Workflow . Provide credentials ​ In order to run this package, you must ensure the following: The Fivetran Platform Connector is set up and has run successfully in Fivetran at least once. Fivetran logs are stored in a destination supported by Atlan. The above destination has been crawled in Atlan. To use the Fivetran Platform Connector: For Atlan Connection , Atlan will use the credentials of your selected connection to read the Fivetran Platform Connector tables associated with that connection. You can either: Create a connection in Atlan for the destination warehouse you configured while setting up the Fivetran Platform Connector in Fivetran. This connection in Atlan must have access to the Fivetran tables created by the Fivetran Platform Connector. Atlan supports the following destinations: Amazon Redshift Databricks Google BigQuery PostgreSQL Snowflake If you have already created a connection in Atlan, select the connection to extract. (To select a connection, the crawler must have already run for a supported destination.) danger If you have an existing connection, you must ensure that the user or other access permissions configured for that connection allow access to the Fivetran tables created by the Fivetran Platform Connector or update them accordingly. For Fivetran Platform Schema , select the destination schema where Fivetran logs are stored . You can only select one schema and must ensure that the connection above has access to all Fivetran log tables stored in this destination schema . Once successful, at the bottom of the screen, click Next . Configure the connection ​ To complete the Fivetran connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. Run the enrichment ​ You can now enrich Atlan with Fivetran metadata: To check for any permissions or other configuration issues before running the enrichment, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the enrichment has completed running, you will see lineage extended upstream from your data platform, warehouse, or lake! 🎉 Tags: connectors data crawl api configuration Previous Set up Fivetran Next What does Atlan crawl from Fivetran? Select the source Provide credentials Configure the connection Run the enrichment"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/crawl-dbt",
    "content": "Connect data ETL Tools dbt Crawl dbt Assets Crawl dbt On this page Crawl dbt Once you have configured dbt Cloud service token or uploaded your dbt Core project files to cloud storage , you can crawl dbt metadata into Atlan. To enrich metadata in Atlan from dbt, review the order of operations and then complete the following steps. Select the source ​ To select dbt as your source: In the top right of any screen, navigate to New and then click New Workflow . From the list of packages, select dbt Assets and then click Setup Workflow . Provide your credentials ​ dbt core ​ To enter your dbt Core credentials: For Extraction method , click Object Storage . Enter the details for the object storage location of your project files. Click the Test Authentication button to confirm connectivity to object storage using these details. Once authentication is successful, navigate to the bottom of the screen and click Next . dbt cloud ​ To enter your dbt Cloud credentials: For Extraction method , click Cloud . For Host Name , enter the domain name of your dbt Cloud instance, if not the default. Include the https:// . For more information on access URLs, refer to dbt documentation . For Authentication Type , Service Account is the default selection for service account token . Change to PAT to enter a personal access token (PAT) instead. For Token , enter the dbt Cloud token you generated . Click the Test Authentication button to confirm connectivity to dbt Cloud using these details. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the dbt connection configuration: Provide a Connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you don't specify any user or group, no one can manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the dbt crawler, you can further configure it. Did you know? If a project appears in both the include and exclude filters, the exclude filter takes precedence. dbt core ​ On the Configuration page for dbt Core, you can override the defaults for any of these options: To limit the enrichment to a particular connection with materialized assets, click Connection and select the relevant option. (This defaults to all connections, if none are specified.) To import existing tags from dbt to Atlan , for Import Tags , click Yes . dbt cloud ​ On the Configuration page for dbt Cloud, you can override the defaults for any of these options: To select the dbt projects and environments you want to exclude from crawling, click Exclude Metadata . (This defaults to no projects, if none are specified.) To select the dbt projects and environments you want to include in crawling, click Include Metadata . (This defaults to all projects, if none are specified.) To limit the enrichment to a particular connection with materialized assets, click Connection and select the relevant option. (This defaults to all connections, if none are specified.) To import existing tags from dbt to Atlan , for Import Tags , click Yes . For Advanced options , click Yes to configure the crawler further: For Enrich Metadata in Materialized Assets , click Yes to enable enrichment for both dbt and materialized assets or No for dbt assets only. Run the crawler ​ To run the dbt crawler, after completing the previous steps: To check for any permissions or other configuration issues before running the crawler, click Preflight checks You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you can see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up dbt Core Next Manage dbt tags Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/update-atlan-through-dbt",
    "content": "Connect data ETL Tools dbt Manage dbt in Atlan Enrich Atlan through dbt Enrich Atlan through dbt Beyond the default mapped dbt Cloud or dbt Core properties, you can update any of Atlan's metadata attributes (except for name , tenantId , and qualifiedName ) through your dbt model's meta property. For example, you can set:\nAnnouncements, atlan domains, certificates, custom metadata, descriptions, owners, atlan readme, tags, and terms on dbt assets and the assets that dbt materializes whenever applicable. For more details on how to do these updates, including various examples, see the dbt tabs in the Common asset actions snippets of our developer documentation: Certify assets Manage announcements Change descriptions Change owners Tag assets Change custom metadata Link terms to assets Link Atlan domains to assets Link Readme to assets Tags: connectors data crawl enrichment Previous Manage dbt tags Next Migrate from dbt to Atlan action"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/manage-dbt-tags",
    "content": "Connect data ETL Tools dbt Manage dbt in Atlan Manage dbt tags On this page Manage dbt tags Did you know? If you have already set up and crawled dbt, you do not need to make any modifications to your dbt Cloud or dbt Core setup. You only need to configure the dbt crawler to import dbt tags. Atlan will then import your existing dbt tags automatically for you. Atlan imports your dbt tags and allows you to update your dbt assets with the imported tags. Once you've crawled dbt : Your dbt assets in Atlan will be automatically enriched with their dbt tags. Imported dbt tags will be mapped to corresponding Atlan tags through case-insensitive name match   -  multiple dbt tags can be matched to a single tag in Atlan. You can also attach dbt tags to your dbt assets in Atlan   -  allowing you to categorize your assets at a more granular level. You can filter your assets by dbt tags. Import dbt tags to Atlan ​ Who can do this? You will need to be an admin user in Atlan to import dbt tags. You will also need to work with your dbt Cloud or dbt Core administrator for additional inputs and approval. Atlan imports existing dbt tags through one-way tag sync. The imported dbt tags are matched to corresponding tags in Atlan through case-insensitive name match and your dbt assets enriched with the tags synced from dbt. To allow Atlan to import and sync dbt tags, you will need to do the following: Create tags or have existing tags in dbt. Set up dbt Cloud or dbt Core to integrate with Atlan. Configure the dbt crawler to import existing tags from dbt to Atlan. For Import Tags , click Yes to import dbt tags or click No to disable it. If you subsequently modify the workflow to disable tag import, for any tags already imported, Atlan will preserve those tags. Once the crawler has completed running, tags synced from dbt will be available to use for tagging assets ! 🎉 View dbt tags in Atlan ​ Once you've crawled dbt Cloud or dbt Core , you will be able to view and manage your dbt tags in Atlan. To view synced dbt tags: From the left menu of any screen, click Governance . Under the Governance heading of the _Governance cente_r, click Tags . (Optional) Under Tags , click the funnel icon to filter tags by source type. Click dbt to filter for tags imported from dbt. In the Overview section, you can view a total count of synced dbt tags. To the right of Overview , click Synced tags to view additional details   -  including tag name, description, total count of linked assets, connection name, and timestamp for last synced. (Optional) Click the Linked assets tab to view linked assets for your dbt tag. (Optional) In the top right, click the pencil icon to add a description and change the tag icon . Tags synced from dbt cannot be renamed. You can now attach dbt tags to your dbt assets in Atlan! 🎉 Tags: connectors crawl setup Previous Crawl dbt Next Enrich Atlan through dbt Import dbt tags to Atlan View dbt tags in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-core",
    "content": "Connect data ETL Tools dbt Get Started Set up dbt Core On this page Set up dbt Core This guide explains how to set up dbt Core in Atlan, including configuring access, organizing your storage bucket, and uploading the necessary metadata files so Atlan can process and analyze your dbt project data. Setup and access management ​ In this section, learn how to configure access for dbt Core so Atlan can connect to your storage location and read the required metadata. Choose between using your own cloud storage bucket or an Atlan-managed bucket. Use your own bucket (recommended) Use Atlan bucket Depending on the cloud provider in use, go to Marketplace → search for dbt → click to set up dbt → select Object Storage, and then choose the desired cloud provider. Atlan supports reading from AWS, Azure, and GCP. The setup process prompts for the information required for each cloud provider. For authentication, refer to the following: Amazon S3 ​ Please follow the instructions below in order to create the right IAM Role with the right permissions Azure ADLS ​ Please follow the instructions below in order to create the right Service principle with the right permissions Google GCS ​ Please follow the instructions below in order to create the right Service account with the right permissions To avoid access issues, Atlan can help you uploading the required files to the same bucket where your tenant is hosted. Amazon S3 ​ Raise a support request to get the details of your Atlan S3 bucket and include the ARN value of the IAM user or IAM role that Atlan can provision access to. You need to create an IAM policy and attach it to the IAM user or role to upload the required files to your Atlan bucket. To create an IAM policy with the necessary permissions, follow the steps below Google Cloud Storage ​ To use Atlan's Google Cloud Storage (GCS) bucket, first you have to create a new service account . Then Raise a support request to share the username of the service account with Atlan. The username is in the following format: [email protected] . The Atlan support team provides you with read and write access to a particular folder in the Atlan GCS bucket. Once Atlan has granted access, you can use the service account to upload the required files. Structure the bucket ​ Once you have configured access, the next step is to organize your storage bucket so that Atlan can correctly identify and process uploaded files. info Atlan uses the metadata.invocation_id and metadata.project_id attributes to uniquely identify and link the uploaded files. Atlan doesn't use the file paths to identify a project or job that the file belongs to. The following directory structure is provided as a guideline Atlan supports extracting dbt metadata from multiple or single dbt projects. The main-prefix has the following format gcs|s3://<BUCKET_NAME>/<PATH_PREFIX> or abfss://<CONTAINER>/<PATH> , if you used Atlan's bucket, the Atlan support team provides it after setting up access policies on your bucket. You need to use the following directory structure, even if you have a single dbt project: main-prefix - project1 - job1 - manifest.json - other files - job2 - manifest.json - other files - job4 - manifest.json - other files - project3 - job5 - manifest.json - other files Upload project files ​ To load correct metadata, Atlan processes the manifest.json and run_results.json files for each job. There are many ways to load the metadata, below are suggested approaches from Atlan. You need to upload the files from the target directory of the dbt project into distinct folders. Upload the run artifacts generated from the following commands: (Required) Compilation results: dbt compile --full-refresh This command generates files that contain a full representation of your dbt project's resources, including models, tests, macros, node configurations, resource properties, and more. Files to upload: manifest.json and run_results.json Alternatively, you can upload the same files by running the dbt run --full-refresh command. (Optional) Test results: dbt test This command executes all dbt tests in a dbt project and generates files that contain the test results. Files to upload: manifest.json and run_results.json (Optional) Catalog: dbt docs generate This command generates metadata about the tables and views produced by the models in your dbt project, for example, column data types and table statistics. Files to upload: manifest.json and catalog.json Tags: atlan documentation Previous Set up dbt Cloud Next Crawl dbt Setup and access management Structure the bucket Upload project files"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/how-tos/crawl-informatica-cdi",
    "content": "Connect data ETL Tools Informatica CDI Crawl Informatica CDI Assets Crawl Informatica CDI assets On this page Crawl Informatica CDI assets Create a crawler workflow to automatically discover and catalog your Informatica Cloud Data Integration assets, including projects, workflows, and data lineage. Prerequisites ​ Before you begin, verify you have: Completed the Set up Informatica CDI guide Access to your Informatica Cloud environment Parameter files downloaded from your Secure Agent machines Create crawler workflow ​ Create a new workflow and select Informatica CDI as your connector source. In the top-right corner of any screen, select New > New Workflow . From the list of packages, select Informatica CDI Assets > Setup Workflow . Configure authentication ​ Set up secure access to your Informatica Cloud environment by providing connection credentials. In the Host field, enter your Informatica CDI domain without the protocol or sub-region. Example If your full URL is: https://usw1.dmp-us.informaticacloud.com/ Enter only: dmp-us.informaticacloud.com Enter the Username and Password for the user you created in the Set up Informatica CDI guide. Select Test Authentication to verify connectivity to Informatica CDI. After successful authentication, select Next . Configure connection ​ Set up connection management and define who can access and manage this connection. Enter a Connection Name that represents your source environment. For example, use values like production, development, gold, or analytics. To modify who can manage this connection, update the users or groups listed under Connection Admins . If you don't specify any user or group, no one can manage the connection, including admins. Select Next to continue. Configure crawler ​ Set up what to crawl and configure advanced options for accurate lineage generation. Configure metadata filters using the Include Metadata and Exclude Metadata fields. If an asset appears in both fields, the exclude metadata field takes precedence. Include Metadata : Select the projects or folders you want to include in crawling. This defaults to all assets if none are specified. Exclude Metadata : Select the projects or folders you want to exclude from crawling. This defaults to no assets if none are specified. Configure advanced options for uploading parameter files: Upload parameter files used by the Informatica CDI projects or folders in a compressed format. MIME types : Windows ZIP or Linux Zip Run crawler ​ Execute the crawler to discover and catalog your Informatica CDI assets. To run the crawler immediately, select Run . To schedule the crawler to run hourly, daily, weekly, or monthly, select Schedule & Run . After the crawler completes, you can view the assets on Atlan's asset page. See also ​ What does Atlan crawl from Informatica CDI : Understand the metadata and assets discovered during crawling Tags: connectors etl-tools informatica cdi crawl workflow Previous Set up Informatica CDI Next Transformations Prerequisites Create crawler workflow See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/fivetran/how-tos/set-up-fivetran",
    "content": "Connect data ETL Tools Fivetran Get Started Set up Fivetran On this page Set up Fivetran Fivetran Platform Connector ​ Who can do this? You need your Fivetran Account Administrator (who can create, view, edit, and delete destinations and connectors) to complete the steps below   -  you may not have access yourself. Note : You also need Fivetran Enterprise (or above) to use this integration. If you're not on such a plan yet, Atlan may be able to help you access a trial period from Fivetran. Just reach out to your Atlan contact! Create a destination ​ The Fivetran Platform Connector delivers your logs and account or destination metadata to a schema in your destination. Fivetran automatically adds this connector to every new destination you create. You need to set up at least one destination to receive the log events. Atlan currently supports the following destinations, refer to Fivetran documentation linked below to configure them in Fivetran: Amazon Redshift Databricks Google BigQuery PostgreSQL Snowflake If you have already configured a destination in Fivetran, skip to the next step. Configure Fivetran Platform Connector ​ Once you have set up a supported destination, follow the steps in this setup guide from Fivetran to set up your Fivetran Platform Connector account-wide. You must configure the Fivetran Platform Connector on an account level. Atlan recommends not excluding any columns , as this can impact the successful execution of the Fivetran enrichment package. The warehouse credentials or role configured in Atlan must have the necessary permissions to query tables in <Fivetran_destination_database>.<FPC_schema_name> . Refer to the Fivetran documentation for available FPC_schema_name values. The role must have the required permissions to access these tables. Refer to the relevant guide below for setting up permissions: Set up Snowflake Set up Google BigQuery Set up Databricks Set up Amazon Redshift Set up PostgreSQL This enables you to sync all the metadata and logs for all the connectors in your account to a single destination. Tags: data integration crawl api configuration Previous Fivetran Next Crawl Fivetran Fivetran Platform Connector"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/how-tos/set-up-matillion",
    "content": "Connect data ETL Tools Matillion Get Started Set up Matillion On this page Set up Matillion Configure user authentication and permissions in Matillion to enable Atlan to crawl metadata from your Matillion instance. This setup creates a dedicated API user with read-only access to guarantee secure metadata extraction. Important This setup guide applies only to Matillion ETL . Matillion Data Productivity Cloud (DPC) isn't supported. Prerequisites ​ Before you begin, make sure you have: Matillion Server Admin access : You need administrator privileges to create users and manage permissions Matillion instance access : Ability to log in to your Matillion instance Create user ​ To create a new user for crawling Matillion : Log in to your Matillion instance. From the top header of your Matillion instance, click Admin , and then from the dropdown, click User Configuration . On the User Configuration page, click the + button to add a new user. In the Add user dialog, enter the following details: For Username , enter a username for the new user. For Password and Repeat Password , enter a password for the new user and confirm it in the next step. For Role , click API to enable the new user to use the Matillion APIs. For Permission Groups , click Reader to enable the new user permission to only view the project and almost all parts of the instance including API profiles, credentials, OAuths, jobs, and variables - without edit access. Click OK to add the new user. On the User Configuration page, click Apply changes to confirm new user creation. Did you know? Atlan only reads metadata from Matillion and never updates or changes any objects in your instance. Set permissions ​ To set permissions for the new user in Matillion: Log in to your Matillion instance. From the top header of your Matillion instance, click Admin , and then from the dropdown, click Manage Permissions . In the Permissions dialog, for the Reader group, click the pencil icon to grant permissions for the group. In the group-specific Permissions - Reader dialog, for User Permissions , under State , click the dropdown and then click Granted . Read permission is now available to members of the group at the project level and can override a Forbidden Expected State . Click OK to confirm your selections. Next steps ​ Crawl Matillion - Use the credentials you just created to establish a connection and crawl metadata from Matillion Tags: connectors etl_tools matillion setup Previous Matillion Next Crawl Matillion Prerequisites Create user Set permissions Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/matillion/how-tos/crawl-matillion",
    "content": "Connect data ETL Tools Matillion Crawl Matillion Assets Crawl Matillion On this page Crawl Matillion Once you have configured the Matillion user permissions , you can establish a connection between Atlan and Matillion. To crawl metadata from Matillion, review the order of operations and then complete the following steps. Select the source ​ To select Matillion as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Matillion Assets . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Matillion credentials: For Extraction method , Direct is the default selection. For Hostname , enter the host name of your Matillion instance. For Authentication , Basic Authentication is the default method. For Username , enter the username you created in Matillion . For Password , enter the password you created for the username . For SSL , keep Enabled to connect via a Secure Sockets Layer (SSL) channel or click Disabled . Click the Test Authentication button to confirm connectivity to Matillion. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Matillion connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . If you don't specify any user or group, no one can manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Matillion crawler, you can further configure it. On the Metadata Filters page, you can override the defaults for any of these options. If an asset appears in both the include and exclude filters, the exclude filter takes precedence. To select the assets you want to include in crawling, click Include Projects . (This defaults to all assets, if none are specified.) To select the assets you want to exclude from crawling, click Exclude Projects . (This defaults to no assets, if none specified.) For Enable Lineage , keep the default option Yes to crawl lineage or click No to disable it. End-to-end lineage is currently not supported for Matillion version 1.68 LTS due to limitations of the Matillion APIs   -  only lineage for asset transformations is supported at present. Run the crawler ​ To run the Matillion crawler, after completing the previous steps: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you can see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Matillion Next What does Atlan crawl from Matillion? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/amazon-mwaa-openlineage/how-tos/integrate-amazon-mwaa-openlineage",
    "content": "Connect data Orchestration & Workflow Amazon MWAA OpenLineage Get Started How to integrate Amazon MWAA/OpenLineage On this page Integrate Amazon MWAA/OpenLineage To integrate Amazon Managed Workflows for Apache Airflow (MWAA) with Atlan, complete the following steps. (Alternatively, you can use the AWS Secrets Manager to store the environment variables and fetch them using the plugin, follow the steps here to do so.) To learn more about OpenLineage, refer to OpenLineage configuration and facets . Did you know? For Apache Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see How to implement OpenLineage in Airflow operators . Also, check the recommended provider package versions for OpenLineage . Create an API token in Atlan ​ Before running the workflow, you need to create an API token in Atlan. Configure the integration in Atlan ​ Select the source ​ To select Amazon MWAA/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Amazon MWAA Airflow Assets and then click Setup Workflow . Create the connection ​ danger A single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets. To configure the Amazon MWAA/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. (Optional) For Host , enter the URL of your Apache Airflow UI   -  do not include any extra paths such as /home in the URL. This will allow Atlan to help you view your assets directly in Amazon MWAA from the asset profile. (Optional) For Port , enter the port number for your Apache Airflow UI. For Enable OpenLineage Events , click Yes to enable the processing of OpenLineage events or click No to disable it. If disabled, new events will not be processed in Atlan. To create a connection, at the bottom of the screen, click the Create connection button. Configure the integration in Amazon MWAA ​ Did you know? You will need the Atlan API token and connection name to configure the integration in Amazon MWAA. This will allow Amazon MWAA to connect with the OpenLineage API and send events to Atlan. danger Atlan does not support integrating with Apache Airflow versions older than 2.5.0. To configure Amazon MWAA to send OpenLineage events to Atlan: Based on your Apache Airflow version on Amazon MWAA, there may be additional prerequisites for using OpenLineage: For Apache Airflow versions 2.7.0 onward, update the requirements.txt file of your Apache Airflow instance with: apache-airflow-providers-openlineage For Apache Airflow versions 2.5.0 onward and prior to 2.7.0, update the requirements.txt file of your Apache Airflow instance: openlineage-airflow To set environment variables, you will need to deploy a custom plugin to Amazon MWAA. Create an env_var_plugin.py file and add the following Python code in the plugin: For Apache Airflow versions 2.7.0 onward: from airflow . plugins_manager import AirflowPlugin import os os . environ [ \"AIRFLOW__OPENLINEAGE__NAMESPACE\" ] = \"<connection_name>\" os . environ [ \"AIRFLOW__OPENLINEAGE__TRANSPORT\" ] = '''{ \"type\": \"http\", \"url\": \"https://<instance>.atlan.com/events/openlineage/airflow-mwaa/\", \"auth\": { \"type\": \"api_key\", \"api_key\": \"<API_token>\" } }''' os . environ [ \"AIRFLOW__OPENLINEAGE__CONFIG_PATH\" ] = \"\" os . environ [ \"AIRFLOW__OPENLINEAGE__DISABLED_FOR_OPERATORS\" ] = \"\" class EnvVarPlugin ( AirflowPlugin ) : name = \"env_var_plugin\" AIRFLOW__OPENLINEAGE__NAMESPACE : replace <connection_name> with the connection name as exactly configured in Atlan. AIRFLOW__OPENLINEAGE__TRANSPORT : specify details of where and how to send OpenLineage events. Replace <instance> with the name of your Atlan instance. Replace <API_token> with the API token generated in Atlan. AIRFLOW__OPENLINEAGE__CONFIG_PATH : specifies that the apache-airflow-providers-openlineage package read the OpenLineage config from environment variables instead of a config file. AIRFLOW__OPENLINEAGE__DISABLED_FOR_OPERATORS : specifies that OpenLineage must send events for all operators   -  only required for the apache-airflow-providers-openlineage package. For Apache Airflow versions 2.5.0 onward and prior to 2.7.0: from airflow . plugins_manager import AirflowPlugin import os os . environ [ \"OPENLINEAGE_URL\" ] = \"https://<instance>.atlan.com/events/openlineage/airflow-mwaa/\" os . environ [ \"OPENLINEAGE_NAMESPACE\" ] = \"<connection_name>\" os . environ [ \"OPENLINEAGE_API_KEY\" ] = \"<API_token>\" class EnvVarPlugin ( AirflowPlugin ) : name = \"env_var_plugin\" OPENLINEAGE_URL : points to the service that will consume OpenLineage events   -  for example, https://<instance>.atlan.com/events/openlineage/airflow-mwaa/ . OPENLINEAGE_NAMESPACE : set the connection name as exactly configured in Atlan. OPENLINEAGE_API_KEY : set the API token generated in Atlan. Amazon MWAA allows you to install a plugin through a zip archive. You can either: Use the following code to zip your env_var_plugin.py file: zip plugins.zip env_var_plugin.py If you already have a plugins.zip file, add the env_var_plugin.py file to your zip file. Upload the plugins.zip and requirements.txt files to the S3 bucket connected to your Amazon MWAA environment. Amazon MWAA requires your DAGs, plugins, and requirements.txt file to be in the same S3 bucket, which serves as the source location for your environment. You will need to specify the path for the latest versions of the plugins.zip and requirements.txt files in Amazon MWAA. To specify the path: Open the Environments page on the Amazon MWAA console. Select an environment and then click Edit . In the DAG code in Amazon S3 section, configure the following: For Plugins file - optional , select the plugins.zip file in the S3 bucket connected to your Amazon MWAA environment or choose the latest plugins.zip version from the dropdown list. For Requirements file - optional , select the latest requirements.txt file version from the dropdown list. Click Next, Update environment. or Next to save your configurations. Verify the Atlan connection in Amazon MWAA ​ To verify connectivity to Amazon MWAA: For Verify connection with MWAA , click the clipboard icon to copy and run the preflight check DAG on your Amazon MWAA instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the preflight checks documentation . Click Done to complete setup. Once your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! 🎉 You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: connectors data configuration Previous Amazon MWAA OpenLineage Next What does Atlan crawl from Amazon MWAA/OpenLineage? Create an API token in Atlan Configure the integration in Atlan Configure the integration in Amazon MWAA Verify the Atlan connection in Amazon MWAA"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/how-tos/implement-openlineage-in-airflow-operators",
    "content": "Connect data Orchestration & Workflow Apache Airflow OpenLineage Implement OpenLineage How to implement OpenLineage in Airflow operators On this page Implement OpenLineage in Airflow operators This document helps you learn how to implement OpenLineage support for any Airflow operator. To implement OpenLineage support, consider the following types of operators: Supported operators ​ If you're using an Airflow operator supported by OpenLineage, the OpenLineage events will contain input and output details. This means that you do not have to modify your current DAG implementation and Atlan will be able to generate data lineage. To install OpenLineage, refer to the documentation for supported sources: Apache Airflow Amazon MWAA Astronomer Google Cloud Composer For Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation . This documentation is automatically updated when OpenLineage support is added to any operator from a provider package. You have to make sure that you're using the latest version of the provider package. For more information, see the recommended provider package versions for OpenLineage . Custom and unsupported operators ​ If you're using a custom or an unsupported operator, your Airflow tasks will still emit OpenLineage events but may not include task-specific metadata such as inputs and outputs, SQL query, and more. This may limit Atlan from being able to generate data lineage. To implement OpenLineage support for custom and unsupported operators, refer to Implementing OpenLineage in Operators documentation . To help you understand the process, following is an example: Sample implementation ​ This approach is recommended when working with your own operators, where you can directly implement OpenLineage methods. You can also refer to OpenLineage documentation for more details. To implement OpenLineage support for a custom or an unsupported operator: Open the Operator class definition to which you want to add OpenLineage support. Implement at least one of the following OpenLineage methods in the Operator class: get_openlineage_facets_on_start() get_openlineage_facets_on_complete() The function should return datasets in the form of inputs and outputs with OpenLineage-compliant dataset names. This allows an OpenLineage consumer such as Atlan to properly match dataset metadata collected from different sources. To learn more about naming conventions, refer to OpenLineage documentation . Example ​ Below is an example of a properly implemented get_openlineage_facets_on_complete method for the GCSToGCSOperator . In this example, since there is some level of processing included in the execute method with no relevant failure data, implementing this single method was sufficient. def get_openlineage_facets_on_complete(self , task_instance) : \"\" \" Implementing _on_complete because execute method does preprocessing on internals. This means we won't have to normalize self.source_object and self.source_objects , destination bucket and so on. \"\" \" from airflow.providers.common.compat.openlineage.facet import Dataset from airflow.providers.openlineage.extractors import OperatorLineage return OperatorLineage( inputs= [ Dataset(namespace=f \"gs://{self.source_bucket}\" , name=source) for source in sorted(self.resolved_source_objects) ] , outputs= [ Dataset(namespace=f \"gs://{self.destination_bucket}\" , name=target) for target in sorted(self.resolved_target_objects) ] , ) Test implementation ​ Atlan recommends that you test your changes locally by running Apache Airflow on local and setting the OpenLineage transport as the \"console\". You can use Astronomer on local as it is easy and quick, but feel free to use any other method. To test your implementation locally: Install the Docker Desktop application in your system. Install Astro CLI . In your root directory, create a directory for the following Astronomer files   - mkdir astro-airflow and cd astro-airflow . Initialize an Astronomer project with the command astro init . This will create the required files in the directory you created above. Open the .env file, add AIRFLOW__OPENLINEAGE__TRANSPORT='{\"type\": \"console\"}' to the file, and save it. Add a test DAG with tasks using your custom operator with OpenLineage support to the astro-airflow/dags folder. Start Astronomer Airflow with the command astro dev start . Open http://localhost:8080/home after Astronomer Airflow has started. Run the DAG that uses your custom operator with OpenLineage support. Open DAG run task logs and locate the OpenLineage events in the logs. (Optional) Format the JSON OpenLineage events in your IDE using this online tool . Ensure that the OpenLineage events contain input and output details. For example: { \"eventTime\" : \"2024-12-27T17:55:24.407459+00:00\" , \"eventType\" : \"COMPLETE\" , \"inputs\" : [ { \"facets\" : { } , \"name\" : \"dir1/dir2/sample.csv\" , \"namespace\" : \"s3a://atlan-test-bucket\" } ] , ... ... \"outputs\" : [ { \"facets\" : { } , \"name\" : \"wide_world_importers.astronomer_assets.sample\" , \"namespace\" : \"databricks://dbc-8d941db8-48cd.cloud.databricks.com\" } ] , ... ... \"schemaURL\" : \"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\" } To view other implementation examples, refer to the following documentation: GCSToGCSOperator AzureBlobStorageToGCSOperator (Optional) Community contribution ​ If you add OpenLineage support to an operator from the list of commonly used provider packages, consider updating the Apache Airflow repository . This allows other users to implement your code and improve it over time. Here is an example of a contribution to the community from a member of the Atlan team. Frequently asked questions ​ Can Atlan extract lineage from PythonOperator or BashOperator? ​ OpenLineage supports both PythonOperator and BashOperator. However, these core operators function as \"black box\" operators, capable of running any code. This in turn may limit the extent of lineage extraction. If the lineage generated is incomplete, Atlan suggests that you use manually annotated lineage (inlets and outlets). Can Atlan extract lineage from KubernetesPodOperator? ​ OpenLineage neither supports KubernetesPodOperator nor a managed service such as EksPodOperator or GKEStartPodOperator . This is because these operators also function as \"black box\" operators, capable of running any code. Limited execution details are exposed to the operator, thus limiting Atlan's ability to extract lineage. Atlan suggests that you use manually annotated lineage (inlets and outlets). Are there other methods to implement OpenLineage support for lineage generation through events? ​ Yes, you can use manually annotated lineage , which requires updating the DAG code. Keep in mind that this is a fallback measure, only recommended for very specific use cases, such as when it is impossible to extract lineage from the operator itself. Manually annotated lineage is also difficult to update and prone to manual errors. Tags: connectors data Previous How to integrate Apache Airflow/OpenLineage Next What does Atlan crawl from Apache Airflow/OpenLineage? Supported operators Custom and unsupported operators Frequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/informatica-cdi/how-tos/set-up-informatica-cdi",
    "content": "Connect data ETL Tools Informatica CDI Get Started Set up Informatica CDI On this page Set up Informatica CDI Configure user authentication and gather required parameter files to enable the Informatica Cloud Data Integration connector in Atlan. This guide walks you through creating a user with the Designer role and preparing the necessary parameter files for accurate lineage generation. Prerequisites ​ Before you begin, make sure you have: Access to Informatica Cloud as an Org Administrator Network connectivity to your Informatica Cloud instance Admin permissions in Atlan to create connections Create user ​ Informatica CDI connector supports Native authentication type . Perform the following steps as an Org Administrator in Informatica Cloud to set up user authentication for the connector. Log in to Informatica IICS as an Org Admin. Go to Administrator → Users . Click the Add User button (➕). Follow the steps in the official document for Creating the user . Assign the Designer role to the user. For more information, see Designer role official documentation Gather parameter files ​ Parameter files are used to define source or target schema and table names referenced in job definitions. You need these files to generate accurate lineage. Identify the IICS CDI Projects or Folders you plan to include in the Atlan workflow. Locate the parameter files on your Informatica Cloud Secure Agent machines where the ETL jobs run. Download the parameter files for each project or folder. These are needed when configuring the crawler in Atlan. Next steps ​ Crawl Informatica CDI assets : Configure and run the crawler to discover and catalog your Informatica CDI assets Tags: connectors etl-tools informatica cdi setup authentication Previous Informatica CDI Next Crawl Informatica CDI assets Prerequisites Create user Gather parameter files Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/crawl-microsoft-azure-data-factory",
    "content": "Connect data ETL Tools Microsoft Azure Data Factory Crawl Microsoft Azure Data Factory Assets Crawl Microsoft Azure Data Factory On this page Crawl Microsoft Azure Data Factory Once you have configured the Microsoft Azure Data Factory permissions , you can establish a connection between Atlan and Microsoft Azure Data Factory. To crawl metadata from Microsoft Azure Data Factor y, review the order of operations and then complete the following steps. Select the source ​ To select Microsoft Azure Data Factory as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Azure Data Factory Assets . In the right panel, click Setup Workflow . Provide credentials ​ To enter your Microsoft Azure Data Factory credentials: For Extraction method , Direct is the default selection. For Authentication , Service Principal is the default selection. For Client ID , enter the application (client) ID you copied for the service principal. For Client Secret , enter the client secret you copied for the service principal. For Tenant ID , enter the directory (tenant) ID you copied for the service principal. Click the Test Authentication button to confirm connectivity to Microsoft Azure Data Factory. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Microsoft Azure Data Factory connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Run the crawler ​ To run the Microsoft Azure Data Factory crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Microsoft Azure Data Factory Next What does Atlan crawl from Microsoft Azure Data Factory? Select the source Provide credentials Configure the connection Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/etl-tools/microsoft-azure-data-factory/how-tos/set-up-microsoft-azure-data-factory",
    "content": "Connect data ETL Tools Microsoft Azure Data Factory Get Started Set up Microsoft Azure Data Factory On this page Set up Microsoft Azure Data Factory Atlan supports service principal authentication for fetching metadata from Microsoft Azure Data Factory. This method requires a client ID, client secret, and tenant ID to fetch metadata. Register app with Microsoft Entra ID ​ Who can do this? You will need your Cloud Application Administrator or Application Administrator to complete these steps   -  you may not have access yourself. This will be required if the creation of registered applications is not enabled for the entire organization. You will need to register your service principal application with Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret. To register your app with Microsoft Entra ID: Log in to the Azure portal . In the search bar, search for Microsoft Entra ID , and select it from the dropdown list. From the left menu of the Microsoft Entra ID page, click App registrations . From the toolbar on the App registrations page, click + New registration . On the Register an application page, for Name , enter a name for your service principal application and then click Register . On the homepage of your newly created application, from the Overview screen, copy the values for the following fields and store them in a secure location: Application (client) ID Directory (tenant) ID From the left menu of your newly created application page, click Certificates & secrets . On the Certificates & secrets page, under Client secrets , click + New client secret . In the Add a client secret screen, enter the following details: For Description , enter a description for your client secret. For Expiry , select when the client secret will expire. Click Add . On the Certificates & secrets page, under Client secrets , for the newly created client secret, click the clipboard icon to copy the Value and store it in a secure location. Set permissions ​ Who can do this? You will need your Microsoft Azure Data Factory administrator to complete these steps   -  you may not have access yourself. You will need to add the service principal to the Reader role . This will allow the service principal read-only access to your Microsoft Azure Data Factory account. To add the service principal to the Reader role: Log in to the Azure portal . Open the menu and search for or select Data factories . On the Data factories page, select the data factory you want to crawl in Atlan. From the left menu of your data factory page, click Access control (IAM) . From the tabs along the top of the Access control (IAM) page, click Add and then click Add role assignment . On the Add role assignment page, configure the following: In the Roles tab, from the list of roles under Job function roles , select Reader -  this allows read-only access to your data factory -  and then click Next . You will need to assign this role to all the data factories you want to crawl in Atlan. In the Members tab, enter the following details: For Assign access to , click User, group, or service principal . For Members , click + Select members and then select the service principal you created. Click Next to proceed to the next step. In the Review + assign tab, click Review + assign to add role assignment. Atlan will extract metadata from all the data factories you specified in your Microsoft Azure Data Factory account with Reader access. Tags: data api authentication Previous Microsoft Azure Data Factory Next Crawl Microsoft Azure Data Factory Register app with Microsoft Entra ID Set permissions"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/astronomer-openlineage/how-tos/integrate-astronomer-openlineage",
    "content": "Connect data Orchestration & Workflow Astronomer OpenLineage Get Started How to integrate Astronomer/OpenLineage On this page Integrate Astronomer/OpenLineage To integrate Astronomer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to OpenLineage configuration and facets . Did you know? For Apache Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see How to implement OpenLineage in Airflow operators . Also, check the recommended provider package versions for OpenLineage . Create an API token in Atlan ​ Before running the workflow, you will need to create an API token in Atlan. Configure the integration in Atlan ​ Select the source ​ To select Astronomer/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Astronomer Airflow Assets and then click Setup Workflow . Create the connection ​ danger A single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets. To configure the Astronomer/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. (Optional) For Host , enter the URL of your Astronomer Airflow UI. This will allow Atlan to help you view your assets directly in Astronomer from the asset profile. (Optional) For Port , enter the port number for your Astronomer Airflow UI. For Enable OpenLineage Events , click Yes to enable the processing of OpenLineage events or click No to disable it. If disabled, new events will not be processed in Atlan. To create a connection, at the bottom of the screen, click the Create connection button. Configure the integration in Astronomer ​ Did you know? You will need the Atlan API token and connection name to configure the integration in Astronomer. This will allow Astronomer to connect with the OpenLineage API and send events to Atlan. danger Atlan does not support integrating with Apache Airflow versions older than 2.5.0. Astronomer has a built-in OpenLineage integration -  Atlan recommends using OpenLineage version 1.2.1 or latest. You will need to use environment variables in Astronomer to set custom values for the integration with Atlan. To configure Astronomer to send OpenLineage events to Atlan: Open your Astronomer console and select a workspace. In the left menu under Workspace , click Deployments and then select the required deployment. On your deployment page, click the Variables tab. On the Variables page, click the Edit variables button. Add the following environment variable keys and corresponding values: For Apache Airflow versions 2.7.0 onward: AIRFLOW__OPENLINEAGE__NAMESPACE : set the connection name as exactly configured in Atlan. OPENLINEAGE_DISABLED and AIRFLOW__OPENLINEAGE__DISABLED : set both to false to enable the OpenLineage listener in Apache Airflow, if disabled by default. AIRFLOW__OPENLINEAGE__TRANSPORT : specify details of where and how to send OpenLineage events in the following JSON string format: { \"type\" : \"http\" , \"url\" : \"https://<instance>.atlan.com/events/openlineage/airflow-astronomer/\" , \"auth\" : { \"type\" : \"api_key\" , \"api_key\" : \"<API_token>\" } } Replace <instance> with the name of your Atlan instance. Replace <API_token> with the API token generated in Atlan. For Apache Airflow versions 2.5.0 onward and prior to 2.7.0: OPENLINEAGE_URL : points to the service that will consume OpenLineage events   -  for example, https://<instance>.atlan.com/events/openlineage/airflow-astronomer/ . OPENLINEAGE_API_KEY : set the API token generated in Atlan. OPENLINEAGE_NAMESPACE : set the connection name as exactly configured in Atlan. OPENLINEAGE_DISABLED and AIRFLOW__OPENLINEAGE__DISABLED : set both to false to enable the OpenLineage listener in Apache Airflow, if disabled by default. Click Update Environment Variables to save your changes. It can take up to two minutes for new variables to be applied to your deployment. Verify the Atlan connection in Astronomer ​ To verify connectivity to Astronomer: For Verify connection with Astronomer , click the clipboard icon to copy and run the preflight check DAG on your Astronomer instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the preflight checks documentation . Click Done to complete setup. Once your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! 🎉 You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: connectors configuration Previous Astronomer OpenLineage Next What does Atlan crawl from Astronomer/OpenLineage? Create an API token in Atlan Configure the integration in Atlan Configure the integration in Astronomer Verify the Atlan connection in Astronomer"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-airflow-openlineage/how-tos/integrate-apache-airflow-openlineage",
    "content": "Connect data Orchestration & Workflow Apache Airflow OpenLineage Get Started How to integrate Apache Airflow/OpenLineage On this page Integrate Apache Airflow/OpenLineage To integrate Apache Airflow/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to OpenLineage configuration and facets . Atlan also supports other Apache Airflow distributions to enhance your data management and workflow capabilities: Amazon MWAA Astronomer Google Cloud Composer Did you know? You will need the Atlan API token and connection name to configure the integration in Apache Airflow/OpenLineage. This will allow Apache Airflow to connect with the OpenLineage API and send events to Atlan. Create an API token in Atlan ​ Before running the workflow, you will need to create an API token in Atlan. Configure the integration in Atlan ​ Select the source ​ To select Apache Airflow/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Airflow Assets and then click Setup Workflow . Create the connection ​ danger A single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets. To configure the Apache Airflow/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. (Optional) For Host and Port , enter the URL and port number of your Apache Airflow UI, respectively. This will allow Atlan to help you view your assets directly in Apache Airflow from the asset profile. For Enable OpenLineage Events , click Yes to enable the processing of OpenLineage events or click No to disable it. If disabled, new events will not be processed in Atlan. To create a connection, at the bottom of the screen, click the Create connection button. Configure the integration in Apache Airflow/OpenLineage ​ Did you know? For Apache Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see How to implement OpenLineage in Airflow operators . Also, check the recommended provider package versions for OpenLineage . danger Atlan does not support integrating with Apache Airflow versions older than 2.5.0. To configure Apache Airflow to send OpenLineage events to Atlan: Based on your Apache Airflow version, there may be additional prerequisites for using OpenLineage: For Apache Airflow versions 2.7.0 onward, download and install the latest apache-airflow-providers-openlineage package and update the requirements.txt file of your Apache Airflow instance with: apache-airflow-providers-openlineage For Apache Airflow versions 2.5.0 onward and prior to 2.7.0, download and install the latest openlineage-airflow library and update the requirements.txt file of your Apache Airflow instance with: openlineage-airflow Add the following environment variables to your project's .env file: danger When deploying Apache Airflow on Kubernetes, set these environment variables in both the Scheduler and Triggerer pods to ensure proper integration. For Apache Airflow versions 2.7.0 onward: AIRFLOW__OPENLINEAGE__NAMESPACE : set the connection name as exactly configured in Atlan. AIRFLOW__OPENLINEAGE__TRANSPORT : specify details of where and how to send OpenLineage events in the following JSON string format: { \"type\" : \"http\" , \"url\" : \"https://<instance>.atlan.com/events/openlineage/airflow/\" , \"auth\" : { \"type\" : \"api_key\" , \"api_key\" : \"<API_token>\" } } Replace <instance> with the name of your Atlan instance. Replace <API_token> with the API token generated in Atlan. For Apache Airflow versions 2.5.0 onward and prior to 2.7.0: OPENLINEAGE_URL : points to the service that will consume OpenLineage events   -  for example, https://<instance>.atlan.com/events/openlineage/airflow/ . OPENLINEAGE_API_KEY : set the API token generated in Atlan. OPENLINEAGE_NAMESPACE : set the connection name as exactly configured in Atlan. Verify the Atlan connection in Apache Airflow ​ To verify connectivity to Apache Airflow: For Verify connection with Airflow , click the clipboard icon to copy and run the preflight check DAG on your Apache Airflow instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the preflight checks documentation . Click Done to complete setup. Once your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! 🎉 You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: connectors data configuration Previous Apache Airflow OpenLineage Next How to implement OpenLineage in Airflow operators Create an API token in Atlan Configure the integration in Atlan Configure the integration in Apache Airflow/OpenLineage Verify the Atlan connection in Apache Airflow"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/apache-spark-openlineage/how-tos/integrate-apache-spark-openlineage",
    "content": "Connect data Orchestration & Workflow Apache Spark OpenLineage Get Started How to integrate Apache Spark/OpenLineage On this page Integrate Apache Spark/OpenLineage Atlan extracts job-level operational metadata from Apache Spark and generates job lineage through OpenLineage. To learn more about OpenLineage, refer to OpenLineage configuration and facets . To integrate Apache Spark/OpenLineage with Atlan, review the order of operations and then complete the following steps. Create an API token in Atlan ​ Before running the workflow, you will need to create an API token in Atlan. Select the source in Atlan ​ To select Apache Spark/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the list of packages, select Spark Assets and then click Setup Workflow . Configure the integration in Atlan ​ You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your jobs run to catalog your assets. To configure the Apache Spark/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. To run the workflow, at the bottom of the screen, click the Run button. Configure the integration in Apache Spark ​ Did you know? You will need the Atlan API token and connection name to configure the integration in Apache Spark/OpenLineage. This will allow Apache Spark to connect with the OpenLineage API and send events to Atlan. Spark has a default SparkListener interface that OpenLineage leverages to collect information about Spark jobs. To configure Apache Spark to send OpenLineage events to Atlan, you can either: To activate the listener, add the following properties to your Spark configuration: # Initialize Spark session spark = ( SparkSession . builder . master ( 'local' ) . appName ( \"SparkJobs\" ) . config ( 'spark.jars.packages' , \"io.openlineage:openlineage-spark:<latest OpenLineage version>\" ) . config ( 'spark.extraListeners' , 'io.openlineage.spark.agent.OpenLineageSparkListener' ) . config ( 'spark.openlineage.transport.type' , 'http' ) . config ( 'spark.openlineage.transport.url' , 'https://<instance>.atlan.com' ) . config ( 'spark.openlineage.transport.endpoint' , '/events/openlineage/spark/api/v1/lineage' ) . config ( 'spark.openlineage.namespace' , '<connection-name>' ) . config ( 'spark.openlineage.transport.auth.type' , 'api_key' ) . config ( 'spark.openlineage.transport.auth.apiKey' , '<Atlan_api_key>' ) . getOrCreate ( ) ) Atlan recommends using the latest available version of the OpenLineage package for the Apache Spark integration. Replace <latest OpenLineage version> with the latest version of OpenLineage . url : set the URL of your Atlan instance   -  for example, https://<instance>.atlan.com . endpoint : points to the service that will consume OpenLineage events   -  for example, /events/openlineage/spark/api/v1/lineage . namespace : set the connection name as exactly configured in Atlan. apiKey : set the API token generated in Atlan. Add the above configuration to your cluster's spark-defaults.conf file or specific jobs on submission via the spark-submit command. Once the data processing tool has completed running, you will see Spark jobs along with lineage from OpenLineage events in Atlan! 🎉 You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: data api authentication configuration Previous Apache Spark OpenLineage Next What does Atlan crawl from Apache Spark/OpenLineage? Create an API token in Atlan Select the source in Atlan Configure the integration in Atlan Configure the integration in Apache Spark"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/how-tos/crawl-dagster",
    "content": "Connect data Orchestration & Workflow Dagster Get Started Crawl Dagster assets On this page Crawl Dagster assets Private Preview Create a crawler workflow in Atlan to capture lineage from your Dagster assets. This workflow connects to Dagster and begins lineage capture. Prerequisites ​ Before you begin, make sure you have: Admin access to your Atlan workspace Configured Dagster for Atlan integration. For more information, see Set up Dagster Create crawler workflow ​ Follow these steps to create a workflow in Atlan that captures lineage from Dagster. In Atlan, select New > New Workflow . From the package list, choose Dagster Assets . Select Setup Workflow . Configure connection ​ Follow these steps to configure the Dagster connection in Atlan and finalize lineage capture. Enter a Connection Name . For example, production , development , or analytics . Assign at least one Connection Admin . Select Run to create the connection. Track the workflow's progress in the Workflow center . After the workflow completes, the Dagster connection is ready to receive lineage events. See also ​ What does Atlan crawl from Dagster : Metadata available from Dagster after integration Tags: connectors lineage dagster Previous Set up Dagster Next What does Atlan crawl from Dagster Prerequisites Create crawler workflow Configure connection See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/dagster/how-tos/set-up-dagster",
    "content": "Connect data Orchestration & Workflow Dagster Get Started Set up Dagster On this page Set up Dagster Private Preview Configure Dagster to send lineage and asset metadata to Atlan. This setup enables automatic capture of Dagster assets and their lineage relationships. Prerequisites ​ Before you begin, make sure you have: Admin access to your Dagster instance Admin access to your Atlan workspace An Atlan API key. Learn how to create an API key Configure Dagster ​ Follow these steps to enable the Atlan integration in Dagster so lineage events from your Dagster assets flow into Atlan. Contact Dagster support to enable the Atlan integration. Share the following details with Dagster support: Your Atlan API key. Your Atlan tenant domain. For example, https://[your-org].atlan.com/ . The connection name you plan to create in Atlan. After the integration is enabled, Dagster assets automatically send lineage events to Atlan. Next steps ​ Crawl Dagster assets : Create a crawler workflow in Atlan to capture lineage from Dagster Tags: connectors lineage dagster Previous Dagster Next Crawl Dagster assets Prerequisites Configure Dagster Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/lineage/google-cloud-composer-openlineage/how-tos/integrate-google-cloud-composer-openlineage",
    "content": "Connect data Orchestration & Workflow Google Cloud OpenLineage Get Started How to integrate Google Cloud Composer/OpenLineage On this page Integrate Google Cloud Composer/OpenLineage To integrate Google Cloud Composer/OpenLineage with Atlan, complete the following steps. To learn more about OpenLineage, refer to OpenLineage configuration and facets . Did you know? For Apache Airflow operators supported for OpenLineage extraction, you can refer to Airflow's Supported operators documentation. To learn how to extract lineage though OpenLineage methods, custom extractors, or manually annotated lineage, see How to implement OpenLineage in Airflow operators . Also, check the recommended provider package versions for OpenLineage . Create an API token in Atlan ​ Before running the workflow, you will need to create an API token in Atlan. Configure the integration in Atlan ​ Select the source in Atlan ​ To select Google Cloud Composer/OpenLineage as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the filters along the top, click Orchestrator . From the list of packages, select Google Cloud Composer Airflow Assets and then click Setup Workflow . Create the connection ​ danger A single connection (namespace) must be used for only one Airflow instance. Using the same connection across multiple instances may cause environment variables to update incorrectly, leading to unexpected behavior. You will only need to create a connection once to enable Atlan to receive incoming OpenLineage events. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the OpenLineage events as and when your DAGs run to catalog your Apache Airflow assets. To configure the Google Cloud Composer/OpenLineage connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. (Optional) For Host , enter the URL of your Google Cloud Composer Airflow UI. This will allow Atlan to help you view your assets directly in Google Cloud Composer from the asset profile. (Optional) For Port , enter the port number for your Google Cloud Composer Airflow UI. For Enable OpenLineage Events , click Yes to enable the processing of OpenLineage events or click No to disable it. If disabled, new events will not be processed in Atlan. To create a connection, at the bottom of the screen, click the Create connection button. Configure the integration in Google Cloud Composer ​ Did you know? You will need the Atlan API token and connection name to configure the integration in Google Cloud Composer. This will allow Google Cloud Composer to connect with the OpenLineage API and send events to Atlan. danger Atlan does not support integrating with Apache Airflow versions older than 2.5.0. To configure Google Cloud Composer to send OpenLineage events to Atlan: You will need to configure Google Cloud Composer for the integration: Open your Google Cloud console and navigate to the Environments page. From the list of environments, click the name of your environment. Configure the following: For Apache Airflow versions 2.7.0 onward, set override Airflow configuration options : In the Environment details page, click the Airflow configuration overrides tab and then click Edit . In the Airflow configuration overrides form, click the Add Airflow configuration override button to specify the first set of values: For Section 1 , enter openlineage . For Key 1 , enter namespace . For Value 1 , enter the connection name as exactly configured in Atlan. Click the Add Airflow configuration override button to specify the second set of values: For Section 2 , enter openlineage . For Key 2 , enter transport . For Value 2 , enter the following: { \"type\" : \"http\" , \"url\" : \"https://<instance>.atlan.com/events/openlineage/airflow-cloud-composer/\" , \"auth\" : { \"type\" : \"api_key\" , \"api_key\" : \"<API_KEY>\" } } For <API_key> , set the API token generated in Atlan. For Apache Airflow versions 2.5.0 onward and prior to 2.7.0, set environment variables : In the Environment details page, click the Environment variables tab and then click Edit . Add the following environment variable names and corresponding values: OPENLINEAGE_URL : points to the service that will consume OpenLineage events   -  for example, https://<instance>.atlan.com/events/openlineage/airflow-cloud-composer/ . OPENLINEAGE_API_KEY : set the API token generated in Atlan. OPENLINEAGE_NAMESPACE : set the connection name as exactly configured in Atlan. Click Save to save your changes. You will also need to install the OpenLineage PyPI package in Google Cloud Composer. To install the OpenLineage PyPI package in your environment: In the Environment details page, click the PyPI packages tab and then click Edit . Click Add package to add a custom package. Under PyPI packages , for Package name , specify the package name. For Apache Airflow versions 2.7.0 onward: apache-airflow-providers-openlineage For Apache Airflow versions 2.5.0 onward: openlineage-airflow Click Save to save your configuration. Verify the Atlan connection in Google Cloud Composer ​ To verify connectivity to Google Cloud Composer: For Verify connection with Cloud Composer , click the clipboard icon to copy and run the preflight check DAG on your Google Cloud Composer instance to test connectivity with Atlan. If you encounter any errors after running the DAG, refer to the preflight checks documentation . Click Done to complete setup. Once your DAGs have completed running in Apache Airflow, you will see Apache Airflow DAGs and tasks along with lineage from OpenLineage events in Atlan! 🎉 You can also view event logs in Atlan to track and debug events received from OpenLineage. Tags: lineage data-lineage impact-analysis api rest-api graphql Previous Google Cloud Composer OpenLineage Next What does Atlan crawl from Google Cloud Composer/OpenLineage? Create an API token in Atlan Configure the integration in Atlan Configure the integration in Google Cloud Composer Verify the Atlan connection in Google Cloud Composer"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/how-tos/crawl-aiven-kafka",
    "content": "Connect data Event/Messaging Aiven Kafka Crawl Aiven Kafka Assets Crawl Aiven Kafka On this page Crawl Aiven Kafka Once you have configured the Aiven Kafka permissions , you can establish a connection between Atlan and Aiven Kafka. Did you know? Atlan currently supports the offline extraction method for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. To crawl metadata from Aiven Kafka after uploading the results to S3 , review the order of operations and then complete the following steps. Select the source ​ To select Aiven Kafka as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Aiven Kafka Assets . In the right panel, click Setup Workflow . Provide credentials ​ Atlan supports the offline extraction method for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Extraction method , Offline is the default selection. For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include topics.json , topic-configs.json , and so on. Based on your cloud platform, enter the following details: If using AWS, for Role ARN , enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3. If using Microsoft Azure, enter the name of your Azure Storage Account and the SAS token for Blob SAS Token . If using Google Cloud Platform, no further configuration is required. When complete, at the bottom of the screen, click Next . Configure the connection ​ To complete the Aiven Kafka connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Aiven Kafka crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude topics regex . (This will default to no assets, if none specified.) To select the assets you want to include in crawling, click Include topics regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Aiven Kafka crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Aiven Kafka Next What does Atlan crawl from Aiven Kafka? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/aiven-kafka/how-tos/set-up-aiven-kafka",
    "content": "Connect data Event/Messaging Aiven Kafka Get Started Set up Aiven Kafka On this page Set up Aiven Kafka Who can do this? You will probably need your Aiven Kafka administrator to complete these steps   -  you may not have access yourself. Atlan supports the S3 extraction method for fetching metadata from Aiven Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. Create user in Aiven Kafka ​ To create a new user for extracting metadata from Aiven Kafka : Log in to your Aiven console and select your active cluster. From the upper right of the cluster Overview page, click the Users tab to create a new user: For Create a service user , under Username , enter a name for the new user and then click Add service user . The new user will be listed under Service users , on the Users page. Copy the username and password for the new user and store them in a secure location. (Optional) If using client certificate authentication , copy the access key and access certificate and store them in a secure location. From the upper right of the cluster Overview page, click the Access Control List (ACL) tab to add a new ACL grant: Under Access Control List (ACL) , for ACL Type , click ACL For Topic . For Add access control entry , enter the following details: For Username , enter the username you created for the new user. For Topic , enter an asterisk * to include all topics. From the Permission dropdown, select Admin -  learn more about ACL permission mapping . Click Add entry to save your selections. Navigate to the Overview tab, copy or download the CA Certificate and store the details in a secure location. Did you know? Once you have extracted metadata on-premises and uploaded the results to S3 , you can crawl the metadata from Aiven Kafka into Atlan. Tags: connectors data Previous Aiven Kafka Next Crawl Aiven Kafka Create user in Aiven Kafka"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/how-tos/set-up-amazon-msk",
    "content": "Connect data Event/Messaging Amazon MSK Get Started Set up Amazon MSK On this page Set up Amazon MSK warning 🤓 Who can do this? You will need your Amazon MSK administrator to run these commands   -  you may not have access yourself. Atlan supports IAM role authentication for fetching metadata from Amazon Managed Streaming for Apache Kafka (Amazon MSK). This method uses an AWS role ARN and region to fetch metadata. For IAM role authentication, Atlan supports TLS encryption to ensure secure and encrypted communication between Atlan and your Amazon MSK cluster. Additionally, Atlan currently only supports the following: Apache Kafka 2.7.1 or higher for Amazon MSK Provisioned deployment Create IAM policy ​ To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"kafka-cluster:Connect\" \"kafka-cluster:DescribeCluster\" , \"kafka-cluster:DescribeGroup\" , \"kafka-cluster:DescribeTopic\" , \"kafka-cluster:DescribeTopicDynamicConfiguration\" , \"kafka-cluster:DescribeClusterDynamicConfiguration\" ] , \"Resource\" : [ \"arn:aws:kafka:<region>:<account_id>:cluster/<cluster_name>/<cluster_uuid>\" , \"arn:aws:kafka:<region>:<account_id>:group/<cluster_name>/<cluster_uuid>/*\" , \"arn:aws:kafka:<region>:<account_id>:topic/<cluster_name>/<cluster_uuid>/*\" ] , } ] } Replace <region> with the AWS region of your Amazon MSK cluster. Replace <account_id> with your AWS account ID. Replace <cluster_name> with the name of your Amazon MSK cluster. Replace <cluster_uuid> with the universally unique identifier (UUID) of your Amazon MSK cluster. IAM permissions ​ Atlan requires the following permissions: kafka-cluster:Connect -  grants permission to connect to the Amazon MSK cluster as a Kafka client, allowing the user or service to interact with Kafka brokers for producing and consuming messages. kafka-cluster:DescribeCluster -  grants permission to extract metadata about the Amazon MSK cluster, such as its configuration, status, and associated brokers. kafka-cluster:DescribeGroup -  grants permission to describe consumer groups in the Kafka cluster. This includes metadata such as consumer group, members, and their assigned partitions. kafka-cluster:DescribeTopic -  grants permission to describe Kafka topics, including metadata such as partitions and replication factor for a topic. kafka-cluster:DescribeTopicDynamicConfiguration -  allows access to view the dynamic configurations of Kafka topics. This includes topic-level overrides for configurations like retention periods, which can be changed without requiring a cluster restart. kafka-cluster:DescribeClusterDynamicConfiguration -  allows access to view the dynamic configuration settings of a Kafka cluster. These configurations can change without restarting the cluster and include parameters like replication settings, broker properties, and more. Role delegation-based authentication ​ Using the policy created above, configure IAM role delegation-based authentication. To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the crawler. Tags: data authentication Previous Amazon MSK Next Set up a private network link to Amazon MSK Create IAM policy IAM permissions Role delegation-based authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/how-tos/crawl-amazon-msk",
    "content": "Connect data Event/Messaging Amazon MSK Crawl Amazon MSK Assets Crawl Amazon MSK On this page Crawl Amazon MSK Once you have configured the Amazon MSK permissions , you can establish a connection between Atlan and Amazon MSK. (If you are also using a private network for Amazon MSK, you will need to set that up first , too.) To crawl metadata from Amazon MSK, review the order of operations and then complete the following steps. Select the source ​ To select Amazon MSK as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Amazon MSK Assets . In the right panel, click Setup Workflow . Provide credentials ​ To enter your Amazon MSK credentials: For Extraction method , Direct is the default selection. For Bootstrap servers , enter the hostname(s) (or PrivateLink cluster connection string ) of your Amazon MSK broker(s)   -  for multiple hostnames, separate each entry with a comma , or semicolon ; . For Authentication , IAM Role is the default authentication method. For Deployment Type , Provisioned is the default deployment type. For Security protocol , SASL_SSL is the default security protocol. For AWS Role ARN , enter the ARN of the IAM role you created in your AWS account . For AWS Region , enter the AWS region of your Amazon MSK cluster. Click the Test Authentication button to confirm connectivity to Amazon MSK. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Amazon MSK connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Amazon MSK crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: For Skip internal topics , keep the default option Yes to skip internal Apache Kafka topics or click No to enable crawling them. To have the crawler ignore topics based on a naming convention, specify a regular expression in the Exclude topic regex field. To have the crawler include topics based on a naming convention, specify a regular expression in the Include topic regex field. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Amazon MSK crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up a private network link to Amazon MSK Next What does Atlan crawl from Amazon MSK? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/how-tos/crawl-apache-kafka",
    "content": "Connect data Event/Messaging Apache Kafka Crawl Apache Kafka Assets Crawl Apache Kafka On this page Crawl Apache Kafka Atlan crawls metadata from your Apache Kafka clust er, allowing you to discover, classify, and govern your Kafka topics and schemas. This guide walks you through the steps to configure and run the Apache Kafka crawler in Atlan. Prerequisites ​ Before you begin, complete the following prerequisites: Apache Kafka setup: You have configured the Apache Kafka permissions , you can establish a connection between Atlan and Apache Kafka. Order of operations: Review the order of operations to understand the sequence of tasks for crawling metadata. Access to Atlan workspace: You must have the required permissions in Atlan to create and manage a connection. Select the source ​ To select Apache Kafka as your source: In Atlan, click New , and from the menu, select New Workflow . From the Marketplace page, click Apache Kafka Assets . Click Setup Workflow in the right panel to proceed with configuration. Provide credentials ​ In Direct extraction, Atlan connects to Apache Kafka and crawls metadata directly. In Offline extraction, you need to first extract metadata yourself and make it available in S3 . Direct extraction method ​ To enter your Apache Kafka credentials: For Bootstrap servers , enter the hostname(s) of your Apache Kafka broker(s)   -  for multiple hostnames, separate each entry with a comma , or semicolon ; . For Authentication , Atlan provides the following authentication methods: No Authentication: If your Apache Kafka cluster does not require authentication, Atlan can connect without any credentials.. Basic Authentication (SASL/PLAIN): Uses a username and password with the SASL_PLAIN mechanism for authentication. SCRAM Authentication (SASL/SCRAM): Uses a username and password with the SASL_SCRAM mechanism (SCRAM-SHA-256 or SCRAM-SHA-512) for secure authentication. Username, enter the username for your Apache Kafka brokers. Password, enter the password for the username. For Security protocol , select Plaintext or SSL for No Auth, and SASL_PLAINTEXT or SASL_SSL for Basic and SCRAM authentication. For SASL Mechanism (optional for SCRAM authentication), choose the appropriate mechanism for your Kafka cluster. Click Test Authentication to confirm connectivity. Once authentication is successful, click Next . Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from Apache Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include topics.json , topic-configs.json , and so on. Based on your cloud platform, enter the following details: If using AWS, for Role ARN , enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3. If using Microsoft Azure, enter the name of your Azure Storage Account and the SAS token for Blob SAS Token . If using Google Cloud Platform, no further configuration is required. When complete, at the bottom of the screen, click Next . Configure the connection ​ To complete the Apache Kafka connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Apache Kafka crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: For Skip internal topics , keep the default option Yes to skip internal Apache Kafka topics or click No to enable crawling them. To select the Apache Kafka assets you want to exclude from crawling, click Exclude topics regex . (This will default to no assets, if none specified.) To select the Apache Kafka assets you want to include in crawling, click Include topics regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Apache Kafka crawler, after completing the steps above: Click Preflight checks to verify configuration. Choose one of the following options: To run the crawler once immediately, click Run . To schedule the crawler, click Schedule & Run . Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Apache Kafka Next Set up on-premises Kafka access Prerequisites Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/amazon-msk/how-tos/set-up-a-private-network-link-to-amazon-msk",
    "content": "Connect data Event/Messaging Amazon MSK Get Started Set up a private network link to Amazon MSK On this page Set up a private network link to Amazon MSK Who can do this? You will need your Amazon MSK or AWS administrator involved   -  you may not have access to run these tasks. AWS PrivateLink creates a secure, private connection between services running in AWS, ensuring that traffic between services remains within the AWS network. This document describes the steps to set this up between Amazon MSK and Atlan. Prerequisites ​ Before you can set up private network connectivity, ensure the following: Amazon MSK version: Apache Kafka 2.7.1 or higher. Authentication type: only IAM role-based authentication is supported. Cluster instance type: must be larger than t3.small. Region alignment: both your Amazon MSK cluster and Atlan tenant must reside in the same AWS region. For more information, refer to Requirements and Limitations for Multi-VPC Private Connectivity . Request Atlan's details ​ To configure private network connectivity between your AWS account and Atlan, contact Atlan support for the following details: Atlan's AWS account ID Enable private network link ​ To verify or enable AWS PrivateLink for Amazon MSK: Sign in to the AWS Management Console and open the Amazon MSK Console . From the left menu, click Clusters . On the Clusters page, under Cluster name , select the cluster for which you want to enable private network link. On your cluster page, below the overview section, click the Properties tab. In the Properties tab, navigate to the Networking settings section to verify or enable AWS PrivateLink connectivity: If you have verified that AWS PrivateLink is turned on, skip to the next section. If AWS PrivateLink is turned off, click the Edit button and then click Turn on multi-VPC connectivity to enable it. In the Turn on multi-VPC connectivity page, for Authentication type , click IAM role-based authentication . At the bottom of the screen, click Turn on selection . The cluster will undergo a rolling update, which may take several minutes to a few hours to complete. Grant access to Atlan ​ Once AWS PrivateLink is enabled for your Amazon MSK cluster, you will need to update the cluster policy to grant access to Atlan. To update your Amazon MSK cluster policy: Sign in to the AWS Management Console and open the Amazon MSK Console . From the left menu, click Clusters . On the Clusters page, under Cluster name , select the cluster for which you enabled private network link. On your cluster page, below the overview section, click the Properties tab. In the Properties tab, navigate to the Security settings section and then click Edit cluster policy . In the Edit cluster policy page, under Cluster policy , configure the following: Click Basic as the new cluster policy. For Account ID(s) that need cluster access , enter Atlan's AWS account ID. Click the Include Kafka service principal checkbox to allow Atlan access to Kafka services only. Click Save changes to save your selections. Notify Atlan support team ​ Once you've completed the steps above, contact the Atlan support team again and provide the following details for your Amazon MSK cluster: Amazon MSK Cluster ARN   -  the unique identifier of your cluster Atlan will create a managed VPC connection to your Amazon MSK cluster . Once completed, Atlan support will send you the cluster connection string (bootstrap servers) required for accessing Amazon MSK via AWS PrivateLink. You can now enter the cluster connection string for the Bootstrap servers field to crawl Amazon MSK . Atlan will securely connect to your Amazon MSK cluster using AWS PrivateLink. Tags: atlan documentation Previous Set up Amazon MSK Next Crawl Amazon MSK Prerequisites Request Atlan's details Enable private network link Grant access to Atlan Notify Atlan support team"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/apache-kafka/how-tos/set-up-apache-kafka",
    "content": "Connect data Event/Messaging Apache Kafka Get Started Set up Apache Kafka On this page Set up Apache Kafka Who can do this? You will probably need your Apache Kafka administrator to run these commands   -  you may not have access yourself. Atlan supports different authentication mechanisms to securely access your Apache Kafka cluster. If the cluster is configured with \"No Auth\" (authentication not needed), Atlan connects directly. If the cluster requires authentication, you must configure it for Atlan to access your Apache Kafka cluster. Atlan supports the following authentication flows: Basic authentication using a username and password with SASL_PLAIN mechanism SCRAM authentication using a username and password with SASL_SCRAM mechanism Use basic authentication with SASL_PLAIN mechanism ​ With basic authentication using SASL_PLAIN , Atlan authenticates with Kafka using a username and password. To authenticate Atlan with Kafka using SASL_PLAIN , complete the following steps on each broker: Create user by defining the user credentials in a `JAAS` login configuration file: KafkaServer { org.apache.kafka.common.security.plain.PlainLoginModule required username=\"<kafka admin username>\" password=\"<kafka admin password>\" user_<username> = \"<password>\"; }; Replace <kafka admin username> and <kafka admin password> with the administrator credentials for Kafka. Replace <username> with the username you want to use in Atlan. Replace <password> with the password you want to use in Atlan. Pass the JAAS file as a JVM configuration option when running the broker: export KAFKA_OPTS=\"-Djava.security.auth.login.config=<path-to-jaas-file>/jaas-kafka-server.conf\" Atlan does not make any API requests or queries that update the resources in your Kafka cluster. Set the minimum necessary permissions for resources: Grant topic permissions to read and describe topics with the following command: ./bin/kafka-acls.sh --topic '*' --add --allow-principals user:atlan --operations Read,Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/server.properties Grant consumer group permissions to read and describe consumer groups with the following command: ./bin/kafka-acls.sh --consumer-group '*' --add --allow-principals user:atlan --operations Read,Describe --allow-host '*' --config /<broker-config-path>/config/server.properties Grant cluster permissions to describe cluster configurations with the following command: ./bin/kafka-acls.sh --cluster --add --allow-principals user:atlan --operations Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/config/server.properties Once you have configured the Apache Kafka brokers, restart your brokers to pick up the new configuration. Use SCRAM authentication with SASL_SCRAM mechanism ​ SCRAM (Salted Challenge Response Authentication Mechanism) provides more security than SASL_PLAIN . To use SCRAM authentication, complete the following steps on each broker: Create the SCRAM user using SCRAM-SHA-256 or SCRAM-SHA-512 mechanism based on the mechanism set up on your Apache Kafka cluster: Use SCRAM-SHA-256 mechanism: bin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type users --entity-name <username> --add-config \"SCRAM-SHA-256=[iterations=4096,password=<password>]\" Replace <username> with the username you want to use in Atlan. Replace <password> with the password you want to use in Atlan. Use SCRAM-SHA-512 mechanism: kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type users --entity-name <username> --add-config \"SCRAM-SHA-512=[iterations=4096,password=<password>]\" Replace <username> with the username you want to use in Atlan. Replace <password> with the password you want to use in Atlan. Use both SCRAM-SHA-256 and SCRAM-SHA-512 mechanisms: kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type users --entity-name <username> --add-config \"SCRAM-SHA-256=[iterations=4096,password=<password>],SCRAM-SHA-512=[iterations=4096,password=<password>]\" Replace <username> with the username you want to use in Atlan. Replace <password> with the password you want to use in Atlan. Verify the user configuration: kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type users --entity-name <username> Replace <username> with the username you want to use in Atlan. The SCRAM authentication needs a JAAS file. If the file doesn't exist, create one with the following content: KafkaServer { org.apache.kafka.common.security.plain.PlainLoginModule required username=\"<kafka admin username>\" password=\"<kafka admin password>\" }; Pass in the JAAS file as a JVM configuration option when running the broker: export KAFKA_OPTS=\"-Djava.security.auth.login.config=<path-to-jaas-file>/jaas-kafka-server.conf\" Atlan does not make any API requests or queries that update the resources in your Kafka cluster. Set the minimum necessary permissions for resources: Grant topic permissions to read and describe topics with the following command: ./bin/kafka-acls.sh --topic '*' --add --allow-principals user:atlan --operations Read,Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/server.properties Grant consumer group permissions to read and describe consumer groups with the following command: ./bin/kafka-acls.sh --consumer-group '*' --add --allow-principals user:atlan --operations Read,Describe --allow-host '*' --config /<broker-config-path>/config/server.properties Grant cluster permissions to describe cluster configurations with the following command: ./bin/kafka-acls.sh --cluster --add --allow-principals user:atlan --operations Describe,DescribeConfigs --allow-host '*' --config /<broker-config-path>/config/server.properties Once you have configured the Apache Kafka brokers, restart your brokers to pick up the new configuration. Tags: atlan documentation Previous Apache Kafka Next Crawl Apache Kafka Use basic authentication with SASL_PLAIN mechanism Use SCRAM authentication with SASL_SCRAM mechanism"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka/how-tos/set-up-confluent-kafka",
    "content": "Connect data Event/Messaging Confluent Kafka Get Started Set up Confluent Kafka On this page Set up Confluent Kafka Who can do this? You will need your OrganizationAdmin , EnvironmentAdmin , or CloudClusterAdmin role to complete these steps   -  you may not have access yourself. Atlan supports the API authentication method for fetching metadata from Confluent Kafka. This method uses an API key and API secret to fetch metadata. Create an API Key ​ This section provides steps for creating an API key to access metadata from your Confluent Kafka environment. info 💪 Note: Atlan does not perform any API requests or queries that modify the resources in your Confluent Kafka environment. Resource-Specific API Key ​ To create a resource-specific API key for crawling Confluent Kafka , follow these steps: Log in to your Confluent Cloud instance with a OrganizationAdmin , EnvironmentAdmin , or CloudClusterAdmin role. From the Cloud Console , select your active cluster. Under Cluster Overview in the left menu for your active cluster, click API Keys . In the upper right of the API Keys page, click + Add key . On the Create key page, enter the following details: For Access Control , under Select Scope for API key , select Granular access to define a specific set of access rules, then click Next to proceed. For Service Account , click Create a new one and enter the following details: For New service account name , enter a meaningful name for your API key. For example, Atlan . (Optional) For Description , add a description for your API key. For example, Atlan crawler connection . Click Next to proceed. For Add ACLs to service account , click + Add ACLs to add and allow the following minimum permissions required for your Confluent Kafka resources: Cluster: DescribeConfigs Group: Describe Topic: Describe , DescribeConfigs Once you've added all the permissions, click Next to proceed. For Create key , under Get your API key , copy or download the API key and secret. Make sure to store them securely, as the secret can't be retrieved later. (Optional) Cloud API Key ​ To access Kafka metrics( sizeInBytes ), you need a Cloud API key. Follow these steps to generate one: Click the hamburger menu (☰) icon in the top right corner to open the menu. In the menu, click API Keys . On the API Key listing screen, click the Add API Key button to add a new API key. On the Select an account for API key screen, select the account appropriate for your service or user account, then click Next . On the Select resource scope for API key screen, choose Cloud resource management , then click Next . On the API details screen , enter the required details: Name : Provide a unique and meaningful name for your API key. For example, Atlan Kafka Metrics Key. Description : Add a description that illustrates the purpose of the key. For example, Atlan Kafka metrics API key with read-only permissions. Click Create API Key to generate the key. On the API key download screen , copy the API key and secret or click Download API Key to save them as a file. Make sure to store them securely, as the secret can't be retrieved later. Click Complete to finish the API key creation process. You will be able to see the generated API key on the API key listing screen. Tags: data api authentication Previous Confluent Kafka Next Crawl Confluent Kafka Create an API Key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/confluent-kafka/how-tos/crawl-confluent-kafka",
    "content": "Connect data Event/Messaging Confluent Kafka Crawl Confluent Kafka Assets Crawl Confluent Kafka On this page Crawl Confluent Kafka Atlan crawls metadata from your Confluent Kafka cl uster, allowing you to discover, classify, and govern your Kafka topics and schemas. This guide walks you through the steps to configure and run the Confluent Kafka crawler in Atlan. Prerequisites ​ Before you begin, complete the following prerequisites: Confluent Kafka setup: You have configured the Confluent Kafka permissions , you can establish a connection between Atlan and Confluent Kafka. Order of operations: Review the order of operations to understand the sequence of tasks for crawling metadata. Access to Atlan workspace: You must have the required permissions in Atlan to create and manage a connection. Select the source ​ To select Confluent Kafka as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Confluent Kafka Assets . In the right panel, click Setup Workflow . Provide credentials ​ In Direct extraction , Atlan connects to Confluent Kafka and crawls metadata directly. In Offline extraction , you need to first extract metadata yourself and make it available in S3. Direct extraction method ​ To enter your Confluent Kafka credentials: For Bootstrap servers , enter the hostname(s) of your Confluent Kafka broker(s). Separate multiple hostnames with a comma , or semicolon ; . For API Key , enter the API key you copied. For API Secret , enter the API secret you copied. For Security protocol, click SASL_PLAINTEXT to connect to Confluent Kafka through a non-encrypted channel or click SASL_SSL to connect via a Secure Sockets Layer (SSL) channel. Click the Test Authentication button to confirm connectivity to Confluent Kafka. Once authentication is successful, navigate to the bottom of the screen and click Next . Offline extraction method ​ Atlan also supports the offline extraction method for fetching metadata from Confluent Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. You will need to first extract the metadata yourself and then make it available in S3 . To enter your S3 details: For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include topics.json , topic-configs.json , and so on. Based on your cloud platform, enter the following details: If using AWS, for Role ARN , enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3. If using Microsoft Azure, enter the name of your Azure Storage Account and the SAS token for Blob SAS Token . If using Google Cloud Platform, no further configuration is required. When complete, at the bottom of the screen, click Next . Configure the connection ​ To complete the Confluent Kafka connection configuration: Provide a Connection Name that represents your source environment. For example, use values like production , development , gold , or analytics . (Optional) To change the users who can manage this connection, update the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Confluent Kafka crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: For Skip internal topics , keep the default option Yes to skip internal Kafka topics or click No to enable crawling them. To select the assets you want to exclude from crawling, click Exclude topics regex . (This will default to no assets, if none specified.) To select the assets you want to include in crawling, click Include topics regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Confluent Kafka crawler: To run the crawler once, immediately, click the Run button at the bottom of the screen. To schedule the crawler to run hourly, daily, weekly, or monthly, click the Schedule & Run button at the bottom of the screen. Once the crawl completes, your assets appear in Atlan! 🎉 Tags: connectors data crawl setup Previous Set up Confluent Kafka Next Set up on-premises Kafka access Prerequisites Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/crawl-microsoft-azure-event-hubs",
    "content": "Connect data Event/Messaging Microsoft Azure Event Hubs Crawl Microsoft Azure Event Hubs Assets Crawl Microsoft Azure Event Hubs On this page Crawl Microsoft Azure Event Hubs Once you have configured the Microsoft Azure Event Hubs permissions , you can establish a connection between Atlan and Microsoft Azure Event Hubs. To crawl metadata from Microsoft Azure Event Hubs, review the order of operations and then complete the following steps. Select the source ​ To select Microsoft Azure Event Hubs as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Azure Event Hubs Assets . In the right panel, click Setup Workflow . Provide credentials ​ Choose your authentication method: In SAS Key , you will need your event hub namespace and a connection string-primary key for authentication. In Service Principal , you will need your event hub namespace and the following: If only fetching metadata from Microsoft Azure Event Hubs, you will need a client ID, client secret, and tenant ID for authentication. If fetching metadata from both Microsoft Azure Event Hubs and Apache Kafka, you will need a connection string-primary key and client ID, client secret, and tenant ID for authentication. SAS key ​ To enter your Microsoft Azure Event Hubs credentials: For Extraction method , Direct is the default selection. For Select which metadata to fetch , click From Only Event hubs to only fetch metadata from Microsoft Azure Event Hubs or click From Both Kafka and Event hubs to fetch metadata from both Microsoft Azure Event Hubs and Apache Kafka. For Bootstrap servers , enter the event hub namespace you copied from Microsoft Azure Event Hubs in the following format   - <your event hub namespace>.servicebus.windows.net:9093 . For Connection string-primary key , enter the connection string-primary key you copied from Microsoft Azure Event Hubs. For Security protocol , SSL is the default selection for connecting via a Secure Sockets Layer (SSL) channel. Click the Test Authentication button to confirm connectivity to Microsoft Azure Event Hubs. Once authentication is successful, navigate to the bottom of the screen and click Next . Service principal ​ To enter your Microsoft Azure Event Hubs credentials: For Extraction method , Direct is the default selection. For Bootstrap servers , enter the event hub namespace you copied from Microsoft Azure Event Hubs in the following format   - <your event hub namespace>.servicebus.windows.net:9093 . For Select which metadata to fetch , you can either: Click From Only Event hubs to only fetch metadata from Microsoft Azure Event Hubs. If you choose not to fetch metadata from Apache Kafka, note that Atlan will not be able to display the message count for your event hubs and consumer groups. To enter your credentials: For Client ID , enter the application (client) ID you copied for the service principal. For Client Secret , enter the client secret you copied for the service principal. For Tenant ID , enter the directory (tenant) ID you copied for the service principal. Click From Both Kafka and Event hubs to fetch metadata from both Microsoft Azure Event Hubs and Apache Kafka and then enter your credentials: For Connection string-primary key , enter the connection string-primary key you copied from Microsoft Azure Event Hubs. For Client ID , enter the application (client) ID you copied for the service principal. For Client Secret , enter the client secret you copied for the service principal. For Tenant ID , enter the directory (tenant) ID you copied for the service principal. For Security protocol , SSL is the default selection for connecting via a Secure Sockets Layer (SSL) channel. Click the Test Authentication button to confirm connectivity to Microsoft Azure Event Hubs. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Microsoft Azure Event Hubs connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Microsoft Azure Event Hubs crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: For Skip internal event hubs , keep the default option Yes to skip internal event hubs or click No to enable crawling them. To select the assets you want to exclude from crawling, click Exclude event hubs regex . (This will default to no assets, if none specified.) To select the assets you want to include in crawling, click Include event hubs regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Microsoft Azure Event Hubs crawler, after completing the steps above: To run the crawler once, immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Microsoft Azure Event Hubs Next What does Atlan crawl from Microsoft Azure Event Hubs? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/microsoft-azure-event-hubs/how-tos/set-up-microsoft-azure-event-hubs",
    "content": "Connect data Event/Messaging Microsoft Azure Event Hubs Get Started Set up Microsoft Azure Event Hubs On this page Set up Microsoft Azure Event Hubs Atlan supports the following authentication methods for Microsoft Azure Event Hubs: SAS key -  this method uses a connection string-primary key to fetch metadata. Service principal -  in addition to a connection string-primary key, this method requires a client ID, client secret, and tenant ID to fetch metadata. SAS key authentication ​ Who can do this? You will need your Microsoft Azure Event Hubs administrator to complete these steps   -  you may not have access yourself. Create a shared access signature policy ​ You will need to create a shared access signature (SAS) policy in Microsoft Azure Event Hubs for authentication in Atlan. The Manage permission is required for the following: Atlan requires read permissions of the configurations set to event hubs and the event hub namespace. Since Atlan currently only supports SAS policy-based authentication, Manage permission is required to provide this type of access. SAS policies do not support granular access control while Send or Listen permission is insufficient to crawl configuration metadata. Granular permissions will only be available once Atlan supports other authentication methods that allow for the granular access control capabilities of Microsoft Azure. To fetch the Azure Event Hub status attribute and Azure Event Hub consumer group assets through the Azure APIs. To create a SAS policy for crawling Microsoft Azure Event Hubs : Log in to the Azure portal . Open the menu and search for or click Event Hubs . On the Event Hubs page, click the namespace of your event hub. Copy your Event Hubs Namespace to use for authentication in Atlan . In the left menu of your event hub namespace, under Settings , click Shared access policies . On the _Shared access policie_s page, click + Add to add a new SAS policy. In the Add SAS policy sidebar, enter the following details: For Policy name , enter a meaningful name   -  for example, Atlan integration policy . To add the Manage permission to your SAS policy, click Manage . Click Create to finish setup. On the _Shared access policie_s page, select the newly created SAS policy. From the corresponding SAS Policy dialog, under Connection string-primary key , click the clipboard icon to copy the connection string-primary key and store it in a secure location. You will need your event hub namespace and the connection string-primary key for crawling Microsoft Azure Event Hubs . Service principal authentication ​ Who can do this? You will need your Microsoft Azure Event Hubs administrator to create a shared access signature policy and Cloud Application Administrator or Application Administrator to register an app with Microsoft Entra ID and add it to the Event Hubs Data Sender role -  you may not have access yourself. You need the following to authenticate the connection in Atlan: Connection string-primary key -  required to crawl Kafka assets Client ID (application ID), client secret , and tenant ID (directory ID)   -  required to crawl Microsoft Azure Event Hubs assets Create a shared access signature policy ​ Follow the steps in Create a shared access signature policy to generate a connection string-primary key for crawling Microsoft Azure Event Hubs . Register app with Microsoft Entra ID ​ You will need to register your service principal application with Microsoft Entra ID and note down the values of the tenant ID, client ID, and client secret. To register your app with Microsoft Entra ID: Log in to the Azure portal . In the search bar, search for Microsoft Entra ID and select it from the dropdown list. From the left menu of the Microsoft Entra ID page, click App registrations . From the toolbar on the App registrations page, click + New registration . On the Register an application page, for Name , enter a name for your service principal application and then click Register . On the homepage of your newly created application, from the Overview screen, copy the values for the following fields and store them in a secure location: Application (client) ID Directory (tenant) ID From the left menu of your newly created application page, click Certificates & secrets . On the Certificates & secrets page, under Client secrets , click + New client secret . In the Add a client secret screen, enter the following details: For Description , enter a description for your client secret. For Expiry , select when the client secret will expire. Click Add . On the Certificates & secrets page, under Client secrets , for the newly created client secret, click the clipboard icon to copy the Value and store it in a secure location. Add app to Event Hubs Data Sender role ​ You will need to add the service principal application created in the previous step to the Azure Event Hubs Data Sender role . To add a service principal to the Azure Event Hubs Data Sender role: Log in to the Azure portal . Open the menu and search for or click Event Hubs . On the Event Hubs page, click the namespace of your event hub. From the left menu of your event hubs namespace page, click Access Control (IAM) . In the upper right of the Access Control (IAM) page, navigate to the Add a role assignment tile and then click Add . On the Add a role assignment page, enter the following details: For Role , click the dropdown to select Azure Event Hubs Data Sender -  this allows send access to Azure Event Hubs resources . For Assign access to , click the dropdown to select Azure AD user, group, or service principal . For Select , choose the service principal application you created in the previous step. Click Save to save your role assignment. Tags: connectors data authentication Previous Microsoft Azure Event Hubs Next Crawl Microsoft Azure Event Hubs SAS key authentication Service principal authentication"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/how-tos/crawl-redpanda-kafka",
    "content": "Connect data Event/Messaging Redpanda Kafka Crawl Redpanda Kafka Assets Crawl Redpanda Kafka On this page Crawl Redpanda Kafka Once you have configured the Redpanda Kafka permissions , you can establish a connection between Atlan and Redpanda Kafka. Did you know? Atlan currently supports the offline extraction method for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. To crawl metadata from Redpanda Kafka after uploading the results to S3 , review the order of operations and then complete the following steps. Select the source ​ To select Redpanda Kafka as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Redpanda Kafka Assets . In the right panel, click Setup Workflow . Provide credentials ​ In offline extraction, you need to first extract metadata yourself and make it available in S3 . To enter your S3 details: For Extraction method , Offline is the default selection. For Bucket name , enter the name of your S3 bucket. For Bucket prefix , enter the S3 prefix under which all the metadata files exist. These include topics.json , topic-configs.json , and so on. Based on your cloud platform, enter the following details: If using AWS, for Role ARN , enter the ARN of the AWS role to assume. This role ARN will be used to copy the files from S3. If using Microsoft Azure, enter the name of your Azure Storage Account and the SAS token for Blob SAS Token . If using Google Cloud Platform, no further configuration is required. When complete, at the bottom of the screen, click Next . Configure the connection ​ To complete the Redpanda Kafka connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Redpanda Kafka crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to exclude from crawling, click Exclude topics regex . (This will default to no assets, if none specified.) To select the assets you want to include in crawling, click Include topics regex . (This will default to all assets, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Redpanda Kafka crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule & Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Redpanda Kafka Next Set up on-premises Kafka access Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/on-premises-event-buses/how-tos/set-up-on-premises-kafka-access",
    "content": "Connect data Event/Messaging Redpanda Kafka Guides Set up on-premises Kafka access On this page Set up on-premises Kafka access Who can do this? You will need access to a machine that can run Docker on-premises. You will also need your Kafka instance details, including credentials. In some cases you won't be able to expose your Kafka instance for Atlan to crawl and ingest metadata. For example, this may happen when security requirements restrict access to sensitive, mission-critical data. In such cases you may want to decouple the extraction of metadata from its ingestion in Atlan. This approach gives you full control over your resources and metadata transfer to Atlan. Prerequisites ​ To extract metadata from your Kafka instance, you will need to use Atlan's kafka-extractor tool. Did you know? Atlan uses exactly the same kafka-extractor behind the scenes when it connects to Kafka in the cloud. Install Docker Compose ​ Docker Compose is a tool for defining and running applications composed of many Docker containers. (Any guesses where the name came from? 😉) To install Docker Compose: Install Docker Install Docker Compose Did you know? Instructions provided in this documentation should be enough even if you are completely new to Docker and Docker Compose. However, you can also walk through the Get started with Docker Compose tutorial if you want to learn Docker Compose basics first. Get the kafka-extractor tool ​ To get the kafka-extractor tool: Raise a support ticket to get the link to the latest version. Download the image using the link provided by support. Load the image to the server you'll use to crawl Kafka: sudo docker load -i /path/to/kafka-extractor-master.tar Get the compose file ​ Atlan provides you with a Docker compose file for the kafka-extractor tool. To get the compose file: Download the latest compose file . Save the file to an empty directory on the server you'll use to access your on-premises Kafka instance. The file is docker-compose.yaml . Define Kafka connections ​ The structure of the compose file includes three main sections: x-templates contains configuration fragments. Keep the default settings; no changes are required. services is where you will define your Kafka connections. volumes contains mount information. Keep the default settings; no changes are required. Define services for Apache Kafka ​ For each Apache Kafka instance, define an entry under services in the compose file. Each entry will have the following structure: # Example Apache Kafka connection connection-name : << : *extract environment : << : *kafka-defaults # Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\" # Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" volumes : # You can change './output/connection-name' to any output location you want - ./output/connection - name : /output Replace connection-name with the name of your connection. <<: *extract tells the kafka-extractor tool to run. environment contains all parameters for the tool. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Apache Kafka connections as you want. Define services for Confluent Kafka ​ For each Confluent Kafka instance, define an entry under services in the compose file. Each entry will have the following structure: # Example Confluent Kafka connection connection-name : << : *extract environment : << : *kafka-defaults # Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\" # Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" KAFKA_FLAVOUR : \"CONFLUENT_CLOUD\" CONFLUENT_AUTH : \"<cloud_api_key>:<cloud_api_secret>\" CONFLUENT_CLUSTER_ID : \"<lkc-xxxx>\" volumes : # You can change './output/connection-name' to any output location you want - ./output/connection - name : /output Replace connection-name with the name of your connection. <<: *extract : Tells the kafka-extractor tool to run. environment : Contains all parameters for the tool. KAFKA_FLAVOUR : Defines the Kafka distribution being used. Use CONFLUENT_CLOUD when working with Confluent Cloud. CONFLUENT_AUTH : Configures authentication using a Cloud API key and secret. Replace cloud_api_key with the Cloud API key retrieved during setup. For more information, see How to set up Confluent Kafka Cloud API Key . Replace cloud_api_secret with the corresponding secret for your Cloud API key. For more information, see How to set up Confluent Kafka Cloud API Key . CONFLUENT_CLUSTER_ID : The unique ID of your Kafka cluster. You can find this in the cluster overview page of the Confluent Cloud console. The cluster ID follows the lkc-xxxx format. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Confluent Kafka connections as you want. Define services for Aiven Kafka ​ For each Aiven Kafka instance, define an entry under services in the compose file. Each entry will have the following structure: # Example Aiven Kafka connection connection-name : << : *extract secrets : - kafka_client_config - kafka_ca_cert # Uncomment the following lines if you are using Aiven Kafka with Client Certificate Authentication #      - kafka_access_cert #      - kafka_access_key environment : << : *kafka-defaults # Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\" # Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" volumes : # You can change './output/connection-name' to any output location you want - ./output/connection - name : /output Replace connection-name with the name of your connection. <<: *extract tells the kafka-extractor tool to run. environment contains all parameters for the tool. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Aiven Kafka connections as you want. Define services for Redpanda Kafka ​ For each Redpanda Kafka instance, define an entry under services in the compose file. Each entry will have the following structure: # Example Redpanda Kafka connection connection-name : << : *extract environment : << : *kafka-defaults # Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\" # Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" volumes : # You can change './output/connection-name' to any output location you want - ./output/connection - name : /output Replace connection-name with the name of your connection. <<: *extract tells the kafka-extractor tool to run. environment contains all parameters for the tool. volumes specifies where to store results. In this example, the extractor will store results in the ./output/connection-name folder on the local file system. You can add as many Redpanda Kafka connections as you want. Did you know? Docker's documentation describes the services format in more detail. Provide credentials ​ To define the credentials for your Kafka connections, you will need to provide a Kafka client configuration file. For managed Kafka instances such as Confluent Cloud and Aiven , this configuration can be copied directly from the console. Here is an example that would be compatible with all Kafka variants: Apache Kafka, Confluent Cloud, and Aiven Kafka. This is just an example, your cluster configuration may vary: # Required connection configs for Kafka producer, consumer, and admin # If SSL enabled, use SASL_SSL, otherwise use SASL_PLAINTEXT (when using with basic auth) security.protocol=SASL_SSL # If basic auth is enabled sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username='{{ USERNAME or CLUSTER_API_KEY }}' password='{{ PASSWORD or CLUSTER_API_SECRET }}'; sasl.mechanism=PLAIN # Best practice for higher availability in Apache Kafka clients prior to 3.0 session.timeout.ms=45000 Redpanda Kafka only supports the SCRAM authentication method . Here is an example configuration: sasl.mechanism=<SCRAM - SHA - 256 or SCRAM - SHA - 512 depending on your config > security.protocol=SASL_SSL sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=\"<username > \" password=\"<password > \"; # Best practice for higher availability in Apache Kafka clients prior to 3.0 session.timeout.ms=45000 Secure credentials ​ Using local files ​ danger If you decide to keep Kafka credentials in plaintext files, we recommend you restrict access to the directory and compose file. For extra security, we recommend you use Docker secrets to store the sensitive passwords. To specify the local files in your compose file: secrets : kafka_client_config : # Change it to the actual location of your kafka config file (MANDATORY) file : ./kafka.client.config kafka_ca_cert : # Change it to the actual location of your kafka CA cert file (OPTIONAL - only use if using custom CA) file : ./ca.pem kafka_access_cert : # Change it to the actual location of your kafka access cert file (OPTIONAL - only use if using Client Certificate auth) file : ./service.cert kafka_access_key : # Change it to the actual location of your kafka access key file (OPTIONAL - only use if using Client Certificate auth) file : ./service.key danger This secrets section is at the same top-level as the services section described earlier. It is not a sub-section of the services section. Using Docker secrets ​ To create and use Docker secrets: Store the Kafka configuration file: sudo docker secret create kafka_client_config path/to/kafka.client.config # Optional sudo docker secret create kafka_ca_cert path/to/ca.pem sudo docker secret create kafka_access_cert path/to/service.cert sudo docker secret create kafka_access_key path/to/service.key At the top of your compose file, add a secrets element to access your secret: secrets : kafka_client_config : external : true name : kafka_client_config kafka_ca_cert : external : true name : kafka_ca_cert kafka_access_cert : external : true name : kafka_access_cert kafka_access_key : external : true name : kafka_access_key The name should be the same one you used in the docker secret create command above. Once stored as a Docker secret, you can remove the local Kafka configuration file. Within the service section of the compose file, add a new secrets element and specify the name of the secret within your service to use it. Example ​ Let's explain in detail with an example: secrets : kafka_client_config : external : true name : kafka_client_config x-templates : # ... services : # Example Apache Kafka connection apache-kafka-example : << : *extract environment : << : *kafka-defaults # Kafka bootstrap servers (semicolon-separated) KAFKA_BOOTSTRAP_SERVERS : \"localhost:9092\" # Skip topics that are internal to Kafka (e.g. __consumer_offsets) SKIP_INTERNAL_TOPICS : \"true\" volumes : # You can change './output/apache-kafka-example' to any output location you want - ./output/apache - kafka - example : /output In this example, we've defined the secrets at the top of the file (you could also define them at the bottom). The kafka_client_config refers to an external Docker secret created using the docker secret create command. The name of this service is apache-kafka-example . You can use any meaningful name you want. The <<: *kafka-defaults sets the connection type to Kafka. KAFKA_BOOTSTRAP_SERVERS tells the extractor about the Kafka hosts or brokers. SKIP_INTERNAL_TOPICS tells the extractor whether to extract internal topics or skip them. The ./output/apache-kafka-example:/output line tells the extractor where to store results. In this example, the extractor will store results in the ./output/apache-kafka-example directory on the local file system. We recommend you output the extracted metadata for different connections in separate directories. The secrets section within services tells the extractor which secrets to use for this service. Each of these refers to the name of a secret listed at the beginning of the compose file. Tags: data crawl Previous Crawl Redpanda Kafka Next Crawl on-premises Kafka Prerequisites Get the compose file Define Kafka connections Provide credentials Secure credentials Example"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/how-tos/integrate-anomalo",
    "content": "Connect data Data Quality & Observability Anomalo Get Started How to integrate Anomalo On this page Integrate Anomalo Once you have configured the Anomalo settings , you can establish a connection between Atlan and Anomalo. To integrate Anomalo with Atlan, review the order of operations and then complete the following steps. Create an API token in Atlan ​ Before running the workflow, you will need to create an API token in Atlan. Configure the integration in Atlan ​ Select the source ​ To select Anomalo as your source, from within Atlan: In the top right of any screen, click New and then click New workflow . From the list of packages, select Anomalo Assets and then click Setup Workflow . Create the connection ​ You will only need to create a connection once to enable Atlan to receive incoming events from Anomalo. Once you have set up the connection, you neither have to rerun the workflow nor schedule it. Atlan will process the Anomalo events as and when your checks run in Anomalo to catalog your check metadata. To configure the Anomalo connection, from within Atlan: For Connection Name , provide a connection name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. For Host Name , enter the URL of your Anomalo instance. For API Key , enter the API key you copied in Anomalo. Click the Test Authentication button to confirm connectivity to Anomalo. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ On the Metadata page, you can override the defaults for any of these options: To select the warehouses you want to include in crawling, click Include warehouses . (This will default to all warehouses, if none are specified.) To select the warehouses you want to exclude from crawling, click Exclude warehouses . (This will default to no warehouses, if none are specified.) To check for any permissions or other configuration issues before running the crawler, click Preflight checks . Navigate to the bottom of the screen and click Next . Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Configure the integration in Anomalo ​ Who can do this? You will need your Anomalo Deployment Admin Superuser to complete these steps   -  you may not have access yourself. As a Deployment Admin Superuser in Anomalo, you will need to create an Organization Integration or Deployment Integration linking a newly created Atlan integration to your Anomalo deployment. This configuration is required only after you have completed integrating Anomalo in Atlan. You will need the following for this configuration: URL of your Atlan instance API token generated in Atlan To create an Atlan integration in Anomalo: Log in to your Anomalo instance as a Deployment Admin Superuser . Create an Atlan integration in the admin interface. Enter your Atlan hostname and the API token you generated in Atlan . Create an Organization Integration or a Deployment Integration linking the new Atlan integration to your Anomalo deployment or a specific organization. Once you have integrated Anomalo and your checks have completed running in Anomalo, Atlan will start receiving and processing events from Anomalo. You will see your Anomalo checks cataloged in Atlan! 🎉 Note that Atlan does not fetch any historical check metadata. As new events are received from Anomalo, Atlan will process these events and catalog your Anomalo checks. Did you know? You can also view event logs in Atlan to track and debug events received from Anomalo. Tags: connectors integration api authentication Previous Set up Anomalo Next What does Atlan crawl from Anomalo? Create an API token in Atlan Configure the integration in Atlan Configure the integration in Anomalo"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/on-premises-event-buses/how-tos/crawl-on-premises-kafka",
    "content": "Connect data Event/Messaging Redpanda Kafka Guides Crawl on-premises Kafka On this page Crawl on-premises Kafka Once you have set up the kafka-extractor tool , you can extract metadata from your on-premises Kafka instances by completing the following steps. Run kafka-extractor ​ Crawl all Kafka connections ​ To crawl all Kafka connections using the kafka-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up Crawl a specific connection ​ To crawl a specific Kafka connection using the kafka-extractor tool: Log into the server with Docker Compose installed. Change to the directory containing the compose file. Run Docker Compose: sudo docker-compose up <connection-name> (Replace <connection-name> with the name of the connection from the services section of the compose file.) (Optional) Review generated files ​ The kafka-extractor tool will generate many folders with JSON files for each service . For example: topics topic-configs consumer-groups consumer-groups-members and many others You can inspect the metadata and make sure it is acceptable for providing metadata to Atlan. Upload generated files to S3 ​ To provide Atlan access to the extracted metadata, you will need to upload the metadata to an S3 bucket. To upload the metadata to S3: Ensure that all files for a particular connection have the same prefix. Upload the files to the S3 bucket using your preferred method   -  include all the files from the output folder generated after running Docker Compose. For example, to upload all files using the AWS CLI : aws s3 cp output/kafka-example s3://my-bucket/metadata/kafka-example --recursive Crawl metadata in Atlan ​ Once you have extracted metadata on-premises and uploaded the results to S3, you can crawl the metadata into Atlan: How to crawl Apache Kafka How to crawl Confluent Kafka How to crawl Aiven Kafka How to crawl Redpanda Kafka Be sure you select S3 for the Extraction method . Tags: connectors data crawl Previous Set up on-premises Kafka access Next What does Atlan crawl from Redpanda Kafka? Run kafka-extractor (Optional) Review generated files Upload generated files to S3 Crawl metadata in Atlan"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/anomalo/how-tos/set-up-anomalo",
    "content": "Connect data Data Quality & Observability Anomalo Get Started Set up Anomalo On this page Set up Anomalo Atlan supports the API authentication method for fetching metadata from Anomalo . This method uses an API key to fetch metadata. Your Anomalo Deployment Admin Superuser must also configure an Atlan integration in your Anomalo deployment to send events to Atlan when your checks run in Anomalo. This will update the check metadata in Atlan in real time. This configuration is required only after you have completed integrating Anomalo in Atlan . You will need your Atlan hostname and an API token generated in Atlan . Generate an API key ​ Who can do this? You must at least have an Anomalo Viewer role to generate an API key . Atlan will require read-only access to your connected data sources in Anomalo. Did you know? Atlan does not make any API requests or queries that will update the objects in your Anomalo environment. You will need to create an API key in Anomalo for integrating with Atlan. To create an API key for crawling Anomalo : Log in to your Anomalo instance. From the left menu of your Anomalo instance, click Settings . On the Settings page, in the Account tab, change to the API keys tab. On the API keys page, to generate a new API key: If you have existing API keys, click the Add an API key button. If you do not have any API keys, click the Create an API key button. In the New API Key dialog, enter the following details: For Description , add a meaningful description for your API key   -  for example, Atlan connection . For Expiration , keep the default selection or select a preferred option. Click Save to finish creating the API key. From the corresponding screen, copy the API Key value and store it in a secure location. danger The API key cannot be retrieved later. You must copy the key value before closing the dialog box. Tags: connectors data integration api authentication configuration Previous Anomalo Next How to integrate Anomalo Generate an API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/messaging/redpanda-kafka/how-tos/set-up-redpanda-kafka",
    "content": "Connect data Event/Messaging Redpanda Kafka Get Started Set up Redpanda Kafka On this page Set up Redpanda Kafka Who can do this? You will probably need your Redpanda Kafka administrator to complete these steps   -  you may not have access yourself. Atlan supports the S3 extraction method for fetching metadata from Redpanda Kafka. This method uses Atlan's kafka-extractor tool to fetch metadata. Create user in Redpanda Kafka ​ To create a new user for extracting metadata from Redpanda Kafka : Log in to your Redpanda Console and select your active cluster. From the left menu of your cluster's Overview page, click the Security tab. From the upper right of the Security page, click Create User to create a new user. In the Create User dialog, enter the following details: For Username , enter a name for the new user   -  for example, Atlan integration . For Password , set a password for the new user. From the Mechanism dropdown, select a SCRAM mechanism . Click Create to finish creating the new user. From the list of users on the Security page, select the newly created user to edit the associated Access Control Lists (ACLs) . In the Edit ACL dialog, enter the following details: For Topics , click Add Topic ACL to allow the following operations   - Describe and DescribeConfigs . For Consumer Groups , click Add Consumer Group ACL to allow the following operation   - Describe . For Transactional ID , click Add Transactional ID ACL to allow the following operation   - Describe . For Clusters , click Add Cluster ACL to allow the following operations   - Describe and DescribeConfigs . Once you have added all the required operations, click OK to finish setup. Did you know? Once you have extracted metadata on-premises and uploaded the results to S3 , you can crawl the metadata from Redpanda Kafka into Atlan. Tags: connectors data Previous Redpanda Kafka Next Crawl Redpanda Kafka Create user in Redpanda Kafka"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/how-tos/set-up-monte-carlo",
    "content": "Connect data Data Quality & Observability Monte Carlo Get Started Set up Monte Carlo On this page Set up Monte Carlo Who can do this? You will probably need your Monte Carlo account owner to complete these steps   -  you may not have access yourself. Atlan supports the API authentication method for fetching metadata from Monte Carlo. This method uses an API key ID and secret to fetch metadata. Create an account-service API key ​ Did you know? Atlan does not make any API requests or queries that will update the objects in your Monte Carlo environment. You will need to create an account-service API key in Monte Carlo for integration with Atlan. To create an account-service API key for crawling Monte Carlo : Log in to your Monte Carlo instance. In the top header of your Monte Carlo instance, click Settings . In the left menu under Settings , click API Access and then click Account Service Keys . From the Account Service Keys page, click the Create Key button. In the Create Account Service Key dialog, enter the following details: For Description , add a meaningful description for your API key   -  for example, Atlan connection . From the Authorization Groups dropdown, select Viewers (All) to provide minimum permissions for crawling Monte Carlo. (Optional) For Expires After , keep the default selection or select a preferred option. Click Create to finish creating the account-service API key. From the corresponding screen, copy the Key ID and Secret and store them in a secure location. danger The API secret cannot be retrieved later. Tags: connectors data integration crawl api authentication Previous Monte Carlo Next Crawl Monte Carlo Create an account-service API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/monte-carlo/how-tos/crawl-monte-carlo",
    "content": "Connect data Data Quality & Observability Monte Carlo Crawl Monte Carlo Assets Crawl Monte Carlo On this page Crawl Monte Carlo Once you have configured the Monte Carlo permissions , you can establish a connection between Atlan and Monte Carlo. To crawl metadata from Monte Carlo, review the order of operations and then complete the following steps. Select the source ​ To select Monte Carlo as your source: In the top right of any screen in Atlan, navigate to +New and click New Workflow . From the Marketplace page, click Monte Carlo . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Monte Carlo credentials: For Authentication , API Key Authentication is the default selection. For API Key ID , enter the API key ID you copied . For API Secret , enter the API secret you copied . Click the Test Authentication button to confirm connectivity to Monte Carlo. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Monte Carlo connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Monte Carlo crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click the Include filter . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click the Exclude filter . (This will default to no assets, if none specified.) To enable crawling assets with specific incident statuses , click Include Incident Statuses and select the relevant option(s). To include unresolved incidents by default, we recommend selecting the No Status and Acknowledged filters. (This will default to all incident statuses, if none are specified.) For Incidents and Alerts time range , specify a date range for which you want to crawl alerts and incidents from Monte Carlo. The default date range is set to the last 30 days, you can either keep the default selection or change to the last 14 or 45 days. (Optional) For Advanced Config , keep Default for the default configuration or click Custom to configure the enrichment: To map Monte Carlo metadata enrichment to assets from specific connections only, for Include Connections , specify the connections in Atlan, or leave it blank to include all connections. If you have specified any connections, Atlan will map monitors, alerts, and incidents only to the assets included in those connections. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Monte Carlo crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Monte Carlo Next What does Atlan crawl from Monte Carlo? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/how-tos/set-up-soda",
    "content": "Connect data Data Quality & Observability Soda Get Started Set up Soda On this page Set up Soda Who can do this? You will need your Soda Cloud administrator to complete these steps   -  you may not have access yourself. You will also need to scan your datasets using the latest version of Soda Library or migrate from Soda Core to Soda Library to ensure the best possible experience in Atlan. Associated checks for datasets scanned using an older version of Soda Library may be unavailable or missing the relationship with datasets in Atlan. Atlan supports the API authentication method for fetching metadata from Soda. This method uses an API key ID and API secret to fetch metadata. Create an API key ​ Did you know? Atlan does not make any API requests or queries that will update the objects in your Soda instance. You will need to create an API key in Soda for integration with Atlan. To create an API key for crawling Soda : Log in to your Soda Cloud instance as an Admin . In the top right of your Soda Cloud account, click on your avatar, and from the dropdown, click Profile . Under your profile name, click the API Keys tab. On the API Keys page, click the + button to generate a new API key. In the API Keys dialog, enter the following details: For Description , enter a meaningful description. For Organization , enter the name of your organization. Click Create to finish setup. From the corresponding screen, copy the API Key ID and API Key Secret and store them in a secure location. danger The API secret cannot be retrieved later. Tags: data api authentication Previous Soda Next Crawl Soda Create an API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/observability/soda/how-tos/crawl-soda",
    "content": "Connect data Data Quality & Observability Soda Crawl Soda Assets Crawl Soda On this page Crawl Soda Once you have configured the Soda permissions , you can establish a connection between Atlan and Soda. To crawl metadata from Soda, review the order of operations and then complete the following steps. Select the source ​ To select Soda as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Soda . In the right panel, click Setup Workflow . Provide your credentials ​ To enter your Soda credentials: For Host Name , enter the hostname or base URL of your Soda instance to connect to the Soda APIs   -  for example, cloud.soda.io , cloud.us.soda.io , or demo.soda.io . For Authentication , API Key Authentication is the default selection. For API Key ID , enter the API key ID you copied . For API Secret , enter the API secret you copied . Click the Test Authentication button to confirm connectivity to Soda. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Soda connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Soda crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the assets you want to include in crawling, click the Include filter . (This will default to all assets, if none are specified.) To select the assets you want to exclude from crawling, click the Exclude filter . (This will default to no assets, if none specified.) (Optional) For Advanced Config , keep Default for the default configuration or click Custom to configure the enrichment: To map Soda metadata enrichment to assets from specific connections only, for Include Connections , specify the connections in Atlan, or leave it blank to include all connections. If you have specified any connections, Atlan will map Soda checks only to the assets included in those connections. Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Soda crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button.\nOnce the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl setup Previous Set up Soda Next What does Atlan crawl from Soda? Select the source Provide your credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid/how-tos/crawl-bigid",
    "content": "Connect data Privacy & Security BigID Crawl BigID Metadata Crawl BigID On this page Crawl BigID Configure the Atlan BigID workflow to crawl metadata from your BigID instance and discover privacy-related data assets in Atlan. This guide walks through setting up the workflow, configure connection, map data sources, and running the crawler. Prerequisites ​ Before you begin, make sure you have: Set up a BigID system user account. If not, follow the Set up BigID guide for detailed instructions. Your BigID domain name and API token which is needed to configure the workflow. To crawl metadata from BigID, review the order of operations . Permissions required ​ To successfully configure the BigID workflow, make sure that your user role has below permissions: Atlan : Admin or Workflow Admin permissions BigID : System user with API access Set up workflow ​ Follow these steps in Atlan to create a BigID workflow: Click New and then click New Workflow to set up a new workflow From the list of packages, select BigID and click on Setup Workflow . Provide below details: Workflow Name : Enter a unique name to help recognize and manage the workflow Host FQDN : Enter the BigID domain name. For private-network setups, use the private DNS associated with the link. Personal Access Token Value : Provide the API token created for the system user in the Set up BigID SSL certificate : Enter the root certificate PEM value if your BigID instance exposes a self-signed certificate Click Test Authentication to confirm connectivity to BigID When successful, at the bottom of the screen click Next Set up connection ​ Set up the connection details and specify who can manage this connection. Follow these steps to configure the connection: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics (Optional) To change the users able to manage this connection, change the users or groups listed under Connection Admins . If you don't specify any user or group, nobody can manage the connection - not even admins. At the bottom of the screen, click Next to proceed Map data sources ​ Map BigID Data Sources to Atlan Connections to establish the relationship between your data assets. Follow these steps to map data sources: Atlan Connection : Select the Atlan Connection that houses the data assets that you're trying to bring BigID metadata for BigID Datasources : Select one or more BigID Data Sources that contain assets associated with the mapped Atlan Connection Click Next to configure custom metadata Configure custom metadata ​ Set up custom metadata to store BigID-discovered attributes in Atlan. Follow these steps to configure custom metadata: Custom Metadata : Create a new Custom Metadata on Atlan named BigID Metadata with a text-based property named Attributes . This is used to house the BigID-discovered, scan-related attributes that the workflow brings over to Atlan Attribute Custom Metadata : Enter the value as BigID Metadata Attribute Custom Metadata Property : Enter the value as Attributes Run crawler ​ Execute the BigID crawler to discover and import metadata. Follow these steps to run crawler: Run immediately : Click the Run button to run the crawler once immediately Schedule run : Click the Schedule Run button to schedule the crawler to run hourly, daily, weekly, or monthly Troubleshooting ​ If you encounter issues during the BigID crawl process: Authentication issues : Verify your API token is valid and has the correct permissions SSL certificate errors : Check that you've provided the correct root certificate PEM value No metadata appears : Check that data source mapping is correct and BigID contains assets for the mapped connections Need help ​ Contact Atlan support : For issues related to Atlan integration, contact Atlan support See also ​ What does Atlan crawl from BigID Tags: connectors data crawl privacy bigid Previous Set up BigID Next What does Atlan crawl from BigID? Prerequisites Permissions required Set up workflow Troubleshooting Need help See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/how-tos/crawl-s3",
    "content": "Connect data Storage Amazon S3 Crawl S3 Assets Crawl S3 assets On this page Crawl S3 assets Catalog your Amazon S3 buckets and objects in Atlan using the S3 Assets workflow. This guide walks you through setting up authentication and running your first crawl. Prerequisites ​ Before you begin, make sure the you have: Completed S3 setup with IAM credentials. Your AWS credentials (Access Key ID and Secret Access Key, or Role ARN) ready. Information about S3 buckets and prefixes you want to catalog. Verified that the AWS account is allowlisted to assume the role when using IAM role-based authentication Set up the destination bucket structure , required only if you plan to use inventory-based ingestion . Set up workflow ​ Create a new S3 Assets workflow: In the top right, select New > New Workflow . From the package list, select S3 Assets . Select Setup Workflow . Configure extraction method ​ Choose how to connect to your S3 environment: Direct extraction Agent extraction Select Direct for the extraction method. Choose your authentication type: IAM User : Enter your Access Key ID and Secret Access Key. IAM Role : Enter your Role ARN. Select the AWS Region where your buckets are located. Select Test Authentication to verify the connection. Select Next . Select Agent for the extraction method. Add the secret keys for your secret store configuration. Follow the Secure Agent configuration guide . Select Next . Choose ingestion method ​ Select your ingestion method: Direct ingestion : Recommended for fewer than 1 million objects. This method crawls S3 buckets and objects directly. Inventory ingestion : Recommended for large-scale use (more than 1 million objects). Uses inventory reports for efficiency. For inventory ingestion, provide: S3 Bucket Name : Bucket holding the inventory reports (without the s3:// prefix). S3 Bucket Prefix : Prefix used in the report configuration. Include a trailing slash ( / ). Leave empty if no prefix was used. note The region for the inventory report is picked from the credentials used in the extraction method . Configure bucket filters ​ Choose which buckets and prefixes to include or exclude. Exclude filters override include filters if both match. For a single bucket : Include Bucket : Exact bucket name (e.g., my-data-bucket ) Include Prefix : Specific prefix to crawl (e.g., processed/2024/ ) Leave all other filters empty. For multiple buckets : Include Bucket : Regex pattern (e.g., prod-.* | analytics-.* ) Exclude Bucket : Regex pattern (e.g., .*-temp | .*-backup ) Include Prefix : Prefixes to include (e.g., data/ | reports/ ) Exclude Prefix : Prefixes to exclude (e.g., archive/ | tmp/ ) Configure connection details ​ Enter a Connection Name to identify your S3 environment. Examples: production-s3 , analytics-lake , raw-data-store Assign Connection Admins to manage access. At least one admin is required. Run crawler ​ You can now start cataloging your assets: Run now : Select Run to start a one-time crawl. Schedule runs : Select Schedule & Run to automate recurring crawls. Monitor crawl progress in the activity log. Once complete, your S3 buckets and objects will appear in Atlan. Troubleshooting ​ Permissions : Confirm all required IAM permissions are set. See the S3 setup guide for details. Need help ​ Contact Atlan support for integration issues or assistance. See also ​ What Atlan crawls from S3 : Full list of assets and metadata included in the crawl. Tags: s3 amazon-s3 crawl data-catalog storage Previous Set up Inventory reports Next S3 Inventory Report Structure Prerequisites Set up workflow Troubleshooting Need help See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/privacy/bigid/how-tos/set-up-bigid",
    "content": "Connect data Privacy & Security BigID Get Started Set up BigID On this page Set up BigID Create a BigID system user account and generate an API token for Atlan to access BigID metadata. This guide walks through creating a custom role, system user, and API token. Permissions required ​ To successfully set up BigID for Atlan integration, confirm that your user role has the necessary permissions: BigID : Administrator access to create roles and system users Create custom role ​ Create a custom role for Atlan to access BigID metadata. Follow these steps that provide privileges only to read metadata of assets and not the actual data contained in Catalog objects. Log in to your BigID instance Navigate to Settings → Access Management → Roles Click Add New Role Give a meaningful and unique name. For example, Atlan Integration as the role name Select root as scope. Add the following permissions: Catalog : Read, Export, Get Attributes Value, View Sensitive Values, Manual Fields (Read), Business Attributes (Read) Data Sources : Read Policies : Read Security Posture : Read Click Save Create system user ​ Atlan uses a system user to authenticate and retrieve metadata from BigID. Follow these steps to create a system user: Navigate to Settings → Access Management → System Users Click Add New Role Fill in the required user details Click Connect Roles and select the Atlan Integration role Click Save Generate API token ​ Atlan uses the API token in Workflow configure to autheticate with BigID. Follow these steps to generate an API token for the system user: Select the system user you just created In the details panel, click Generate under Tokens Set the token expiry period and click Generate Copy and save the token securely for use in Atlan workflow configuration Click Save Need help ​ If you encounter issues during the BigID setup process: BigID documentation : Refer to the BigID documentation for detailed information about roles, system users, and API tokens Contact Atlan support : For issues related to Atlan integration, contact Atlan support Next steps ​ Crawl BigID Tags: connectors data crawl privacy bigid Previous BigID Next Crawl BigID Permissions required Create custom role Create system user Generate API token Need help Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/how-tos/set-up-inventory-reports-for-s3",
    "content": "Connect data Storage Amazon S3 Inventory reports Set up Inventory reports On this page Set up Inventory reports Set up inventory reports for Amazon S3 to enable inventory-based ingestion through the crawler. This guide shows you how to configure inventory reports in the format required by Atlan's S3 crawler. Prerequisites ​ Before you begin, make sure you have: AWS permissions : Access to configure inventory reports on source buckets. Follow the official AWS documentation on inventory report configuration for permissions. Destination bucket : A dedicated S3 bucket to store inventory reports. Create destination bucket ​ First, create a dedicated S3 bucket to store your inventory reports. Sign in to the AWS Management Console. Navigate to S3 → Buckets . Click Create bucket . Enter a unique bucket name (for example, atlan-inventory-reports ). Make a note of the bucket name as it’s required when configuring the Atlan workflow for inventory-based ingestion. Select the appropriate region (keep this consistent for all inventory reports). Make a note of the region as it’s required when configuring the Atlan workflow for inventory-based ingestion. Configure other settings as needed. Click Create bucket . Configure inventory reports ​ Now configure inventory reports for each S3 bucket you want to catalog in Atlan. Navigate to S3 → Buckets . Select the source bucket you want to catalog. Go to the Management tab. Scroll down to Inventory configurations Click Create inventory configuration and configure the following settings: Inventory configuration name : Enter a meaningful name, such as atlan-inventory-config Inventory scope : Optionally choose a prefix to limit the report to specific objects. You can also use filters in your workflow. Object versions : Select Current version only (Atlan doesn't support Include all versions ). Configure the Report details : Destination bucket : Select the destination bucket you created earlier. Optionally specify a prefix to organize reports in a folder. Note : If you use a prefix, remember it for your Atlan workflow configuration and keep it consistent across all bucket reports. Report frequency : Choose daily or weekly. Report format : Select CSV or Apache Parquet (only these formats are supported). Status : Enable the inventory report by selecting Enabled . Encryption : Leave encryption disabled. Atlan's S3 crawler requires unencrypted inventory reports. Metadata fields : Select all available metadata fields. This ensures Atlan receives complete metadata information about your S3 objects. Review all settings and click Create . For multiple inventory reports, your destination bucket must follow a specific structure. See Inventory Report Structure for details. Need help? ​ If you run into issues while setting up inventory reports: AWS documentation : See the AWS S3 Inventory documentation for more information. Atlan support : If you have issues related to Atlan integration, contact Atlan support . Next steps ​ Once you've configured your inventory reports: Crawl your S3 assets : Follow steps to crawl your S3 assets. Tags: connectors data crawl storage amazon-s3 aws Previous Set up Amazon S3 Next Crawl S3 assets Prerequisites Create destination bucket Configure inventory reports Need help? Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/how-tos/set-up-confluent-schema-registry",
    "content": "Connect data Event/Messaging Confluent Schema Registry Get Started Set up Confluent Schema Registry On this page Set up Confluent Schema Registry Who can do this? You will probably need your Schema Registry administrator to complete these steps   -  you may not have access yourself. Atlan supports the API authentication method for fetching metadata from Confluent Schema Registry. This method uses an API key and secret to fetch metadata. Create an API key ​ Did you know? Atlan does not make any API requests or queries that will update the resources in your Confluent Cloud Schema Registry environment. To create an API key for crawling Confluent Schema Registry : Log in to your Confluent Cloud instance. From the left menu of the Cloud Console , click Environments and then select your environment. On your environment page, under Stream Governance API in the right menu, for Endpoint , click the clipboard icon to copy the endpoint and store it in a secure location. Under Credentials in the right menu, click View & manage if there are any existing API keys or click 0 keys if there are none to open the API credentials dialog. In the API credentials dialog, click + Add key if there are any existing API keys or click Create key if there are none to create a new schema registry API key. From the Create a new Schema Registry API key dialog: For Key , click the clipboard icon to copy the API key and store it in a secure location. For Secret , click the clipboard icon to copy the API secret and store it in a secure location. (Optional) For Description , enter a description for the new API key   -  for example, Atlan integration API key . Did you know? You will need the schema registry endpoint, API key, and API secret for crawling Confluent Schema Registry . Tags: connectors data crawl api authentication Previous Confluent Schema Registry Next Crawl Confluent Schema Registry Create an API key"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs/how-tos/crawl-gcs",
    "content": "Connect data Storage Google GCS Crawl GCS Assets Crawl GCS assets On this page Crawl GCS assets The Google Cloud Storage crawler fetches assets from Google Cloud Storage and publishes them to Atlan for discovery. The crawler catalogs buckets and objects from your GCS environment. Prerequisites ​ Before you begin, make sure you have: Completed the GCS setup guide . GCS credentials (Project ID and Service Account JSON key) ready. Determined which GCS buckets and prefixes you want to catalog. Create crawler workflow ​ Start by creating a new GCS Assets workflow: Navigate to the bottom right of any screen and select Workflow . Select Marketplace from the top if you are creating a new workflow, or select Manage if you want to use an existing workflow. Select Google Cloud Storage from the package list. Select Setup Workflow . Configure extraction method ​ Choose how to connect to your GCS environment. Connect directly to GCS using Atlan's credential store: Add the Project ID (Google Cloud project ID that contains the buckets). Add the Service Account JSON key that you created in the GCS setup guide . Select Test Authentication to verify connectivity. Select Next . Configure metadata filters ​ Set up filters to control which buckets and objects get cataloged: Bucket prefix : Publish to Atlan only the buckets that start with the specified prefix. Leave empty if you need all buckets. Object prefix : Publish to Atlan only the objects that start with the specified prefix. Leave empty if you need all objects. Object delimiter (applicable only if inventory report isn't selected): Use this to list all blobs in a \"folder,\" for example \"public/.\" The delimiter argument restricts results to only the \"files\" in the given \"folder.\" Without the delimiter, the entire tree under the prefix is returned. For example, given these blobs: a/1.txt a/b/2.txt If prefix = 'a/', without a delimiter, the following blobs are published to Atlan: a/1.txt a/b/2.txt However, if prefix = 'a/' and delimiter = '/', only the file directly under 'a/' is published to Atlan: a/1.txt Bucket exclusion list : List of buckets (comma separated) to be excluded. Configure ingestion method ​ Choose how to ingest data from GCS: Direct crawling Inventory report Configure direct crawling options: Build abstraction layer : Whether to build abstraction layer on top of files (default: No). Publish as-is patterns : List of comma-separated patterns to be published as-is (without abstraction layer). Applicable only if Build abstraction layer = Yes. Regex to match characters to replace : Regular expression to match characters to replace. It acts on the file full name (without bucket prefix). Regex with replacement characters : Regular expression with replacement characters. It acts on the file full name (without bucket prefix). Configure inventory report options: Inventory bucket name : Bucket where the inventory is stored. Inventory prefix : Prefix within the inventory bucket where the inventory is located. Inventory file format : File format used to generate the inventory report (CSV or Parquet). The following permissions must be granted to the role assigned to the Service Account: storage.buckets.list , storage.objects.list , and roles/storage.objectViewer . Build abstraction layer : Whether to build abstraction layer on top of files (default: No). Publish as-is patterns : List of comma-separated patterns to be published as-is (without abstraction layer). Applicable only if Build abstraction layer = Yes. Regex to match characters to replace : Regular expression to match characters to replace. It acts on the file full name (without bucket prefix). Regex with replacement characters : Regular expression with replacement characters. It acts on the file full name (without bucket prefix). Configure asset handling ​ Control how assets are created and updated: Input handling : How to handle assets in the CSV file that don't exist in Atlan: Create full : Create a full-fledged asset that can be discovered and maintained like other assets in Atlan. Create partial : Create a \"partial\" asset. These are only shown in lineage and can't be discovered through search. These are useful when you want to represent a placeholder for an asset that you lack full context about, but also don't want to ignore completely. Update only : Only update assets that already exist in Atlan, and don't create any asset of any kind. Note: READMEs and links in Atlan are technically separate assets—these are still created, even in Update only mode. Delta handling : Whether to treat the input file as an initial load, full replacement (deleting any existing assets not in the file), or only incremental (no deletion of existing assets). Remove attributes : How to delete any assets not found in the latest file. Reload which assets : Which assets to reload from the latest input CSV file. Changed assets only calculates which assets have changed between the files and only attempts to reload those changes. Configure connection ​ Set up the connection details: Connection : Name of the connection that's created in Atlan. The connection name must be unique across all Google Cloud Storage connections. Need help ​ If you run into issues during the GCS crawling process: GCP documentation : Refer to the Google Cloud Storage documentation for detailed information about buckets and objects. Contact Atlan support : For issues related to Atlan integration, contact Atlan support . See also ​ What does Atlan crawl from GCS : Learn about the GCS metadata that Atlan discovers and catalogs. Tags: gcs google-gcs crawl data-catalog storage Previous Set up Google Cloud Storage Next What does Atlan crawl from Google GCS Prerequisites Create crawler workflow Need help See also"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/amazon-s3/how-tos/set-up-s3",
    "content": "Connect data Storage Amazon S3 Get Started Set up Amazon S3 On this page Set up Amazon S3 This guide walks you through creating IAM permissions and authentication credentials to allow Atlan to catalog your S3 buckets and objects. warning This integration catalogs only S3 buckets and objects. It doesn't support data lineage. Prerequisites ​ Before you begin: Set up S3 inventory reports , required only if you plan to use inventory-based ingestion . Permissions required ​ To complete this setup, you'll need: AWS Administrator access to create IAM policies and users/roles in AWS Management Console Atlan workflow access to configure connectors and workflows in Atlan Access to configure S3 inventory reports only if you plan to use inventory ingestion Create IAM policy ​ Choose the appropriate policy depending on your ingestion method. Direct ingestion Inventory ingestion In AWS, go to IAM → Policies Click Create policy Select the JSON tab and paste: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAccessToBuckets\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListAllMyBuckets\" , \"s3:ListBucket\" , \"s3:GetObject\" , \"s3:GetEncryptionConfiguration\" , \"s3:GetBucketVersioning\" ] , \"Resource\" : [ \"arn:aws:s3:::<s3_bucket_1>\" , \"arn:aws:s3:::<s3_bucket_1>/*\" , \"arn:aws:s3:::<s3_bucket_2>\" , \"arn:aws:s3:::<s3_bucket_2>/*\" ] } ] } Replace <s3_bucket> with your actual bucket name or pattern. Click Next , name your policy (e.g. AtlanS3CrawlerDirectPolicy ), and create it. In AWS, go to IAM → Policies Click Create policy Select the JSON tab and paste: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowInventoryAccess\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:ListBucket\" , \"s3:GetObject\" , \"s3:SelectObjectContent\" ] , \"Resource\" : [ \"arn:aws:s3:::<s3_bucket>\" , \"arn:aws:s3:::<s3_bucket>/*\" ] } ] } Replace <s3_bucket> with your actual bucket name or pattern. Click Next , name your policy (e.g. AtlanS3CrawlerInventoryPolicy ), and create it. Set up authentication ​ Choose between IAM user (simpler) and IAM role (more secure and recommended for production). IAM user IAM role In AWS, go to IAM → Users Click Add users , give a name (e.g. atlan-s3-crawler ) Select Attach policies directly and choose the policy you just created Complete the steps and create an access key Save the Access Key ID and Secret Access Key — you'll need them in Atlan Contact Atlan support for the Node Instance Role ARN of your Atlan EKS cluster In AWS, go to IAM → Roles → Create role Select Trusted entity type: AWS account Enter Atlan’s AWS account ID (available via support) Attach the policy you created earlier Name the role (e.g. AtlanS3CrawlerRole ) and create it Edit the trust relationship with this policy: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" } ] } Share the role name and AWS account ID with Atlan support Once Atlan confirms access, copy the Role ARN (e.g. arn:aws:iam::<account-id>:role/<role-name> ) for use in the workflow warning Wait for confirmation from Atlan before proceeding to workflow configuration. Need help? ​ Check AWS IAM documentation for detailed reference Contact Atlan support for help with setup or integration Next steps ​ Crawl S3 assets : Configure your workflow and crawl S3 assets. Tags: connectors data crawl storage amazon-s3 aws Previous Amazon S3 Next Set up Inventory reports Prerequisites Permissions required Create IAM policy Set up authentication Need help? Next steps"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/schema/confluent-schema-registry/how-tos/crawl-confluent-schema-registry",
    "content": "Connect data Event/Messaging Confluent Schema Registry Crawl Schema Registry Assets Crawl Confluent Schema Registry On this page Crawl Confluent Schema Registry Once you have configured the Confluent Schema Registry access permissions , you can establish a connection between Atlan and Confluent Schema Registry. To crawl metadata from Confluent Schema Registry, review the order of operations and then complete the following steps. Select the source ​ To select Confluent Schema Registry as your source: In the top right of any screen in Atlan, navigate to +New and click New workflow . From the Marketplace page, click Confluent Schema Registry Assets . In the right panel, click Setup Workflow . Provide credentials ​ To enter your Confluent Schema Registry credentials: For Host , enter your schema registry endpoint . For API Key , enter the API key you copied . For API Secret , enter the API secret you copied . Click the Test Authentication button to confirm connectivity to Confluent Schema Registry. Once authentication is successful, navigate to the bottom of the screen and click Next . Configure the connection ​ To complete the Confluent Schema Registry connection configuration: Provide a Connection Name that represents your source environment. For example, you might use values like production , development , gold , or analytics . (Optional) To change the users who are able to manage this connection, change the users or groups listed under Connection Admins . danger If you do not specify any user or group, no one will be able to manage the connection   -  not even admins. Navigate to the bottom of the screen and click Next to proceed. Configure the crawler ​ Before running the Confluent Schema Registry crawler, you can further configure it. On the Metadata page, you can override the defaults for any of these options: To select the subjects you want to exclude from crawling, click Exclude subjects . (This will default to no subjects, if none specified.) To select the subjects you want to include in crawling, click Include subjects . (This will default to all subjects, if none are specified.) Did you know? If an asset appears in both the include and exclude filters, the exclude filter takes precedence. Run the crawler ​ To run the Confluent Schema Registry crawler, after completing the steps above: To check for any permissions or other configuration issues before running the crawler, click Preflight checks . You can either: To run the crawler once immediately, at the bottom of the screen, click the Run button. To schedule the crawler to run hourly, daily, weekly, or monthly, at the bottom of the screen, click the Schedule Run button. Once the crawler has completed running, you will see the assets on Atlan's asset page! 🎉 Tags: connectors data crawl Previous Set up Confluent Schema Registry Next What does Atlan crawl from Confluent Schema Registry? Select the source Provide credentials Configure the connection Configure the crawler Run the crawler"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/getting-started-with-the-apis",
    "content": "Get Started Quick Start Guides Developers Software development kits (SDKs) Software development kits (SDKs) You can integrate with Atlan in several ways. We recommend using one of our software development kits (SDKs) , if possible, which: encode best practices, are built for performance, and add validations and simplifications not found in the raw REST APIs themselves. Did you know? Our SDKs are nothing more than client libraries that wrap our underlying REST APIs. They prevent you from needing to reinvent the wheel by wrapping the REST APIs yourself. Tags: api rest-api graphql Previous Custom solutions Next Atlan's open API"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/custom-solutions",
    "content": "Get Started Quick Start Guides Developers Custom solutions Custom solutions Atlan provides custom packages tailored to your unique use cases. These publicly-available extensions broaden the scope of our out-of-the-box connectivity options . Reach out to your customer success manager to install any of the selected packages in your Atlan tenant: Asset change notification -  sends email notifications when assets are created, updated, and/or deleted. Asset export (advanced) -  exports a set of assets to different locations based on the filtering criteria you have applied. Asset export (basic) -  identifies and extracts assets that have been enriched in Atlan. You can modify the resulting CSV file by updating the metadata for your assets, and then load it back into Atlan using the asset import package. Asset import -  loads metadata from a CSV file that matches the format of one extracted using either of the asset export packages ( basic or advanced ). Lineage builder -  creates lineage between any source and target asset. Lineage generator (no transformations) -  automatically detects assets with the same (or similar) name between two connections and creates lineage between them. Relational assets builder -  creates (and updates) net-new relational assets: connections, databases, schemas, tables, views, materialized views, and columns. Tags: integration connectors Previous API authentication Next Software development kits (SDKs)"
  },
  {
    "url": "https://docs.atlan.com/apps/connectors/storage/google-gcs/how-tos/set-up-gcs",
    "content": "Connect data Storage Google GCS Get Started Set up Google Cloud Storage On this page Set up Google Cloud Storage This guide walks you through setting up Google Cloud Storage (GCS) to enable secure data ingestion from your GCS buckets. This guide walks you through setting up Google Cloud Storage (GCS) to enable secure data ingestion from your GCS buckets. The connector catalogs GCS buckets and objects only. Prerequisites ​ GCS bucket containing the data you want to ingest Appropriate permissions to create service accounts and manage IAM roles Permissions required ​ Make sure you (or an administrator) can assign the following IAM roles to the service account that the connector uses: Storage Bucket Viewer ( roles/storage.bucketViewer ) Storage Object Viewer ( roles/storage.objectViewer ) You also need permission to create a service account and generate its key. Create a service account ​ Select your project from the project dropdown.\nCreating a dedicated service account avoids using personal credentials and lets you manage access centrally. In the left navigation menu, go to IAM & Admin > Service accounts . Select Create service account . Enter a name for your service account (for example, atlan-gcs-connector ). Add an optional description. Select Create and continue . Assign roles and permissions ​ Add the following roles to your service account:\nThese roles grant read-only access so the connector can discover buckets and objects without modifying data. Storage Bucket Viewer : Lets you read bucket details ( storage.buckets.list ). Storage Object Viewer : Lets you list objects and read object metadata ( storage.objects.list ). Select Done . Generate a service account key ​ In the left navigation menu, go to IAM & Admin > Service accounts .\nThe JSON key file is used by the connector to authenticate to GCP programmatically. Select Create key . Download and store the key file securely. Select JSON as the key type. Select Create . Download the JSON file and store it securely. Configure bucket permissions ​ Navigate to Cloud Storage .\nGrant the service account access to every bucket you want Atlan to crawl. Select your bucket. Go to the Permissions tab. Select Add principal . Enter your service account email (for example, [email protected] ). Assign the Storage Object Viewer role. Select Save . Need help ​ If you run into issues during the GCS setup process: GCP documentation : Refer to the Google Cloud IAM documentation for detailed information about roles and permissions. Contact Atlan support : For issues related to Atlan integration, contact Atlan support . Next steps ​ Crawl GCS assets : Follow this guide to configure the crawler workflow and ingest metadata from your GCS buckets. Tags: connectors data crawl storage google-gcs gcp Previous Google Cloud Storage Next Crawl GCS assets Prerequisites Permissions required Create a service account Assign roles and permissions Generate a service account key Configure bucket permissions Need help Next steps"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-discovery-of-process-assets",
    "content": "Configure Atlan Administration Feature Management How to enable discovery of process assets On this page Enable  discovery of process assets Who can do this? You will need to be an admin user in Atlan to enable discovery and tracking of process assets. Processes represent the movement or transformation of assets in Atlan. By default, process assets are hidden on the assets page and reporting center to ensure an efficient asset search, filtering , and discovery experience. To create a more customizable experience for your users, you can turn on discovery and tracking of process assets. Did you know? Even if discovery of process assets is turned off, users will still be able to view process assets on the lineage graph and sidebar. Enable process asset discovery ​ To enable discovery and tracking of process assets: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Assets heading of the Labs page, turn on Discover and track processes . Your users will now be able to search, filter, discover, and track process assets ! 🎉 If you'd like to disable this feature, follow the steps above to turn it off. Tags: atlan documentation Previous How to enable associated terms Next How to enable sample data download Enable process asset discovery"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/allow-guests-to-request-updates",
    "content": "Configure Atlan Administration Get Started Allow guests to request updates On this page Allow guests to request updates Who can do this? You will need to be an admin user in Atlan to allow guest users to request metadata updates. Guest users in Atlan can only suggest changes to asset metadata if enabled from the admin center. To allow guest users to request updates, follow these steps. Enable guest users to request updates ​ To enable your guest users to request metadata updates: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access control heading of the Labs page, turn on Allow guests to raise requests for metadata updates . Your guest users will now be able to raise requests for metadata updates on assets! 🎉 If you'd like to disable this option for your guest users, follow the steps above and then turn it off. Tags: atlan documentation Previous Administration Next Allow members to view reports Enable guest users to request updates"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/allow-members-to-view-reports",
    "content": "Configure Atlan Administration Get Started Allow members to view reports On this page Allow members to view reports Who can do this? You will need to be an admin user in Atlan to allow member users to view the reporting center . Admin users can control access to the reporting center for member users in their organization. If enabled, member users will be able to view the following dashboards: Assets dashboard to monitor assets Glossary dashboard to track metrics for glossaries, categories, and terms Insights dashboard to track metrics for queries Usage and cost dashboard to track asset usage and associated costs danger Permission to view the governance and automations dashboards in the reporting center is reserved for admin users only. To allow member users to view the reporting center , follow these steps. Enable member users to view reports ​ To enable member users to view the reporting center: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access control heading of the Labs page, turn on Allow member users to access Reporting Center . Your member users will now be able to view the assets , glossary , Insights , and usage and cost dashboards in the reporting center! 🎉 If you would like to revoke access, follow the steps above to turn it off. Tags: dashboards visualization analytics Previous Allow guests to request updates Next Disable user activity Enable member users to view reports"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-admins",
    "content": "Get Started Quick Start Guides Administrators On this page Administrators User management ​ User management is a critical part of data governance. Atlan's user management capabilities should be a mainstay of how you organize and control access for people in your organization. Add and manage users from the admin center ​ It's super simple to invite and remove users from Atlan from the Admin center . You can also manage existing users by adding them to groups, changing their roles, or set up SSO , SCIM , and SMTP configurations. Manage access control from the governance center ​ The Governance center is where you can build access control mechanisms to manage user access . Personas allow you to group users into teams, such as Financial Analysts or Cloud Engineers , and set policies based on the access those personas should have. Purposes are where you can build policies based on the actions or access that a user might need. For example, you can use Atlan's policy-based access controls to manage access to PII and other sensitive data. This is a best practice for data governance. Once you set these policies, Atlan will enforce them throughout your users' experience. This means that users who don't have access to a particular type of data will not be able to see it. Governance workflows help you set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. Asset profile ​ The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. Glossary ​ The Atlan glossary is a rich tool for defining and organizing your data terminology to improve transparency and share knowledge. No need to ask around for what a column name means. The glossary functions as a source of truth for teams to understand their data assets. Start keeping all your definitions in one searchable place. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as: Owners of your data, so you know who to ask for clarification. Certificate status, to easily understand if metadata enrichment is still in progress or the asset is ready to be used. Linked assets that are relevant to the term, so you can explore other helpful material. Did you know? The glossary helps power Atlan's powerful search tool , so tagging and defining assets are critical to helping your team find what they need. Discovery ​ We rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The discovery tool is Atlan's powerful in-platform search, powered by the terms and descriptions you've added to your data assets. Here are a few of the things that make Atlan's discovery awesome: Every attribute of your data is searchable in Atlan   -  saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need. Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Search assets from just about any page in Atlan using Cmd/Ctrl+K or by clicking Search assets across Atlan at the top of any page. Control your search by using facets about your data (such as the verification status or owner) to find what's most important to you. Sort by popularity to quickly discover what assets your teammates are using every day. Tags: get-started quick-start Previous What is Atlan? Next Data consumers User management Asset profile Glossary Discovery"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-sample-data-download",
    "content": "Configure Atlan Administration Feature Management How to enable sample data download On this page Enable  sample data download Who can do this? You will need to be an admin user in Atlan to enable sample data downloads. Atlan allows admin users to enable or disable downloading sample data . This can help you enforce better governance across your organization. To enable sample data download, follow these steps . Enable sample data download ​ To enable sample data download: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Insights heading of the Labs page, turn on Download data . Your users will now be able to export sample data in a CSV file! 🎉 If you'd like to disable sample data download, follow the steps above to turn it off. Tags: atlan documentation Previous How to enable discovery of process assets Next How to enable scheduled queries Enable sample data download"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-data-consumers",
    "content": "Get Started Quick Start Guides Data consumers On this page Data consumers Discovery ​ We rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The discovery tool is Atlan's powerful in-platform search, powered by the terms, tags, and definitions you've added to your data. Here are a few of the things that make Atlan's discovery awesome: Every attribute of your data is searchable in Atlan   -  saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need. Discovery allows you to toggle the type of data asset you are looking for. Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Search assets from just about any page in Atlan using Cmd/Ctrl+K or by clicking Search assets across Atlan at the top of any page. Control your search by using facets about your data (such as the verification status or owner) to find what's most important to you. Sort by popularity to quickly discover what assets your teammates are using every day. Insights ​ Atlan was built by data teams for data teams, so we know that querying your data is crucial to answering important business questions. SQL is the most common way that data teams query their data, so we wanted to make it easy for Atlan users to seamlessly query and share SQL work with their team. Atlan's Insights workspace allows teams to find the data they need, query it, and save and manage all SQL queries in one place. You can write SQL queries from scratch or use the Visual Query Builder to write SQL scripts quicker. Did you know? The Visual Query Builder is a great way to start building complex queries without fluency in SQL. Think of it as \"fill in the blank\" but for queries! Some of our favorite features about Insights: Customize the SQL editor's look and feel through preferences Examine a data asset's lineage alongside your SQL editor, so you can effortlessly understand your data asset in relation to other assets Save, organize, and share your SQL queries Tags: get-started quick-start Previous Administrators Next Contributors Discovery Insights"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/enable-scheduled-queries",
    "content": "Configure Atlan Administration Feature Management How to enable scheduled queries On this page Enable  scheduled queries Who can do this? You will need to be an admin user in Atlan to enable scheduled queries. To enable scheduled queries, follow these steps. Enable scheduled queries ​ To enable scheduled queries: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Insights heading of the Labs page, turn on Schedule queries . In the Turn on scheduled queries dialog box, click Yes to confirm. Your users will now be able to schedule queries in Insights ! 🎉 If you'd like to disable scheduled queries, follow the steps above to turn it off. Did you know? Once scheduled queries has been enabled, your users will also be able to export large results via email . Tags: atlan documentation Previous How to enable sample data download Next Restrict asset visibility Enable scheduled queries"
  },
  {
    "url": "https://docs.atlan.com/platform/how-tos/generate-har-files-and-console-logs",
    "content": "Get Started Administration Generate HAR files and console logs On this page Generate HAR files and console logs Atlan is built on REST APIs , so you can see the requests being sent by the UI to the API gateway through your browser's developer console. To help Atlan troubleshoot issues, you may be asked to create and send a HAR file and browser console logs: Console or network logs frequently provide critical error details that are required to determine the underlying cause of the issue or bug that you are experiencing. HAR files include all the network traffic from when you started recording, including sensitive information like passwords and private keys. To avoid including such information in a HAR file, Atlan recommends using a text editor to manually edit the file and remove any sensitive content before sending it to Atlan support. Generate in Google Chrome ​ Launch Google Chrome and navigate to the relevant webpage in Chrome. In the upper-right corner of your screen, click the three vertical dots. From the Chrome menu, click More Tools and then click Developer Tools . In the left menu, click on the Network tab and then select Fetch/XHR as your filtering option. Under the Network tab, click Preserve log . A red circle will appear on the left to show that you have started recording the network log. If you see a black circle, click on it to turn it red and start recording. To allow Google Chrome to record the interaction between the browser and website, refresh the page. Confirm if you can view new entries in the console. Next to the red circle icon, click the circle slash icon to clear logs. Replicate the issue that you experienced in the browser. For example, if it's a particular click that triggers an error, perform this action so that the error is recorded in the console. Once the page has loaded, navigate to the Console tab and right-click in the console box. Select Save as... and enter a name for the file. Return to the Network tab and right-click the element that triggered an error. This is typically marked in red in the console. Click Save as HAR with content to save the HAR file. (Recommended) To remove any sensitive information from the HAR file: Open the HAR file in a text editor of your choice. Search for all instances of passwords and replace the values with a placeholder value such as ***** . For example, in the following sample password content, you can replace <YourPrivateKey> and <YourPrivateKeyPassword> with placeholder values: \"headersSize\": -1, \"bodySize\": 3762, \"postData\": { \"mimeType\": \"application/json\", \"text\": \"{\\\"host\\\":\\\"<YourHostName>\\\",\\\"port\\\":<port>,\\\"authType\\\":\\\"keypair\\\",\\\"username\\\":\\\"PRD_SDCSHOP_TU_ETL_CATALOG\\\",\\\"password\\\":\\\"-----BEGIN ENCRYPTED PRIVATE KEY-----\\\\<YourPrivateKey>\\\\n-----END ENCRYPTED PRIVATE KEY-----\\\",\\\"extra\\\":{\\\"role\\\":\\\"GLOBAL_TALEND_CATALOG_DBADMIN\\\",\\\"warehouse\\\":\\\"PRD_SDCSHOP_ETL\\\",\\\"private_key_password\\\":\\\"<YourPrivateKeyPassword>\\\"},\\\"connectorConfigName\\\":\\\"atlan-connectors-snowflake\\\",\\\"query\\\":\\\"show atlan schemas\\\",\\\"schemaExcludePattern\\\":[\\\"INFORMATION_SCHEMA\\\"]}\" } } Save the HAR file. Upload the HAR file and browser console log to your Atlan support ticket. Tags: api rest-api graphql Previous Cloud logging and monitoring Next Tenant access management Generate in Google Chrome"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/disable-user-activity",
    "content": "Configure Atlan Administration Feature Management Disable user activity On this page Disable user activity Who can do this? You will need to be an admin user in Atlan to disable user activity on asset profiles. You can view recently visited users and total views on your assets in Atlan by default. To disable asset profile visitors, complete these steps. Disable asset profile visitors ​ To disable asset profile visitors for your assets in Atlan: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Assets heading of the Labs page, turn off Asset profile visitors . Your users will not be able to view user activity on your assets in Atlan! 🎉 If you'd like to enable user activity, follow the steps above and then turn it on. Tags: atlan documentation Previous Allow members to view reports Next How to enable associated terms Disable asset profile visitors"
  },
  {
    "url": "https://docs.atlan.com/get-started/how-tos/quick-start-for-contributors",
    "content": "Get Started Quick Start Guides Contributors On this page Contributors Asset profile ​ The asset profile in Atlan gives you a quick and clear understanding of what a data asset contains. You can think of the asset profile as the TL;DR about your data. What's in an asset profile? Summary : The numbers of columns and rows, certification status (e.g. verified, draft, etc.), owner of the asset, and more Column Preview : An overview of column names and definitions Sample Data : A snapshot of what the data looks like (with anything sensitive hidden, according to your policies) Readme : An in-depth description of the asset that should provide all the context, knowledge, and links needed to fully understand the asset Lineage : Visualization of how your data asset relates to other assets, which helps you figure out upstream and downstream dependencies Glossary ​ The Atlan glossary is a rich tool for defining and organizing your data terminology to improve transparency and share knowledge. No need to ask around about what a column name means. The glossary functions as a source of truth for teams to understand their data assets. Start keeping all your definitions in one (searchable!) place. The glossary provides key intel on your data assets so you can quickly understand important attributes of your data, such as... Owners of your data, so you know who to ask for clarification Certificate status, to easily understand if the data is still in progress or ready to be used Linked Assets that are relevant to the term, so you can explore other helpful material Did you know? The glossary helps power Atlan's powerful search tool, so tagging and defining assets are critical to helping your team find what they need. Discovery ​ We rely on search bars to find things in almost every corner of the internet. Atlan uses a similar search tool to help you explore your data assets. The discovery tool is Atlan's powerful in-platform search, powered by the terms, tags, and definitions you've added to your data. Here are a few of the things that make Atlan's discovery awesome: Every attribute of your data is searchable in Atlan   -  saved SQL queries, schemas, links, and more. This lets you search far and wide to find exactly what you need. Discovery allows you to toggle the type of data asset you are looking for. Intelligent keyword recognition sees through your typos to show exactly what you wanted, no matter what you actually typed. Search assets from just about any page in Atlan using Cmd/Ctrl+K or by clicking Search assets across Atlan at the top of any page. Control your search by using facets about your data (such as the verification status or owner) to find what's most important to you. Sort by popularity to quickly discover what assets your teammates are using every day. Insights ​ Atlan was built by data teams for data teams, so we know that querying your data is crucial to answering important business questions. SQL is the most common way that data teams query their data, so we wanted to make it easy for Atlan users to seamlessly query and share SQL work with their team. Atlan's Insights workspace allows teams to find the data they need, query it, and save and manage all SQL queries in one place. You can write SQL queries from scratch or use the Visual Query Builder to write SQL scripts quicker. Did you know? The Visual Query Builder is a great way to start building complex queries without fluency in SQL. Think of it as \"fill in the blank\" but for queries! Some of our favorite features about Insights: Customize the SQL editor's look and feel through preferences Examine a data asset's lineage alongside your SQL editor, so you can effortlessly understand your data asset in relation to other assets Save, organize, and share your SQL queries Tags: get-started quick-start Previous Data consumers Next API authentication Asset profile Glossary Discovery Insights"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/restrict-asset-visibility",
    "content": "Configure Atlan Administration Feature Management Restrict asset visibility On this page Restrict asset visibility Who can do this? You will need to be an admin user in Atlan to restrict asset visibility. Note that glossary access works slightly differently . To build effective data governance, you need to control data access across your organization. Atlan handles this through access control policies . By default in Atlan, all users can see the existence of all assets. The All assets view includes all assets in your Atlan data estate. To change this default setting, see Disable all assets view . Generally, you may want to limit specific teams from being able to see all assets in Atlan. All you need to do is turn off the default behavior, so that your member and guest users will only have access to the assets curated through their personas and purposes . Admin users will still have full access to all assets, even when this default behavior is turned off. Summary ​ View all assets in Labs is OFF: To view any assets, a persona is required. Personas are necessary to grant access for viewing. View all assets in Labs is ON: All assets are visible by default, even without a persona. However, only the default metadata will be displayed and the rest will be locked. Personas are needed to restrict access to view specific assets. Example ​ Imagine a user who belongs to an Insurance Member purpose . Once you turn off the default behavior, this user will have access to: View details only about the curated assets in the Insurance Member purpose. Search and discover only the curated assets in the Insurance Member purpose. View details only about the related assets in the Insurance Member purpose. View only the linked assets in the Insurance Member purpose for glossary terms. View all assets in the lineage graph, but only view details in the sidebar for assets in the Insurance Member purpose. If the user is a member of multiple purposes and personas, the assets they can access will be a superset of assets across all those purposes and personas. danger Turning off the default visibility of all assets currently does not apply to the Explorer section in Insights or the requests widget. Member and guest users will still be able to view all assets in Insights and the requests widget. Disable all assets view ​ The All assets view on the Assets page is enabled for all users by default. To turn it off for your member and guest users, complete the following steps. To disable all assets view: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access Control heading of the Labs page, turn off View \"All assets\" in Assets Discovery . Your member and guest users will now only have access to the curated assets for their persona or purpose by default! 🎉 If you'd like to enable the all assets view, follow the steps above and then turn it on. Did you know? If a user does not belong to any persona or purpose and the All assets view is disabled, then the user will be prompted to reach out to their Atlan administrator and request to be added to a persona or purpose. For more questions on restricting asset visibility, head over here . Tags: glossary business-terms definitions Previous How to enable scheduled queries Next Restrict glossary visibility Summary Example Disable all assets view"
  },
  {
    "url": "https://docs.atlan.com/product/administration/labs/how-tos/restrict-glossary-visibility",
    "content": "Configure Atlan Administration Feature Management Restrict glossary visibility On this page Restrict glossary visibility Private Preview Who can do this? You will need to be an admin user in Atlan to restrict glossary visibility. Note that asset access works slightly differently . Once you have restricted glossary visibility: If a glossary policy has been configured for a selected persona, only the user belonging to that persona be able to view the glossaries curated through glossary policies. If a user isn't part of any persona, they're unable to view any glossaries in Atlan. If a user is part of a persona that doesn't have glossary policies specifically or any policies at all configured, the user is unable to view all glossaries. The restriction is applicable to the Assets page, glossary tree on the Glossary page, terms filter on the Assets page, terms in an asset sidebar, linked assets , and related terms . The All glossaries view on the Glossary page is enabled for all users by default. To turn it off for your member and guest users, complete the following steps. To disable all glossaries view: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access control heading on the Labs page, turn on Persona switcher in Glossary . Your member and guest users now only have access to the curated glossaries for their persona! 🎉 If you'd like to restore the default all glossaries view, repeat steps 1 to 3 and then turn off the toggle. Summary ​ Persona switcher in Glossary in Labs is OFF: Users can see all glossaries. Persona switcher in Glossary in Labs is ON: Users can't see any glossaries until provided in a persona. Did you know? If a user doesn't belong to any persona and the All glossaries view is disabled, then the user is prompted to reach out to their Atlan administrator and request to be added to a persona. Tags: glossary business-terms definitions Previous Restrict asset visibility Next How to view event logs Summary"
  },
  {
    "url": "https://docs.atlan.com/product/administration/logs/how-tos/view-event-logs",
    "content": "Configure Atlan Administration Monitoring How to view event logs On this page View event logs Who can do this? You will need to be an admin user in Atlan to view event logs. Event logs help you track and debug events received from supported connectors, providing you with greater observability in Atlan. Event logs are currently stored in Atlan for 7 days. You will first need to configure any of the following supported sources to receive events: Airflow/OpenLineage Amazon MWAA/OpenLineage Astronomer/OpenLineage Google Cloud Composer/OpenLineage Apache Spark/OpenLineage Anomalo Once you have configured a supported source, you can view event logs for your events from the admin center: View a list of the 20 most recently received events. For every event, you can also view the timestamp for when it was received in Atlan based on your local timezone and 24-hour time notation, name of the connector configured, and event details. Filter events by connectors — Airflow (also includes all other supported distributions, Amazon MWAA, Astronomer, and Google Cloud Composer), Apache Spark, and Anomalo. Expand any event to view the full JSON code. View event logs ​ You can view event logs in Atlan through two methods, depending on your role and workflow preferences: Via admin panel ​ To view event logs: From the left menu of any screen in Atlan, click Admin . Under the Logs heading of your admin Workspace , click Event logs . On the Event logs page, you can view a list of up to 20 most recently received events in Atlan. (Optional) Click the Connector dropdown to filter events by the connector configured: Click Airflow to view events received through Airflow , Amazon MWAA , Astronomer , or Google Cloud Composer connections. Click Spark to view events received through Apache Spark connections. Click Anomalo to view events received through Anomalo connections. (Optional) Click the refresh button to refresh event logs and view the latest events. For any event listed in the event logs, you can view the timestamp for when it was received in Atlan, name of the connector configured, and event details. (Optional) Click any event to view more details in the Event details sidebar: View the JSON code, connector name, and timestamp for the event received. When viewing the code, you can also click the brackets to collapse or expand them. Click the copy icon to copy the event details. Click the expand icon to view the JSON code in fullscreen mode. Via connection profile ​ caution 🤓 Who can do this? You must be an admin user in Atlan to view event logs. Navigate to Workflow > Manage . Select the Listeners tab. Click or search for the connection you want. Click the connection to open its profile. Click the Events tab. This view shows events specific to that connection. Tags: lineage data-lineage impact-analysis integration connectors Previous Restrict glossary visibility Next How to view query logs View event logs"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-documentation",
    "content": "Configure Atlan Atlan AI Documentation How to use Atlan AI for documentation On this page Use Atlan AI for documentation ➕ Available to customers in Enterprise and Business-Critical platform editions Who can do this? Before using Atlan AI, your admin user must enable Atlan AI in your Atlan workspace. Atlan AI helps you automate the process of documenting your data assets. You can use Atlan AI to generate meaningful context for your assets and then simply review the content for accuracy and relevance. Accept, reject, or edit any AI-powered suggestions, the choice is yours! You can use Atlan AI to: Document tables and views with AI-generated descriptions Document child assets with AI-generated descriptions Document terms and categories with AI-generated descriptions Document terms with AI-generated READMEs Add an Atlan AI-generated alias to supported assets Did you know? To ensure full transparency, any changes made using Atlan AI will be marked as Updated using Atlan AI in the activity log . Add a description to a table or view ​ You can use Atlan AI to add descriptions to your assets in Atlan and provide helpful context to your teams. Supported asset types include: Amazon QuickSight analyses, dashboards, and datasets dbt Cloud and dbt Core models, sources, and tests Looker dashboards, explores, looks, models, tiles, and views Microsoft Power BI workspaces, dashboards, datasets, data sources, pages, reports, tables, and tiles Mode charts, queries, and reports Redash dashboards, queries, and visualizations Salesforce objects, dashboards, and reports Sigma workbooks, datasets, pages, and data elements Snowflake streams SQL tables, views, databases, and schemas Tableau workbooks, worksheets, dashboards, data sources, and metrics ThoughtSpot answers and liveboards To add a description to a table or view using Atlan AI: From the left menu on any screen, click Assets . (Optional) Under the search bar on the Assets page, click the Table tab. (Optional) In the Filters menu on the left, click Properties to expand the menu and select Description to search for assets without a description. Click an asset to view the Overview tab in the sidebar. Under Description , you can either: For assets without a description, navigate to the text box and click use Atlan AI to add an Atlan AI-suggested description. For assets with an existing description, click the text box and then click the Improve using Atlan AI button to replace the existing description with an Atlan AI-suggested one. (Optional) At the bottom of the Atlan AI is writing... box, click Enhance now to briefly describe your organization and help Atlan AI make more relevant suggestions   -  this option is only visible to admin users. Once Atlan AI has generated a description, you can: Click anywhere in the text box to edit the text and then click Apply . Click Apply to add the description to your asset. Click Discard to discard the AI-generated description. Click the retry button to generate a new description, compare the two to select the most relevant option, and then click Apply . (Optional) From the sidebar tabs on the right, click the Activity tab to view the changelog   -  including the Updated using Atlan AI stamp, user information, and timestamp for the update. Your AI-suggested table or view description is now live! 🎉 Add a description to a column ​ You can use Atlan AI to add descriptions to your columns from the following: Overview tab in the sidebar for column assets Column preview section in the asset profile for table and view assets Columns tab in the sidebar for table and view assets Supported asset types include: Amazon QuickSight analysis visuals, dashboard visuals, and dataset fields dbt Cloud and dbt Core columns Looker fields for explores and views Microsoft Power BI columns and measures Salesforce fields Sigma dataset columns and data element fields SQL columns Tableau data source fields and calculated fields ThoughtSpot visualizations In this example, we'll use Atlan AI to add a description to a column from the Column preview section in a table asset profile . To add a description to a column using Atlan AI: From the left menu on any screen, click Assets . (Optional) Under the search bar on the Assets page, click the Table tab. Right-click an asset and then select Open profile to view its asset profile. From the asset profile, navigate to the Column Preview section and select a column to document. Under Description , click use Atlan AI to add an AI-generated description to the column. (Optional) At the bottom of the Atlan AI is writing... box, click Enhance now to briefly describe your organization and help Atlan AI make more relevant suggestions   -  this option is only visible to admin users. Once Atlan AI has generated a description, you can: Click anywhere in the text box to edit the text and then click Apply . Click Apply to add the description to your asset. Click Discard to discard the AI-generated description. Click the retry button to generate a new description, compare the two to select the most relevant option, and then click Apply . (Optional) From the sidebar tabs on the right, click the Activity tab to view the changelog   -  including the Updated using Atlan AI stamp, user information, and timestamp for the update. Your AI-suggested column descriptions are now live! 🎉 Add a description to glossary assets ​ You can use Atlan AI to add descriptions to your terms and categories in Atlan and provide useful business context for your linked assets . To add a description to a term using Atlan AI: From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to document using Atlan AI. You can either add a description from the term profile or sidebar. Under Description , you can either: For terms without a description, navigate to the text box and click use Atlan AI to add an Atlan AI-suggested description. For terms with an existing description, click the text box and then click the Improve using Atlan AI button to replace the existing description with an Atlan AI-suggested one   -  this option is only available in the sidebar. (Optional) At the bottom of the Atlan AI is writing... box, click Enhance now to briefly describe your organization and help Atlan AI make more relevant suggestions   -  this option is only visible to admin users. Once Atlan AI has generated a description, you can: Click anywhere in the text box to edit the text and then click Apply . Click Apply to add the description to your term. Click Discard to discard the AI-generated description. Click the retry button to generate a new description, compare the two to select the most relevant option, and then click Apply . (Optional) From the sidebar tabs on the right, click the Activity tab to view the changelog   -  including the Updated using Atlan AI stamp, user information, and timestamp for the update. Your AI-suggested term description is now live! 🎉 Add a README to terms ​ You can use Atlan AI to generate comprehensive READMEs for your terms in Atlan. This provides you with a solid foundation for documenting business context. You can then simply edit the Atlan AI-generated README and customize the format accordingly. To add a README to a term using Atlan AI: From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to document using Atlan AI. In the Readme section of the asset profile, click Use Atlan AI to add an Atlan AI-suggested README to the term. Click anywhere in the text box to edit or format the text and then click Save . Did you know? If there are automated suggestions for asset descriptions, the option to use Atlan AI to document such assets will be unavailable. Automated suggestions are based on user-generated descriptions for similar assets and may be more accurate than Atlan AI-generated descriptions. Tags: atlan documentation Previous How to implement the Atlan MCP server Next How to use Atlan AI for lineage analysis Add a description to a table or view Add a description to a column Add a description to glossary assets Add a README to terms"
  },
  {
    "url": "https://docs.atlan.com/product/administration/readme-templates/how-tos/create-readme-templates",
    "content": "Configure Atlan Administration Templates Create README templates On this page Create README templates Who can do this? You will need to be an admin user in Atlan to create and manage README templates. Admin users in Atlan can create, curate, and manage README templates from the governance center. Once admin users have created the templates, other users will be able to select these templates and enrich their assets with READMEs . They will also be able to see a rich preview of each template before adding the relevant documentation. Create a README template ​ To create a README template: In the left menu in Atlan, click Governance . Under the Governance heading, click Readme templates . On the Readme templates page, click Get started . In the Untitled Template dialog box, enter the following details: For template name, enter a name for your template. (Optional) For Add description , add a description for your template. (Optional) To add an icon, click on the image icon. Click Create to proceed. In the text editor, add your template. Click Save . (Optional) To edit the template, in the upper right of the screen, click the pencil icon. Your README template is now available in the templates manager! 🎉 Tags: integration connectors Previous How to view query logs Create a README template"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/use-atlan-ai-for-lineage-analysis",
    "content": "Configure Atlan Atlan AI Lineage Analysis How to use Atlan AI for lineage analysis On this page Use Atlan AI for lineage analysis ➕ Available to customers in Enterprise and Business-Critical platform editions Who can do this? Before using Atlan AI, your admin user must enable Atlan AI in your Atlan workspace. Atlan AI can help you understand lineage transformations using natural language. You can use Atlan AI to create a natural language explanation for assets with SQL attributes and help you better understand the transformation logic. Explain lineage transformations ​ To use Atlan AI to explain lineage transformations: From the left menu of any screen, click Assets . Select an asset, and from the top right of the asset card, click the View lineage icon to open the lineage graph. On the lineage graph, click any circular process button to view more details in the sidebar. From Overview in the sidebar, under Query , click the Atlan AI icon to explain the SQL query. You can now understand lineage transformations using Atlan AI! 🎉 Did you know? The lineage graph in Atlan provides a granular view of the data flows and transformations for your assets, learn more here . Tags: lineage data-lineage impact-analysis Previous How to use Atlan AI for documentation Next Atlan AI security Explain lineage transformations"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-models/how-tos/view-data-models",
    "content": "Configure Atlan Data Models Get Started How to view data models On this page view data models Once you have ingested your ER model assets in Atlan , you can: Search, discover, and filter models, entities, attributes, relationships, and mappings, cataloged as native assets in Atlan. Link ER entities at the conceptual, logical, or physical layer to your crawled data assets. Note that the Atlan UI is optimized for the following linking design: Conceptual entity → Logical entity → Physical entity → Database table or view. Trace the object lifecycle across various layers of abstraction or implementation: Business glossary ER models Crawled data assets   -  tables and views Models ​ To view an entity–relationship (ER) model: From the left menu of any screen in Atlan, click Assets . In the Filters menu on the left, click Source . Click Choose connection to filter for assets in a Data Model connection. Under the search bar on the Assets page, click the Asset type dropdown. From the Asset type dropdown, select a Model to search by a specific asset type. Select a model asset to view the asset sidebar or open the asset profile. Asset preview ​ The model asset preview includes basic information about the asset, including technical name, alias , model type, description, and total count of associated entities. Asset sidebar ​ The sidebar to the right of the asset preview provides high-level information about the asset. Here's what you can view specific to model assets: Overview offers a preview of the key characteristics of the asset, including model name, type, and description. You can add an alias , enrich metadata, link domains to your data models, and more from the asset sidebar. Entities displays a list of entities in a model, along with entity type and description. This tab also provides you with a search bar to search and sort entities. Asset profile ​ The model profile summarizes important details about the asset. Overview tab displays details such as technical name, alias , entity count, model type, description, certification status, and owners. The Entities section offers a snapshot of all the entities in a model, listing the entity name and description. Entities tab allows you to update metadata such as tags and terms for your entities directly from the model profile. Entities ​ To view an entity: From the left menu of any screen in Atlan, click Assets . In the Filters menu on the left, click Source . Click Choose connection to filter for assets in a Data Model connection. Under the search bar on the Assets page, click the Asset type dropdown. From the Asset type dropdown, select a Entity to search by a specific asset type. Select an entity asset to view the asset sidebar or open the asset profile. Asset preview ​ The entity asset preview includes basic information about the asset, including technical name, alias , entity type, description, and total count of associated attributes. Asset sidebar ​ The sidebar to the right of the asset preview provides high-level information about the asset. Here's what you can view specific to entity assets: Overview offers a preview of the key characteristics of the asset, including entity name, type, and description. You can add an alias , enrich metadata, link domains to your entities, and more from the asset sidebar. Attributes displays a list of attributes in an entity, along with data type, description, and primary key indicator, if any. This tab also provides you with a search bar to search and sort attributes. Relations shows a list of entity relationships: Associations displays relation name, associated entity, and cardinality. Generalization displays parent-child relationships between entities. Layers organizes different levels of abstraction in a tree-like representation: Conceptual entity → Logical entity → Physical entity → Database table or view. Asset profile ​ The entity profile displays important details about the asset. Overview tab summarizes details such as technical name, alias , attribute count, entity type, associated model, description, certification status, and owners. The Layers section organizes different levels of abstraction in a tree-like representation: Conceptual entity → Logical entity → Physical entity → Database table or view. Click an entity name to view more details in the asset sidebar. The Attributes section offers a snapshot of all the attributes of an entity, including the attribute name, primary key indicator, data type, nullability, and description. Attributes tab allows you to update metadata such as tags and terms for your attributes directly from the entity profile. Entity Diagram provides a visual representation of entity relationships . Entity diagram ​ An entity diagram is a visual representation of entity relationships. It also allows you to navigate to the next set of related entities as you explore the graph. Note that the entity diagram in Atlan is not a replacement for ER modeling tools, which are optimized for editing objects and displaying an entire model. The entity diagram in Atlan: Is entity-focused only Uses crow’s foot notation to represent one-to-many relationships Lists attributes that are part of an entity: Attributes that form part of a relationship are shown at the top of the list. Clicking on one highlights the attribute in the related entity that forms the basis of the association. The association itself can be highlighted and has a sidebar with detailed information. Layers ​ The Layers section of an entity or database table helps you navigate across different abstraction layers of entities. A layer is a mechanism to have multiple abstractions for a model or an entity. Conceptual   -  most abstract Logical   -  more defined Physical   -  well-defined and conformant to a target database system Tags: data crawl model Previous Data Models Next What are data models? Models Entities Entity diagram Layers"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/atlan-ai/how-tos/implement-the-atlan-mcp-server",
    "content": "Configure Atlan Atlan AI Get Started How to implement the Atlan MCP server On this page Implement Atlan MCP Server The Model Context Protocol (MCP) is an open standard that enables AI agents to access contextual metadata from external systems. Atlan provides a reference implementation of MCP through the Atlan MCP server . This server acts as a bridge between Atlan’s metadata platform and AI tools such as Claude and Cursor. You can use the Atlan MCP server to support AI-driven use cases like searching for assets, understanding lineage, or updating metadata, all using real-time context from Atlan. Get started ​ The Atlan MCP server can be configured in multiple environments depending on your preferred development setup and integration target. Follow the instructions below to set up the server with your desired tool: Cursor Claude Local development Set up the Atlan MCP server in Cursor: Using uv : uv is a fast Python package manager designed to run and manage virtual environments locally without the overhead of Docker. Using Docker : Use Docker to run the MCP server in an isolated, containerized environment. Set up the Atlan MCP server in Claude Desktop: Using uv : uv is a fast Python package manager designed to run and manage virtual environments locally without the overhead of Docker. Using Docker : Use Docker to run the MCP server in an isolated, containerized environment. Build and run the Atlan MCP server locally: Local build guide : Walkthrough for cloning, building, and running the server locally. Available tools ​ The Atlan MCP server exposes a set of tools that enable AI agents to interact with your metadata programmatically. Search assets : Search for assets in Atlan using filters such as name, type, tags, and domains. Query by DSL : Retrieve specific assets using Atlan’s DSL query language. Explore lineage : Explore upstream or downstream lineage for a given asset. Update assets : Modify asset metadata, including descriptions and certification status. Need help? ​ For troubleshooting and feature requests, see the GitHub repo . Contact Atlan support for help with setup or integration. See also ​ Atlan MCP Server README on GitHub Tags: Atlan MCP setup model Previous Atlan AI Next How to use Atlan AI for documentation Get started Available tools Need help? See also"
  },
  {
    "url": "https://docs.atlan.com/product/administration/logs/how-tos/view-query-logs",
    "content": "Configure Atlan Administration Monitoring How to view query logs View query logs Who can do this? You will need to be an admin user in Atlan to view query logs. The query log helps you track all queries run in Atlan, including: saved and unsaved queries in the Insights query editor queries run through both the Atlan UI and API sample data previews from asset profiles You can also view additional details and run status for each query and use filters to track specific queries. Query logs are persisted throughout the lifecycle of the Atlan instance for your organization. To view query logs: From the left menu of any screen in Atlan, click Admin . Under the Logs heading of your admin Workspace , click Query logs . On the Query logs page, you can view all the queries that your users have run or are running in Atlan. (Optional) Click the funnel icon to filter queries and then: Click Status to filter queries by run status — Succeeded , Failed , or Aborted . Click Users to filter queries by Atlan users. (Optional) Use the search bar to search for queries using specific keywords. The default date range is set to 30 days. Use the date filter to view query logs for the last 7 days, past 3 or 6 months, or a custom date range of your choice. For any query listed in the query logs, you can view the query name, connection, execution details, user that run the query, and timestamp for when the query was run. (Optional) Click any query to view more details in the Query details sidebar: In the Query details sidebar, you can view the full query, connection, database, schema, and asset name, query status, and query run time. Click the copy icon to copy the query and use it as a template for writing your own queries. Click the expand icon to see the full query. For Query Source , click Copy ID to copy the query ID. Tags: data api Previous How to view event logs Next Create README templates"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/add-stakeholders",
    "content": "Configure Atlan Data Products Stakeholder Management Add stakeholders On this page Add stakeholders Who can do this? Before you can create a stakeholder, you will need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You will need to have update permissions through domain policies for the specific domain(s) or subdomain(s) to create and manage stakeholders. Stakeholders help you define the people and their responsibilities within a data domain in Atlan. You can map stakeholders to all or selected domains with responsibilities that align with their function within those domains. This can also help you ensure accountability among stakeholders and improve collaboration between your teams. For example, for the data domain Customer Service , you may want to define a Customer Service Manager stakeholder and assign it to the people who serve that function. This way, your customer support team will know whom to contact for questions or escalate any issues. Once you have created stakeholders or want to use the default options from Atlan: You can add stakeholder information for any and all domains. This will help you provide additional metadata for your domain, but does not help enforce access control currently. For the latter, you will need to create domain policies . You can assign any user as a stakeholder within a specific domain. Each user can be mapped to only one responsibility within a domain. However, users can have multiple stakeholder responsibilities across multiple domains. Create a stakeholder ​ Atlan provides you with predefined stakeholders for your data domains. You can either use the default options and assign them to your users or create new definitions for stakeholders based on your business needs. To create a stakeholder: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , you can either: If you have enabled the products module , click Domains & products . If you have not enabled the products module, click Domains . On the Domains & products or Domains page, change to the Settings tab. Under Stakeholders , you can view the default stakeholders   -  domain owner, data engineer, data product owner, and data architect. To the right of each responsibility, click the right-facing arrow to view the domains it applies to and users assigned to that responsibility. You can neither delete nor make any changes to the default options. To create a new stakeholder, click Add . In the Add responsibility dialog, enter the following: For Name , enter a meaningful name for your stakeholder. Atlan recommends following a naming convention for responsibilities across your domains to help users find stakeholders more easily. For Applies to , click the dropdown to select domains that this responsibility should apply to: To select All domains , you must have update permissions on all data domains in Atlan. To select any one specific domain or multiple domains, you must have update permissions for the specific data domain(s). Add a stakeholder ​ You can add stakeholders directly from the domain or subdomain profile. To add a stakeholder to your data domain or subdomain: From the left menu of any screen in Atlan, click Products . To select a domain or subdomain, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the View all button to view more domains. From the domain or subdomain profile, under Stakeholders , click + Add stakeholder . In the right Stakeholders pane, click + Stakeholders . In the Add Stakeholders dialog, configure the following: For Users or groups , select the users or groups that the stakeholder responsibility should apply to. Click the stakeholder dropdown to select a default stakeholder, a custom one you created, or click Create new to create a new one. Click Add to add the stakeholder to the domain or subdomain. Tags: atlan documentation Previous Create data domains Next Create data products Create a stakeholder Add a stakeholder"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/monitor-data-domains",
    "content": "Configure Atlan Data Products Domain Management Monitor data domains On this page Monitor data domains Who can do this? Before you can monitor domains, you will need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You will need to be a domain owner or admin in Atlan to track domain activity and usage. The Statistics tab within a data domain helps you take the pulse of what's happening to your data domains in Atlan and gain actionable insights. You can view a summary of data products, track metrics for domain enrichment, view data product creation over time, monitor domain usage, and more. Summarize data products ​ The Summary section provides a high-level overview of your domain and data products. To summarize data products: From the left menu of any screen in Atlan, click Products . From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the Browse all button to view more domains. From the tabs along the top of your data domain page, click the Statistics tab. The Summary section provides you with a high-level overview of data product metrics: Under Product summary , you can view a total count of data products and output ports as well as information about domains and subdomains. Under Products by status , you can view a total count of products grouped by status: Active -  data products active for users to consume. Sunset -  data products planned for retirement. Archived -  data products archived and no longer available to users. Review enrichment completion ​ The Activity section displays a total count of data products grouped by type of metadata enrichment. You can also view the total count of products that need to be updated   -  for example, products without a description. To review metadata enrichment for data products: From the left menu of any screen in Atlan, click Products . From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the Browse all button to view more domains. From the tabs along the top of your data domain page, click the Statistics tab. Under Products by enrichment , navigate to the card you want to view   -  for this example, we'll select the With Description card. (Optional) In the With Description card, click the products remaining button to view all the products without a description in the sidebar. Track product creation over time ​ The Products created over time graph provides visual insights into your data product creation process. To track product creation over time: From the left menu of any screen in Atlan, click Products . From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the Browse all button to view more domains. From the tabs along the top of your data domain page, click the Statistics tab. Under Products created over time , view a time graph for product creation within a specific domain. Monitor domain usage ​ The Usage tab allows you to track and monitor domain usage. You can also view top domain visitors in this section. To track domain usage: From the left menu of any screen in Atlan, click Products . From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the Browse all button to view more domains. From the tabs along the top of your data domain page, click the Statistics tab. Under Usage , you can either: Track views over time for your data products   -  total views and views by date. View a set of top domain visitors. Tags: lineage data-lineage impact-analysis Previous Create domain policies Next What is business lineage? Summarize data products Review enrichment completion Track product creation over time Monitor domain usage"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/access-archived-assets",
    "content": "Use data Discovery Asset Management Access archived assets On this page Access archived assets To access archived assets in Atlan, complete the f ollowing steps. Archived assets ​ Did you know? Archived assets still exist in Atlan. They are only \"soft-deleted\" and do not appear in search results (by default). You can only access archived assets through discovery. Assets can be archived when: You lose permissions to an asset at source. The asset name is changed at source. An asset is removed at source or moved to a different schema or database. Delete your assets either by deleting a connection or configuring it in Atlan. Search for archived assets ​ To search for archived assets: From the left menu of any page, click Assets . Under Filters on the left, expand Properties . At the bottom of the list of properties, click Is Archived and then Yes . Now use any additional filters , and the results will include only archived assets. View details of an archived asset ​ To view the details of an archived asset: Search for the asset using the steps above. Narrow down your results using the other filters or search details. Click the card for the asset you want to view. Use the right sidebar to view the details of the asset. The icons on the far right of the sidebar allow you to review different aspects of the asset. Deleted assets ​ Assets that have been (hard-)deleted no longer exist in Atlan. So you cannot find them by searching, or even access them through a direct link. Tags: data asset-profile Previous Search and discover assets Next Add an alias Archived assets Deleted assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/create-domain-policies",
    "content": "Configure Atlan Data Products Domain Management Create domain policies On this page Create domain policies Who can do this? Before you can create a domain policy, you will need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You will need to be a domain admin in Atlan to create and manage domain policies. Domain policies allow you to control which users can (or cannot) take certain actions within a data domain or subdomain   -  for example, creating subdomains and data products, updating metadata, and more. In Atlan, you can define domain policies for data domains through personas . This framework allows you to ensure that your data products are secure and only accessible to the individuals or teams involved in managing the data. Create a persona ​ To create a persona: From the left menu of any screen, click Governance . Under Access Control , click Personas . If this is the first persona, click the Get started button. Otherwise click the New persona button. Enter a meaningful name for the persona, (optional) a description, and then click Create . You now have an empty persona. It won't do much until you complete the next steps, too! Add users and groups ​ To add users and groups to the persona, from within the persona: To the right of the Users and groups box, click the Add button. Select users and / or groups: Under the single-user icon, select users to add to the persona. Under the double-user icon, select groups to add to the persona. Click the Update button to save the users and groups to the persona. Now this persona will be available to those users and groups. It still won't do much, though, without some policies... (Optional) Add rich documentation ​ To add rich documentation describing the persona: Under Summary , then Channels , add any Slack channels relevant to the persona. Under Resources , add links to external resources like PDFs, repositories, Notion, Confluence, Google Drive   -  anything that has a URL. Under Readme , write a richly-formatted description of the persona. Add a domain policy ​ To add a domain policy to the persona, from within the persona: Change to the Policies tab. Click the New Policy button and choose the type of policy. Choose Domain policy . Under Name , briefly describe the policy's intention. Under Select domains , choose the domain on which to apply the policy   -  for example, Finance . (Optional) For Configure permissions , choose the permissions the policy will grant. By default, all permissions will be granted. To select others: To the right of Configure permissions , click the Edit link. Select the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one. You can enable or limit the following: Read : permission to view metadata, resources, and READMEs for data domains Update Domains : permission to update metadata, resources, and READMEs for data domains Create Sub-domains : permission to create new data subdomains within a domain Update Sub-domains : permission to update metadata, resources, and READMEs for data subdomains Create Products : permission to create new data products within a domain Update Products : permission to update metadata, resources, and READMEs for data products Delete Products : permission to delete data products within a domain Update Custom Metadata For Domains : permission to update custom metadata for data domains Update Custom Metadata For Sub-Domains : permission to update custom metadata for data subdomains Update Custom Metadata For Products : permission to update custom metadata for data products At the bottom of the list, click Save . At the bottom of the Domain policy sidebar, click Save . (Optional) Set preferences ​ Did you know? You can also personalize the details users will see in the sidebar or filters menu when in a persona. This is great to limit information overload, by showing only what is relevant to a given set of users. To set preferences for the persona: Change to the Preferences tab of the persona. From the left menu, configure the following: To set the default landing page for the persona: Click Navigation to view landing page preferences. For Set default landing page , click the dropdown and then click Products so that your users land on the data products page when they log into Atlan. To limit the out-of-the-box tabs that should be visible to the persona: Click Asset sidebar to view asset sidebar preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To limit the asset filters that should be visible to the persona: Click Asset filters to view asset filter preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To limit the custom metadata that should be visible to the persona: Click Custom metadata to view custom metadata preferences. Click the checkbox to the left of each custom metadata structure to include it (on) or exclude it (off) for the persona. To personalize your data products, click Products and then click the Add cover image button to set a cover image for data products within your selected domain. Tags: atlan documentation Previous Create data products Next Monitor data domains Create a persona Add users and groups (Optional) Add rich documentation Add a domain policy (Optional) Set preferences"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-a-resource",
    "content": "Use data Discovery Asset Management Add a resource Add a resource Need to redirect users to important information that's outside Atlan? You can add links to internal or external URLs wit hin an asset profile. These can help your team better understand the contextual information for your asset. To add resources to your assets, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on an asset to open the asset profile. In the navigation bar to the right of the asset profile, click Resources . Click +Add Resource and paste your URL. For Title , type a title for your resource. Your resource is now ready to use! 🎉 Once you've added a resource, click +Add in the Resource menu to add more resources for your asset. You can also edit or delete them to curate your list of resources. Did you know? You can also add any web page as a resource to your assets using Atlan's Chrome extension . Tags: atlan documentation Previous Add certificates Next Configure language settings"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/create-data-domains",
    "content": "Configure Atlan Data Products Get Started Create data domains On this page Create data domains Who can do this? Before you can create a data domain, you will need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You will need to be a domain owner or domain admin in Atlan or have access through domain policies to create and manage data domains. If you do not wish to enable the products module but still want to create domains in Atlan, refer to How to manage domains instead. Data domains provide a logical way of grouping data products within a specific domain or business entity   -  for example, functions like finance and sales, business units for different products and services, regions of operation, and more. Most importantly, data domains help in fostering collaboration and promote shared ownership and domain-level governance in your organization. Data domains in Atlan take true meaning from: Key stakeholders of a specific domain and their roles Data products within that specific domain Create a data domain ​ To create a data domain: From the left menu of any screen in Atlan, click Products . On the Products page, click Get started . For Overview , enter the basic details for your domain: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. (Optional) For Theme , choose from the available color options to add a theme to your domain. For Name , enter a meaningful name for your data domain   -  for example, Customer Service . The character limit for a domain name is 80 characters. (Optional) Click the domain icon to change the icon for your domain. (Optional) For Description , enter a description for your domain. For Owners , assign additional users or groups as domain owner. In the top right of the screen, click the Create button to complete setup. Congrats on creating your data domain in Atlan! 🎉 (Optional) Create a data subdomain ​ danger You will first need to create a data domain before you can add a data subdomain to it. Data subdomains help you logically segment your data domains according to business needs. To create a data subdomain: From the left menu of any screen in Atlan, click Products . To select a domain or subdomain, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the View all button to view more domains. In the upper right of your data domain page, click the + Add button and then click New sub-domain to add a data subdomain. For Overview , enter the basic details for your subdomain: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. (Optional) For Theme , choose from the available color options to add a theme to your subdomain. For Name , enter a meaningful name for your subdomain   -  for example, Social Media . The character limit for a subdomain name is 80 characters. (Optional) Click the domain icon to change the icon for your subdomain. (Optional) For Description , enter a description for your subdomain. For Owners , assign additional users or groups as subdomain owner. In the top right of the screen, click the Create button to complete setup. Congrats on creating your data subdomain in Atlan! 🎉 Your data producers can now add data products to your data domain. Update a data domain ​ The domain profile includes essential details about the data domain. You can also curate what your domain users will be able to view. To update a data domain: From the left menu of any screen in Atlan, click Products . To select a domain or subdomain, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the View all button to view more domains. On your data domain page, the Overview tab displays important details about the domain. (Optional) From the top right, click the + Add button and then: Click New sub-domain to add data subdomains. Click New product to add data products. danger Even as a domain owner or admin, you will need to have create, update, and delete permissions through domain policies for a specific domain or subdomain to create and manage data products. Under Summary , view a total count of data products in your domain and domain description: (Optional) Click + Add stakeholder to add stakeholders . (Optional) Click the Description field to update the description. (Optional) For Owners , click the pencil icon to add or remove owners . (Optional) If custom metadata properties are available, you can add custom metadata to your domain. (Optional) Click + Add resource to add a resource to your domain. Under Readme , click + Add to add a README to your data domain or use Atlan AI for documentation . From the top right of the data domain profile: Click the user avatars to view a list of recently visited users, total views on your domain, total number of unique visitors, and total views by user. Use the days filter to filter domain views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your domain and bookmark it for easy access. Click the Slack or Teams icon to post on a Slack or Microsoft Teams channel. Click the 3-dot icon to add an announcement or a resource to your domain. Switch to the Products tab to view data products within your domain. Switch to the Statistics tab to monitor domain usage . Switch to the Lineage tab to view business lineage for your domain . Move a subdomain or product ​ You can move subdomains and products within and across domains to better organize your business entity. Move data products to a different subdomain or domain, or create subdomains within the same domain or across your domains in Atlan. You will need the following permissions: Moving a subdomain or product from one domain to another   - Update Domains permission on both the source and target domains. Moving a subdomain or product within the same domain   - Update Domains permission on the domain you want to reorganize. To move an existing subdomain or product: From the left menu of any screen in Atlan, click Products . In the left menu of the Products page, you can either: Drag and drop a subdomain or product into the relevant domain within the same or a different parent domain. In the popup, click Move to confirm the changes. To the right of the subdomain or product name, click the three dots icon and then click Move to . In the Move to dialog, select a relevant parent domain within the same or a different domain and then click Move to confirm the changes. Convert a subdomain to a domain ​ You can convert subdomains into parent domains. For example, as your organization grows, some small teams may evolve into major departments. In that case, you may want your subdomains in Atlan to better reflect your organizational architecture and convert them to domains. You will need the Update Domains permission on the subdomain and parent domain of the subdomain you want to convert. To convert a subdomain into a domain: From the left menu of any screen in Atlan, click Products . In the left menu of the Products page, to the right of the subdomain you want to convert, click the three dots icon and then click Convert to domain . In the Convert to domain dialog, click Convert to domain to confirm your changes. Archive a data domain ​ You can archive your data domains and subdomains when they are no longer in use. Note that Atlan does not allow archiving a domain that contains any active subdomains or products. Ensure that your domain content is inactive before you proceed. To archive a data domain: From the left menu of any screen in Atlan, click Products . To select a domain or subdomain to archive, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the View all button to view more domains. From the top right of the domain or subdomain profile, click the 3-dot icon and then click Archive . From the Archive domain? dialog, click Archive to archive your data domain. Did you know? To programmatically create, update, and manage data domains using API, refer to our developer documentation . Tags: atlan documentation Previous Data Products Next Add stakeholders Create a data domain (Optional) Create a data subdomain Update a data domain Move a subdomain or product Convert a subdomain to a domain Archive a data domain"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/data-products/how-tos/create-data-products",
    "content": "Configure Atlan Data Products Product Management Create data products On this page Create data products ➕ Available via the Data Marketplace package Who can do this? Before you can create a data product, you need your Atlan admin to enable the products module in your Atlan workspace . Once enabled, first review the order of operations . You need to have create, update, and delete permissions through domain policies for the specific domain or subdomain to create and manage data products. Data products help your data consumers easily discover and work with data assets. As you get started, here are some questions to consider: What use case(s) are you trying to solve as an organization? How to define a common vocabulary and approach for creating data products and ensuring interoperability across domains? How to design data products to power discovery and drive usage among data consumers? Data products in Atlan can be highly adaptable to the needs of your organization. Create a data product ​ You can either create a data product from the products module or lineage graph. To create a data product, complete these steps. From the products module: ​ From the left menu of any screen in Atlan, click Products . To select a domain or subdomain, you can either: From the navigation menu on the Products homepage, use the search bar or select the relevant domain or subdomain. From the top right of Discover domains , select your data domain of interest. If you cannot find your data domain, click the See all button to view more domains. In the upper right of your data domain or subdomain page, click the + Add button, and then from the dropdown, click New product to add a new data product. From the lineage graph: ​ From the left menu of any screen in Atlan, click Assets . (Optional) In the Filters menu on the left, expand the Properties menu and then click Has lineage to filter for assets with data lineage. Select an asset, and from the top right of the asset card, click the View lineage icon to open the lineage graph. On the lineage graph, select an asset to create a data product. Atlan will only include assets that are visible on the lineage graph. To include more assets: (Optional) Hover over the + button to the right of any asset and then click the Expand all button to include assets further upstream or downstream horizontally in the data product. (Optional) Click Show all to include assets further upstream or downstream assets vertically in the data product. From the top right of the lineage graph, click the box with plus icon for data products to create a data product from the lineage graph. In the New product via lineage form, configure the following: For Add assets , any assets that are visible on the lineage graph will be automatically included in the data product. Note that process , child, and partial assets are currently not supported for data product creation from the lineage graph. You can either keep all the asset selections or deselect any assets. (Optional) The asset you had selected on the lineage graph will be automatically set as an output port. You can keep that selection, click Output port to remove the current selection, or click Mark as output port on any other assets to set additional output ports. Click Continue to proceed. Provide details ​ To provide details: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. For Name , enter a meaningful name for your data product   -  for example, Social Media Marketing . The character limit for a product name is 80 characters. For Domain , select a data domain from the dropdown   -  for example, Customer Service . (Optional) For Description , enter a description for your domain. (Optional) For Criticality , select a level of business criticality from the dropdown   -  choose from High , Medium , and Low . (Optional) For Sensitivity , assign a data sensitivity level from the dropdown   -  choose from Public , Internal , and Confidential . (Optional) For Owners , assign additional users or groups as data product owner. (Optional) For Visibility , select who can access and monitor the data product throughout its entire lifecycle: Private to members of this domain -  only members of a specific domain can access the data product. Private to selected members -  only members of a specific domain and other selected users or groups can access the data product. Public -  everyone in the organization can access the data product. If creating a data product from the products module, in the top right of the screen, click the Continue button. If creating a data product from the lineage graph, at the bottom of the form, click the Continue button and skip to the Review the data product section. Add assets ​ To select assets to include in the data product, you can select via the asset browser or using filters. Add via browser ​ Click the checkbox to select individual assets to include in your data product. (Optional) Use the search bar to search for assets by the technical name of the asset. (Optional) Filter assets by specific asset types. Click the 3-dot icon to view more asset type filters. (Optional) Click the Show: all dropdown and change to Selected assets to only view your asset selection. (Optional) Click the All filters dropdown, and then from the All filters pane: Click Hierarchy to filter assets by connection, database, and schema. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported tags . Click Terms to filter assets by linked terms . Click Properties to filter assets by common asset properties . In the top right of the screen, click the Continue button. Add via rules ​ Under Select assets , click Add via filters to add assets to your data product using asset filters. To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. For Attributes , select a relevant option: Click Connection and then select an existing connection. (Optional) To further refine your asset selection: Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported tags . Click Glossary, terms, & categories to filter by a specific glossary or category to bulk update all the nested terms or by multiple glossaries and categories. Click Linked terms to filter assets by linked terms . Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. For Operator , select Is one of for values to include or Is not for values to exclude. Depending on the selected attribute(s), you can also choose from additional operators : Select Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, in the Filters card, click View assets for a preview. In the top right of the screen, click the Continue button. (Optional) Select output ports ​ Did you know? Output ports determine the relationships between your data products. These relationships are visually represented as business lineage . To select output ports: From the list of assets, select output port(s) to allow your data consumers to consume the data product across domains. These assets will serve as the consumption layer for your data product. For Input ports , Atlan displays a total count of input ports for your data product. These assets are designated as output ports in other data products, and serve as input ports for your data product. Click View assets to view all input port assets. In the top right of the screen, click the Continue button. Review the data product ​ Once you have reviewed your data product, you can either: Click Save as draft to save your changes in a draft version and publish when ready. Only you and any other users you add as owners to the product will be able to search for, view, and edit your draft products, depending on their permissions . Click Create and publish to publish it immediately. Congrats on creating a data product in Atlan! 🎉 You can also use governance workflows to govern the creation and change in status of data products. Update a data product ​ Did you know? You can move your data products within and across subdomains or domains to reorganize them as needed. Once you have created a data product, you will also need to monitor and manage it during its entire lifecycle. This helps ensure that the data product stays fresh, up-to-date, and trustworthy. The data product profile in Atlan allows you to curate how your users can use the data product. To update a data product: From the left menu of any screen in Atlan, you can either: Click Products . To select a data product, you can: From the navigation menu on the Products homepage, use the search bar or expand the relevant domain or subdomain. In the Data products section, select a trending or recently viewed data product. The list of Trending products is sorted by the total count of views on each product, with the most viewed product listed at the top. From the top right of any screen in Atlan, click the star icon . From the Starred assets popup, select a starred data product. From the left navigation menu or Products homepage, click My drafts to continue working on your draft products. Products in draft mode are only visible to product owners until these are published. If your Atlan admin has enabled the Show products in asset discovery toggle , click Assets to search for data products from asset discovery . On your data domain page, next to the Overview tab, click the Products tab and select your data product of interest. (Optional) From the top right of the product profile, click the Published dropdown to update the status of your data product: Draft -  data product is only visible to product owners. Published -  data product is published for users to consume. Sunset -  data product is planned for retirement. Archived -  data product is archived and no longer visible to users. To restore an archived data product, click the Restore product button and then click Restore . Atlan will restore the archived data product and it will reappear in product and asset discovery, domain profile, and product lineage. Under Summary , view details about the data domain your data product belongs to, including criticality, sensitivity, and freshness. (Optional) Click the pencil icon to update Criticality to signify business impact: High -  high business impact _Medium   - _ moderate business impact Low -  internal or a non-business impact (Optional) Click the pencil icon to update Sensitivity to assign a data classification: Public -  may be freely accessible Internal -  may only be distributed within the organization Confidential -  may only be limited to a specific domain or team within the organization (Optional) Click Add resource to add a resource to your asset. Under Product Score , view a scorecard for your data product, calculated based on metadata completeness. Under At A Glance , view a total count of linked assets and output ports. Under Readme , click + Add to add a README to your data product or use Atlan AI for documentation . Under Details , you can view and update metadata for your data product   -  including visibility, terms , owners , tags , certificates , and custom metadata . You can also set up playbooks to update product metadata in bulk. Switch to the Assets tab to view linked assets. (Optional) Click the Edit button to add or remove assets from your data product. (Optional) Select an asset to view its asset profile in a sidebar. (Optional) Filter assets by asset types   -  for example, use the Table filter to view table assets only. (Optional) Click Disable as output port to remove an output port or click Set as output port to set an asset as an output port. Switch to the Lineage tab to view business lineage for your data product. Hover over any data product to view the metadata popover for more context. Click the view output ports menu to view output ports for your data product. In the upper right of the lineage graph, click the downward arrow to download an image of the product lineage graph. Switch to the Activity tab to view the activity log for your data product. View details about changes made to the data product and filter for specific types of metadata changes . View top and recent users for your data product. View a list of data producers for your data product. Switch to the Contracts tab to view any linked contracts for the output ports in your data product. From the top right of the data product profile: Click the user avatars to view a list of recently visited users, total views on your asset, total number of unique visitors, and total views by user. Use the days filter to filter asset views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your data product and bookmark it for easy access. Click the Slack or Teams icon to post on a Slack or Microsoft Teams channel. Click the 3-dot icon to add an announcement or a resource to your data product. If you have enriched your draft products with terms or tags , your draft products will be visible to other users as linked assets when viewed from the term or tag profile, respectively. However, only a product owner with the requisite permissions to update the product can make any changes to the draft product. Did you know? To programmatically create, update, and manage data products using API, refer to our developer documentation . Tags: lineage data-lineage impact-analysis Previous Add stakeholders Next Create domain policies Create a data product Update a data product"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-custom-metadata",
    "content": "Use data Discovery Configuration Add custom metadata Add custom metadata Who can do this? Any user with permission to edit custom metadata can add or change custom metadata values on assets. (An admin user must first create the custom metadata structures for those values, though.) To add custom metadata to an asset: Navigate to the asset. You could do this by searching, through discovery, or even through our Chrome extension . In the right sidebar, click the icon for the metadata structure you want to enrich. If no values yet exist, click the Start editing button. Otherwise, in the upper-right of the custom metadata panel in the sidebar, click the Edit button. Enter values for any properties you want to enrich. (You do not need to fill in every property.) In the upper-right of the custom metadata panel in the sidebar, click the Update button. That's it, the asset is now enriched with your organization's own custom metadata! 🎉 Tags: data integration asset-profile Previous How to use the filters menu Next How do I use the filters menu?"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-certificates",
    "content": "Use data Discovery Asset Management Add certificates On this page Add certificates How many times has someone complained to you that the data is incomplete or has issues? And how many times have you responded that it's still a work in progress or they're using the wrong data! Wouldn't it be really convenient if the data could answer these questions? Certificates in Atlan can help! The certification tags help users quickly identify whether a data asset is ready to use, a work in progress, or has some issues. You can add the following four certification tags to any data asset: Verified for ready to be used. Draft for work in progress. Deprecated if the asset no longer exists. No certificate if not required or needs documentation down the line. Did you know? Certificates can be used to quickly filter data assets on the Assets page. This helps build trust in your data assets among users. Add certificates to your assets ​ To add or update the certificate for your data assets, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on the asset to open its asset profile. In the right menu, click + under Certificate and choose the relevant certification option. Write a message to add more context. Now your data can proudly display its status for all to see! 🎉 Once you have selected a certification tag for your data asset, you will get a popup that your certificate has been saved. Tags: atlan documentation Previous Add descriptions Next Add a resource Add certificates to your assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-an-alias",
    "content": "Use data Discovery Asset Management Add an alias On this page Add an alias Who can do this? Any non-guest user with edit access to an asset's metadata can add an alias. This only includes admin and member users. An alias is a business-oriented, alternate name that you can specify for your assets in Atlan. You can either manually add a more descriptive and user-friendly alias or use Atlan AI to do the same, if Atlan AI is enabled in your Atlan workspace . This can help you improve the readability of your asset names while providing useful context to your users. Atlan recommends adding an alias that's unique to each asset, in a one-to-one relationship. To relate your assets, you can instead attach tags to group them by use case or link terms to group them conceptually. Atlan currently supports adding an alias to everything EXCEPT the following asset types: Database Schema Connection Process, including ConnectionProcess BIProcess ColumnProcess Query Example ​ For an asset with a technical name like FCT_SUPPLIER_TRANSACTIONS , you can add Supplier Transaction Records as an alias. Once you have added an alias, you can: Search for assets either with their technical names or aliases, Atlan will match on the most relevant title. Use the Title property filter to filter for assets either by their technical names or aliases. Set asset name display preferences for personas , choosing whether the technical name or alias should be displayed prominently in search and discovery. View aliases in search results , asset preview, asset profile , asset sidebar , and lineage graph . Add an alias ​ To add an alias to your asset: From the left menu of any screen in Atlan, click Assets . On the Assets page, click any asset with a technical name. To add an alias, you can either: Open the asset profile, and to the right of the asset name, click the pencil icon. Navigate to the Overview sidebar, and to the right of the asset name, click the pencil icon. In the Add an alias dialog, you can either: In the Type a business-friendly name field, enter an alias for your asset. If Atlan AI is enabled , under Atlan AI Suggested , click an Atlan AI-suggested alias for your asset. (Optional) Click the refresh icon to regenerate Atlan AI suggestions and compare different sets of suggestions. Click Add to save your preferred alias for the asset. (Optional) Once you have added an alias, you can: Hover over the asset name to view both the technical name and alias in a popup. From the asset sidebar tabs on the right, click the Activity tab to view alias activity by user and timestamp of update. To edit your alias, click the pencil icon to make any changes. From the filters menu on the left, click Properties and then click Title to filter assets by the technical name or alias. Your asset will now display an alias! 🎉 You can also set asset name display preferences to technical name or alias for your personas. If no preference has been specified and an alias is available, then Atlan will display the alias over the technical name on an asset by default. Tags: atlan documentation Previous Access archived assets Next Star assets Example Add an alias"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-descriptions",
    "content": "Use data Discovery Asset Management Add descriptions On this page Add descriptions You can add descriptions to your assets in Atlan, including tables, views, and individual columns. You can even add a description in the form of a README . Doing so will enrich your data asset with the relevant contextual information. Did you know? The description editor in Atlan also supports Markdown syntax. You can crawl descriptions in Markdown at source, view them in the asset sidebar and profile, and make edits in Markdown directly in Atlan. Add descriptions to your assets ​ To add or update a description for your data asset, follow these steps: From the left menu on any screen, click Assets . On the Assets page, click on an asset to view its Overview in the right menu. Under Description , click on the text box to add a description. Once you've added new text or updated the existing one, click anywhere to automatically save your changes. Your asset description is now live! 🎉 You can also add or edit asset descriptions directly from the asset profile. Did you know? The size limit for description values is 32766 bytes. Depending on the types of characters used, the character limit for descriptions can range from 8191 to 32766 characters. Add column descriptions ​ You can also add a description for a single column rather than an entire data asset. To add column descriptions, follow these steps: On the Assets page, click on an asset to view its asset profile. Under Column Preview in the asset profile, navigate to a column and click +Add under Description . Once you've entered the text, click anywhere to save it. Search using asset descriptions ​ You can also filter your assets by the keywords in your asset descriptions. Here are the steps: In the Filters menu on the Assets page, click Properties . Next, click Description from the dropdown menu. In the Description popup display, enter a keyword in the text box. Click on the downward arrow and then select the preferred matching condition. To add multiple description filters, click + in the Description popup. The Assets page will now display a list of assets filtered by your description text. You can click Clear All in the Filters menu at any time to remove all your filters. danger If you're using integration code or custom packages to update asset descriptions, there may be additional nuances to consider since these can override either (or both) description attributes: description and userDescription . Tags: data integration crawl asset-profile Previous Add owners Next Add certificates Add descriptions to your assets Add column descriptions Search using asset descriptions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/add-owners",
    "content": "Use data Discovery Asset Management Add owners On this page Add owners Atlan allows you to add owners for each data asset. This enriches the asset profile and helps build trust among users. Users can then quickly reach out to the owner of the asset for any questions about the data. The owner is responsible for maintaining the data asset. They are the right person to contact for questions about the data's frequency of update, progress, status, and more. Add owners to your assets ​ To add or update owners for a data asset, follow these steps: On the Atlan homepage, click Assets in the left menu. Click on an asset to view its asset profile. In the Overview menu to the right, click + under Owners to access a list of users. To assign owners, you can either: Click the checkbox next to an individual user's name. Toggle to groups in the top right and select an entire group of users. Click Save . Your asset profile will now display the asset owners! 🎉 Search by asset owners ​ You can also filter your assets by asset owners. Here are the steps: In the Filters menu on the Assets page, click Owners . Click the checkbox next to an owner name to view their owned assets. Did you know? You can select No Owners in the Owners filter to view assets that currently do not have assigned owners and assign accordingly if needed. Tags: atlan documentation Previous Star assets Next Add descriptions Add owners to your assets Search by asset owners"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/configure-language-settings",
    "content": "Use data Discovery Configuration Configure language settings On this page Configure language settings How does Atlan speak to you? Decide for yourself! Atlan enables you to customize language settings from the default English to your preferred language at a workspace level. Atlan admins can change the default language for their Atlan workspace from the admin center. Individual users can also set a personal language preference, which overrides the default workspace settings - English or otherwise. Atlan currently supports the following additional languages: French Japanese Portuguese To enable any of the additional supported languages or request ones not listed in this section in your Atlan workspace, Atlan admins must contact Atlan support . Configure workspace language settings ​ Who can do this? Once Atlan has enabled a preferred language for your organization, you need to be an admin user in Atlan to configure language settings for your Atlan workspace. To configure workspace language settings: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Workspace settings heading of the Labs page, for Default workspace language , click the English language dropdown to set a preferred language. Your users can now use Atlan in their preferred language! 🎉 Configure personal language settings ​ Who can do this? Once Atlan has enabled a preferred language for your organization, anyone with access to Atlan - admin, member, or guest user - can update language settings for their Atlan instance. To set a personal language preference: In the top right corner of your Atlan instance, click your name, and then from the dropdown, click Language . From the Language dropdown, select a preferred language for your Atlan instance. You can now use Atlan in your preferred language! 🎉 Troubleshooting ​ If you wish to improve translations, such as for some specific technical term that's not translating correctly, you can directly submit that through Atlan's open source language support . Note that you need to have a Github account to be able to do so. Enter src/locales/default and choose the language you wish to update translations for. Submit a pull request to edit any existing translations or add new terms. Alternatively you can submit a request to Atlan support . Tags: atlan documentation Previous Add a resource Next How to use the filters menu Configure workspace language settings Configure personal language settings Troubleshooting"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/how-do-i-use-the-filters-menu",
    "content": "Use data Discovery Configuration How do I use the filters menu? How do I use the filters menu? To learn how to use the filters menu in Atlan, follow the instructions in this guide . Tags: atlan documentation Previous Add custom metadata Next How to interpret timestamps"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/star-assets",
    "content": "Use data Discovery Asset Management Star assets On this page Star assets Who can do this? Anyone with access to Atlan   -  admin, member, or guest user   -  can star assets. Atlan allows you to star your most used assets and bookmark them for quick and easy access. Once you have starred your assets, you can: Access your starred assets from anywhere in Atlan Get a personalized view of your homepage with starred assets Filter by your starred assets Sort your assets by star count View starred activity in the activity log Set up Slack or Microsoft Teams notifications for metadata updates Star an asset ​ Navigate to the left menu of any screen in Atlan and click Assets to begin: From asset sidebar ​ To star an asset from the asset sidebar: From the Assets page, click an asset you want to star. Navigate to the the upper right of the Overview sidebar and click the star button to star the asset and add it to your list of starred assets. From asset profile ​ To star an asset from the asset profile: From the Assets page, right-click an asset you want to star and select Open profile . Navigate to the top right of the asset profile and click the star button to star the asset and add it to your list of starred assets. Click the star button again to remove the asset from your list of starred assets. Did you know? Only you have the power to edit stars from your starred assets. However, for assets you've starred, other users can view your username while hovering over the star button and from the activity log. View starred assets ​ Once you have starred your assets, you can use the Starred assets widget to view them from anywhere in Atlan. The starred assets widget will show you a total count and complete list of your starred assets. To view starred assets: From the top right of any screen in Atlan, click the star icon. This will also show you a total count of your starred assets. From the Starred assets popup, you can view starred assets sorted by most recently starred or use the search bar to search for specific starred assets. Click Open in Assets to view all your starred assets. The corresponding Assets page will only show all your starred assets with the Starred assets filter applied. To further refine your search, add more filters . (Optional) Next to the search bar on the Assets page, click the sort button. From the Star count sorting menu, click Most starred to view most starred assets or Fewest starred to view least starred assets. Click any starred asset to view more details: In the top right of the Overview sidebar, hover over the star button to view total star count and users who have starred it in a popover. From the sidebar tabs on the right, click the Activity tab to view starred activity by user and timestamp. Enable metadata update alerts ​ You can set up Slack or Microsoft Teams notifications for metadata updates on all your starred assets in Atlan. You can also customize the type of change alerts you want to receive. With real-time alerts directly delivered to your Slack or Microsoft Teams account, you can stay informed about the latest changes to your starred assets. Before you can set up notifications for starred assets, you will need to: Slack: Integrate Slack and Atlan Link your Slack account to Atlan Microsoft Teams: Integrate Microsoft Teams and Atlan : For any existing Microsoft Teams integrations prior to February 22, 2024 only, you will need to update the Atlan app in your Microsoft Teams workspace to use this feature. For all new Microsoft Teams integrations from February 22, 2024 onward, no additional configuration required. Link your Microsoft Teams account to Atlan To enable notifications for starred assets: From the top right of any screen in Atlan, click the star icon. From the bottom left of the Starred assets popup, click Enable notifications . To link your Slack or Microsoft Teams account: If you have already linked your Slack account , skip this step. To link your Slack account to Atlan , in the Receive notifications for starred assets dialog, click Slack , and from the corresponding screen, click Allow to continue. If you have already linked your Microsoft Teams account , skip this step. To link your Microsoft Teams account to Atlan , in the Receive notifications for starred assets dialog, click Teams , and from the corresponding screen, click Allow to continue. In the notifications setup dialog, for Notify me about , you can either: Click All updates to receive notifications for all the changes listed in Custom updates made to your starred assets. Click Custom updates to limit notifications to specific types of metadata updates: Click Name to receive an alert when the name of an asset is updated. Click Description to receive an alert when a description is added to, updated, or removed from an asset. Click Announcement to receive an alert when an announcement is added to, updated, or removed from an asset. Click Certificate to receive an alert when a certificate is added to, updated, or removed from an asset. Click Owners to receive an alert when an owner is assigned to or removed from an asset. Click Readme to receive an alert when a README is added to, updated, or removed from an asset. Click Terms to receive an alert when a term is linked to or unlinked from an asset . Click Tags to receive an alert when a tag is attached to or removed from an asset. Click Propagated Tags to receive an alert when a tag is propagated to or removed from an asset. Click Save to save your notification preferences. (Optional) To edit notification settings, from the bottom left of the Starred assets popup, click Enabled . In the notifications setup dialog, you can further customize your notifications. (Optional) To remove notifications, from the bottom left of the Starred assets popup, click Enabled . In the notifications setup dialog, click Disable notifications to reset your notification settings. You will now receive Slack or Microsoft Teams notifications for changes made to all your starred assets in Atlan! 🎉 Tags: data asset-profile Previous Add an alias Next Add owners Star an asset View starred assets Enable metadata update alerts"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/search-and-discover-assets",
    "content": "Use data Discovery Get Started Search and discover assets On this page Search and discover assets Atlan is a living catalog of all your data assets and knowledge. It lets you quickly discover and access your data, along with the tribal knowledge and business context. Its Amazon-like search and filtering experience isn't just for data tables. It also extends to a variety of data assets, like columns, databases, SQL queries, BI dashboards, and much more. To ensure a high-quality search experience, Atlan recommends the following: Certify your assets Link terms to your assets to add business context Enrich your assets with descriptions Star your assets for easy access Did you know? You can bookmark your search results with applied filters or share them with other Atlan users in your organization for quick and easy access. Search superpowers ​ Let's find out what makes Atlan's search intuitive and super quick. Intelligent keyword recognition Atlan supports powerful, intelligent search. When you search using keywords, the keywords in the matching search results will be highlighted for easy recognition. Even if your keyword contains an underscore _ or a period . -  for example, instacart_order -  both keywords will be highlighted across all search results. For keyword-based search: If the keywords you're searching by is present in the asset name, description , or linked term , only then will the asset appear in your search results. Atlan displays search results based on asset names   -  technical name and alias -  that match your keyword(s). If the keyword is a glossary term linked to assets or present in asset descriptions , such assets will be boosted in search results. Whether your search query is incomplete ( insta ) or misspelled ( instacrt ordr ), Atlan's powerful search can still help you discover exactly what you need. Search from anywhere ​ There are multiple ways to start your search: Click the Search assets across Atlan bar on the homepage. Click Assets in the left-side panel. Use Cmd/Ctrl + K to open the search page from anywhere in Atlan. Search using context ​ The Assets section offers a variety of filters to narrow down your search. Here are the different types of filters that you can use: Source : Search by connectors , chosen from a list of connections within Atlan. Domains : Filter assets by domains , such as a single domain, multiple domains, or no domain. Certificate : Search based on the certificate attached to data assets, such as Verified , Draft , Deprecated , and No certificate . Owners : Filter by selecting one or more users. You can also toggle between users and groups to filter based on a group of users. Tags : Filter by user-generated tags, such as public , PII , and more. Terms : Filter by terms from your glossaries, such as cost , revenue , or P&L . Properties : Filter assets by other properties, like technical name or alias , description , last updated, and so on. Atlan's search results include a quick count of all the resulting data assets grouped by type. As you apply the filters, you'll see these counts change in real time. You can also enter a keyword in the search bar and filter your results by a specific type of data asset. For instance, enter the keyword order in the search bar and then click the Column checkbox to view column results for your searched keyword. Sort search results ​ Atlan allows you to sort your search results in different ways. This helps you quickly find the assets you're interested in. Sorting options include: Relevance : Sort by how closely the search results match your searched keywords. Name : Sort by the asset name in an alphabetical or a reverse alphabetical order. Updated on Atlan : Sort by the newest or oldest updated assets. Star count : Sort assets by most or fewest stars . Order : Sort the search results in an ascending or descending order. Popularity : Sort Snowflake and Google BigQuery assets by the most or least popular assets . Did you know? The sorting options may vary depending on the asset type selected. For example, if you are viewing the results while filtering by the Table tab, you'll also have the option of sorting by the most or fewest number of rows and columns. Search with patterns ​ You can refine your search in Atlan with the following patterns: Exact match search : Wrap the keywords within single '' or double \"\" quotation marks when typing them in the search bar   -  for example, \"instacart_total_users\" . Only the asset names with case-insensitive exact match and following the order of the keywords will be boosted in the search results   -  for example, instacart_total_users or Instacart_Total_Users . If the keywords are contained in the asset description or linked terms , such assets will show up next. Additionally, you can use exact match to search by the qualifiedName or globally unique identifier (GUID) of an asset. Combined string of database, schema, and table : For a more data-friendly search experience, copy the combined string of database.schema.table (or schema.table ) from your SQL editor and paste it in the search bar   -  for example, atlan_db.public.instacart_total_orders . Multiple phrase match : When you enter two or more keywords, Atlan will find assets with asset names that partially match the keywords or a combination of them to narrow down the search results. See only what you want to see ​ Atlan gives you the option to customize your search. Want to show or hide certain fields in your search results? Click the 3-dot icon next to the search bar to set display preferences for each field: Description Terms Tags Connection Tags: data asset-profile Previous Discovery Next Access archived assets Search superpowers"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/discovery/how-tos/use-the-filters-menu",
    "content": "Use data Discovery Configuration How to use the filters menu On this page Use the filters menu You can refine the search for your assets in Atlan using the filters menu. Add filters to your asset search to find assets that are more relevant to you. Once you have added filters, you can also: Bookmark your search results with applied filters for quick access. Copy the browser URL with applied filters and share it with other users in your organization   -  they may need to log into Atlan first to view the search results. Export filtered assets to spreadsheets and enrich asset metadata in bulk. Did you know? You can resize the filters panel. Hover over the edge of the panel and then click and drag the slider arrow to resize it. Atlan will remember your preferred size for the duration of your session. The panel will be returned to the default size when you log in again. Use the following filters to help with asset disco very. Start by either clicking Assets in the left panel or the search bar from any screen in Atlan: Source ​ You can use source-specific filters to curate a list of relevant assets to search from. To filter by a specific source: In the Filters menu on the left, click Source . Click Choose connection to filter by a supported connector . (Optional) Select an existing connection for a selected connector: Click All Databases to filter by databases for a selected connection. Click All Schemas to filter by schemas for a selected connection. Asset type ​ You can filter your search results by specific types of assets. The asset type filter also includes a quick count of all the resulting assets grouped by type. You can select multiple asset types to group search results by asset types. For example, if you would only like to view column assets: Under the search bar on the Assets page, click the Asset type dropdown. From the Asset type dropdown, click the Column tab to only view column assets. Only column assets will be displayed in the results, with the tab showing a total count of the column assets. (Optional) In the Filters menu on the left, under Asset Type Filters , click Column to add a type-specific property filter to further refine your search: Click Parent asset type to filter columns by a parent asset type   -  tables and views. Click Parent asset name to filter columns by the name of a table or view, or set a matching condition such as pattern match. Click Data type to filter columns by data types. Click any of the column keys to filter by column keys. (Optional) If an asset type you're filtering for does not match the search keywords but there are other asset types that match, click Check other matches to view those assets or click Clear search to clear your search and start over. Domains ​ Domains provide a logical way of mapping and organizing assets within a specific domain or business entity. You can filter assets by a single domain, multiple domains, or no domains. To filter assets by domains: In the Filters menu on the left, click Domains to expand the menu. Under Domains , to filter assets by domains: Check the boxes to select one or more domains or subdomains to filter your assets. Click No domains to filter assets not mapped to any domain. Metadata filters ​ Certificate ​ You can filter your asset search based on the certificate attached to the data assets   - Verified , Draft , Deprecated , and No certificate . For example, if you would only like to view verified assets: In the Filters menu on the left, click Certificate to expand the menu. Under Certificate , click Verified to only view verified assets in your search results. Owners ​ You can filter your asset search by selecting one or more owners . You can also toggle between individual users and groups to filter results based on a group of users. Or, select No Owners in the owners filter to view assets that currently do not have any owners and assign them if needed. For example, if you would like to filter assets by a group of owners: In the Filters menu on the left, click Owners to expand the menu. Under Owners , click the group icon . Click the group name by which you want to filter your assets. Tags ​ You can filter your assets by user-generated tags , such as public , PII , and more. You can also select No Tag in the tags filter to view assets that currently do not have any tags and add them if needed. For example, if you would like to filter assets for a data compliance check: In the Filters menu on the left, click Tags to expand the menu. Under Tags , click the relevant option   -  for example, PII . (Optional) Filter for assets by tags imported from supported sources: Select a synced Snowflake tag to view tagged Snowflake assets only. To filter by Snowflake tag values, next to the tag name, click the rightward arrow to open the tag value menu. In the Filter by tag value dialog, click the Select tag value dropdown and then select a tag value to filter assets. You can either search by predefined allowed values or tag values. Select a synced dbt tag to view tagged dbt Cloud or dbt Core assets only. Select a synced Databricks tag to view tagged Databricks assets only. You can also filter by Databricks tag values. Terms ​ You can filter your asset search by terms from your glossaries . You can also select No Terms in the terms filter to view assets that currently do not have any linked terms and add them if needed. For example, if you would like find assets linked to a specific term: In the Filters menu on the left, click Terms to expand the menu. Under Terms , click the relevant term   -  for example, Marketing Analysis -  to discover assets linked to that term. (Optional) Next to the search bar in the Terms filter, click the Advanced options icon to set a matching condition. Click the operators dropdown, and then: Click Or to filter assets that match any selected term(s). Click And to filter assets that match all selected terms. Click None of to filter assets that do not match any of the selected term(s). Click Not empty to filter assets that have one or more linked terms . Click Empty to filter assets without any linked terms . Properties ​ Properties offer a variety of filters to narrow down your asset search. You can filter your assets by common asset properties, such as name, description, last updated, and more. To search by common asset properties: In the Filters menu on the left, click Properties to expand the menu. From the Properties menu, you can: Click Title to search by the technical name or alias of an asset. From the dialog, set your preferred matching condition (see Description filter). Click Description to search by the description of an asset in Atlan or from the source. To set a matching condition, from the Description dialog, click the operators dropdown to: Click Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Click Starts With or Ends With to filter assets using the starting or ending sequence of values. Click Contains to find assets with specified values contained within the property. Click Pattern to filter assets using supported Elastic DSL regular expressions . Click Is Empty or Is Not Empty to filter assets with or without null values. Click Starred assets to filter for all your starred assets . Click Has lineage to filter for assets with or without data lineage . Click Has readme to filter for assets with or without README files . Click Has resources to filter for assets with or without resources . Click Announcement to find assets with a specific announcement type. From the Announcement dialog, click the dropdown menu to select Information , Issue , Warning , or No announcement . Click Unique identifier to find assets with a unique ID. From the dialog, select your preferred matching condition and type the relevant information. Click Qualified name to filter by a unique name for the asset. From the dialog, select your preferred matching condition and type the relevant information (see Description filter). Click Last updated (in source) or Last updated (in Atlan) to find assets by when they were last updated and where. From the dialog, set the condition to Before or After and then select a date. Click Created (in source) or Created (in Atlan) to find assets by when they were created and where. From the dialog, set the condition to Before or After and then select a date. Click Created by (Atlan) or Last updated by (Atlan) to filter assets by a user who created or last updated the asset in Atlan. From the dialog, from the Select user dropdown, select the user name. Click Is archived to view archived assets in the search results. (Optional) Click the + sign to add more filtering options   -  currently only available with the Title , Description , Unique identifier , Qualified name , Last updated (in source and Atlan), and Created (in source and Atlan) filters. dbt ​ danger The dbt filters will only appear in the filters menu if dbt assets have been crawled . The dbt filters allow you to filter your dbt Cloud and dbt Core assets to find the most relevant results. For example, if you need to find assets from a specific project in dbt: In the Filters menu on the left, click dbt to expand the menu. Under dbt , click Project name to filter by a specific project. In the Project name dialog: Select the relevant matching condition. For Type , type the project name to filter your assets   -  for example, food-beverage . (Optional) Click the + sign to add more filtering options. (Note: This may not be available for all the dbt filters). Usage ​ The usage filters allow you to filter your assets by usage metadata. For example, you can: Filter assets with zero queries and archive them. Find costly assets to better optimize your operations. Discover recently updated assets and follow up on the updates. Custom metadata filters ​ danger You first need to set up custom metadata properties and toggle on the Show in filter slider during setup. When you add custom metadata in Atlan, you can also choose to set custom metadata properties as filters to help with quicker asset discovery. For example, if you would like to filter your assets by custom user roles metadata: In the Filters menu on the left, click the custom metadata filter   -  for example, Stewards . Under the custom metadata filter, select the custom metadata property   -  such as Data Steward . In the property dialog, select the matching condition and user to filter your assets. (Optional) For custom metadata option properties only: You can either: For custom metadata properties with five or fewer options, click the operators dropdown. For custom metadata properties with more than five options, next to the search bar, click the Advanced options icon and then click the operators dropdown. From the operators dropdown, you can: Click Or to filter assets that match any selected value(s). Click And to filter assets that match all selected values   -  only supported if multiple values are allowed for custom metadata options. Click None of to filter assets that do not match any of the selected value(s)   -  only supported if multiple values are allowed for custom metadata options. Click Not empty to filter assets that have one or more assigned values for the selected property. Click Empty to filter assets without any assigned values for the selected property. Asset type filters ​ You can choose from two different sets of filters   -  type-specific or connector-specific. Type-specific filters ​ If you're filtering by a specific asset type , you can select type-specific property filters to further refine your search. For example, if you're filtering by: Tables or views   -  you can filter these asset types by a specific row or column count. Columns   -  you can filter column assets by parent asset type and name, data type, or column keys . Process   -  you can filter process assets by the SQL query. Query   -  you can filter saved queries by visual queries . Connector-specific filters ​ Connector-specific filters will appear in the filters menu only if there are crawled assets for a supported source and asset-type filters specific to the connector are applied. Connector-specific filters are currently supported for the following sources: Anomalo Apache Kafka , Confluent Kafka , Aiven Kafka , Redpanda Kafka , and Amazon MSK Apache Airflow/OpenLineage , Amazon MWAA , Astronomer , Google Cloud Composer , and Apache Spark Google BigQuery Matillion Microsoft Azure Cosmos DB Microsoft Azure Event Hubs Microsoft Power BI MicroStrategy MongoDB Monte Carlo Redash Salesforce Sisense Snowflake Soda Tableau ThoughtSpot Qlik Sense Cloud and Qlik Sense Enterprise on Windows Google Cloud Storage buckets and objects Microsoft Azure Data Lake Storage accounts, containers, and objects Preset datasets, dashboards, and workspaces API paths Files -  supported file types include DOC, Excel, PPT, CSV, TXT, JSON, XML, and ZIP files To use a connector-specific filter: From the Assets page, click the Asset type filter, and then from the dropdown, select an asset type from a supported connector   -  in this example, we'll select Soda Checks . In the Filters menu on the left, click the Soda filter to expand the menu. From the Soda filter menu, filter your Soda checks by check status, owner, or last scanned at date. (Optional) For properties that allow selecting multiple values, you can set your preferred matching condition. Click the operators dropdown and then: Click Or to filter assets that match any selected value(s). Click And to filter assets that match all selected values. Click None of to filter assets that do not match any selected value(s). Click Not empty to filter assets that have one or more assigned values for the selected property. Click Empty to filter assets without any assigned values for the selected property. Tags: data integration Previous Configure language settings Next Add custom metadata Source Asset type Domains Metadata filters Custom metadata filters Asset type filters"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/how-tos/create-a-purpose",
    "content": "Configure Atlan Access control Get started Create purpose On this page Create purpose Who can do this? You will need to be an admin user to create purposes. danger A purpose acts on at least one tag. This tag must be created before creating the purpose. To create a purpose: From the left menu of any screen, click Governance . Under Access Control , click Purposes . If this is the first purpose, click the Get started button. Otherwise, in the top right, click the New Purpose button. For Purpose name , enter a meaningful name for the purpose. (Optional) Add a description for the purpose. In the lower-left corner of the dialog, click Select tag . Select one or more tags from the list, and then click on the purpose box again to close the list. Click Create to create the purpose. You now have an empty purpose. Did you know? The purpose will not yet control any access. Your users can still use the purpose to quickly browse assets with any of the tags selected, though. (Optional) Add rich documentation ​ To add rich documentation describing the purpose: Under Summary , then Channels , add any Slack channels relevant to the purpose. Under Resources , add links to external resources like PDFs, repositories, Notion, Confluence, Google Drive   -  anything that has a URL. Under Readme , write a richly formatted description of the purpose. Add policies ​ Did you know? For the purpose to control access, you need to define one or more policies. Repeat the following steps for each set of users and permissions you want to control through the purpose. To add policies to the purpose, from within the purpose: Change to the Policies tab. Click the New Policy button and choose the type of policy. Add a metadata policy ​ To grant or restrict permissions to change metadata: Choose Metadata policy . Under Name , briefly describe the policy's intention. (Optional) Under Users and Groups , choose the users to whom to apply the policy. By default, all users will be included. To select others: In the Users and Groups box, click the x . Under Users and Groups , click the Add link. Search for and select the users and groups to control with the policy, and then click anywhere in the Metadata policy sidebar. (Optional) For Configure permissions , choose the permissions the policy will grant . By default, all permissions will be granted. To select others: To the right of Configure permissions , click the Edit link. Select the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one. At the bottom of the list, click Save . (Optional) For Deny selected permissions , choose whether you want to explicitly deny these permissions. danger If enabled, this will override all grants _take_priority) of those permissions from any other policies for the same users. At the bottom of the Metadata policy sidebar, click Save . Add a data policy ​ To grant or restrict permissions to query or preview data: Choose Data policy . Under Name , briefly describe the policy's intention. (Optional) Under Users and Groups , choose the users to whom to apply the policy. By default, all users will be included. To select others: In the Users and Groups box, click the x . Under Users and Groups , click the Add link. Search for and select the users and groups to control with the policy, and then click anywhere in the Data policy sidebar. (Optional) For Querying Permissions , choose whether you want to explicitly deny the ability to query and preview data on these assets. danger If enabled, this will override all grants _take_priority) of those permissions from any other policies for the same users. This will also deny at the table level   -  even if only 1 out of 100 columns in a table has the tag, previewing and querying will be denied for the entire table. (Optional) For Configure permissions , choose the masking policy to apply. By default, no masking will be applied. To apply masking, under Masking (Optional) , select the type of masking to apply. If you are unsure what they do, hover over each one to see a more detailed description and an example. At the bottom of the Data policy sidebar, click Save . (Optional) Set preferences ​ Did you know? You can also personalize the details users will see in the sidebar or filters menu when in a purpose. This is great to limit information overload, by showing only what is relevant to a given set of users. To set preferences for the purpose: Change to the Preferences tab of the purpose. From the left menu, configure the following: To limit the asset types that should be visible to the purpose: Click Asset types to view asset type preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the purpose. To limit the out-of-the-box tabs that should be visible to the purpose: Click Asset sidebar to view asset sidebar preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the purpose. To limit the asset filters that should be visible to the purpose: Click Asset filters to view asset filter preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the purpose. To limit the custom metadata that should be visible to the purpose: Click Custom metadata to view custom metadata preferences. Click the checkbox to the left of each custom metadata structure to include it (on) or exclude it (off) for the purpose. Tags: atlan documentation Previous Create persona Next Manage users (Optional) Add rich documentation Add policies (Optional) Set preferences"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/contracts/how-tos/create-data-contracts",
    "content": "Build governance Contracts Get Started Create data contracts On this page Create data contracts Private Preview A data contract is an agreement between a data producer and consumer that specifies requirements for generating and using high-quality, reliable data. As a powerful tool for data management, data contracts can help you standardize contractual obligations between data producers and consumers, organize your assets with embeddable contract metadata, and enforce them with data quality rules. In Atlan, you can directly add a data contract to supported assets and provide helpful context to your downstream users. For a data contract to help build trust in your assets, it should be: Templatized and easily comprehensible   -  use Atlan's YAML contract template to create standardized contracts and push to Atlan. Version-controlled   -  continuously validate and monitor your data contracts either in runtime or real-time. Embeddable   -  embed the contract as metadata for a supported asset. Enforceable   -  enforce your contracts with data quality rules. Extensible   -  identify new specifications, generate new versions, and then compare and contrast them. Did you know? You can create webhooks for data contracts and receive notifications for when a contract is added or updated to a URL of your choice. Supported asset types ​ You can create data contracts for the following asset types: Tables Views Materialized views Output port assets of data products Supported asset metadata ​ Atlan maps the following asset metadata properties to it contract properties: Metadata property Contract property name dataset typeName type userDescription or description description ownerUsers owner.users ownerGroups owners.groups certificateStatus certification.status certificateStatusMessage certification.message announcementType announcement.type announcementTitle announcement.title announcementMessage announcement.description meaningNames terms classificationDef.displayName tags.name classifications.propagate tags.propagate classifications.restrict_propagation_through_lineage tags.restrict_propagation_through_lineage classifications.restrict_propagation_through_hierarchy tags.restrict_propagation_through_hierarchy column.name columns.name column.userDescription or column.description columns.description column.dataType columns.data_type column.isPrimary columns.primary !column.isNullable columns.required column.precision columns.precision column.numericScale columns.scale tags on column columns.tags column.meaningNames columns.terms custom metadata (CM) custom_metadata.<CM> Add a data contract to an asset ​ Who can do this? Any non-guest user with edit access to an asset's metadata can create, deploy, and manage data contracts. This only includes admin and member users in Atlan. To add a data contract to an asset, you can either: Create a contract directly in Atlan from the Contracts tab of the asset profile. You can create and maintain data contracts as easily as editing a word document. Use Atlan CLI to import an existing contract from your local machine to Atlan directly or through a CI/CD pipeline. Atlan CLI is a command-line tool that you can download directly from Atlan to your local machine to create and push data contracts to Atlan. Once you have published the contract, you can also sync metadata from a contract to the governed asset in Atlan. Once created, you will be able to monitor and manage your data contracts in Atlan. To add a data contract: From the left menu of any screen in Atlan, click Assets . (Optional) From the Filters menu on the left, click Properties and then click Has contract . Click No to filter for assets without a contract. From the Assets page, select an asset to open the asset sidebar. In the left Overview sidebar, click Add contract . In the Contract tab of the asset profile, you can either: Click Create contract to create a draft contract directly in Atlan based on asset metadata. Click Import contract to use Atlan CLI to import an existing contract from your local environment to Atlan. You will first need to install and connect Atlan CLI and then push the contract to Atlan. Refer to our developer documentation to complete the steps. (Optional) Click the Edit button to edit the contract. Congrats on adding a data contract in Atlan! 🎉 View a data contract ​ To view a data contract: From the left menu of any screen in Atlan, click Assets . From the Assets page, select an asset to open the asset sidebar. From the left Overview sidebar, click View contract to navigate to the Contract tab in the asset profile. (Optional) In the Contract tab, you can view the contract specifications for your asset in a YAML format. You can also: Click the Document icon to open a read-only, simplified view of your contract. Next to Published version , click the version dropdown to view the latest version of the contract. Select an older version and then click Compare with published version to compare them side by side. Click the Edit button to edit the contract. Click the clipboard icon to copy the YAML code. Under Timeline , view a timeline for the evolution of your contract. Under Summary , view details of who last updated your contract and when. Tags: atlan documentation Previous Contracts Next Add contract impact analysis in GitHub Supported asset types Supported asset metadata Add a data contract to an asset View a data contract"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/contracts/how-tos/add-contract-impact-analysis-in-github",
    "content": "Build governance Contracts Impact Analysis Add contract impact analysis in GitHub On this page Add contract impact analysis in GitHub Private Preview Impact analysis helps you identify how modifications to your data contracts might impact downstream processes, data quality, and overall business operations. This can help you analyze proposed changes and mitigate potential risks before implementation. If you have ever changed a data contract only to find out later that it broke a downstream table or dashboard, Atlan provides a GitHub Action to help you out. This action places Atlan's impact analysis right into your pull request. So, you can view the potential downstream impact of your changes before merging the pull request. Prerequisites ​ Before running the action, you will need to create an Atlan API token . You will also need to assign a persona to the API token and add a metadata policy that provides the requisite permissions on assets for the Atlan action to work. For example, you can add the following permissions: Asset, such as a table   - Read only Any downstream connections, such as Microsoft Power BI   - Read only You will need to configure the default GITHUB_TOKEN permissions. Grant Read and write permissions to the GITHUB_TOKEN in your repository to allow the atlan-action to seamlessly add or update comments on pull requests. Refer to GitHub documentation to learn more. Configure the action ​ To set up the Atlan action in GitHub: Create repository secrets in your repository: ATLAN_INSTANCE_URL with the URL of your Atlan instance. ATLAN_API_TOKEN with the value of the API token. Add the GitHub Action to your workflow: Create a workflow file in your repository   - .github/workflows/atlan-action.yml . Add the following code to your workflow file: name : Atlan action on : pull_request : types : [ opened , edited , synchronize , reopened , closed ] jobs : get-downstream-impact : name : Get Downstream Assets runs-on : ubuntu - latest steps : - name : Checkout uses : actions/checkout@v4 - name : Run Action uses : atlanhq/atlan - action@v1 with : GITHUB_TOKEN : $ { { secrets.GITHUB_TOKEN } } ATLAN_INSTANCE_URL : $ { { secrets.ATLAN_INSTANCE_URL } } ATLAN_API_TOKEN : $ { { secrets.ATLAN_API_TOKEN } } ATLAN_CONFIG : .atlan/config.yaml Test the action ​ After you've completed the configuration above, create a pull request with a changed Atlan data contract file to test the action. You should see the Atlan GitHub action running and then adding comments in your pull request: The GitHub workflow will add and update a single comment for every file change. The impacted assets in the comment will be displayed in a collapsible section and grouped by source and asset type. The comment will include some metadata for your impacted assets   -  such as descriptions, owners, and linked glossary terms. View impacted assets directly in Atlan. Inputs ​ Name Description Required GITHUB_TOKEN For writing comments on PRs to print downstream assets true ATLAN_INSTANCE_URL For making API requests to the user's tenant true ATLAN_API_TOKEN For authenticating API requests to the user's tenant true ATLAN_CONFIG For impact analysis of Atlan data contracts , if included in a GitHub PR true Tags: data api Previous Create data contracts Prerequisites Configure the action Test the action Inputs"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/access-control/how-tos/create-a-persona",
    "content": "Configure Atlan Access control Get started Create persona On this page Create persona Who can do this? You must be an admin user to create personas. To create a persona: From the left menu of any screen, click Governance . Under Access Control , click Personas . If this is the first persona, click the Get started button. Otherwise click the New persona button. Enter a meaningful name for the persona, (optional) a description, and then click Create . You now have an empty persona. It won't do much until you complete the next steps, too! Add users and groups ​ To add users and groups to the persona, from within the persona: To the right of the Users and groups box, click the Add button. Select users and / or groups: Under the single-user icon, select users to add to the persona. Under the double-user icon, select groups to add to the persona. Click the Update button to save the users and groups to the persona. Now this persona becomes available to those users and groups. It still doesn't do much, though, without some policies... Add rich documentation (optional) ​ To add rich documentation describing the persona: Under Summary , then Channels , add any Slack channels relevant to the persona. Under Resources , add links to external resources like PDFs, repositories, Notion, Confluence, Google Drive — anything that has a URL. Under Readme , write a richly-formatted description of the persona. Add policies ​ For the persona to really do anything, you need to define one or more policies. Repeat the following steps for each set of assets and permissions you want to control through the persona. Did you know? The higher level at which you can define the assets, the better. For example, if you create a policy at a database level, all future schemas and tables (in that database) are also covered by the policy. To add policies to the persona, from within the persona: Change to the Policies tab. Click the New Policy button and choose the type of policy. Add a metadata policy ​ Add a metadata policy to grant or restrict permissions to change metadata. You can also control access to data quality rules through metadata policies. For more information about data quality rules, see data quality rules . Follow these steps to set up a metadata policy: Choose Metadata Policy . Under Name , briefly describe the policy's intention. Under Select a connection , choose the connection on which to apply the policy. (Optional) For Asset selector , choose the assets the policy controls. By default, all assets in the connection are included. To select others: In the All assets box, click the x . Under Asset selector , click the Add link. To search for and select the assets to control with the policy, in the Add Assets dialog: Click Browse to search and select assets from all databases in the connection. Click Search to search from and select individual assets in the connection. Click Custom to search and select assets by their qualified name. Click Save to confirm your selections. (Optional) For Configure permissions choose the permissions the policy grants . By default, all permissions are granted. To select others: To the right of Configure permissions click the Edit link. Select the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one. At the bottom of the list, click Save . (Optional) For Deny selected permissions choose whether you want to explicitly deny these permissions. danger If enabled, this overrides all grants from any other policies for the same users. At the bottom of the Metadata Policy sidebar, click Save . Did you know? When using the custom asset selector for metadata and data policies, you can add /* after the database name to select all the schemas in that database. Add a data policy ​ To grant or restrict permissions to query or preview data: Choose Data Policy . Under Name , briefly describe the policy's intention. Under Select a connection , choose the connection on which to apply the policy. (Optional) For Asset selector , choose the assets the policy controls. By default, all assets in the connection are included. To select others: In the All assets box, click the x . Under Asset selector , click the Add link. To search for and select the assets to control with the policy, in the Add Assets dialog: Click Browse to search and select assets from all databases in the connection. Click Search to search from and select individual assets in the connection. Click Custom to search and select assets by their qualified name. Click Save to confirm your selections. (Optional) For Deny Query choose whether you want to explicitly deny the ability to query and preview data on these assets. danger If enabled, this overrides all grants from any other policies for the same users. At the bottom of the Data Policy sidebar, click Save . Add a glossary policy ​ To grant or restrict permissions to change glossary contents: Choose Glossary Policy . Under Name , briefly describe the policy's intention. Under Select glossary , choose the glossary or glossaries on which to apply the policy. (Optional) For Configure permissions choose the permissions the policy grants . By default, all permissions are granted. To select others: To the right of Configure permissions click the Edit link. Select the permissions required. If you are unsure what they do, hover over the checkbox to see a more detailed description of each one. At the bottom of the list, click Save . At the bottom of the Glossary Policy sidebar, click Save . Set preferences (optional) ​ Did you know? You can also personalize the details users see in the sidebar or filters menu when in a persona. This is great to limit information overload, by showing only what's relevant to a given set of users. To set preferences for the persona: Change to the Preferences tab of the persona. From the left menu, configure the following: To set the default landing page for the persona: Click Navigation to view landing page preferences. For Set default landing page , click the dropdown and then select the page where your users land when they log into Atlan. (See also What will be the default landing page for users with two or more personas? ) Keep Home as the default selection for homepage. Click Assets to direct your users to asset search and discovery. Click Glossary to direct your users to glossaries. Click Insights to direct your users to the query editor. Click Products to direct your users to the homepage for data products. Click Custom to set a custom path of your choice. For Atlan page link , specify a path to the page in Atlan you want your users to land on and make sure it's available to users in the persona. Click Save to save your preference. You must turn off View \"All assets\" in Assets Discovery from Labs in the admin center to make sure that users only land on your preselected page. Complete the steps in How to restrict asset visibility to do so. To limit the asset types that are visible to the persona: Click Asset types to view asset type preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To limit the out-of-the-box tabs that are visible to the persona: Click Asset sidebar to view asset sidebar preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To limit the asset filters that are visible to the persona: Click Asset filters to view asset filter preferences. Click the checkbox to the left of each tab's name to include it (on) or exclude it (off) for the persona. To set the default asset name that's visible to the persona: Click Asset name to set display preferences for asset name. Click the Prefer display name over technical name checkbox to display the display name , if added, over the technical name for the persona. Uncheck the box to display the technical name instead. To limit the custom metadata that's visible to the persona: Click Custom metadata to view custom metadata preferences. Click the checkbox to the left of each custom metadata structure to include it (on) or exclude it (off) for the persona. To personalize your data products , click Products and then click the Add cover image button to set a cover image for data products within your selected domain. Tags: atlan documentation Previous Create groups Next Create purpose Add users and groups Add rich documentation (optional) Add policies Set preferences (optional)"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/add-custom-metadata-badges",
    "content": "Build governance Custom Metadata Badge Management Add custom metadata badges On this page Add custom metadata badges Who can do this? You must be an admin user to be able to add badges for custom metadata . Bringing visibility to custom metadata and enriching it with additional context are critical requirements for data teams. Atlan enables you to highlight the most critical custom metadata with badges. Admin users can use badges to highlight custom metadata right in the asset overview, ensuring greater visibility. Custom metadata badges can help users quickly get the context they need for their data assets, for example: If an Airflow DAG was successful If a table's data quality checks failed Atlan currently supports creating badges for custom metadata properties with the following input types   -  text, number, options, and boolean. Note that Atlan currently does not support creating badges for multivalued types. If Allow multiple values is toggled on , you will not be able to create a badge for that custom metadata property. Create custom metadata badges ​ danger Before creating a new badge, you'll need to have created at least one custom metadata structure . To create a custom metadata badge: From the left menu of any screen, click Governance . Under the Metadata heading, click Badges . Under the Badges heading, click + Create new . In the Create new badge popover, add the following details: For Custom Metadata property , select the property you want to create a badge for   -  in this example, we'll select Last Run Status for Airflow ETL Details as the property. For Name , add a name to your badge, such as Airflow Run Status . (Optional) For Description , add a description. Click Create to create your badge. To add options to your custom metadata badge, click +New option . To define the options for your custom metadata badge, add the following details: To set a matching condition for your values, select Equals (=) or Not Equals (!=) . For Enter value , enter a value   -  such as, Successful . Click the grey box to choose a color for your badge. (Optional) Click the eye icon to preview the badge. (Optional) Click +New option to add more options to your badge. Click Save to save your badge options. Your custom metadata badge is now live! 🎉 Add custom metadata badges to assets ​ Once you've created your custom metadata badges, you can add them to your assets. Did you know? If you edit the custom metadata properties of an asset, the badges will appear automatically. To add a custom metadata badge to an asset: From the left menu of any screen, click Assets . On the Assets page, select an asset to add a badge. In the asset sidebar on the right, click the custom metadata tab   -  in this example, we'll select Airflow ETL Details . In the custom metadata panel in the sidebar, click Start editing . Enter the value(s) you want for the custom metadata properties on the asset and click Update . The custom metadata badge will now be displayed in the asset profile and sidebar! 🎉 Tags: atlan documentation Previous Disable data access Next Manage custom metadata structures Create custom metadata badges Add custom metadata badges to assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/control-access-metadata-data",
    "content": "Build governance Custom Metadata Access Management Control access to metadata and data? On this page Control access to metadata and data? You can customize access for users through several mechanisms. User roles ​ The most general mechanism is a user role . These define the very broad permissions a user has in Atlan   -  for example, whether they can administer other users, or only discover metadata. When it comes to what metadata and data a user can access, though, we need to use the additional mechanisms below. Connection admins ​ Connection admins are users who manage connectivity to a data source. By default, these users can: Read and write all metadata on assets from that connection. Preview and query the data in all data assets from that connection. Manage access policies to grant others access to the assets from that connection. You define the connection admin when crawling a new data source for the first time. A connection admin can also extend the list of connection admins on their connection at any time. Access policies ​ Who can do this? A user must be both an admin user and a connection admin to define access policies for the connection's assets. Access policies either allow or restrict access to certain assets. These allow you to be much more creative (and granular) about access than the all-or-nothing privileges of connection admins. You start by defining which assets to control with each policy. There are two complementary mechanisms to do this in Atlan   - personas and purposes . Once you have defined the subset of assets, you can then define granular access to both metadata and data: Metadata policies ​ Metadata policies control what users can do with the assets' metadata. Through them, you can control who can: Read : view an asset's activity log, custom metadata, and SQL queries Update : change asset metadata, including description, certification, owners, README, and resources Update Custom Metadata Values for the assets Add Tags to the assets Remove Tags from the assets Add Terms to the assets Remove Terms from the assets Create : create new assets within the selected connection (via API) Delete : delete assets within the selected connection (via API) Data policies ​ Data policies control what users can do with the assets' data. Through them, you can control who can: Query and preview the data within the assets Whether to hide any data, through various masking techniques: Show first 4 : replaces all the data with X except the first 4 characters of data. For example 1234 5678 9012 3456 would become 1234XXXX . Show last 4 : replaces all the data with X except the last 4 characters of data. For example 1234 5678 9012 3456 would become XXXX3456 . Hash : replaces the data with a consistent hashed value. Because the hash is consistent you can still join on it across assets. For example 1234 5678 9012 3456 would become f43jknscakc12nk21ak . Nullify : replaces the data with the null value. For example 1234 5678 9012 3456 would become null . Redact : replaces all alphabetic data with x and all numeric data with 0. For example 1234 Street Name would become 0000 Xxxxxx Xxxx . Glossary policies ​ Glossary policies control what users can do with glossary metadata   -  terms and categories. Through them, you can control who can do the following against each glossary: Read permission on terms, categories, and glossaries exists by default and cannot be modified. Glossary policies do not restrict users from viewing any glossary and its contents within the Glossary section. Create terms and categories inside the glossary Update descriptions, certification, owners, READMEs, and resources for the glossary, terms and categories Link terms in the glossary with all other assets Delete terms and categories inside the glossary Add tags to the terms Remove tags from the terms Update custom metadata values for the terms and categories inside the glossary Glossary policies can only be defined through personas. Interactions ​ All the mechanisms above can coexist. This is powerful, but can also be a bit overwhelming to think about. What takes priority when a user is under the control of all these mechanisms? 😵‍💫 It's actually not as bad as you might think   -  only these three rules: Access is denied by default (implicitly) ​ By default, users will not have the permissions listed above. This remains true until you explicitly grant a user a permission. For example, imagine you have not set up any access policies and a new user joins. They will not have any of the permissions above against any assets in Atlan. Did you know? Users have read permission on terms, categories, and glossaries by default in Atlan. Explicit grants (allows) are combined ​ When you grant a user a permission, this is combined with all other permissions you have granted the user. Continuing our example, imagine you add the new user to a group defined as the connection admins for Snowflake. The user will now have full read/write access to all metadata for Snowflake assets, and be able to query and preview the data in those assets. Then you add the user to a persona that gives read/write access to a Looker project. The user will now have access to all Snowflake assets and a Looker project's assets. Explicit restrictions (denies) take priority ​ danger When you explicitly deny a user a permission, this takes priority over all other permissions you have granted the user. Continuing our example, imagine you define a purpose with a data policy that masks PII data. The user will still have full read/write access to all metadata for Snowflake assets and a Looker project's assets. In general, they will still be able to query and preview the data in the Snowflake assets. However, any PII data in Snowflake will now be masked. Then you add a metadata policy to the purpose that denies permission to remove the PII tag. The user will no longer have full read/write access to all metadata for Snowflake assets and a Looker project's assets. The user can no longer remove the PII tag from any of these assets. Did you know? The combination of mechanisms in the example above shows their power. Through a small number of controls we can define wide-ranging but granular access permissions. Tags: atlan documentation Previous Add options Next Disable data access User roles Connection admins Access policies Interactions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/add-options",
    "content": "Build governance Custom Metadata Get Started Add options On this page Add options Who can do this? You must be an admin user in Atlan to create options for custom metadata properties. Options in Atlan stand for enumerations or enumerated data types. Options allow you to create your own set of predefined and related values. Once you've created your options, you can add them to your custom metadata properties to ensure consistency of usage across the organization. Example ​ Imagine that you would like to denote values for the data quality level of your metadata in Atlan. To solve for this, you could create an option Data quality and define three indicative values: Bronze -  for freshly crawled metadata Silver -  for asset enrichment in progress Gold -  for well-documented assets Once you've created the option, you can add it as a custom metadata property . Then you can enrich your assets with this additional context for your data teams. Create options ​ To create an option: From the left menu of any screen, click Governance . Under the Metadata heading, click Options . Under the Options heading, click Get started . In the New option dialog, enter the following details: For Name , enter a meaningful name for your option   -  for example, Data quality . For Values , enter a list of values considered valid, separate each value with a semicolon ; - Gold , Silver , and Bronze . Click Create to add your option. You have just created an option! 🎉 To edit the values for your option, click on the pencil icon in the top right to make your changes and then save them. Did you know? Atlan currently only supports deleting options through API . Tags: data crawl Previous Custom Metadata Next Control access to metadata and data? Example Create options"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/disable-data-access",
    "content": "Build governance Custom Metadata Access Management Disable data access On this page Disable data access Who can do this? You will need to be an admin user in Atlan to configure these options. What if you want to block access to data for your users, and only allow them to access metadata? There are different ways to do this in Atlan. From the most wide-reaching to the most granular: Block all querying ​ To stop all users from querying data, across all data assets: From the left menu of any screen, click Admin . Under Workspace , click Labs . Toggle off the Insights option. (This should also deactivate all sub-options of Insights.) danger Users will still be able to preview sample data, even with Insights turned off. Block by source ​ When setting up a crawler ​ To stop all users from accessing data for a source, when setting up the crawler: Set Allow SQL Query to No to stop users from querying any data in the source. Set Allow Data Preview to No to stop users from previewing any data in the source. So to block all access to data for that source, set both options to No . When setting up a connection's credentials ​ You can configure the credentials for some data sources without data access permissions. If the credentials cannot query data, Atlan will not be able to query or preview data. If Atlan cannot query or preview data, no users in Atlan can query or preview data either. danger This depends on the connector   -  some connectors need a level of data access even to crawl metadata. The specific set up guide for each connector gives you the minimal set of permissions. Block by asset ​ Who can do this? In addition to being an Admin user, you will need to be a connection admin for the source containing the assets. To stop users from querying or previewing data for specific assets: Define a persona with a data policy that denies access to those assets. Add the users to that persona. danger To ensure Atlan blocks data access for all users (including connection admins) the data policy must explicitly deny query access. A lack of data policy (implicit deny) will not prevent connection admins from querying and previewing data. Block by tag ​ To stop users from querying or previewing data that has a particular tag : Define a purpose on that tag. Within the purpose, define a data policy that denies query access for those users. (This will also apply to data previews.) Even if only a single column has the tag, Atlan will block querying and previewing of the entire asset. Tags: data crawl Previous Control access to metadata and data? Next Add custom metadata badges Block all querying Block by source Block by asset Block by tag"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks/how-tos/enable-data-quality",
    "content": "Build governance Data Quality Studio Databricks Data Quality Configure data quality Enable data quality on connection On this page Enable data quality on connection Private Preview Enable data quality on your Databricks connection in Atlan to start monitoring data quality. This guide helps you configure the connection with the necessary credentials and permissions. Prerequisites ​ Before you begin, complete the following steps: Set up Databricks for data quality completed Have the service principal credentials created during Databricks setup Identify the Databricks connection where you want to enable data quality Enable data quality ​ Follow these steps to enable data quality on your Databricks connection. Turn on the data quality feature: Navigate to Settings in Atlan Find the Labs section Turn on the Data Quality toggle Select your connection and configure credentials: IMPORTANT Currently, you can only enable data quality on one connection in Atlan. If you wish to enable it on another connection, raise a support request . Data Quality Page Connection Settings Navigate to Governance > Data Quality Select your Databricks connection from the list Click Enable data quality for your selected connection Enter the following credential details: Client ID : The service principal client ID created in Databricks setup Client Secret : The service principal client secret Tenant ID : The tenant ID (Azure only) Workspace URL : Your Databricks workspace URL SQL Warehouse : Your preferred SQL warehouse for DQ operations Click Run permissions check to verify: Credentials have necessary permissions in Databricks Databricks setup completed correctly Click Update to save the credentials Navigate to Governance > Connections Select your Databricks connection Open Connection settings from the sidebar Enter the following credential details: Client ID : The service principal client ID created in Databricks setup Client Secret : The service principal client secret Tenant ID : The tenant ID (Azure only) Workspace URL : Your Databricks workspace URL SQL Warehouse : Your preferred SQL warehouse for DQ operations Click Run permissions check to verify: Credentials have necessary permissions in Databricks Databricks setup completed correctly Click Update to save the credentials Next steps ​ After completing these steps: Atlan takes approximately 10 minutes to complete the setup in the background Once finished, you'll see data quality options available on your Databricks assets You can start creating data quality rules on tables and views Need help ​ If you have questions or need assistance with enabling data quality on your connection, reach out to Atlan Support by submitting a support request . See also ​ Data quality permissions - Learn about the data quality permission scopes and configuration Configure alerts for data quality rules - Set up real-time notifications for rule failures Tags: databricks data-quality setup atlan Previous Set up Databricks Next Setup and configuration Prerequisites Enable data quality Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/databricks/how-tos/set-up-databricks",
    "content": "Build governance Data Quality Studio Databricks Data Quality Get Started Set up Databricks On this page Set up Databricks Private Preview This guide walks through configuring Databricks to work with Atlan's data quality studio by creating the required service principal, setting up authentication, and granting the necessary privileges. Atlan recommends using serverless SQL warehouses for instant compute availability. System requirements ​ Before setting up the integration, make sure you meet the following requirements: Databricks Premium or Enterprise edition Serverless Compute for Jobs & Notebooks enabled Dedicated SQL warehouse for running DQ-related queries Outbound network access permitted from Serverless Compute (Enterprise tier only) Prerequisites ​ Before you begin, complete the following steps: Obtain Workspace admin and Metastore Admin or CREATE CATALOG privilege Identify your dedicated SQL warehouse for DQ operations Create an API token in Atlan that's stored in Databricks for authentication Review Data Quality permissions to understand required privileges Create service principal ​ Create the service principal that Atlan uses to perform Data Quality (DQ) operations within your Databricks workspace. Follow the appropriate guide based on your Databricks deployment environment: Creating a Service Principal in AWS based Databricks Accounts Creating a Service Principal in Azure based Databricks Accounts Store the following credentials securely: client_id client_secret tenant_id (Azure only) Service principal name Atlan recommends naming it: atlan-dq-service-principal Set up authentication: Choose one of the following authentication methods for your service principal: OAuth (Recommended) : Use the client_id , client_secret , and tenant_id (Azure only) from the service principal created in the previous step No additional configuration required Personal access token (pat) : Follow the Databricks Personal Access Token guide to generate a token for the service principal Store the token securely for use in the next steps Grant warehouse access: Grant the service principal access to a SQL warehouse that's used to run Data Quality queries. Go to your Databricks workspace UI Navigate to SQL > SQL Warehouses Click on the warehouse you want Atlan to use Click on the Permissions button Select the Service Principal ( atlan-dq-service-principal ) from the list Assign the Can Use permission Click Add Once access is granted, Atlan can use this warehouse to run SQL queries related to Data Quality operations. Set up Databricks objects ​ Create the required Databricks objects needed for the functioning of the Atlan Data Quality Studio. Create the atlan_dq catalog ​ The atlan_dq catalog is used by Atlan to store metadata, DQ rule execution results, and internal processing tables. Run the following SQL command in a Databricks notebook or SQL editor: CREATE CATALOG IF NOT EXISTS atlan_dq ; Set up secret scope and secret ​ Create a Databricks Secret Scope to securely store the Atlan API token. This token enables the service principal to authenticate and interact with Atlan's APIs. note Secret scopes and secret ACLs can only be managed using the Databricks CLI or REST API. These operations aren't supported through SQL. Create a new Secret Scope named atlan_dq : databricks secrets create-scope atlan_dq Save the Atlan API token in a secret named api_token in the scope: databricks secrets put-secret --json '{ \"scope\": \"atlan_dq\", \"key\": \"api_token\", \"string_value\": \"<ATLAN_API_TOKEN>\" }' Replace <ATLAN_API_TOKEN> with the API token value you created in Atlan. Grant privileges ​ Grant the following privileges to atlan-dq-service-principal so it can create internal objects, read the Atlan API token, and query data for quality checks. Replace placeholders with real values. Manage the atlan_dq catalog GRANT USE CATALOG ON CATALOG atlan_dq TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT CREATE SCHEMA ON CATALOG atlan_dq TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; Read the API token stored in the atlan_dq secret scope databricks secrets put-acl atlan_dq <SERVICE_PRINCIPAL_CLIENT_ID> READ Access data for quality checks (choose one scope) Catalog level GRANT USE CATALOG ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT USE SCHEMA ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT SELECT ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; Schema level GRANT USE CATALOG ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT USE SCHEMA ON SCHEMA < SCHEMA > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT SELECT ON SCHEMA < SCHEMA > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; Table level GRANT USE CATALOG ON CATALOG < CATALOG > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT USE SCHEMA ON SCHEMA < SCHEMA > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; GRANT SELECT ON TABLE < TABLE > TO '<SERVICE_PRINCIPAL_CLIENT_ID>' ; These grants let Atlan create its internal schemas, fetch the API token securely, and run SELECT queries needed for rule execution. Next steps ​ Enable data quality on connection - Configure your Databricks connection for data quality monitoring Need help ​ If you have questions or need assistance with setting up Databricks for data quality, reach out to Atlan Support by submitting a support request . See also ​ Configure alerts for data quality rules - Set up real-time notifications for rule failures Tags: databricks data-quality setup governance Previous Databricks Data Quality Studio Next Enable data quality on connection System requirements Prerequisites Create service principal Set up Databricks objects Create the atlan_dq catalog Set up secret scope and secret Grant privileges Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/manage-custom-metadata-structures",
    "content": "Build governance Custom Metadata Structure Management Manage custom metadata structures On this page Manage custom metadata structures Who can do this? You must be an admin user to manage custom metadata structures, including defining new ones. Before users or integrations can enrich assets with custom metadata, you must first define its structure. Create custom metadata structure ​ To create a new custom metadata structure: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Start adding custom metadata heading, click the + Get started to add a new structure: For Name enter a name for the custom metadata structure. (In our examples , this would be IPR or ETL .) (Optional) To personalize your custom metadata, to the left of the name, click the image icon. From the upper right of the corresponding dialog: Click Icons to add an icon to your custom metadata. Click the gray box to change the color of the icon to green, yellow, or red. Click Emoji to add an emoji to your custom metadata. Click Upload Image to upload an image for your custom metadata. The recommended size for image uploads is 24x24 pixels. (Optional) Add a description of the custom metadata below these. At the bottom right of the dialog, click the Create button. Create properties in the structure ​ To create custom metadata properties within a custom metadata structure: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Custom Metadata heading, select the custom metadata structure you want to change. Click the New property button (no properties yet) or Add property button (to add more properties): For Name , enter a name for one property. (In our examples , this would be one of License type , Provider , Job link , and so on.) For Type , select the type of value you expect users to use for this property: The Text type allows free-form text values. The Integer type allows only whole numbers (no decimals). The Decimal type allows fractional numbers (those with decimal points). The Boolean type allows only a Yes or a No value. The Date type allows both date and time values in the following format   -  day, month, year, hours, minutes, and seconds. The Options type allows you to define your own set of predefined options for values that are valid. The Users type allows only existing Atlan users as values. The Groups type allows only existing Atlan groups as values. The URL type allows only web links. The SQL type allows only SQL code. (Optional) For Description , enter an explanation for how you expect users to use this property. If you chose Options as the type, either: Under Select Options , select an existing set of options to reuse. Click the Create New link to create a new set of options. Under Option name , give the options a name. Under Values , enter the list of values considered valid (separated by ; ). (Optional) Under Assets , you can configure the connections and asset types on which this custom metadata should be visible to: For Connections , select the connection to which you want to limit users to be able to enrich assets with this property. For example, you may want a property to only apply to a specific Snowflake connection. For Applicable asset types , select the kinds of assets you want users to be able to enrich with this property. For example, you may want a property to only apply to SQL assets like tables and views, and not to BI assets. (Optional) Under Glossary assets , you can configure the glossaries and glossary asset types on which this custom metadata should be visible to: For Glossaries , select the glossaries to which you want to limit users to be able to enrich assets with this property. For Applicable asset types , select the glossary assets you want users to be able to enrich with this property. For example, you may want a property to only apply to terms within a glossary, and not to categories. (Optional) Under Domain assets , you can configure the data domains, subdomains, and products on which this custom metadata should be visible to: For Domains , select the domains or subdomains to which you want to limit users to be able to enrich with this property. For Applicable asset types , select the domains, subdomains , or products you want users to be able to enrich with this property. For example, you may want a property to only apply to products within a specific subdomain, and not to the parent domain. (Optional) Under Other assets , for Applicable asset types , select assets that neither fall under the rubric of a connection or glossary   -  currently only file assets are supported. (Optional) Under Configurations toggle any extra settings for the property: Allow multiple values controls whether users can enter more than a single value for this property. (Note: this is only available for some types.) Show in filter controls whether users can filter on this property when doing asset discovery. Show in overview controls whether the property will show up in the Overview sidebar tab of assets. (All properties will show in the custom metadata's own tab, but those with this Show in overview enabled will also show in the Overview tab.) That's it, your users can now enrich assets with this custom metadata ! 🎉 Delete properties from a structure ​ danger Deleting a custom metadata property will remove the values for that property from any assets. To delete custom metadata properties from a custom metadata structure: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Custom Metadata heading, select the custom metadata structure you want to change. In the properties table on the right, click the delete icon on the far right of the row containing the property to delete the property. When prompted for confirmation, click the Confirm button. Delete custom metadata structure ​ You can also delete an entire custom metadata structure. danger Deleting a custom metadata structure will remove all its properties and all its custom metadata values from any assets. You might want to consider using personas to hide the custom metadata , until you confirm it is no longer needed. To delete a custom metadata structure: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Custom Metadata heading, select the custom metadata structure you want to delete. In the upper right of the custom metadata structure, click the red delete icon. When prompted for confirmation, click the Delete button. View linked assets ​ Once users in your organization have enriched their assets with custom metadata , you will be able to view the linked assets right from the governance center. To view assets with custom metadata: From the left menu of any screen, click Governance . Under the Metadata heading, click Custom Metadata . Under the Custom Metadata heading, click Linked Assets to view all the assets linked to the custom metadata. (Optional) Click any asset to open the asset sidebar for more details. Tags: data integration Previous Add custom metadata badges Next What happens when users do not have access to metadata? Create custom metadata structure Create properties in the structure Delete properties from a structure Delete custom metadata structure View linked assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/how-tos/configure-alerts",
    "content": "Build governance Data Quality Studio Advanced configuration Configure alerts On this page Configure alerts Set up real-time notifications for data quality rule failures in Atlan. Receive immediate alerts via Slack or Microsoft Teams when rules fail, enabling quick response to data quality issues. This guide shows how to configure organization-level alert destinations and set rule-specific alert priorities. Prerequisites ​ Before you begin, make sure you have: Administrative access to Atlan (for organization-level configuration) Access to a public Slack or Microsoft Teams channel Data quality rules already configured in your environment Configure organization-level alerts ​ Set up the alert destination for all data quality rules in your organization. This configuration applies to all rules and determines where alerts are sent. Only instance administrators can access this setting. IMPORTANT Only public channels are supported. Alerts can't be routed to private channels or Direct Messages at this time. Navigate to the Admin panel from the profile menu In the left-hand menu, select Integrations Choose your preferred messaging platform: Slack or Microsoft Teams Within the selected platform, scroll to the Data Quality section Enter the name of the public channel where rule failure alerts are delivered Click Update to activate the integration Once saved, the alerting configuration is in effect for all data quality rules based on their priority settings. Configure rule-level alert priority ​ Set alert priorities during rule creation or editing. This determines how frequently alerts are sent for each specific rule. Navigate to the data quality rule you want to configure Open the rule for editing or create a new rule In the rule creation workflow, scroll to the Additional Settings section All rules default to Normal priority unless explicitly changed by the user. Under Alerts , select the desired priority level: Normal (default): Alerts are sent up to three times per failure, then suppressed until the rule passes. Use this for most data quality rules. Urgent : Alerts are sent every time the rule fails. Use this for critical business rules. Low : No alerts are sent. Failures are silently logged. Use this for non-critical monitoring. Next steps ​ After completing these steps: Rule failures trigger alerts in the designated public Slack or Teams channel, based on priority Each alert includes full context - rule name, asset, severity, and relevant metadata - to aid quick triage and action Rule-level alert priority settings can be modified at any time by editing the rule Need help ​ If you have questions or need assistance with configuring alerts, reach out to Atlan Support by submitting a support request . See also ​ Set up Databricks - Configure Databricks for data quality monitoring Set up Snowflake - Configure Snowflake for data quality monitoring Tags: data-quality alerts notifications Next What's auto re-attachment Prerequisites Configure organization-level alerts Configure rule-level alert priority Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/enable-auto-re-attachment",
    "content": "Build governance Data Quality Studio Snowflake Data Quality Configure data quality Enable auto re-attachment of rules On this page Enable auto re-attachment of rules This guide explains how to configure your Snowflake environment to enable automatic reattachment of rules in Atlan. Prerequisites ​ Make sure the following conditions are met: The recreated asset has the same name and similar structure. The original rule is still present and not deleted. The feature is enabled for your tenant. Grant privileges ​ Below are the minimum privileges the atlan_dq_service_role role needs to reattach rules automatically. Replace <database-name> and similar placeholders with your own object names. Database and schema usage : Enables Atlan to discover and reference objects. GRANT USAGE ON DATABASE < database - name > TO ROLE atlan_dq_service_role ; GRANT USAGE ON ALL SCHEMAS IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; Table and view reference : Required for rules that reference columns in these objects. GRANT REFERENCES ON ALL TABLES IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; GRANT REFERENCES ON ALL VIEWS IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; DMF schema and function usage : Enables execution of Snowflake Data Metric Functions created by Atlan. -- Grant usage on the helper database / schema holding DMF functions GRANT USAGE ON DATABASE ATLAN_DQ_DQ TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ_DQ . DMFS TO ROLE atlan_dq_service_role ; -- Grant usage on all existing and future functions within the DMF schema GRANT USAGE ON ALL FUNCTIONS IN SCHEMA ATLAN_DQ_DQ . DMFS TO ROLE atlan_dq_service_role ; GRANT USAGE ON FUTURE FUNCTIONS IN SCHEMA ATLAN_DQ_DQ . DMFS TO ROLE atlan_dq_service_role ; Future-proofing privileges : Ensures newly created objects are covered without manual grants. GRANT USAGE ON FUTURE SCHEMAS IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; GRANT REFERENCES ON FUTURE TABLES IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; GRANT REFERENCES ON FUTURE VIEWS IN DATABASE < database - name > TO ROLE atlan_dq_service_role ; After these grants are applied, any table or view that's recreated with the same name automatically regains its attached rules, keeping your data quality checks continuous. What’s next ​ Once privileges are configured, rule reattachment happens automatically whenever matching assets are recreated in Snowflake. You can now continue monitoring your data quality workflows without needing to manually reapply rules. See also ​ Auto re-attachment rules : Understand how Atlan automatically reattaches rules to recreated Snowflake assets to maintain continuous data quality enforcement. Tags: snowflake data-quality auto-re-attachment Previous Enable data quality on connection Next Upgrade to Snowflake data quality studio Prerequisites Grant privileges What’s next See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/enable-data-quality",
    "content": "Build governance Data Quality Studio Snowflake Data Quality Configure data quality Enable data quality on connection On this page Enable data quality on connection Private Preview Enable data quality on your Snowflake connection in Atlan to start monitoring data quality. This guide helps you configure the connection with the necessary credentials and permissions. Prerequisites ​ Before you begin, complete the following steps: Set up Snowflake for data quality completed Have the credentials for the atlan_dq_user created during Snowflake setup Identify the Snowflake connection where you want to enable data quality Enable data quality ​ Follow these steps to enable data quality on your Snowflake connection. Turn on the data quality feature: Navigate to Settings in Atlan Find the Labs section Turn on the Data Quality toggle Select your connection and configure credentials: IMPORTANT Currently, you can only enable data quality on one connection in Atlan. If you wish to enable it on another connection, raise a support request . Data Quality Page Connection Settings Navigate to Governance > Data Quality Select your Snowflake connection from the list Click Enable data quality for your selected connection Enter the following credential details: Username : The atlan_dq_user created in Snowflake setup Password : The password for your Atlan DQ user Role : atlan_dq_service_role Warehouse : Your preferred warehouse for DQ operations Click Run permissions check to verify: Credentials have necessary permissions in Snowflake Snowflake setup completed correctly Click Update to save the credentials Navigate to Governance > Connections Select your Snowflake connection Open Connection settings from the sidebar Enter the following credential details: Username : The atlan_dq_user created in Snowflake setup Password : The password for your Atlan DQ user Role : atlan_dq_service_role Warehouse : Your preferred warehouse for DQ operations Click Run permissions check to verify: Credentials have necessary permissions in Snowflake Snowflake setup completed correctly Click Update to save the credentials Next steps ​ After completing these steps: Atlan takes approximately 10 minutes to complete the setup in the background Once finished, you'll see data quality options available on your Snowflake assets You can start creating data quality rules on tables and views Need help ​ If you have questions or need assistance with enabling data quality on your connection, reach out to Atlan Support by submitting a support request . See also ​ Operations - Learn about the data quality operations and monitoring capabilities Configure alerts for data quality rules - Set up real-time notifications for rule failures Tags: snowflake data-quality setup atlan Previous Set up Snowflake Next Enable auto re-attachment of rules Prerequisites Enable data quality Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/domains/how-tos/organize-assets",
    "content": "Build governance Domains Asset Organization How to organize assets On this page Organize assets Who can do this? Any non-guest user with edit access to an asset's metadata can add assets to domains. This only includes admin and member users. Domain policies currently do not have any impact outside the products module . You can map and organize your assets into domains and subdomains . Domains provide you with a logical structure to group and govern your assets that aligns with business needs and ensures a curated discovery experience. To add assets to a domain, note the following: You can link assets to domains irrespective of whether or not you use data products . You can only add assets to any one specific domain or subdomain. Assets may be used across multiple domains, but can only belong to one domain or subdomain. You can filter assets by a single domain, multiple domains, or no domains. Atlan currently does not support adding glossaries, categories, and terms to domains. Atlan currently does not support raising a request to add assets to domains. Admin users can bulk add assets to domains using playbooks . Add an asset to a domain ​ Did you know? You can also set up playbooks to bulk add assets to your domains and subdomains. You will need to be an admin user in Atlan to create playbooks. To add an asset to a domain, complete the following steps. To add an asset to a domain: From the left menu of any screen in Atlan, click Assets . On the Assets page, click an asset to open the asset sidebar. In the Overview sidebar, under Domains , click Add to domain . In the popup, check the boxes to select the domain or subdomain to which you want to add the asset. You can only select any one parent domain or nested subdomain. (Optional) Hover over the linked domain or subdomain to view details in a popover, including the user that added the domain. You can also: Click View domain to view the domain profile from the governance center. If the products module is turned off , you will need to be an admin user in Atlan to view the domain. If the products module is turned on , domain policies will determine your ability to view the domain. Click the unlink icon to remove the asset from the linked domain. (Optional) Click the pencil icon to change to a different domain or remove it from the asset. (Optional) In the Filters menu on the left, click Domains to filter assets by domains: Check the boxes to select one or more domains or subdomains to filter your assets. Click No domains to filter assets not mapped to any domain. Did you know? To programmatically add assets to a domain or remove them from a linked domain, refer to our developer documentation . Tags: atlan documentation Previous Manage domains Add an asset to a domain"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/how-tos/bulk-upload-terms-in-the-glossary",
    "content": "Build governance Glossary Term Management Bulk upload terms in the glossary On this page Bulk upload terms in the glossary Who can do this? Currently, only member users with edit access on glossaries and admin users in Atlan can start a bulk glossary upload   -  you may not have access yourself. You can upload multiple terms and categories into a glossary using the bulk upload option: Download the template ​ A dynamic template can be generated per glossary. The template contains drop-down values for Type, Categories, Owner Users, Owner Groups and Tags. To download the template from within Atlan: Click on the desired glossary, choose Bulk Upload tab and proceed to download the sample template . Or to download the glossary bulk terms template from within Atlan: From the left menu, click Glossary . Terms and categories can only be added/updated to an existing glossary . Click All glossaries and then click the glossary you want to add terms to. To the right of the glossary's name in the glossary tree, click the Bulk Upload tab and proceed to download the sample template . Read the notes on each column before you start to edit the template. A file named {glossary_name}.xlsx downloads. You can edit this file with your existing terms and definitions in Microsoft Excel or Google Sheets. Populate the template ​ Populate the downloaded template following the template field structure. You can find the same details below in the comments embedded in the {glossary_name}.xlsx file. Column explanations ​ danger Except for the terms and category, these objects must already exist: users, groups, and tags. Term name ​ Enter the name of the term under the Name column. This is a mandatory field. Enter the term or category name. Avoid using @ in the name. If the name already exists, it updates the asset metadata based on the rest of the columns. Term names must be unique within a glossary, categories with same name can exist only at different levels in a glossary. Add all categories if term exists already. Avoid having empty rows in between. Did you know? You can use bulk upload to bulk edit term or category information. Type ​ Select the type of the entry in the Type column. This is a mandatory field. Specify whether the entry is a term or a category. Only Term or Category are valid options. Use drop-down to fill the value. Categories ​ Enter the categories to which to add the term in the Categories column. Multiple categories can be mentioned, as comma-separated values. You can manually add categories or select from drop-down options. Categories that don't exist get auto created. Even sub categories can get auto created. Use @ to define hierarchy (for example, Sales@USA), where sub-category USA is present/created under category Sales. Categories with same name can exist only at different levels. For example, one can add Term \"Customer\" at level 2 category with path Users@Users . Description ​ Enter the definition of the entry in the Description column. Business name ​ Enter the alternative name of the entry in the Business name column. User owners ​ Enter the usernames of owners in the User owners column. The users must already exist in Atlan. User owners are responsible for maintaining the data asset. You can manually add usernames or select from drop-down options. Enter valid Atlan usernames to assign ownership. Separate multiple owners with commas. Group owners ​ Enter the group names of owners in the Group owners column. The groups must already exist in Atlan. You can manually add group names or select from drop-down options. Enter valid group names responsible for the term. Separate multiple groups with commas. Certification ​ Enter the certification status in the Certification column. The value must be one of Verified , Draft , Deprecated , or leave empty. Keeping the cell empty removes the certificate status if it exists already. Certification message ​ Enter any additional information related to the certification in the Certification message column. Certification message is added only when Certification column contains non-empty value. Tags ​ Enter any tags to apply to the term in the Tags column. You can manually add tags or select from drop-down options. Tags must already exist in Atlan before being used. Comma-separated values can add multiple tags. Did you know? Tag propagation is disabled by default in Atlan. You can enable tag propagation to any assets linked to the term. Example ​ Following is a simple example of a filled in template, pivoted for readability: Column name Example value Name* Monthly Active Users Type* Term Description The number of users who are active over a month, often abbreviated MAU. Categories Metrics@Critical Metrics User Owners jsmith,jdoe Group Owners finance,ops Certification Verified Certification Message Agreed by governance council on May 10, 2021. Tags PII, Confidential Save as CSV or XLSX file ​ Before uploading the completed template to Atlan, save it as a CSV ( .csv ) or an XLSX ( .xlsx ) file. You may also need to check the following: Atlan requires the file to be comma-separated. If you are in a region where your preferred spreadsheet editor saves CSV or XLSX files with a separator other than a comma , you need to modify your spreadsheet's settings to use commas as separators. If your file contains special characters   -  for example, ñ , ç , õ , and others   -  Atlan recommends uploading the completed template in an XLSX file to retain the special characters. If uploaded in a CSV file format, the bulk upload and decoding of special characters may fail. Important! For optimal results when converting XLSX files to CSV format, it's highly advised to use Google Spreadsheets, as this approach ensures better compatibility and accuracy during the conversion process. Upload the file ​ To upload your CSV or XLSX file to Atlan: From the left menu, click Glossary . Under All Glossaries , click the glossary you want to add terms to. (Terms can only be added to an existing glossary .) To the right of the glossary's name, click the Bulk Upload tab, and then use Click to upload or drag it here. . Click Select a CSV or XLSX file to upload * or drag and drop the file into the dialog. Review the summary of changes displayed and proceed . Monitor the upload ​ To track the status of your upload, from the glossary: View the history from the same Bulk Upload tab. If there are errors in your file, you see a message indicating Failed to upload . If you see the link Download file with errors , click it to download a copy of the file with notations on what the errors are. If you don't see this link, retry the process or reach out to Atlan support. Update your file, save your changes, and upload the edited file following the steps earlier. If your upload is successful, you see a message indicating Success . You can hover on the record to see View Summary . That's it, your terms and categories are now available in Atlan! 🎉 Tags: glossary business-terms definitions Previous Set up glossaries Next Link terms to assets Download the template Populate the template Save as CSV or XLSX file Upload the file Monitor the upload"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/domains/how-tos/manage-domains",
    "content": "Build governance Domains Get Started Manage domains On this page Manage domains ➕ Available via the Data Marketplace package Who can do this? You must be an admin user in Atlan to create and manage domains. Any non-guest users must be granted the update metadata permission to be able to add assets to a domain. Domain policies currently don't have any impact outside the products module . Domains provide a logical way of mapping and organizing assets within a specific domain or business entity. For example, you can create domains for the following: Functions such as finance, sales, and human resources Business units or brands for different products and services Geographic regions of operation Environments such as development and production Most importantly, domains help promote shared ownership and domain-level governance in your organization. To create a domain, complete the following steps. Add a domain ​ To add a domain: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Domains . If you have enabled the products module , refer to How to create data domains instead. On the Domains page, under All domains , click Create domain . For Overview , enter basic details for your domain: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. (Optional) For Theme , choose from the available color options to add a theme to your domain. For Name , enter a meaningful name for your domain   -  for example, Product Operations . (Optional) Click the domain icon to change the icon for your domain. (Optional) For Description , enter a description for your domain. For Owners , assign additional users or groups as domain owner(s). In the top right of the screen, click the Create button to complete setup. Congrats on creating a domain in Atlan! 🎉 Your users can now add assets to your domain . (Optional) Add a subdomain ​ danger You will first need to create a domain before you can add subdomains. Subdomains help you logically segment your domains according to business needs. To add a subdomain: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Domains . If you have enabled the products module , refer to How to create data domains instead. On the Domains page, under All domains , select a domain or subdomain to add a subdomain. From the upper right of your domain page, click the + Add button and then click New sub-domain . For Overview , enter basic details for your subdomain: (Optional) For Cover , click the Change button to select an image from the gallery or upload an image of your own. Click Reposition to drag and reposition the cover image and then click Save position to save your preferences. (Optional) For Theme , choose from the available color options to add a theme to your subdomain. For Name , enter a meaningful name for your subdomain   -  for example, Analytics . (Optional) Click the domain icon to change the icon for your subdomain. (Optional) For Description , enter a description for your subdomain. For Owners , assign additional users or groups as subdomain owner(s). In the top right of the screen, click the Create button to complete setup. Congrats on creating a subdomain in Atlan! 🎉 Manage a domain ​ The domain profile includes essential details about the domain. You can also curate what your domain users will be able to view. To manage a domain: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Domains . If you have enabled the products module , refer to How to create data domains instead. On the Domains page, under All domains , hover over a domain or subdomain to: View domain owners in the Owners column. Click + Add personas to add a persona for governing assets within a domain or subdomain. danger Any non-guest users must be granted the update metadata permission to be able to add assets to a domain. Click the star button to star your domain and bookmark it for easy access. For subdomains only, to the right of the subdomain name, click the 3-dot icon and then: Click Move to to move a subdomain to a different parent domain. In the Move to dialog, select a relevant parent domain within the same or a different domain and then click Move to confirm the changes. Click Convert to domain to convert a subdomain into a parent domain. In the Convert to domain dialog, click Convert to domain to confirm your changes. Click a domain or subdomain to navigate to the domain or subdomain profile, respectively. On your domain page, the Overview tab displays important details about the domain. (Optional) From the top right, click the + Add button and then click New sub-domain to add data subdomains. Under Summary , view a total count of assets in your domain and the domain description: (Optional) Click + Add stakeholder to add stakeholders . (Optional) Click the Description field to update the description. (Optional) For Owners , click the pencil icon to add or remove owners . (Optional) If custom metadata properties are available, you can add custom metadata to your domain. (Optional) Click + Add resource to add a resource to your domain. Under Readme , click + Add to add a README to your data domain or use Atlan AI for documentation . From the top right of the domain profile: Click the user avatars to view a list of recently visited users, total views on your domain, total number of unique visitors, and total views by user. Use the days filter to filter domain views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your domain and bookmark it for easy access. Click the Slack or Teams icon to post on a Slack or Microsoft Teams channel. Click the 3-dot icon to configure the following: Click Add announcement to add an announcement to your domain. Click Add a resource to add resources to your domain. Click Archive to archive your domain   -  ensure that your domain is empty before you archive it. Change to the Assets tab to view assets within your domain. Change to the Statistics tab to monitor domain usage . If you have enabled the products module , change to the Lineage tab to view business lineage for your domain . Did you know? You can set up playbooks to bulk add assets to your domains and subdomains or remove them. Tags: atlan documentation Previous Domains Next How to organize assets Add a domain (Optional) Add a subdomain Manage a domain"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/create-forms",
    "content": "Build governance Stewardship Form Management Create forms On this page Create forms Who can do this? You must be an admin user in Atlan to create and manage forms. Anyone with access to Atlan   -  admin, member, or guest user   -  can fill out forms while raising requests . You can use Atlan's form builder to create reusable forms for managing user requests. The form builder allows you to create and customize forms from one central location. Forms can support multiple input types, including text, dropdown, date, and more. You can use forms to: Standardize request details, improving governance and automation. Track responses and enable structured data collection from user requests. Forms can currently be embedded within the following governance workflow templates only: Access management New entity creation Create a form ​ To create a new form: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Forms . Click the + New form button to create a new form. For Heading , configure the following: For Heading text , enter a name indicative of the form's purpose   -  for example, Data Access Request . (Optional) For Subheading text , enter a brief description. Click anywhere on the screen to save the name and description. To add a field to your form, in the Configure form, enter the following details: For Type , select the type of value you expect users to use for this field: Text type allows free-form text values. Dropdown type allows creating a list of predefined set of values. For Options , click Add option to set as many values as you want for your dropdown list. Email type allows creating a text field for email addresses. Number type allows numeric values. Date type allows date values in the UTC date format   -  year, month, day. Custom metadata property type allows defining existing custom metadata properties. For Property and Options , select the associated property and options for your custom metadata structure. For Name , enter a name that describes the purpose of the input field. For Description , enter a description for your input field. For Placeholder text , add instructions to help users complete the field. (Optional) Toggle on Required to prevent form submission if the field is empty. (Optional) Click the + Add another field button to add more fields to your form. (Optional) Click an existing field to perform additional operations: Click the reorder icon to drag and drop the selected field to reorder your form. Click the trash icon to delete the selected field. Click the copy icon to create a duplicate copy of the selected field. (Optional) Once you have completed your form, in the top right of the screen, you can: Click Preview to view a draft of the form before publishing it. Click Save as draft to save your changes in a draft version and publish when ready. Click Publish to publish your completed form immediately. Your form is now live! 🎉 Once published, you will be able to embed forms in governance workflows to collect more information from requesters. Manage forms ​ Once you have created forms, you can manage and modify your forms and monitor responses from the Forms dashboard. Form deletion is currently not supported. To manage forms: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Forms . From the Overview tab, you can view the following: Filter your existing forms by Published , Draft , or Disabled status. To edit a form, click the name of your form and edit it. To disable a form, select a form and then change the status from Draft or Published to Disabled . Change to the Monitor responses tab to view all form responses: Click the Submitted by filter to filter responses from specific users. Click any response to view more details. You can also view all other responses to a specific form. Frequently asked questions ​ Can Markdown syntax be used in the form description? ​ Atlan currently does not support Markdown syntax in the form description or any other fields. Tags: atlan documentation Previous Revoke data access Next Troubleshooting policies Create a form Manage forms Frequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/automate-policy-compliance",
    "content": "Build governance Stewardship Policy Management Automate policy compliance On this page Automate policy compliance ➕ Available via the Advanced Policy & Compliances package Who can do this? You must be an admin user in Atlan to enable , create , manage , and approve data governance policies. Data governance policies form the backbone of effective data governance. These help you define how to store, manage, access, and use data within an organization. You can establish guiding principles, validation rules, and best practices for monitoring data and policy compliance. An organization may have data that can be publicly available, secure and confidential, or a combination of both. Atlan can help you ensure that all your data assets are governed by data governance policies and used in compliance with applicable laws and regulations. The policy center in Atlan helps you build policies to align with your organizational goals for securing and managing data. You can currently choose from six different types of policies: Data quality -  maintain data accuracy, consistency, and reliability by establishing standards and processes for data validation, cleansing, and quality control. Data privacy -  govern the collection, processing, and sharing of personal and/or sensitive data to ensure compliance with data protection regulations and safeguarding privacy rights. Data security -  outline measures and controls to protect data from unauthorized access, breaches, and loss, often including encryption, access controls, and incident response procedures. Data lifecycle -  define the stages of data from creation and usage to archival and disposal. This ensures data is retained only as long as necessary and compliant with legal and business requirements. Data ethics -  set guidelines for responsible, ethical, and acceptable data use, and address issues such as bias, fairness, and responsible AI implementation. Data definitions and models -  standardize data definitions, taxonomies, and models to ensure consistent and accurate understanding and usage of data across the organization. Enable policy center ​ Who can do this? You must be an admin user in Atlan to enable the policy center module for your organization. As a prerequisite, you must have the governance workflows and inbox module enabled . The policy center provides a single control plane to link your data governance policies to your assets in Atlan. To enable the policy center for your Atlan users: From the left menu of any screen in Atlan, click Admin . Under the Workspace heading, click Labs . On the Labs page, under Governance center , configure the following: Ensure that the Governance Workflows and Inbox module is enabled . Turn on Policy Center to create and enforce governance policies on your assets in Atlan. In the Enable Policy Center dialog, click Enable to confirm. If you'd like to disable the Policy Center module from your organization's Atlan workspace, follow the steps above to turn it off. Once enabled, you can also temporarily disable the module and turn it on again as needed. For any policies you may have created, this will not result in any data loss. Tags: atlan documentation Previous Manage tasks Next Create policies Enable policy center"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/how-tos/link-terms-to-assets",
    "content": "Build governance Glossary Term Management Link terms to assets On this page Link terms to assets Once you've set up a glossary , you can link terms from your glossary to your data assets in Atlan. Linking glossary terms with your data assets can help you: Provide additional context for your assets to other users in your organization. Create common definitions once and apply them many times to multiple assets. Offer an abstract point for applying tags to be propagated to all linked assets   -  including their downstream and child assets   -  if propagation is enabled . Example ​ If your data assets include personal information   -  for example, email addresses   -  you can link your assets to an Email Address term to provide context to your users. You can define the term Email Address once in the glossary. You can link the term to all the columns where an individual's email address appears. You can also tag the term as PII -  and all of the linked assets will be tagged as PII . Link terms to your assets ​ danger You will first need to create a glossary and add terms to it before you can link terms to your assets. To link a term to an asset: From a term ​ From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to link to your assets. In the term profile, next to Overview , click Linked assets . Click + Link Assets to get started. (Optional) In the sidebar on the right, under the search bar, click an asset type to filter your assets   -  for example, Column . In the sidebar on the right, select the asset(s) to which you would like to link the term. At the bottom of the sidebar, click Link asset(s) to confirm your selections. (Optional) Under Linked Assets , next to the search bar, click the export icon to export linked assets for terms to spreadsheets. From an asset ​ From the left menu on any screen, click Assets . On the Assets page, select the asset to which you would like to link a term. Under Terms in the asset sidebar, click the + sign to add a term to your asset. In the dialog, expand the glossary menu and then click the term you would like to link to your assets. Click Save to confirm your selections. You can now view linked assets for your glossary term! 🎉 Did you know? You can also set up playbooks to automate the task of updating asset metadata, such as terms and more. Unlink terms from your assets ​ To unlink a term from an asset: From a term ​ From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under your glossary name, click the category in which your term is nested and then click the term you would like to unlink from your assets. In the term profile, next to Overview , click Linked assets . (Optional) Under Linked Assets , next to the search bar, click the export icon to export linked assets for terms to spreadsheets before unlinking them. Under Linked assets , navigate to the asset(s) from which you would like to unlink the term. To the right of the asset name, click the three dots and then click Unlink asset . From an asset ​ From the left menu on any screen, click Assets . On the Assets page, select the asset from which you would like to unlink a term. Under Terms in the asset sidebar, hover over the term, and in the top right of the term popover, click the unlink button to unlink the term from the asset. Your assets will now be unlinked from the glossary term. Tags: glossary business-terms definitions Previous Bulk upload terms in the glossary Next What is a glossary? Example Link terms to your assets Unlink terms from your assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/glossary/how-tos/set-up-glossaries",
    "content": "Build governance Glossary Get Started Set up glossaries On this page Set up glossaries The Atlan glossary allows you to add new terms and categories, search for existing glossary definitions, and archive old ones. You can also nest terms under categories and subcategories to create a glossary hierarchy. Set up a glossary ​ To define the relevant terms and categories for your data assets, you will first need to set up a glossary. To create a glossary: From the left menu of any screen in Atlan, click Glossary and then click Get started . In the Create new glossary dialog, enter the following details: For Glossary name , enter a name for your glossary   -  for example, Finance . The character limit for a glossary name is 80 characters. In the top right, Draft is set as the default certificate. To change the certificate, click the dropdown arrow and select the certificate you'd like to apply to your glossary. Click the glossary icon to personalize the icon for your glossary. For Description , write a short or detailed description for your glossary   -  size limit for description values is 32766 bytes. For Add owners , add yourself or anyone else in your team as owners of the glossary. Click Create to add your glossary. (Optional) From the top right of the glossary profile: Click the user avatars to view a list of recently visited users, total views on your glossary, total number of unique visitors, and total views by user. Use the days filter to filter glossary views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your glossary . Click the clipboard icon to copy the link for your glossary. Click the pencil icon to edit the glossary name, description, and icon. Click the Slack or Teams icon to share directly on a Slack or Microsoft Teams channel. Click the bell icon to enable Slack or Microsoft Teams notifications for glossary updates in Atlan. Click the 3-dot icon and then: Click Add announcement to add an announcement to your glossary. Click Bulk upload terms to bulk upload terms to your glossary. Click Export to export nested categories and terms within a glossary to spreadsheets. Click Archive to archive the glossary. Your glossary is now ready for you to start adding terms and categories ! 🎉 Add new glossary terms ​ Terms are the building blocks of your glossary. While defining a new glossary term, add as much information as possible for your term so that your team fully understands how to use it. To add a new glossary term: On the Glossary page, click the + icon next to All glossaries and then click Add term from the dropdown. In the Create new term dialog, enter the following details: For Select glossary , select a glossary for your term. In this example, we'll select the Finance glossary. For Term name , enter a name for your term   -  for example, Credit Score . The character limit for a term name is 80 characters. In the top right, Draft is set as the default certificate. To change the certificate, click the dropdown arrow and select the certificate you'd like to apply to your term. (Optional) For Alias , add an alias to your term. For Description , write a short or detailed description for your term   -  size limit for description values is 32766 bytes. For Add owners , add yourself or anyone else in your team as owners of the glossary. (Optional) Turn on Create multiple to create more terms from the same dialog. Click Create to add your term. (Optional) From the top right of the term profile: Click the user avatars to view a list of recently visited users, total views on your term, total number of unique visitors, and total views by user. Use the days filter to filter views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your term . Click the clipboard icon to copy the link for your term. Click the pencil icon to edit the term name and description or add an alias to your term. Click the Slack or Teams icon to share directly on a Slack or Microsoft Teams channel. Click the bell icon to enable Slack or Microsoft Teams notifications for glossary updates in Atlan. Click the 3-dot icon to add an announcement or archive the term. Did you know? Once you've added terms to your glossaries, you can also link them to your assets . Update your glossary terms ​ You can also add a term to your glossary without attaching a certificate or adding an owner at first. Once you have completed adding a term, navigate to the sidebar next to the term profile: Click + under Owners to assign owners for a term. Click Draft to update the certificate for a term. Choose from four certificate options   - Draft , Verified , Deprecated , and No certificate . Click + under Tags to classify the key characteristics of your term and configure tag propagation for linked assets . Click + under Categories to assign a term to a particular category. Click + under any of the associated term options to create relationships between terms. Did you know? Adding an owner to your term can help your teammates figure out who is an expert on a glossary term. This is the person they should reach out to if they have any questions about the term or would like to collaborate on updating it. Add new glossary categories ​ You can add categories to your glossary to better organize your terms and create a hierarchy of information. To add a category to your glossary: On the Glossary page, next to the name of your glossary in the left, click the three horizontal dots icon and then click Add category . In the Create new category dialog, enter the following details: For Category name , enter a name for your category   -  for example, Personal Finance . The character limit for a category name is 80 characters. In the top right, Draft is set as the default certificate. To change the certificate, click the dropdown arrow and select the certificate you'd like to apply to your category. For Description , write a short or detailed description for your category   -  size limit for description values is 32766 bytes. For Add owners , add yourself or anyone else in your team as owners of the glossary. (Optional) Turn on Create multiple to create more categories from the same dialog. Click Create to add your category. Next to the category name in the left menu, click the three horizontal dots icon and then add new terms or subcategories to your category. (Optional) From the top right of the category profile: Click the user avatars to view a list of recently visited users, total views on your category, total number of unique visitors, and total views by user. Use the days filter to filter views and user activity in the last 7, 30, and 90 days. This feature is turned on by default   -  admins can turn off user activity . Click the star button to star your category . Click the clipboard icon to copy the link for your term. Click the pencil icon to edit the category name and description. Click the Slack or Teams icon to share directly on a Slack or Microsoft Teams channel. Click the bell icon to enable Slack or Microsoft Teams notifications for glossary updates in Atlan. Click the 3-dot icon and then: Click Add announcement to add an announcement to your category. Click Export to export nested terms within a category to spreadsheets. Click Archive to archive the category. Move terms and categories ​ You can move terms and categories within and across glossaries to better organize your business context. Move terms to a different category or create subcategories within the same glossary or across your glossaries in Atlan. You will need the following permissions: Moving a term or category from one glossary to another   -  read, update, and delete permissions on both glossaries. Moving a term or category within the same glossary   -  update permission on the glossary you want to reorganize. To move an existing term or category: From the left menu of any screen in Atlan, click Glossary . In the left menu of the Glossary page, you can either: Drag and drop a term or category into the relevant category within the same or a different glossary. In the popup, click Confirm to confirm the changes. To the right of the term or category name, click the three dots icon and then click Move to . In the Move to dialog, select a relevant category within the same or a different glossary and then click Move to confirm the changes. Search for glossary terms ​ There are two ways to search for glossary terms: In the left panel of the Glossary page, type the name of your term in the search bar and select your preferred option from the search results. Click the > icon preceding the name of a category to expand the full list of nested terms in that category. Add READMEs to your assets ​ For glossaries, terms, and categories, the asset profile provides a helpful summary. For example, the Linked assets section displays all the data assets that are linked to a particular term. This is also where you can add a README . The secret to making your glossary really useful is to provide as much information as possible. Adding a README will allow you to state your objectives for defining a glossary unit in greater detail. Inspect glossary terms and categories ​ The navigation bar to the right of the asset profile provides high-level information about the glossary item you are looking at. Here's what you can view: Overview shows key characteristics of a glossary term or category and helps you understand its relationship to other items in a glossary. Activity displays the changelog for your glossary items. For instance, you can find out who updated a term and when. Resources are links to internal or external URLs that can help your team better understand your glossary items. You can add links from GitHub, Google Docs, Google Sheets, or more as resources to your glossary item to provide additional context. Requests for a particular glossary item can be filtered by their status, such as Pending , Approved , and Rejected . Properties show the unique identification number of a glossary item and other properties. Integrations show Slack or Teams messages and Jira tickets pertaining to a particular glossary item. Add associated terms ​ Who can do this? You will need your Atlan administrator to enable associated terms -  except related terms. In order to inter-relate your terms , you will first need to set up a glossary and then add terms . To add relationships between terms: From the left menu on any screen, click Glossary . Under Glossary in the left menu, click the name of your glossary. Under the glossary name, click the category in which your term is nested and then click the term you would like to enrich with an associated term. In the Overview tab of the term sidebar to the right, under Associated terms , click + to add relationships to your term. In the Associate terms dialog, configure the following: To select a term relationship: Click Related to to add a term that is related in some way . Click Recommended to add a standard form of use for the term . Click Synonyms to add a term that is similar in meaning . Click Antonyms to add a term that is opposite in meaning . Click Translates to to add a translated version of the term . Click Valid values for to add applicable values for the term . Click Classifies to add an umbrella category for the term . Click Classified By to add a term that falls under the purview of the term in use . For Select terms , select existing terms to associate. Click Associate terms to confirm your selections. The interrelated terms will reflect the relationships automatically in the term profile and sidebar. (Optional) Under Associated terms in the term profile, you can view a visual representation of your term relationships: Click any term attribute to focus on that specific term relationship. Click the minus or plus icons to zoom out or zoom in on the graph, respectively. Click the expand icon to enlarge the graph. Tags: glossary business-terms definitions Previous Glossary Next Bulk upload terms in the glossary Set up a glossary Add new glossary terms Update your glossary terms Add new glossary categories Move terms and categories Search for glossary terms Add READMEs to your assets Inspect glossary terms and categories Add associated terms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/migrate-snowflake",
    "content": "Build governance Data Quality Studio Snowflake Data Quality Upgrade setup Upgrade to Snowflake data quality studio On this page Upgrade to Snowflake data quality studio Private Preview Upgrade your existing Snowflake data quality setup to the latest version to access new features and improvements. This guide helps you migrate from an older version of the Snowflake data quality integration to the latest version. Prerequisites ​ Before you begin, make sure you have: Existing Snowflake data quality setup configured Permissions required ​ You need the following Snowflake roles: dq_admin role access atlan_dq_service_role role access Upgrade data quality setup ​ Follow these steps to upgrade your existing Snowflake data quality setup to the latest version. Switch to dq_admin role: USE ROLE dq_admin ; Drop the existing procedure: If you have already set up custom SQL previously, use the following command: DROP PROCEDURE IF EXISTS ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING ) ; If you haven't set up custom SQL , use this command: DROP PROCEDURE IF EXISTS ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING ) ; info If you’re not sure which command to use, go to the ATLAN_DQ.SHARED schema in your Snowflake environment and check the procedure signature for MANAGE_DMF to confirm which version you’re using. ::: Create the required schema: CREATE SCHEMA IF NOT EXISTS ATLAN_DQ . DMFS ; Create the updated procedure: View procedure code /** * Manages Data Metric Functions (DMF) operations for Snowflake tabular entities. * This procedure handles various DMF operations including: * - Creating and managing DMFs (CREATE_DMF) * - Attaching/detaching DMFs to entities (ATTACH_DMF, DETACH_DMF) * - Managing DMF schedules (UPDATE_SCHEDULE) * - Executing SQL expressions (EXECUTE_SQL) * - Validating SQL permissions (VALIDATE_SQL_PERMISSIONS) * * The procedure runs with the privileges of the procedure owner and includes comprehensive * validation of all inputs and permissions before executing any operations. * * @param { string } ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE, CREATE_DMF, EXECUTE_SQL, VALIDATE_SQL_PERMISSIONS) * @param { string } ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE) * @param { string } ENTITY_NAME - Fully qualified name of the entity (database.schema.name) * @param { string } [ DMF_NAME = null ] - Fully qualified name of the DMF (database.schema.name) * @param { string } [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations * @param { string } [ SCHEDULE_TYPE = null ] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED) * @param { string } [ SCHEDULE_VALUE = null ] - Schedule value based on type * @param { string } [ DMF_DEFINITION = null ] - SQL expression defining the DMF * @param { string } [ ROLE_TO_CHECK = null ] - Role to check permissions for * @param { string } [ DATABASES_TO_CHECK = null ] - Comma-separated list of databases to validate permissions for * @param { string } [ SCHEMAS_TO_CHECK = null ] - Comma-separated list of schemas to validate permissions for * @param { string } [ TABLES_TO_CHECK = null ] - Comma-separated list of tables to validate permissions for * @returns { string } - JSON string with operation status and result message */ CREATE OR REPLACE SECURE PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( ACTION STRING , ENTITY_TYPE STRING DEFAULT NULL , ENTITY_NAME STRING DEFAULT NULL , DMF_NAME STRING DEFAULT NULL , DMF_ARGUMENTS_JSON STRING DEFAULT '[]' , SCHEDULE_TYPE STRING DEFAULT NULL , SCHEDULE_VALUE STRING DEFAULT NULL , DMF_DEFINITION STRING DEFAULT NULL , ROLE_TO_CHECK STRING DEFAULT NULL , DATABASES_TO_CHECK STRING DEFAULT NULL , SCHEMAS_TO_CHECK STRING DEFAULT NULL , TABLES_TO_CHECK STRING DEFAULT NULL ) RETURNS STRING LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS $$ // -----------------------------------------------------UTILITY FUNCTIONS----------------------------------------------------- /** * Executes a SQL query with parameters * @param { string } sqlText - SQL statement to execute * @param { Array } [binds=[]] - Array of bind parameters for the query * @param { boolean } [ returnFirstRow = false ] - Whether to return only the first row * @returns { Object } Object containing execution result or error information */ function executeQuery ( sqlText , binds = [ ] , returnFirstRow = false ) { try { if ( ! sqlText ) return { isErrored : true , message : \"SQL Text is required\" , result : null , } ; const statement = snowflake . createStatement ( { sqlText , binds } ) ; const result = statement . execute ( ) ; const response = { isErrored : false , message : \"\" , result : null , } ; if ( returnFirstRow ) { response . result = result . next ( ) ? result : null ; return response ; } response . result = result ; return response ; } catch ( err ) { return { isErrored : true , message : ` ${ err . code } - ${ err . message } - ${ sqlText } with binds: ${ binds . join ( \", \" ) } ` , result : null , } ; } } /** * Safely parses a JSON string * @param { string } jsonString - JSON string to parse * @returns { Object } Parsed JSON object or null if invalid */ function safelyParseJSON ( jsonString ) { try { return JSON . parse ( jsonString ) ; } catch ( err ) { return null ; } } /** * Validates a number within a range * @param { string } value - Number to validate * @param { number } min - Minimum value * @param { number } max - Maximum value * @returns { boolean } True if number is valid * @returns { boolean } False if number is invalid */ function isNumberValid ( value , min , max ) { const num = parseInt ( value , 10 ) ; return ! isNaN ( num ) && num >= min && num <= max ; } /** * Escapes and quotes a Snowflake identifier * @param { string } identifier - Raw identifier to normalize * @returns { string } Properly quoted identifier safe for SQL */ function normalizeIdentifier ( identifier ) { return ` \" ${ identifier . replace ( / \" / g , '\"\"' ) } \" ` ; } /** * Retrieves all columns for a given entity. Validates that the entityexists and procedure owner has access to it. * @param { string } entityName - Fully qualified entity name * @returns { Array } Array of column objects with name and dataType properties * @throws { Error } If entity doesn't exist or is inaccessible */ function getAllColumnsForEntity ( entityName ) { const sqlText = \"SHOW COLUMNS IN IDENTIFIER(?)\" ; const binds = [ entityName ] ; const { result , isErrored , message } = executeQuery ( sqlText , binds ) ; if ( isErrored ) { // Validates that the entity exists and procedure owner has access to it throw new Error ( message ) ; } const columns = [ ] ; while ( result . next ( ) ) { const column = { name : result . getColumnValue ( \"column_name\" ) , dataType : JSON . parse ( result . getColumnValue ( \"data_type\" ) ) . type , } ; if ( column . dataType === \"FIXED\" ) column . dataType = \"NUMBER\" ; columns . push ( column ) ; } return columns ; } /** * Validates that the DMF is valid and exists * @param { string } dmfName - Fully qualified name of the DMF * @param { string } dmfArguments - Arguments for the DMF * @returns { boolean } Whether the DMF is valid * @throws { Error } If DMF is invalid */ function isDMFValid ( dmfName , dmfArguments ) { const { isErrored , message } = executeQuery ( ` DESCRIBE FUNCTION IDENTIFIER(?)( ${ dmfArguments } ) ` , [ dmfName ] , true ) ; if ( isErrored ) throw new Error ( message ) ; return true ; } /** * Checks if a timezone is valid * @param { string } timezone - Timezone to validate * @returns { boolean } True if timezone is valid * @returns { boolean } False if timezone is invalid */ function isTimezoneValid ( timezone ) { const result = executeQuery ( ` SELECT CONVERT_TIMEZONE(?, CURRENT_TIMESTAMP()) ` , [ timezone ] , true ) ; return ! result . isErrored ; } /** * Generates a DMF type signature based on the arguments and entity columns * @param { Array } dmfArguments - Array of DMF arguments * @param { Object } entityColumnsMap - Map of entity names to column objects in the format { <ENTITY_NAME>: [ { name: <COLUMN_NAME> , dataType: <DATA_TYPE> } ] } * @param { string } baseEntityName - Name of the base entity * @returns { string } DMF type signature * @throws { Error } If entity not found in the cache */ function generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) { if ( ! dmfArguments || ! dmfArguments . length ) return \"\" ; const baseEntityColumns = entityColumnsMap [ baseEntityName ] ; if ( ! baseEntityColumns ) { throw new Error ( ` Entity ${ baseEntityName } not found in the cache ` ) ; } const baseEntityColumnArguments = dmfArguments . filter ( param => param . type === \"COLUMN\" ) . map ( param => { const column = baseEntityColumns . find ( col => col . name === param . name ) ; return column ? column . dataType : null ; } ) . join ( \", \" ) ; const baseEntityArguments = ` TABLE( ${ baseEntityColumnArguments } ) ` ; const referencedEntityArguments = dmfArguments . filter ( param => param . type === \"TABLE\" ) . map ( entityParam => { const entityName = entityParam . name ; const entityColumns = entityColumnsMap [ entityName ] ; if ( ! entityColumns ) { throw new Error ( ` Entity ${ entityName } not found in the cache ` ) ; } const columnTypes = entityParam . nested . map ( nestedParam => { const column = entityColumns . find ( col => col . name === nestedParam . name ) ; return column ? column . dataType : null ; } ) . filter ( Boolean ) . join ( \", \" ) ; return ` TABLE( ${ columnTypes } ) ` ; } ) ; const arguments = [ baseEntityArguments , ... referencedEntityArguments ] . join ( \", \" ) ; return arguments ; } /** * Generates DMF arguments for SQL statements * @param { string } dmfArguments - Array of DMF arguments * @returns { string } Formatted DMF arguments for SQL statements */ function generateDMFColumnArguments ( dmfArguments ) { return dmfArguments . map ( param => { if ( param . type === \"COLUMN\" ) { return normalizeIdentifier ( param . name ) ; } // Handle TABLE type with nested columns return ` TABLE( ${ normalizeIdentifier ( param . name ) } ( ${ param . nested . map ( nested => normalizeIdentifier ( nested . name ) ) . join ( \", \" ) } )) ` ; } ) . join ( \", \" ) ; } /** * Generates function parameters from DMF arguments * @param { Array } dmfArguments - Array of DMF arguments * @returns { string } Formatted function parameters */ function generateFunctionParameters ( dmfArguments ) { return dmfArguments . map ( param => { if ( param . type === \"TABLE\" ) { const nestedParams = param . nested . map ( nested => ` ${ nested . name } ${ nested . dataType } ` ) . join ( \", \" ) ; return ` ${ param . name } TABLE( ${ nestedParams } ) ` ; } return ` ${ param . name } ${ param . dataType } ` ; } ) . join ( \", \" ) ; } // -----------------------------------------------------VALIDATION FUNCTIONS----------------------------------------------------- /** * Validates that mandatory arguments are provided and valid * @throws { Error } If any mandatory argument is missing or invalid */ function validateMandatoryArguments ( ) { const VALID_ACTIONS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" , \"UPDATE_SCHEDULE\" , \"CREATE_DMF\" , \"EXECUTE_SQL\" , \"VALIDATE_SQL_PERMISSIONS\" ] ) ; const VALID_ENTITY_TYPES = new Set ( [ \"TABLE\" , \"VIEW\" , \"MATERIALIZED VIEW\" , \"EXTERNAL TABLE\" , \"ICEBERG TABLE\" ] ) ; const DMF_OPS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] ) ; const VALID_SCHEDULE_TYPES = new Set ( [ \"MINUTES\" , \"CRON\" , \"ON_DATA_CHANGE\" , \"NOT_SCHEDULED\" ] ) ; const SCHEDULE_TYPES_THAT_REQUIRE_VALUE = new Set ( [ \"MINUTES\" , \"CRON\" ] ) ; if ( ! VALID_ACTIONS . has ( ACTION ) ) throw new Error ( ` Invalid ACTION: \" ${ ACTION } \". Valid options are ${ Array . from ( VALID_ACTIONS ) . join ( \", \" ) } ` ) ; if ( ENTITY_TYPE && ! VALID_ENTITY_TYPES . has ( ENTITY_TYPE ) ) throw new Error ( ` Invalid ENTITY_TYPE: \" ${ ENTITY_TYPE } \". Valid options are ${ Array . from ( VALID_ENTITY_TYPES ) . join ( \", \" ) } ` ) ; if ( DMF_OPS . has ( ACTION ) && ! DMF_NAME ) throw new Error ( \"DMF_NAME is required for DMF related actions\" ) ; if ( ACTION === \"UPDATE_SCHEDULE\" ) { if ( ! SCHEDULE_TYPE ) throw new Error ( \"SCHEDULE_TYPE is required for SCHEDULE action\" ) ; if ( ! VALID_SCHEDULE_TYPES . has ( SCHEDULE_TYPE ) ) throw new Error ( ` Invalid schedule type: \" ${ SCHEDULE_TYPE } \". Valid options are ${ Array . from ( VALID_SCHEDULE_TYPES ) . join ( \", \" ) } ` ) ; if ( SCHEDULE_TYPES_THAT_REQUIRE_VALUE . has ( SCHEDULE_TYPE ) && ! SCHEDULE_VALUE ) throw new Error ( \"SCHEDULE_VALUE is required for SCHEDULE action\" ) ; } if ( ACTION === \"EXECUTE_SQL\" && ! DMF_DEFINITION ) { throw new Error ( \"Please provide a SQL query to execute.\" ) ; } if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { if ( ! DMF_DEFINITION ) { throw new Error ( \"Please provide a SQL query to validate permissions.\" ) ; } if ( ! ROLE_TO_CHECK ) { throw new Error ( \"Failed to fetch role to check permissions. Please ensure the role exists and is accessible.\" ) ; } if ( ! DATABASES_TO_CHECK && ! SCHEMAS_TO_CHECK && ! TABLES_TO_CHECK ) { throw new Error ( \"No databases, schemas, or tables provided. Please provide at least one database, schema, or table to validate permissions for.\" ) ; } } } /** * Parses a fully qualified name into its components * @param { string } fullyQualifiedName - Fully qualified name to parse * @returns { Object } Object with database, schema, and name properties * @throws { Error } If invalid fully qualified name */ function validateFullyQualifiedName ( fullyQualifiedName ) { const parts = fullyQualifiedName . split ( \".\" ) . map ( part => part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length !== 3 ) throw new Error ( ` Invalid fully qualified name: ${ fullyQualifiedName } . Expected format: database.schema.name ` ) ; } /** * Validates the structure of DMF arguments JSON * @param { string } rawDMFArguments - Raw JSON string of DMF arguments * @throws { Error } If DMF arguments structure is invalid */ function validateDMFArgumentsStructure ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) => param . type === \"TABLE\" ) ; if ( referencedEntities . length > 1 ) throw new Error ( \"Only one referenced entity is allowed\" ) ; const validationFunctions = { arrayItem : ( param ) => [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) && param . name , nestedItem : ( param ) => [ \"COLUMN\" ] . includes ( param . type ) && param . name , } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; if ( referencedEntities . length > 0 ) { for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } } /** * Validates that all specified columns exist in an entity * @param { Array } columnsToCheck - Array of column names to validate * @param { Array } entityColumns - Array of column metadata from the entity * @param { string } entityName - Name of the entity for error message * @throws { Error } If any column doesn't exist in the entity */ function validateColumnsExistInEntity ( entityName , allColumnsInEntity , columnsToCheck ) { for ( const column of columnsToCheck ) { if ( ! allColumnsInEntity . some ( col => col . name === column ) ) throw new Error ( ` Column ${ column } not found in entity ${ entityName } ` ) ; } } /** * Validates that all provided identifiers exist and are accessible * Checks entity names, column names, and DMF compatibility * @param { string } entityName - Fully qualified name of the entity * @param { string } dmfName - Fully qualified name of the DMF * @param { Array } dmfArguments - Array of DMF arguments * @throws { Error } If any identifier doesn't exist or is inaccessible */ function validateProvidedIdentifiers ( entityName , dmfName = \"\" , dmfArguments = [ ] ) { if ( ! entityName ) { throw new Error ( \"Please provide a valid entity name. The entity name is required to validate identifiers.\" ) ; } validateFullyQualifiedName ( entityName ) ; // Validate the provided entity names and store all the columns for each entity in a map const baseEntityName = entityName ; const baseEntityAllColumns = getAllColumnsForEntity ( entityName ) ; const entityColumnsMap = { [ baseEntityName ] : baseEntityAllColumns } ; const allReferencedEntities = dmfArguments . filter ( param => param . type === \"TABLE\" ) ; for ( const referencedEntity of allReferencedEntities ) { const columns = getAllColumnsForEntity ( referencedEntity . name ) ; entityColumnsMap [ referencedEntity . name ] = columns ; } // Valite all of the provided columns are valid and exist in their respective entities const allBaseEntityColumnsInArguments = dmfArguments . filter ( param => param . type === \"COLUMN\" ) . map ( param => param . name ) ; validateColumnsExistInEntity ( baseEntityName , baseEntityAllColumns , allBaseEntityColumnsInArguments ) ; for ( const referencedEntity of allReferencedEntities ) { const columnsInArguments = referencedEntity . nested . map ( nestedParam => nestedParam . name ) ; validateColumnsExistInEntity ( referencedEntity . name , entityColumnsMap [ referencedEntity . name ] , columnsInArguments ) ; } if ( dmfName ) { // Validate that the DMF is valid and exists const generatedTypeSignature = generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) ; isDMFValid ( dmfName , generatedTypeSignature ) ; } // All provided identifiers are valid, actually exist and are accessible to the procedure owner } /** * Validates CRON expression syntax * Performs detailed validation of all CRON components and timezones to protect against SQL injection * @param { string } cronExpression - CRON expression to validate * @throws { Error } If CRON expression is invalid */ function validateCronExpression ( cronExpression ) { if ( cronExpression . length > 100 ) throw new Error ( \"Cron expression is too long\" ) ; const cronFields = cronExpression . trim ( ) . split ( / \\s + / ) ; if ( cronFields . length !== 6 ) throw new Error ( \"Invalid cron expression. Expected 6 fields\" ) ; const [ minute , hour , dayOfMonth , month , dayOfWeek , timezone ] = cronFields ; const isTimezoneValidResult = isTimezoneValid ( timezone ) ; if ( ! isTimezoneValidResult ) throw new Error ( \"Invalid timezone provided in the cron expression\" ) ; const regexPatterns = { minute : / ^ ( \\* | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , hour : / ^ ( \\* | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , dayOfMonth : / ^ ( \\* | L | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , month : / ^ ( \\* | \\d + | JAN | FEB | MAR | APR | MAY | JUN | JUL | AUG | SEP | OCT | NOV | DEC | \\* \\/ \\d + | \\d + \\- \\d + | [ A - Z ] {3} \\- [ A - Z ] {3} | \\d + ( , \\d + ) * | ( [ A - Z ] {3} ( , [ A - Z ] {3} ) * ) ) $ / i , dayOfWeek : / ^ ( \\* | \\d + | SUN | MON | TUE | WED | THU | FRI | SAT | \\d + L | [ A - Z ] {3} L | \\* \\/ \\d + | \\d + \\- \\d + | [ A - Z ] {3} \\- [ A - Z ] {3} | \\d + ( , \\d + ) * | ( [ A - Z ] {3} ( , [ A - Z ] {3} ) * ) ) $ / i , } ; if ( minute . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( minute , 0 , 59 ) ) throw new Error ( \"Invalid minute value\" ) ; if ( hour . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( hour , 0 , 23 ) ) throw new Error ( \"Invalid hour value\" ) ; if ( dayOfMonth . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfMonth , 1 , 31 ) ) throw new Error ( \"Invalid day of month value\" ) ; if ( month . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( month , 1 , 12 ) ) throw new Error ( \"Invalid month value\" ) ; if ( dayOfWeek . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfWeek , 0 , 6 ) ) throw new Error ( \"Invalid day of week value\" ) ; if ( ! regexPatterns . minute . test ( minute ) || ! regexPatterns . hour . test ( hour ) || ! regexPatterns . dayOfMonth . test ( dayOfMonth ) || ! regexPatterns . month . test ( month ) || ! regexPatterns . dayOfWeek . test ( dayOfWeek ) ) throw new Error ( \"Invalid cron expression\" ) ; } /** * Validates schedule-specific arguments * Ensures schedule type and value are compatible and valid * @throws { Error } If schedule configuration is invalid */ function validateProvidedArgumentsForSchedule ( ) { const VALID_MINUTES = new Set ( [ \"5\" , \"15\" , \"30\" , \"60\" , \"720\" , \"1440\" ] ) ; if ( SCHEDULE_TYPE === \"MINUTES\" && ! VALID_MINUTES . has ( SCHEDULE_VALUE ) ) throw new Error ( ` Invalid SCHEDULE_VALUE for MINUTES. Valid options are ${ Array . from ( VALID_MINUTES ) . join ( \", \" ) } ` ) ; if ( SCHEDULE_TYPE === \"CRON\" ) validateCronExpression ( SCHEDULE_VALUE ) ; // SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE } /** * Validates DMF arguments with dataType checks * @param { string } rawDMFArguments - Raw JSON string of DMF arguments * @throws { Error } If DMF arguments structure is invalid or dataType is missing */ function validateDMFArgumentsWithDataType ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const validationFunctions = { arrayItem : ( param ) => { if ( ! [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) || ! param . name ) { return false ; } if ( param . type === \"COLUMN\" && ! param . dataType ) { throw new Error ( ` Missing dataType for COLUMN parameter: ${ param . name } ` ) ; } return true ; } , nestedItem : ( param ) => { if ( ! [ \"COLUMN\" ] . includes ( param . type ) || ! param . name ) { return false ; } if ( ! param . dataType ) { throw new Error ( ` Missing dataType for nested COLUMN parameter: ${ param . name } ` ) ; } return true ; } } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) => param . type === \"TABLE\" ) ; for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } /** * Validates DMF name format * @param { string } dmfName - Fully qualified name of the DMF * @throws { Error } If DMF name format is invalid */ function validateDmfName ( dmfName ) { const parts = dmfName . split ( \".\" ) . map ( part => part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length !== 3 ) { throw new Error ( ` Invalid DMF_NAME: ${ dmfName } . Expected format: database.schema.name ` ) ; } } /** * Validates that the provided SQL is read-only and doesn't contain dangerous operations * @param { string } sqlExpression - SQL to validate * @returns { boolean } Whether the SQL is safe * @throws { Error } If SQL contains potentially dangerous operations */ function validateSqlExpression ( sqlExpression ) { if ( ! sqlExpression ) { throw new Error ( \"Please provide a SQL query. The SQL expression cannot be empty.\" ) ; } // Step 1: Normalize Unicode characters to prevent encoding-based attacks const normalizedSql = sqlExpression . normalize ( 'NFKC' ) ; // Step 2: Check for multiple statements (handled by splitIntoSqlStatements) splitIntoSqlStatements ( normalizedSql ) ; // Step 3: Check whether it is a read-query or not if ( ! isReadQuery ( normalizedSql ) ) { throw new Error ( \"Your query must start with SELECT or WITH. Only read operations are allowed.\" ) ; } // Step 4: Check for suspicious patterns that might bypass filters checkForSuspiciousPatterns ( normalizedSql ) ; // Step 5: Check for dangerous operations const dangerousOperation = containsDangerousOperation ( normalizedSql ) ; if ( dangerousOperation ) { throw new Error ( \"For security reasons, this operation is not permitted. Please use only read operations in your query.\" ) ; } return true ; } /** * Enhanced detection of suspicious SQL patterns * @param { string } sql - SQL query to check * @throws { Error } If suspicious patterns are detected */ function checkForSuspiciousPatterns ( sql ) { // Create a copy where string literals are masked to prevent false positives const sqlWithoutStrings = sql . replace ( / ' [ ^ ' ] * ' / g , \"'STRING_LITERAL'\" ) . replace ( / \" [ ^ \" ] * \" / g , '\"STRING_LITERAL\"' ) ; const suspiciousPatterns = [ // Common SQL injection techniques { pattern : / \\b OR \\s + [ 0 - 9 ] + \\s * = \\s * [ 0 - 9 ] + \\b / i , message : \"Suspicious always-true condition detected\" } , // Alias abuse detection { pattern : / \\b AS \\s + [ '\"` ] ? . *? ( DELETE | INSERT | UPDATE | DROP | ALTER | EXEC ) \\b / i , message : \"Suspicious alias detected\" } , // Hex encoding and other obfuscation techniques { pattern : / 0x [ 0 - 9 a - f ] {10,} / i , message : \"Suspicious hex encoding detected\" } , { pattern : / CHAR \\s * \\( \\s * \\d + ( \\s * , \\s * \\d + ) + \\s * \\) / i , message : \"Character code conversion functions are not allowed\" } , ] ; // Check for suspicious patterns outside of string literals for ( const { pattern , message } of suspiciousPatterns ) { if ( pattern . test ( sqlWithoutStrings ) ) { throw new Error ( message ) ; } } } /** * Splits SQL into separate statements based on semicolons not in quotes * @param { string } sql - SQL query * @returns { string } - SQL query without semicolons */ function splitIntoSqlStatements ( sql ) { let inSingleQuote = false ; let inDoubleQuote = false ; for ( let i = 0 ; i < sql . length ; i ++ ) { const char = sql [ i ] ; // Handle quotes if ( char === \"'\" && sql [ i - 1 ] !== '\\\\' ) { inSingleQuote = ! inSingleQuote ; } else if ( char === '\"' && sql [ i - 1 ] !== '\\\\' ) { inDoubleQuote = ! inDoubleQuote ; } // If semicolon outside of quotes, throw error if ( char === ';' && ! inSingleQuote && ! inDoubleQuote ) { throw new Error ( \"Do not use semicolons to break or end your SQL statement. Submit your query without any semicolons.\" ) ; } } // If we get here, there were no semicolons outside quotes return sql . trim ( ) ; } /** * Checks if the SQL is a read-only query * @param { string } sql - SQL query without comments * @returns { boolean } - True if it's a read-only query */ function isReadQuery ( sql ) { const normalizedSql = sql . replace ( / \\s + / g , ' ' ) . toUpperCase ( ) . trim ( ) ; if ( normalizedSql . startsWith ( 'SELECT ' ) ) { return true ; } if ( normalizedSql . startsWith ( 'WITH ' ) ) { return true ; } return false ; } /** * Checks if SQL contains any dangerous operations - using single keywords with word boundaries * @param { string } sql - SQL query without comments * @returns { string | null } - The dangerous operation found or null if safe */ function containsDangerousOperation ( sql ) { // Normalize whitespace and convert to uppercase for comparison const normalizedSql = sql . replace ( / \\s + / g , ' ' ) . toUpperCase ( ) ; // Snowflake-specific dangerous commands - using single keywords with high precision const dangerousCommands = [ // Data Modification 'INSERT' , 'UPDATE' , 'DELETE' , 'MERGE' , 'TRUNCATE' , 'COPY' , // DDL statements 'CREATE' , 'DROP' , 'ALTER' , 'COMMENT' , 'GRANT' , 'REVOKE' , 'UNDROP' , // Transaction control 'BEGIN' , 'COMMIT' , 'ROLLBACK' , // System & session commands 'SET' , 'UNSET' , 'USE' , 'PUT' , 'GET' , 'REMOVE' , 'LIST' , // Information Schema & Metadata 'SHOW' , 'DESCRIBE' , // Procedures and functions 'CALL' , 'EXECUTE' , 'EXEC' , // Additional Snowflake operations 'EXPLAIN' ] ; // Dangerous functions specific to Snowflake const dangerousFunctions = [ 'SYSTEM' , 'CURRENT_USER' , 'CURRENT_ROLE' , 'CURRENT_ACCOUNT' , 'DATABASE' , 'VERSION' , 'SLEEP' , 'CALL_INTEGRATION' , 'PARSE_JSON' , 'RUN_JAVASCRIPT' , 'CALL_JAVASCRIPT' , 'TO_JAVASCRIPT' ] ; // Create a regex pattern with word boundaries for all dangerous commands const commandPattern = new RegExp ( ` \\\\b( ${ dangerousCommands . join ( '|' ) } )\\\\b ` , 'i' ) ; const functionPattern = new RegExp ( ` \\\\b( ${ dangerousFunctions . join ( '|' ) } )\\\\s*\\\\( ` , 'i' ) ; // Check for dangerous commands const commandMatch = normalizedSql . match ( commandPattern ) ; if ( commandMatch ) { return ` Dangerous operation detected: ${ commandMatch [ 0 ] } ` ; } // Check for dangerous functions const functionMatch = normalizedSql . match ( functionPattern ) ; if ( functionMatch ) { return ` Dangerous function call detected: ${ functionMatch [ 1 ] } ` ; } // Check for access to sensitive metadata if ( / \\b INFORMATION_SCHEMA \\b | \\b ACCOUNT_USAGE \\b / i . test ( normalizedSql ) ) { return 'Access to sensitive system metadata detected' ; } return null ; } /** * Executes SQL and returns a numeric result * @param { string } sql - SQL to execute * @returns { number } Numeric result * @throws { Error } If execution fails or result is not numeric */ function executeSqlAndReturnNumber ( sql ) { try { // Execute without returnFirstRow to get full result set const result = executeQuery ( sql , [ ] , false ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } // Check if the result set exists if ( ! result . result ) { throw new Error ( \"Your query didn't return any results. Please check your SQL and try again.\" ) ; } // Check number of columns const columnCount = result . result . getColumnCount ( ) ; if ( columnCount !== 1 ) { throw new Error ( \"Your query should return exactly one column. Please modify your query to return a single numeric value.\" ) ; } // Check if we have exactly one row if ( ! result . result . next ( ) ) { throw new Error ( \"Your query didn't return any rows. Please check your query and try again.\" ) ; } // Get the value const value = result . result . getColumnValue ( 1 ) ; // Check if it's a number if ( typeof value !== 'number' ) { throw new Error ( \"Your query must return a number. Please modify your query to calculate a numeric result.\" ) ; } // Check if there are more rows if ( result . result . next ( ) ) { throw new Error ( \"Your query returned multiple rows. Please modify your query to return a single result.\" ) ; } return value ; } catch ( err ) { throw new Error ( ` ${ err . message } ` ) ; } } /** * Validates all parameters for DMF creation * @throws { Error } If any validation fails */ function validateCreateDmf ( ) { validateDmfName ( DMF_NAME ) ; validateSqlExpression ( DMF_DEFINITION ) ; validateDMFArgumentsWithDataType ( DMF_ARGUMENTS_JSON ) ; } /** * Validates all provided arguments * Performs comprehensive validation on input parameters * @throws { Error } If any validation fails */ function validateAllArguments ( ) { validateMandatoryArguments ( ) ; // Validates all mandatory arguments are provided in the correct format if ( ACTION === \"CREATE_DMF\" ) { validateCreateDmf ( ) ; return ; } else if ( ACTION === \"EXECUTE_SQL\" ) { validateSqlExpression ( DMF_DEFINITION ) ; return ; } else if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { validateSqlExpression ( DMF_DEFINITION ) ; return ; } else if ( ACTION === \"UPDATE_SCHEDULE\" ) { validateProvidedArgumentsForSchedule ( ) ; // Validates the provided schedule type and value } else { validateDMFArgumentsStructure ( DMF_ARGUMENTS_JSON ) ; } validateProvidedIdentifiers ( ENTITY_NAME , DMF_NAME , safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; // All provided arguments are valid and legal } // -----------------------------------------------------MAIN FUNCTION----------------------------------------------------- /** * Extracts database, schema and table name from fully qualified entity name * @param { string } entityName - Fully qualified entity name * @returns { Object } Object containing database, schema and table name */ function parseEntityName ( entityName ) { const [ db , schema , table ] = entityName . split ( \".\" ) ; return { db , schema , table } ; } /** * Gets the owner of a table from information schema * @param { string } db - Database name * @param { string } schema - Schema name * @param { string } table - Table name * @returns { Object } Object containing success status and table owner */ function getTableOwner ( db , schema , table ) { const query = ` SELECT TABLE_OWNER FROM ${ db } .INFORMATION_SCHEMA.TABLES WHERE TABLE_CATALOG = ? AND TABLE_SCHEMA = ? AND TABLE_NAME = ? ` ; const result = executeQuery ( query , [ db , schema , table ] , true ) ; if ( result . isErrored ) { return { isSuccessful : false , message : ` Failed to get table owner: ${ result . message } ` , owner : null } ; } const owner = result . result ?. getColumnValue ( \"TABLE_OWNER\" ) ; if ( ! owner ) { return { isSuccessful : false , message : ` Could not find owner for table ${ db } . ${ schema } . ${ table } ` , owner : null } ; } return { isSuccessful : true , message : \"Successfully retrieved table owner\" , owner } ; } /** * Grants required permissions to a role * @param { string } role - Role to grant permissions to * @returns { Object } Object containing success status and message */ function grantPermissions ( role ) { const query = ` BEGIN GRANT USAGE ON SCHEMA ATLAN_DQ.DMFS TO ROLE \" ${ role } \"; GRANT USAGE ON DATABASE ATLAN_DQ TO ROLE \" ${ role } \"; GRANT USAGE ON ALL FUNCTIONS IN SCHEMA ATLAN_DQ.DMFS TO ROLE \" ${ role } \"; END; ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { return { isSuccessful : false , message : ` Failed to grant permissions: ${ result . message } ` } ; } return { isSuccessful : true , message : ` Successfully granted permissions to role ${ role } ` } ; } /** * Handles permissions for DMF operations * @param { string } entityName - Fully qualified entity name * @returns { Object } Object containing success status and message */ function handleDMFPermissions ( entityName ) { try { // Parse entity name const { db , schema , table } = parseEntityName ( entityName ) ; // Get table owner const ownerResult = getTableOwner ( db , schema , table ) ; if ( ! ownerResult . isSuccessful ) { return ownerResult ; } // Grant permissions return grantPermissions ( ownerResult . owner ) ; } catch ( err ) { return { isSuccessful : false , message : ` Error handling permissions: ${ err . message } ` } ; } } /** * Parses comma-separated object lists into arrays * @param { string } databasesToCheck - Comma-separated list of databases * @param { string } schemasToCheck - Comma-separated list of schemas * @param { string } tablesToCheck - Comma-separated list of tables * @returns { Object } Object with parsed arrays */ function parseCommaSeparatedLists ( databasesToCheck , schemasToCheck , tablesToCheck ) { return { databases : databasesToCheck ? databasesToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] , schemas : schemasToCheck ? schemasToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] , tables : tablesToCheck ? tablesToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] } ; } /** * Checks database access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } databases - Array of databases to check * @returns { Array } Array of accessible databases */ function checkDatabaseAccess ( roleToCheck , databases ) { const accessibleDatabases = [ ] ; for ( const database of databases ) { try { const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE = 'DATABASE' AND OBJECT_NAME = ' ${ database } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"USAGE\" || privilege === \"OWNERSHIP\" ) { accessibleDatabases . push ( database ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for database ' ${ database } '. Role ' ${ roleToCheck } ' may not have access or the database may not exist. ` ) ; } } return accessibleDatabases ; } /** * Checks schema access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } schemas - Array of schemas to check (format: database.schema) * @returns { Array } Array of accessible schemas */ function checkSchemaAccess ( roleToCheck , schemas ) { const accessibleSchemas = [ ] ; for ( const schema of schemas ) { try { const parts = schema . split ( '.' ) ; if ( parts . length !== 2 ) { throw new Error ( ` Invalid schema format: ' ${ schema } '. Expected format: database.schema ` ) ; } const [ database , schemaName ] = parts ; const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE = 'SCHEMA' AND OBJECT_CATALOG = ' ${ database } ' AND OBJECT_NAME = ' ${ schemaName } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"USAGE\" || privilege === \"OWNERSHIP\" ) { accessibleSchemas . push ( schema ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for schema ' ${ schema } '. Role ' ${ roleToCheck } ' may not have access or the schema may not exist. ` ) ; } } return accessibleSchemas ; } /** * Checks table access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } tables - Array of tables to check (format: database.schema.table) * @returns { Array } Array of accessible tables */ function checkTableAccess ( roleToCheck , tables ) { const accessibleTables = [ ] ; for ( const table of tables ) { try { const parts = table . split ( '.' ) ; if ( parts . length !== 3 ) { throw new Error ( ` Invalid table format: ' ${ table } '. Expected format: database.schema.table ` ) ; } const [ database , schema , tableName ] = parts ; const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE IN ('TABLE', 'VIEW') AND OBJECT_CATALOG = ' ${ database } ' AND OBJECT_SCHEMA = ' ${ schema } ' AND OBJECT_NAME = ' ${ tableName } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"SELECT\" || privilege === \"OWNERSHIP\" ) { accessibleTables . push ( table ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for table ' ${ table } '. Role ' ${ roleToCheck } ' may not have access or the table may not exist. ` ) ; } } return accessibleTables ; } /** * Validates SQL permissions for a given role and returns accessible objects * @param { string } sql - SQL to validate * @param { string } roleToCheck - Role to check permissions for * @param { string } databasesToCheck - Comma-separated list of databases to check access for * @param { string } schemasToCheck - Comma-separated list of schemas to check access for * @param { string } tablesToCheck - Comma-separated list of tables to check access for * @returns { Object } Object with validation result and accessible objects * @throws { Error } If SQL validation fails */ function validateSqlPermissions ( sql , roleToCheck , databasesToCheck , schemasToCheck , tablesToCheck ) { try { // Step 1: Run EXPLAIN command to validate SQL syntax and plan const explainSql = ` EXPLAIN ${ sql } ` ; const explainResult = executeQuery ( explainSql , [ ] ) ; if ( explainResult . isErrored ) { throw new Error ( \"We couldn't validate your query. Please check your SQL syntax and permissions and try again.\" ) ; } // Step 2: Parse objects to check const objectsToCheck = parseCommaSeparatedLists ( databasesToCheck , schemasToCheck , tablesToCheck ) ; // Step 3: Check access for each object type const accessibleDatabases = checkDatabaseAccess ( roleToCheck , objectsToCheck . databases ) ; const accessibleSchemas = checkSchemaAccess ( roleToCheck , objectsToCheck . schemas ) ; const accessibleTables = checkTableAccess ( roleToCheck , objectsToCheck . tables ) ; return { isSuccessful : true , message : \"SQL permissions validation successful\" , accessibleObjects : { databases : accessibleDatabases , schemas : accessibleSchemas , tables : accessibleTables } } ; } catch ( err ) { throw new Error ( ` ${ err . message } ` ) ; } } /** * Main function to manage DMF operations * Validates all arguments and executes the main logic * @returns { string } JSON string with operation status and result message * @throws { Error } If any operation step fails */ function main ( ) { validateAllArguments ( ) ; // Handle permissions for DMF attachment/detachment operations if ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] . includes ( ACTION ) ) { const permissionResult = handleDMFPermissions ( ENTITY_NAME ) ; if ( ! permissionResult . isSuccessful ) { return JSON . stringify ( permissionResult ) ; } } // If the provided arguments are valid, proceed with the main logic const dmfArguments = generateDMFColumnArguments ( safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; const SQL_TEMPLATES = { ATTACH_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } ADD DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) ` , DETACH_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } DROP DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) ` , SUSPEND_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } MODIFY DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) SUSPEND ` , RESUME_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } MODIFY DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) RESUME ` , UPDATE_SCHEDULE : { MINUTES : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = ' ${ SCHEDULE_VALUE } MINUTE' ` , CRON : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = 'USING CRON ${ SCHEDULE_VALUE } ' ` , ON_DATA_CHANGE : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES' ` , NOT_SCHEDULED : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } UNSET DATA_METRIC_SCHEDULE ` , } , } ; let sqlText = \"\" ; let returnMessage = \"\" ; let binds = [ ] ; if ( ACTION === \"UPDATE_SCHEDULE\" ) { sqlText = SQL_TEMPLATES [ ACTION ] [ SCHEDULE_TYPE ] ; returnMessage = ` Successfully updated schedule for ${ ENTITY_NAME } to ${ SCHEDULE_TYPE } ${ SCHEDULE_VALUE } ` ; } else if ( ACTION === \"CREATE_DMF\" ) { const DOLLAR = String . fromCharCode ( 36 ) ; // ASCII code for $ const dmfArguments = safelyParseJSON ( DMF_ARGUMENTS_JSON ) ; const functionParams = generateFunctionParameters ( dmfArguments ) ; sqlText = \"CREATE OR REPLACE DATA METRIC FUNCTION \" + DMF_NAME + \" (\" + functionParams + \" )\" + \"RETURNS NUMBER AS \" + DOLLAR + DOLLAR + \" \" + DMF_DEFINITION + \" \" + DOLLAR + DOLLAR ; returnMessage = ` DMF ${ DMF_NAME } registered successfully ` ; } else if ( ACTION === \"EXECUTE_SQL\" ) { // Execute SQL and get numeric result const result = executeSqlAndReturnNumber ( DMF_DEFINITION ) ; const response = { isSuccessful : true , message : \"SQL executed successfully\" , result : result } ; return JSON . stringify ( response ) ; } else if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { const validationResult = validateSqlPermissions ( DMF_DEFINITION , ROLE_TO_CHECK , DATABASES_TO_CHECK , SCHEMAS_TO_CHECK , TABLES_TO_CHECK ) ; return JSON . stringify ( validationResult ) ; } else { sqlText = SQL_TEMPLATES [ ACTION ] ; returnMessage = ` ACTION: ${ ACTION } performed successfully on ${ ENTITY_NAME } with DMF: ${ DMF_NAME } and DMF Arguments: ${ dmfArguments } ` ; } const result = executeQuery ( sqlText , binds ) ; return JSON . stringify ( { isSuccessful : ! result . isErrored , message : result . isErrored ? result . message : returnMessage , } ) ; } // Execute the main function and return the result try { return main ( ) ; } catch ( err ) { return JSON . stringify ( { isSuccessful : false , message : err . message , } ) ; } $$ ; Grant usage to procedure and schema to the service role: GRANT USAGE ON PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING ) TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ . DMFS TO ROLE atlan_dq_service_role ; Need help ​ If you have questions or need assistance with migrating your Snowflake data quality setup, reach out to Atlan Support by submitting a support request . See also ​ Data quality permissions - Review the permissions required for the upgraded setup Tags: snowflake data-quality migration governance Previous Enable auto re-attachment of rules Next Operations Prerequisites Permissions required Upgrade data quality setup Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/automate-data-governance",
    "content": "Build governance Stewardship Get Started Automate data governance On this page Automate data governance Who can do this? You must be an admin user in Atlan to enable , create , and manage governance workflows. Anyone with access to Atlan   -  admin, member, or guest user   -  can use the inbox . You can streamline your data governance requirements in Atlan with governance workflows and manage alerts, approvals, and tasks using the inbox . Governance workflows enable you to set up robust controls on data access management, metadata enrichment, new entity creation, and more, with out-of-the-box workflow templates and automated execution. For example, instead of allowing your users to directly query data or update the certification status of an asset, you can specify assets that require advanced controls and create governance workflows to govern them. These workflows will run in the background, ensure that all required approvals are in place, and only then approve users with appropriate permissions to perform any action. You can use governance workflows to ensure: Risk mitigation -  determine how data is used and shared in your organization with automated access policies. Data security -  manage requests for data access and processing to only allow access to authorized individuals or teams. Metadata change management -  monitor and audit metadata changes to align with established organizational standards. New entity creation -  manage and audit documentation of business context such as glossaries and tags to align with established organizational standards. Policy compliance -  set up repeatable processes and approval flows for your data assets in Atlan to adhere to regulatory requirements   -  currently only applicable if you have also enabled the policy center module . Workflow properties ​ A common set of properties are applicable to all governance workflows in Atlan: Only an admin user can create, update, or delete governance workflows. Out-of-the-box workflow templates. Predefined steps based on workflow selection. Must be associated with an asset type or action. Set up auto-approval rules for users, groups, or owners based on metadata attributes and policies. Activity logs for all workflows available by default. Visibility into the transition states of a workflow. Overlapping workflows   -  governance workflows provide you with the flexibility of creating workflows per team or business domain on the same set of assets instead of creating one complex workflow to cover all your use cases. Atlan will handle all the complexities, only allowing approvals to go through once all approval conditions have been met. Workflow templates ​ You can choose from the following workflow templates to govern your assets and manage access: Change management ​ This template allows you to control changes to metadata within your organization's data management and governance framework. Use cases include requests to: Add, update, and remove descriptions manually and using Atlan AI Add, update, and remove certificates Add, update, and remove an alias Link and remove terms from asset profile Add, update, and remove owners Attach, update, and remove tags Add, update, and remove custom metadata Add, update, and remove domains Add, update, and remove READMEs Add, update, and remove announcements Update and archive glossaries, categories, and terms Move terms and categories Change management workflows will override any permissions assigned through user roles or access policies . For example, even for users with edit access , metadata update requests will go through change management workflows. If there are no change management workflows in place, then users with edit access will be able to update metadata while users without edit access will only be able to suggest changes to metadata . New entity creation ​ This template allows you to control the creation and publication of new entities in Atlan. The new entity creation workflow will override existing glossary policies and user role permissions to create new entities. Creation of the following entities is currently supported for the new entity creation workflow: Glossaries Categories Terms Tags Data products : Creation of a new data product Change of a data product's status from Sunset , Archived , or Draft to Published Whether you are an admin or a member user in Atlan, the existence of a new entity creation workflow means you will need to submit a request for creating new entities. Guest users are neither allowed to directly create nor suggest the creation of glossaries, categories, terms, and tags. Access management ​ This template allows you to automate the process of requesting, approving, and revoking access to data assets in Atlan. It includes the combination of a self-service approach as well as mandating human intervention for approval. You can also revoke data access in Atlan or other data sources. For data sources other than Atlan, you can configure additional actions to revoke data access in the data source. Use cases include requests to query data or view sample data for the following supported asset types   -  tables, views, and materialized views. Grant access in Atlan   -  allow requesters to request data access for querying data in Insights and previewing sample data within Atlan only. Raise Jira ticket to grant or revoke data access on source   -  allow requesters to request or revoke data access for any tool. Atlan will create a support ticket in Jira Cloud for your team to grant or revoke data access and display the status of your request in Atlan. You will need to: Integrate Jira Cloud and Atlan . Link your individual Jira Cloud account to Atlan . Install or register a webhook . Create an access management workflow to enable or revoke access everywhere using Jira. Add a Jira project and issue type and specify an issue status while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in Jira. Raise ServiceNow request to grant or revoke data access on source   -  allow requesters to request or revoke data access for any tool. Atlan will create a request in the Atlan Data Access catalog for your team in ServiceNow to grant or revoke data access and display the status of your request in Atlan. You will need to: Integrate ServiceNow and Atlan . Link your individual ServiceNow account to Atlan . Create a data access approval workflow to enable or revoke access everywhere using ServiceNow. Specify the request state(s) for approval while creating the data access workflow. Your users will be granted access or their access will be revoked once the request is approved in ServiceNow. Trigger a webhook   -  allow requesters to request or revoke data access for any tool. Atlan will trigger a webhook to a URL of your choice for your team to grant or revoke data access. For URL , enter the URL for where you want to receive events, including details on requester, approver, and asset, and then validate the URL. danger Atlan will send a sample payload to test if the webhook URL is correct. You must respond with a 2xx status for the validation to succeed. Atlan will also run this validation before you save your webhook as a precautionary measure. Copy the Secret Key and store it in a secure location to verify data access approval or revocation requests from Atlan. Policy approval ​ You must enable the policy center module to use the policy approval workflow template. This template allows you to automate approvals for your data governance policies in Atlan. Automated policy approval workflows can help you streamline the approval process, facilitate compliance with regulatory standards, and simplify data governance for your organization. Use cases include requests to: Create new policies Revise existing policies Enable governance workflows and inbox ​ Who can do this? You must be an admin user in Atlan to enable the governance workflows and inbox module for your organization. To enable governance workflows and inbox for your Atlan users: From the left menu of any screen in Atlan, click Admin . Under the Workspace heading, click Labs . On the Labs page, under Governance center , turn on Governance Workflows and Inbox to govern your assets and manage alerts, approvals, and tasks in Atlan more effectively. If you'd like to disable the Governance Workflows and Inbox module from your organization's Atlan workspace, follow the steps above to turn it off. Once enabled, you can also temporarily disable the module and turn it on again as needed. For any governance workflows you may have created or existing requests , this will not result in any data loss. Interactions with existing access control mechanisms ​ Once you have turned on governance workflows and inbox, the module will interact with existing access control mechanisms in Atlan as follows: Requests : Atlan will channel requests and approvals through governance workflows and land them in the inbox. New requests   -  once you have enabled governance workflows and inbox, the requests widget will be replaced by an inbox and your member and guest users will not be able to raise any new requests until an admin user has created at least one governance workflow. To enable your member and guest users to raise new requests in Atlan: Create a change management governance workflow . Select all connections present in your Atlan workspace . Skip auto-approval . Select Anyone approves and list the users or groups designated as your Atlan admins . Publish your first governance workflow! Once published, this comprehensive workflow will allow your member and guest users to raise requests. Now you can focus on creating more use-case-driven workflows and consequently removing governed assets from the first workflow until you no longer need it. Existing requests   -  only admin users can take action on existing requests from the requests center . Your member and guest users will only be able to raise new requests on governed assets. Personas and purposes : Metadata policies -  your users must have read access to an asset for triggering governance workflows. If an asset is governed by a governance workflow, your users will be able to raise a request on that asset regardless of all allow/deny permissions in metadata policies. Data policies : No data policy exists   -  if the workflow connection allows querying and previewing sample data but a data policy has not been configured, your users will be able to raise a data access request on governed assets in the connection. Data policy with explicit restrictions   -  if an existing data policy denies querying and previewing sample data and assets are governed by a governance workflow, your users will not be able to raise a data access request on governed assets in the connection. Data policy with explicit grants   -  if an existing data policy allows querying and previewing sample data and assets are governed by a governance workflow, your users will be able to raise a data access request on governed assets in the connection. Glossary policies -  if an asset (glossaries, categories, and terms) is governed by a governance workflow, your users will be able to raise a request on that asset regardless of all allow/deny permissions in glossary policies. Domain policies -  governance workflows are currently not applicable to domain policies. User roles -  if an asset is governed by a governance workflow, your users will be able to raise a request on that asset regardless of their role or permissions. For any asset not governed by a governance workflow, default role permissions will apply. Connection admins -  if an asset is governed by a governance workflow, connection admins will have to go through the approval process for governed assets in the connection. Governance workflows will currently not be triggered for the following actions: Add associated terms Add, update, and remove categories for terms from term profile Add, update, and remove resources Add a README to a term using Atlan AI Link and remove terms from term profile Bulk updates through spreadsheet tools Bulk updates using playbooks Bulk updates using Atlan AI Bulk updates through API, SDK, and CLI operations Metadata updates in supported tools using Atlan browser extension Tags: alerts monitoring notifications workflow automation orchestration Previous Stewardship Next Create governance workflows Workflow properties Workflow templates Enable governance workflows and inbox"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/manage-governance-workflows",
    "content": "Build governance Stewardship Workflow Management Manage governance workflows On this page Manage governance workflows Who can do this? You must be an admin user in Atlan to enable , create , and manage governance workflows. Once you have created governance workflows, you can manage and modify your workflows and monitor requests from the Governance workflows dashboard. Manage governance workflows ​ To manage governance workflows: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Governance workflows . From the Overview tab, you can view the following: In the Activity section, monitor governance workflows by status and type. In the Requests section, monitor requests on assets within the scope of your workflows. The default date range for requests is set to 14 days. You can also view requests for the last 7, 30, or 45 days, or a custom date range of your choice. In the Workflows section, view the governance workflows you created or recently viewed. Change to the Definitions tab to modify your workflows: Filter your existing workflows by Published , Draft , or Disabled status. To edit a workflow, click the name of your workflow and edit it. To disable a workflow, hover over a workflow and then click the horizontal 3-dot icon. From the dropdown, click Disable workflow to disable it. Change to the Monitor tab to view all requests on governed assets: Filter requests on governed assets by Open , Approved , Rejected , or Archived status. Click the Request by filter to filter requests from specific users. Click any request to track the progress on that request and view the governance trail in detail. You can also view all other requests on a specific workflow. Tags: workflow automation orchestration Previous Create governance workflows Next Manage tasks Manage governance workflows"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/create-policies",
    "content": "Build governance Stewardship Policy Management Create policies On this page Create policies ➕ Available via the Advanced Policy & Compliances package Who can do this? You must be an admin user in Atlan to enable , create , manage , and approve data governance policies. You can create a policy to document guidelines for the following: How's data processed and managed within your organization? Who is responsible for the data under various circumstances? What can you do to reduce potential business problems from the improper use of data? Before you can create a data governance policy, you must have an Atlan admin enable the policy center module in your Atlan workspace. To create a policy, complete the following steps. Create a policy ​ To create a new policy: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Policy center . From the Policy Center , click the + New policy button to create a new policy. In the Create a new policy dialog, enter the following details: For Policy name , enter a meaningful name for your policy. For Policy type , choose a policy type. For Owners , assign individual users or groups as policy owners. Click Create to get started. Define the policy ​ Add a purpose ​ Once you have created a policy, you can define its purpose. This is the mission statement of your policy, where you can outline what you want the policy to accomplish. To define the purpose of your policy: In the Overview tab of the policy page, under Purpose , you can either: Click Edit to manually describe the purpose of your policy and then click Save . Click ask Atlan AI to add an Atlan AI-generated description. In the Generate purpose using Atlan AI form, enter the following details: For Enter industry , enter the name of your industry   -  for example, Finance . To define the area of impact of your policy, click the dropdown to select Global , Regional , or Local policy . For compliance type, select Standard , Regulation , or Other . For Enter compliance , enter the compliance regulation. For Describe the policy , enter a brief description of your policy. Click Generate Purpose to generate an Atlan AI-generated purpose. (Optional) Click + Add resource to add resources to your policy description. Describe the policy ​ You can set out the best practices, goals, and guidelines for your policy document. To describe your policy: Switch to the Purpose tab of your policy page. In the Policy section, click Edit to write your policy. You can either manually draft the policy description or use Atlan AI to do the same and then edit as needed. Click Save to save your changes. (Optional) Add policy exceptions ​ A policy exception is a method for maintaining a policy but granting exceptions to authorized individuals or entities. Doing so will allow them to circumvent one or more restrictions. To add a policy exception: In the Purpose tab of the policy page, under Policy Exceptions , click Add policy exception . In the New policy exception form, enter the following details: For Exception name , enter a meaningful name. For Purpose , briefly describe the purpose of this exception. For Users , select the individual users or groups to whom this exception should apply. Click Add exception to save your changes. (Optional) Click + Add new to add more policy exceptions. Define scope and rules ​ Did you know? Your selected assets will not be linked to the draft policy until after it has been approved. It may also take a few hours after the policy has been approved for the assets to be linked while the linkage workflow runs in the background. Select asset scope ​ You can determine the assets within the scope of your policy. Policy rules will only apply to the filtered subset of assets you select. To select assets: Switch to the Scope & Rules tab of your policy page. For Asset scope , use the asset filters to select the relevant assets. The operators and values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter . (Optional) To preview the assets included in the scope of your policy, click View all . Click Save scope to save asset selection. (Optional) To the right of any filter, click the three horizontal dots and then: To remove a filter, click Delete . To turn off a filter, click Disable . Click Enable to turn on any disabled filters. Create compliance rules ​ To implement and enforce your policy, you can create a set of rules to specify permitted or restricted actions, enable compliance with data standards, and ensure accountability. If the assets scoped to the policy do not comply with all the rules, Atlan will trigger an incident to alert you. This incident can help you understand the specific rules that have been violated by the assets, making them noncompliant with the policy. Atlan currently supports creating 10 rules per policy. To define compliance rules: In the Scope & Rules tab of your policy page, Assets must comply with the scope defined above is the default rule for all policies. You must first determine your asset scope before you can create compliance rules. For Compliance rules , use the attribute filters to create a rule with which scoped assets must comply. The operators and values will vary depending on the selected attributes. Atlan currently supports creating policy rules based on the following metadata attributes: Certificates Owners Terms Tags Custom metadata (Optional) To add more rules, click Add another rule . Click Save rules to save the rules you created for the policy. Atlan will scan scoped assets to ensure that these match all the rules. An incident will be triggered for any asset that does not comply with all the policy rules. (Optional) To the right of any rule, click the three horizontal dots and then: To remove a rule, click Delete . To turn off a rule, click Disable . Click Enable to turn on any disabled rules. Define policy validity ​ To define the validity period of your policy: In the Policy Details sidebar of the Overview tab, for Valid till , click the pencil icon to set a validity period. From the calendar, set a date for when the policy will expire. For Review period , click the pencil icon to set a review period. For ...days before expiry , enter a numeric value for when the policy should be reviewed before its expiration date. By default, Atlan will display a warning message on the policy 30 days prior to its expiration date. You can adjust the review period to set a different timeline. During the review period, you can either revise the expiring policy or extend its validity period. Select approval workflow ​ To select an approval workflow: Switch to the Relationships tab of your policy page. From the left menu, select Approval Workflows . In the Approval Workflows section, click Add approval workflow . In the Select Approval Workflow dialog, click the relevant approval workflow for your policy. (Optional) Hover over Approvers to view a list of approvers. Click Save to save your selections. (Optional) Add terms related to this policy ​ You can add business context to your policies in Atlan. In the Overview tab of the policy page, under Linked Terms , click the + button to add related terms. (Optional) Add related policies ​ You may want to group data governance policies by policy type, business function, and more. You can optionally create relationships between your policies in Atlan to build a more comprehensive framework of data governance. To add related policies: Switch to the Relationships tab of your policy page. From the left menu, select Related Policies . In the Related Policies section, click Add related policies . In the left menu of the Add policies related to ... dialog, click the relevant policies to connect to your policy. Click Add policies to save your selections. Submit for approval ​ Once you have reviewed your policy, in the top right of your screen, click Submit for approval to submit your policy for approval. If the policy has been approved and the workflow linking the policy to your selected assets has run successfully, the policy you created will become active and govern linked assets. For governed assets, linked policies will appear on the asset sidebar . You can hover over a linked policy in the asset sidebar to view details in a popover, including policy type, purpose, and certification status, and even navigate to the policy in the policy center. Did you know? If you have any questions about setting up policies, head over to Troubleshooting policies . Tags: atlan documentation Previous Automate policy compliance Next Manage policies Create a policy Define the policy Define scope and rules Define policy validity Select approval workflow (Optional) Add terms related to this policy (Optional) Add related policies Submit for approval"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/create-governance-workflows",
    "content": "Build governance Stewardship Workflow Management Create governance workflows On this page Create governance workflows Who can do this? You must be an admin user in Atlan to enable , create, and manage governance workflows. Atlan provides no-code governance workflow templates with predefined steps. To create a governance workflow , complete the following steps. Create a governance workflow ​ To create a governance workflow: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Governance workflows . Click the + Workflow button to create a new governance workflow. Select a workflow template ​ To select a workflow template: From the Create new workflow menu, select the workflow template best suited to your use case: Change management New entity creation Access management Policy approval -  if policy center module is enabled In the upper right of the screen, click the Use template button to begin. In the New workflow dialog, enter the following details: For Name , enter a meaningful name for your workflow. (Optional) For Description , enter a brief description of your workflow. (Optional) Select an icon to represent your workflow. Click Create to create your workflow. Select the scope of workflow ​ Governance workflows must either be associated with assets, including data products, or certain actions in Atlan. If a user submits an access or update request, your workflow will be triggered to provision access or approve the update request, respectively. To select assets for the scope of your workflow: For When assets match rules , define the scope of your workflow to specific assets. To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. For Attributes , select a relevant option: Click Connection and then select an existing connection. (Optional) To further refine your asset selection: Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported Snowflake and dbt tags. Click Glossary, terms, & categories to filter by a specific glossary or category to bulk update all the nested terms or by multiple glossaries and categories. Click Linked terms to filter assets by linked terms . Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. For Operator , select Is one of for values to include or Is not for values to exclude. Depending on the selected attribute(s), you can also choose from additional operators : Select Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, click View for a preview. At the bottom of the form, click the Save & Continue button. (Optional) Conditional Branching ​ Conditional branching enables you to define dynamic paths within governance workflows based on specific rules. Each branch can have its own set of conditions, approvers, forms, and auto-approval settings. Conditional branching is currently supported only for the Change Management and New Entity Creation workflow templates. Change Management Template ​ Condition based on Metadata Attribute Type Apply approval logic only when specific metadata attributes are changed (e.g., tags, owners, certificates). Enable up to 10 conditional branches Supported attributes include: Name Alias Announcement Description Certificate Domain Owners Tags Terms Readme Custom metadata Each branch can include multiple conditions — if any condition is satisfied, the branch will trigger. Conditions cannot be reused across branches. Fallback Path (Change Management) Triggered when no other branch conditions match Allows exclusions — you can uncheck specific attributes that should not trigger approval Metadata changes grouped as: All asset metadata changes (common attributes) Glossary-specific changes AI asset-specific changes Product and domain-specific changes Only select applicable change types based on the asset universe. For example, if glossary assets aren't in scope, glossary-specific changes won't be triggered. New Entity Creation Template ​ Use this template to route requests based on where a new term, category, or product is being created. Define routing conditions based on the entity's destination. You can also set conditions at the category level. Each branch can include multiple conditions; if any condition is met, that branch is triggered. Attach different input forms to each branch—or reuse the same form, to collect context-specific information from requesters. This enables personalized context collection depending on the route taken. If no conditions are met, the fallback path is triggered automatically. (Optional) Collect information from requesters ​ Linking forms to your governance workflows allows you to collect more information from requesters as they raise requests, thus enabling approvers to make informed decisions. You can currently embed forms within the following governance workflow templates: Access management New entity creation To enable collecting information from requesters: For Collect information from requester , toggle on the Require requestor information slider. To link a form to your governance workflow, you can either: Click + Create new to create a new form . For Input form , click the dropdown to select an existing form. You can optionally preview or edit your selected form. At the bottom of the form, click the Save & Continue button. (Optional) Enable auto-approval ​ You can set up specific conditions for auto-approval of requests to reduce the need for human intervention. For example, you can enable auto-approval of: Data access requests from new users in your team to facilitate faster onboarding. Metadata update requests for assets with no restrictions. To enable auto-approval of requests: For If auto approval is enabled , toggle on the Auto approve request slider. Once you have turned on auto-approval, you can specify a subset of assets or a list of approved users that qualify for automated approval. You can either: For Filter assets for auto approval , create a subset of assets for auto-approval. Follow the steps in Select the scope of workflow to filter your selections. For Auto-approve eligible users requests , configure the following: Click + Add users/groups to select individual users or groups whose requests can be cleared for auto-approval. Click + Add owners to select asset owners whose requests can be cleared for auto-approval. At the bottom of the form, click the Save & Continue button. Set up manual approval ​ You can set up the approval process for requests and identify approvers. This ensures that each request is reviewed and authorized by designated approvers. Approvers can be individual users, user groups, or a combination of both. To set up the manual approval process: For Select approvers and process , determine the approval strategy from the following options: To enable any one approver from a list of approvers to approve requests, click Anyone approves . This means if any one approver approves or rejects a request, the workflow will be completed. To enable all selected approvers to approve requests in no particular order or simultaneously, click All approve - Parallel . This means that requests will go to all designated approvers and must be approved by all. To enable all selected approvers to approve requests in a predefined order, click All approve - Sequential . This means that the request will go through a particular order for approval and must be approved by all. For Who can approve requests , designate approvers: Click + Add users/groups to select individual users or groups as approvers. If you have selected the sequential option, you can drag and sort approvers to create a custom order for approval. Click + Add owners to select asset owners as approvers. If you have selected the sequential option, you can drag and sort approvers to create a custom order for approval. Click + Add users/groups to add at least one additional approver other than the selected asset owner(s). This is a mandatory step if you have designated asset owners as approvers. For Request expiry period , set the minimum or maximum number of days during which the approval window will be open. Non-approval will lead to automatic rejection of the request. At the bottom of the form, click the Save Continue button. Did you know? Atlan recommends that you limit your group selection for automated and manual approval to groups with fewer than 100 users. This will ensure that your governance workflows and approvals run smoothly. Review and publish workflow ​ If you'd like to continue working on your workflow, you can save it as a draft. If your workflow is ready, you can proceed to publishing it. Review all your selections, and then to publish your governance workflow: In the upper right of the screen, click the Publish button. Congratulations, your governance workflow is now active! 🎉 Any requests on assets within the scope of your workflow will be immediately routed through the workflow you just created. Requesters will be notified about the outcome of their requests through the task inbox . Did you know? For governed assets, you can open the activity log to view whether it was updated using governance workflows, an approval timeline, who requested the change, and approvers that approved it. Tags: workflow automation orchestration Previous Automate data governance Next Manage governance workflows Create a governance workflow Select a workflow template Select the scope of workflow (Optional) Conditional Branching (Optional) Collect information from requesters (Optional) Enable auto-approval Set up manual approval Review and publish workflow"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/attach-a-tag",
    "content": "Build governance Tags Tag Management Attach a tag On this page Attach a tag Atlan allows users to add tags to assets. You can use them to identify key characteristics of assets or group them together for usage or data protection. Atlan also supports attaching tags imported from the following supported sources: Databricks dbt Google BigQuery Snowflake For tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar. Did you know? Tag propagation is disabled by default in Atlan. You can enable tag propagation to child and downstream assets. Directly tag an asset ​ To directly tag an asset: In the left menu from any screen in Atlan, click Assets . On the Assets page, click an asset to view its asset profile. Under Tags in the right menu, click the + icon. In the popup, check the boxes to select one or more tags for the asset. No propagation is the default setting. Next to your selected tag(s) in the popup, click Edit to configure the propagation of tags: Click Hierarchy & lineage to allow propagation of tags to the child and downstream assets. Click Hierarchy only (no lineage) to allow propagation of tags to the child assets only. Click No propagation to disallow any propagation of tags. (Optional) For tags imported from supported sources, you can configure the following: For Snowflake assets , you can attach a Snowflake tag . If reverse sync is enabled , any updates made in Atlan will also be synced to Snowflake. If reverse sync is disabled, updates will be restricted to Atlan. Under Snowflake tags , select a synced Snowflake tag and then: Click the Select tag value dropdown to attach an allowed value from a predefined list, if available. For Add value , enter a tag value of your choice, if no predefined allowed values are present. Tag values added in Atlan are case-sensitive. For dbt Cloud or dbt Core assets, you can attach a dbt tag . For Google BigQuery assets, you can attach a Google BigQuery tag . For Databricks assets, you can attach a Databricks tag and tag values. If reverse sync is enabled , any updates made in Atlan will also be synced to Databricks. If reverse sync is disabled, updates will be restricted to Atlan. danger If there are multiple synced tags mapped to an Atlan tag , you will only be able to select one synced tag. You can also only select imported tags that belong to the same connection as the selected asset. Click Update to confirm your selections. Click Save to save the tag(s) to your asset. (Optional) Hover over the attached tag to view tag propagation details in a popover, including username of the user who applied the tag, mode of tag propagation, and when the tag was configured. (Optional) Filter tagged assets by attached tags , including tags imported from supported sources. For reverse sync to work for tags imported from Snowflake and Databricks , first ensure that reverse sync is enabled on the imported tag and then you must attach the imported tag to the asset (complete step 6 above). Did you know? You can remove tags from your tagged assets. You can also add tags to your column assets directly from Google Sheets. Tags: connectors data Previous Delete a tag Next Remove a tag Directly tag an asset"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/manage-policies",
    "content": "Build governance Stewardship Policy Management Manage policies On this page Manage policies Who can do this? You must be an admin user in Atlan to enable , create , manage , and approve data governance policies. Once you have created policies, you can manage and revise your policies, monitor policy breaches, report incidents, and more from the Policy center dashboard. Monitor policies ​ To monitor your policies and take action: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Policy center . From the Overview tab in the Policy Center , you can: Review the Action Required section: Click Open incidents to take action on any open incidents. You can filter incidents by Closed , Open , or In-progress status. Select any incident to view more details and assets linked to the incident. Click Policy breaches to examine policy breaches, which occur when any one condition is breached or violated for a particular policy. View the total count of ungoverned assets in your Atlan workspace. Ungoverned assets refer to assets that are not governed by any policy in Atlan. Click Policies to approve to review and approve policies pending your approval. In the Policies section, view recent, draft, or starred policies. In the At a glance section, view policies by policy type. Under My Policies , view all the policies you have either created or are designated as an owner. Change to the All policies tab to view all the policies in your Atlan workspace. You can filter policies by status   - Active , Draft , or Deprecated . Select any policy to view more details. Change to the Reporting tab to monitor activity related to all policies in Atlan. For All Policies , visualize your policies in Atlan by type or status. For Assets governed over time , visualize trends in your governed assets over time. For Active policies by type , view active policies by policy type. For Assets with multiple policies , view assets that fall under the purview of multiple policies. For Policies with exceptions , view policies with defined exceptions. For Activity , view an activity log for all your policies in Atlan. Revise a policy ​ Atlan currently only supports revising an active policy that is within its validity period. If a policy comes to the end of its validity period and you decide not to extend or revise it, the policy will be deprecated at the end of its validity period. Any assets within the scope of that expired policy will be automatically delinked and no new incidents will be generated. You will still be able to view the deprecated policy and assets within the scope of that policy in the policy center. If you revise an existing policy, Atlan will create a new draft of that existing policy with the same details. You can then revise the policy details and set a new validity period. Only when the new version has been approved and becomes active, the previous version will be deprecated, and stop scanning assets and generating any new incidents. Note that the workflow linking the revised policy to your selected assets has to run successfully in the background for the revised policy to become active and govern linked assets. To revise an existing policy: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Policy center . Select a policy to revise. From the top right, click the Revise policy button. This will create a new draft version of your existing policy. The previous policy will still be under enforcement until the new policy is approved, becomes active, and successfully linked to selected assets. Edit the draft policy to make any changes, such as updating the validity period. Submit the revised policy for approval. Only when the new version has been approved and becomes active, the previous version will be deprecated. You can only delete a draft or published policy in Atlan if you're an owner of that policy. Report an incident ​ Once you have created data governance policies, Atlan will scan governed assets for any incidents and report them in near real time. If any changes not compliant with your policy definition and compliance rules are detected among governed assets, Atlan will generate incidents automatically and notify the policy owners from the policy center. You can then take action on open incidents. In addition to automated incident reporting, you can manually report incidents that may warrant attention. To manually report an incident: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Policy center . Under Action Required , click Open incidents to open the Incidents sidebar. From the Incidents sidebar, click Add new incident to manually report an incident. In the Report new incident , enter the following details: For Brief Description , enter a brief description of the issue you have detected. For In-depth description , add details about the issue, including steps to reproduce the issue. (Optional) For Related Policy , select an impacted policy, if any. (Optional) For Add assets , select any and all impacted assets that may apply. Click Submit incident to submit your incident report. Tags: atlan documentation Previous Create policies Next Revoke data access Monitor policies Revise a policy Report an incident"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/manage-tasks",
    "content": "Build governance Stewardship Workflow Management Manage tasks On this page Manage tasks Who can do this? Anyone with access to Atlan   -  admin, member, or guest user   -  can use the inbox. The inbox in Atlan allows you to take action on and monitor requests waiting for your approval. Additionally, you can track any requests that you may have raised. For requesters, the task can help you: Track the progress of your requests as soon as you have raised them. Manage your alerts and requests all in one place. View requests listed in order of when the approval timeline expires. For approvers, the inbox can help you: Get notified immediately for requests that require your approval. Manage your alerts, approvals, and tasks all in one place. View tasks listed in order of when the approval timeline expires. Approve or reject requests, along with comments. Completed requests and tasks in your inbox are retained throughout the lifecycle of the Atlan instance for your organization. Manage requests ​ To manage requests: From the top right of any screen in Atlan, click the Inbox icon. In the inbox, switch to the Created by me tab to track your requests. (Optional) Filter requests by Open , Approved , Rejected , or Archived status. (Optional) To withdraw your request, in the top right of your request page, click the Withdraw button. Manage tasks ​ To manage tasks: From the top right of any screen in Atlan, click the Inbox icon. In the inbox, from the Assigned to me tab, select a task to review. (Optional) Filter tasks by status   - Open , Approved, Rejected , or Archived . To take action on a task, from the top right of the task page, you can either: Click Approve to approve the request. (Optional) In the Approve dialog, for Add comment , add a comment and then click Approve . Click Reject to reject the request and send back for revision. (Optional) In the Reject dialog, for Add comment , add a comment and then click Reject . Get notified on Slack ​ If your organization's Slack account is integrated with Atlan , you will receive Slack notifications for your approvals and requests. To receive Slack notifications on your approvals and requests: Integrate Slack and Atlan -  for Request notifications , toggle on the slider to receive Slack notifications when requests are raised in Atlan and approve or reject them directly from Slack. The email address used for Slack and Atlan should be the same, even if you haven't personally integrated the accounts. The Slack app should have been installed before August 12, 2022. If installed later, you'll need to update Slack. If different email addresses were used for Slack and Atlan, you'll first need to link your Slack account with Atlan . Tags: alerts monitoring notifications Previous Manage governance workflows Next Automate policy compliance Manage requests Manage tasks Get notified on Slack"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/data-quality/snowflake/how-tos/set-up-snowflake",
    "content": "Build governance Data Quality Studio Snowflake Data Quality Get Started Set up Snowflake On this page Set up Snowflake Private Preview This guide walks through configuring Snowflake to work with Atlan's data quality studio by creating the required roles, setting up database objects, and granting the necessary privileges. System requirements ​ Before setting up the integration, make sure you meet the following requirements: Snowflake Enterprise or Business Critical edition Dedicated Snowflake warehouse for running DQ-related queries Prerequisites ​ Before you begin, complete the following steps: Obtain ACCOUNTADMIN role or equivalent administrative privileges in Snowflake Identify your dedicated warehouse name for DQ operations Have access to create a new Snowflake user for Atlan Review Data Quality permissions to understand required privileges Create roles ​ Create two roles for the integration: -- Create DQ Admin Role CREATE ROLE IF NOT EXISTS dq_admin ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE dq_admin ; -- Create Atlan Service Role CREATE ROLE IF NOT EXISTS atlan_dq_service_role ; GRANT OPERATE , USAGE ON WAREHOUSE \"<warehouse-name>\" TO ROLE atlan_dq_service_role ; Create user ​ Create a dedicated Snowflake user for Atlan following your organization's standards, then grant the service role: GRANT ROLE atlan_dq_service_role TO USER < atlan_dq_user > ; Set up database objects ​ Create the database, schemas, and stored procedure required for Atlan data quality operations. Create the required database structure: -- Create database CREATE DATABASE ATLAN_DQ ; -- Create schemas CREATE SCHEMA ATLAN_DQ . SHARED ; CREATE SCHEMA IF NOT EXISTS ATLAN_DQ . DMFS ; The ATLAN_DQ database serves as a container for all objects related to Atlan Data Quality. The ATLAN_DQ.SHARED schema provides a separate namespace for shared procedures and functions, while the ATLAN_DQ.DMFS schema is required for custom Data Metric Functions (DMFs). Create stored procedure for DMF management: View procedure code /** * Manages Data Metric Functions (DMF) operations for Snowflake tabular entities. * This procedure handles various DMF operations including: * - Creating and managing DMFs (CREATE_DMF) * - Attaching/detaching DMFs to entities (ATTACH_DMF, DETACH_DMF) * - Managing DMF schedules (UPDATE_SCHEDULE) * - Executing SQL expressions (EXECUTE_SQL) * - Validating SQL permissions (VALIDATE_SQL_PERMISSIONS) * * The procedure runs with the privileges of the procedure owner and includes comprehensive * validation of all inputs and permissions before executing any operations. * * @param { string } ACTION - Operation to perform (ATTACH_DMF, DETACH_DMF, SUSPEND_DMF, RESUME_DMF, UPDATE_SCHEDULE, CREATE_DMF, EXECUTE_SQL, VALIDATE_SQL_PERMISSIONS) * @param { string } ENTITY_TYPE - Type of entity (TABLE, VIEW, MATERIALIZED VIEW, EXTERNAL TABLE, ICEBERG TABLE) * @param { string } ENTITY_NAME - Fully qualified name of the entity (database.schema.name) * @param { string } [ DMF_NAME = null ] - Fully qualified name of the DMF (database.schema.name) * @param { string } [DMF_ARGUMENTS_JSON='[]'] - JSON string containing column configurations * @param { string } [ SCHEDULE_TYPE = null ] - Schedule type (MINUTES, CRON, ON_DATA_CHANGE, NOT_SCHEDULED) * @param { string } [ SCHEDULE_VALUE = null ] - Schedule value based on type * @param { string } [ DMF_DEFINITION = null ] - SQL expression defining the DMF * @param { string } [ ROLE_TO_CHECK = null ] - Role to check permissions for * @param { string } [ DATABASES_TO_CHECK = null ] - Comma-separated list of databases to validate permissions for * @param { string } [ SCHEMAS_TO_CHECK = null ] - Comma-separated list of schemas to validate permissions for * @param { string } [ TABLES_TO_CHECK = null ] - Comma-separated list of tables to validate permissions for * @returns { string } - JSON string with operation status and result message */ CREATE OR REPLACE SECURE PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( ACTION STRING , ENTITY_TYPE STRING DEFAULT NULL , ENTITY_NAME STRING DEFAULT NULL , DMF_NAME STRING DEFAULT NULL , DMF_ARGUMENTS_JSON STRING DEFAULT '[]' , SCHEDULE_TYPE STRING DEFAULT NULL , SCHEDULE_VALUE STRING DEFAULT NULL , DMF_DEFINITION STRING DEFAULT NULL , ROLE_TO_CHECK STRING DEFAULT NULL , DATABASES_TO_CHECK STRING DEFAULT NULL , SCHEMAS_TO_CHECK STRING DEFAULT NULL , TABLES_TO_CHECK STRING DEFAULT NULL ) RETURNS STRING LANGUAGE JAVASCRIPT EXECUTE AS OWNER AS $$ // -----------------------------------------------------UTILITY FUNCTIONS----------------------------------------------------- /** * Executes a SQL query with parameters * @param { string } sqlText - SQL statement to execute * @param { Array } [binds=[]] - Array of bind parameters for the query * @param { boolean } [ returnFirstRow = false ] - Whether to return only the first row * @returns { Object } Object containing execution result or error information */ function executeQuery ( sqlText , binds = [ ] , returnFirstRow = false ) { try { if ( ! sqlText ) return { isErrored : true , message : \"SQL Text is required\" , result : null , } ; const statement = snowflake . createStatement ( { sqlText , binds } ) ; const result = statement . execute ( ) ; const response = { isErrored : false , message : \"\" , result : null , } ; if ( returnFirstRow ) { response . result = result . next ( ) ? result : null ; return response ; } response . result = result ; return response ; } catch ( err ) { return { isErrored : true , message : ` ${ err . code } - ${ err . message } - ${ sqlText } with binds: ${ binds . join ( \", \" ) } ` , result : null , } ; } } /** * Safely parses a JSON string * @param { string } jsonString - JSON string to parse * @returns { Object } Parsed JSON object or null if invalid */ function safelyParseJSON ( jsonString ) { try { return JSON . parse ( jsonString ) ; } catch ( err ) { return null ; } } /** * Validates a number within a range * @param { string } value - Number to validate * @param { number } min - Minimum value * @param { number } max - Maximum value * @returns { boolean } True if number is valid * @returns { boolean } False if number is invalid */ function isNumberValid ( value , min , max ) { const num = parseInt ( value , 10 ) ; return ! isNaN ( num ) && num >= min && num <= max ; } /** * Escapes and quotes a Snowflake identifier * @param { string } identifier - Raw identifier to normalize * @returns { string } Properly quoted identifier safe for SQL */ function normalizeIdentifier ( identifier ) { return ` \" ${ identifier . replace ( / \" / g , '\"\"' ) } \" ` ; } /** * Retrieves all columns for a given entity. Validates that the entityexists and procedure owner has access to it. * @param { string } entityName - Fully qualified entity name * @returns { Array } Array of column objects with name and dataType properties * @throws { Error } If entity doesn't exist or is inaccessible */ function getAllColumnsForEntity ( entityName ) { const sqlText = \"SHOW COLUMNS IN IDENTIFIER(?)\" ; const binds = [ entityName ] ; const { result , isErrored , message } = executeQuery ( sqlText , binds ) ; if ( isErrored ) { // Validates that the entity exists and procedure owner has access to it throw new Error ( message ) ; } const columns = [ ] ; while ( result . next ( ) ) { const column = { name : result . getColumnValue ( \"column_name\" ) , dataType : JSON . parse ( result . getColumnValue ( \"data_type\" ) ) . type , } ; if ( column . dataType === \"FIXED\" ) column . dataType = \"NUMBER\" ; columns . push ( column ) ; } return columns ; } /** * Validates that the DMF is valid and exists * @param { string } dmfName - Fully qualified name of the DMF * @param { string } dmfArguments - Arguments for the DMF * @returns { boolean } Whether the DMF is valid * @throws { Error } If DMF is invalid */ function isDMFValid ( dmfName , dmfArguments ) { const { isErrored , message } = executeQuery ( ` DESCRIBE FUNCTION IDENTIFIER(?)( ${ dmfArguments } ) ` , [ dmfName ] , true ) ; if ( isErrored ) throw new Error ( message ) ; return true ; } /** * Checks if a timezone is valid * @param { string } timezone - Timezone to validate * @returns { boolean } True if timezone is valid * @returns { boolean } False if timezone is invalid */ function isTimezoneValid ( timezone ) { const result = executeQuery ( ` SELECT CONVERT_TIMEZONE(?, CURRENT_TIMESTAMP()) ` , [ timezone ] , true ) ; return ! result . isErrored ; } /** * Generates a DMF type signature based on the arguments and entity columns * @param { Array } dmfArguments - Array of DMF arguments * @param { Object } entityColumnsMap - Map of entity names to column objects in the format { <ENTITY_NAME>: [ { name: <COLUMN_NAME> , dataType: <DATA_TYPE> } ] } * @param { string } baseEntityName - Name of the base entity * @returns { string } DMF type signature * @throws { Error } If entity not found in the cache */ function generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) { if ( ! dmfArguments || ! dmfArguments . length ) return \"\" ; const baseEntityColumns = entityColumnsMap [ baseEntityName ] ; if ( ! baseEntityColumns ) { throw new Error ( ` Entity ${ baseEntityName } not found in the cache ` ) ; } const baseEntityColumnArguments = dmfArguments . filter ( param => param . type === \"COLUMN\" ) . map ( param => { const column = baseEntityColumns . find ( col => col . name === param . name ) ; return column ? column . dataType : null ; } ) . join ( \", \" ) ; const baseEntityArguments = ` TABLE( ${ baseEntityColumnArguments } ) ` ; const referencedEntityArguments = dmfArguments . filter ( param => param . type === \"TABLE\" ) . map ( entityParam => { const entityName = entityParam . name ; const entityColumns = entityColumnsMap [ entityName ] ; if ( ! entityColumns ) { throw new Error ( ` Entity ${ entityName } not found in the cache ` ) ; } const columnTypes = entityParam . nested . map ( nestedParam => { const column = entityColumns . find ( col => col . name === nestedParam . name ) ; return column ? column . dataType : null ; } ) . filter ( Boolean ) . join ( \", \" ) ; return ` TABLE( ${ columnTypes } ) ` ; } ) ; const arguments = [ baseEntityArguments , ... referencedEntityArguments ] . join ( \", \" ) ; return arguments ; } /** * Generates DMF arguments for SQL statements * @param { string } dmfArguments - Array of DMF arguments * @returns { string } Formatted DMF arguments for SQL statements */ function generateDMFColumnArguments ( dmfArguments ) { return dmfArguments . map ( param => { if ( param . type === \"COLUMN\" ) { return normalizeIdentifier ( param . name ) ; } // Handle TABLE type with nested columns return ` TABLE( ${ normalizeIdentifier ( param . name ) } ( ${ param . nested . map ( nested => normalizeIdentifier ( nested . name ) ) . join ( \", \" ) } )) ` ; } ) . join ( \", \" ) ; } /** * Generates function parameters from DMF arguments * @param { Array } dmfArguments - Array of DMF arguments * @returns { string } Formatted function parameters */ function generateFunctionParameters ( dmfArguments ) { return dmfArguments . map ( param => { if ( param . type === \"TABLE\" ) { const nestedParams = param . nested . map ( nested => ` ${ nested . name } ${ nested . dataType } ` ) . join ( \", \" ) ; return ` ${ param . name } TABLE( ${ nestedParams } ) ` ; } return ` ${ param . name } ${ param . dataType } ` ; } ) . join ( \", \" ) ; } // -----------------------------------------------------VALIDATION FUNCTIONS----------------------------------------------------- /** * Validates that mandatory arguments are provided and valid * @throws { Error } If any mandatory argument is missing or invalid */ function validateMandatoryArguments ( ) { const VALID_ACTIONS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" , \"UPDATE_SCHEDULE\" , \"CREATE_DMF\" , \"EXECUTE_SQL\" , \"VALIDATE_SQL_PERMISSIONS\" ] ) ; const VALID_ENTITY_TYPES = new Set ( [ \"TABLE\" , \"VIEW\" , \"MATERIALIZED VIEW\" , \"EXTERNAL TABLE\" , \"ICEBERG TABLE\" ] ) ; const DMF_OPS = new Set ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] ) ; const VALID_SCHEDULE_TYPES = new Set ( [ \"MINUTES\" , \"CRON\" , \"ON_DATA_CHANGE\" , \"NOT_SCHEDULED\" ] ) ; const SCHEDULE_TYPES_THAT_REQUIRE_VALUE = new Set ( [ \"MINUTES\" , \"CRON\" ] ) ; if ( ! VALID_ACTIONS . has ( ACTION ) ) throw new Error ( ` Invalid ACTION: \" ${ ACTION } \". Valid options are ${ Array . from ( VALID_ACTIONS ) . join ( \", \" ) } ` ) ; if ( ENTITY_TYPE && ! VALID_ENTITY_TYPES . has ( ENTITY_TYPE ) ) throw new Error ( ` Invalid ENTITY_TYPE: \" ${ ENTITY_TYPE } \". Valid options are ${ Array . from ( VALID_ENTITY_TYPES ) . join ( \", \" ) } ` ) ; if ( DMF_OPS . has ( ACTION ) && ! DMF_NAME ) throw new Error ( \"DMF_NAME is required for DMF related actions\" ) ; if ( ACTION === \"UPDATE_SCHEDULE\" ) { if ( ! SCHEDULE_TYPE ) throw new Error ( \"SCHEDULE_TYPE is required for SCHEDULE action\" ) ; if ( ! VALID_SCHEDULE_TYPES . has ( SCHEDULE_TYPE ) ) throw new Error ( ` Invalid schedule type: \" ${ SCHEDULE_TYPE } \". Valid options are ${ Array . from ( VALID_SCHEDULE_TYPES ) . join ( \", \" ) } ` ) ; if ( SCHEDULE_TYPES_THAT_REQUIRE_VALUE . has ( SCHEDULE_TYPE ) && ! SCHEDULE_VALUE ) throw new Error ( \"SCHEDULE_VALUE is required for SCHEDULE action\" ) ; } if ( ACTION === \"EXECUTE_SQL\" && ! DMF_DEFINITION ) { throw new Error ( \"Please provide a SQL query to execute.\" ) ; } if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { if ( ! DMF_DEFINITION ) { throw new Error ( \"Please provide a SQL query to validate permissions.\" ) ; } if ( ! ROLE_TO_CHECK ) { throw new Error ( \"Failed to fetch role to check permissions. Please ensure the role exists and is accessible.\" ) ; } if ( ! DATABASES_TO_CHECK && ! SCHEMAS_TO_CHECK && ! TABLES_TO_CHECK ) { throw new Error ( \"No databases, schemas, or tables provided. Please provide at least one database, schema, or table to validate permissions for.\" ) ; } } } /** * Parses a fully qualified name into its components * @param { string } fullyQualifiedName - Fully qualified name to parse * @returns { Object } Object with database, schema, and name properties * @throws { Error } If invalid fully qualified name */ function validateFullyQualifiedName ( fullyQualifiedName ) { const parts = fullyQualifiedName . split ( \".\" ) . map ( part => part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length !== 3 ) throw new Error ( ` Invalid fully qualified name: ${ fullyQualifiedName } . Expected format: database.schema.name ` ) ; } /** * Validates the structure of DMF arguments JSON * @param { string } rawDMFArguments - Raw JSON string of DMF arguments * @throws { Error } If DMF arguments structure is invalid */ function validateDMFArgumentsStructure ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) => param . type === \"TABLE\" ) ; if ( referencedEntities . length > 1 ) throw new Error ( \"Only one referenced entity is allowed\" ) ; const validationFunctions = { arrayItem : ( param ) => [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) && param . name , nestedItem : ( param ) => [ \"COLUMN\" ] . includes ( param . type ) && param . name , } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; if ( referencedEntities . length > 0 ) { for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } } /** * Validates that all specified columns exist in an entity * @param { Array } columnsToCheck - Array of column names to validate * @param { Array } entityColumns - Array of column metadata from the entity * @param { string } entityName - Name of the entity for error message * @throws { Error } If any column doesn't exist in the entity */ function validateColumnsExistInEntity ( entityName , allColumnsInEntity , columnsToCheck ) { for ( const column of columnsToCheck ) { if ( ! allColumnsInEntity . some ( col => col . name === column ) ) throw new Error ( ` Column ${ column } not found in entity ${ entityName } ` ) ; } } /** * Validates that all provided identifiers exist and are accessible * Checks entity names, column names, and DMF compatibility * @param { string } entityName - Fully qualified name of the entity * @param { string } dmfName - Fully qualified name of the DMF * @param { Array } dmfArguments - Array of DMF arguments * @throws { Error } If any identifier doesn't exist or is inaccessible */ function validateProvidedIdentifiers ( entityName , dmfName = \"\" , dmfArguments = [ ] ) { if ( ! entityName ) { throw new Error ( \"Please provide a valid entity name. The entity name is required to validate identifiers.\" ) ; } validateFullyQualifiedName ( entityName ) ; // Validate the provided entity names and store all the columns for each entity in a map const baseEntityName = entityName ; const baseEntityAllColumns = getAllColumnsForEntity ( entityName ) ; const entityColumnsMap = { [ baseEntityName ] : baseEntityAllColumns } ; const allReferencedEntities = dmfArguments . filter ( param => param . type === \"TABLE\" ) ; for ( const referencedEntity of allReferencedEntities ) { const columns = getAllColumnsForEntity ( referencedEntity . name ) ; entityColumnsMap [ referencedEntity . name ] = columns ; } // Valite all of the provided columns are valid and exist in their respective entities const allBaseEntityColumnsInArguments = dmfArguments . filter ( param => param . type === \"COLUMN\" ) . map ( param => param . name ) ; validateColumnsExistInEntity ( baseEntityName , baseEntityAllColumns , allBaseEntityColumnsInArguments ) ; for ( const referencedEntity of allReferencedEntities ) { const columnsInArguments = referencedEntity . nested . map ( nestedParam => nestedParam . name ) ; validateColumnsExistInEntity ( referencedEntity . name , entityColumnsMap [ referencedEntity . name ] , columnsInArguments ) ; } if ( dmfName ) { // Validate that the DMF is valid and exists const generatedTypeSignature = generateDMFTypeSignature ( dmfArguments , entityColumnsMap , baseEntityName ) ; isDMFValid ( dmfName , generatedTypeSignature ) ; } // All provided identifiers are valid, actually exist and are accessible to the procedure owner } /** * Validates CRON expression syntax * Performs detailed validation of all CRON components and timezones to protect against SQL injection * @param { string } cronExpression - CRON expression to validate * @throws { Error } If CRON expression is invalid */ function validateCronExpression ( cronExpression ) { if ( cronExpression . length > 100 ) throw new Error ( \"Cron expression is too long\" ) ; const cronFields = cronExpression . trim ( ) . split ( / \\s + / ) ; if ( cronFields . length !== 6 ) throw new Error ( \"Invalid cron expression. Expected 6 fields\" ) ; const [ minute , hour , dayOfMonth , month , dayOfWeek , timezone ] = cronFields ; const isTimezoneValidResult = isTimezoneValid ( timezone ) ; if ( ! isTimezoneValidResult ) throw new Error ( \"Invalid timezone provided in the cron expression\" ) ; const regexPatterns = { minute : / ^ ( \\* | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , hour : / ^ ( \\* | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , dayOfMonth : / ^ ( \\* | L | \\d + | \\* \\/ \\d + | \\d + \\- \\d + | \\d + ( , \\d + ) * ) $ / , month : / ^ ( \\* | \\d + | JAN | FEB | MAR | APR | MAY | JUN | JUL | AUG | SEP | OCT | NOV | DEC | \\* \\/ \\d + | \\d + \\- \\d + | [ A - Z ] {3} \\- [ A - Z ] {3} | \\d + ( , \\d + ) * | ( [ A - Z ] {3} ( , [ A - Z ] {3} ) * ) ) $ / i , dayOfWeek : / ^ ( \\* | \\d + | SUN | MON | TUE | WED | THU | FRI | SAT | \\d + L | [ A - Z ] {3} L | \\* \\/ \\d + | \\d + \\- \\d + | [ A - Z ] {3} \\- [ A - Z ] {3} | \\d + ( , \\d + ) * | ( [ A - Z ] {3} ( , [ A - Z ] {3} ) * ) ) $ / i , } ; if ( minute . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( minute , 0 , 59 ) ) throw new Error ( \"Invalid minute value\" ) ; if ( hour . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( hour , 0 , 23 ) ) throw new Error ( \"Invalid hour value\" ) ; if ( dayOfMonth . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfMonth , 1 , 31 ) ) throw new Error ( \"Invalid day of month value\" ) ; if ( month . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( month , 1 , 12 ) ) throw new Error ( \"Invalid month value\" ) ; if ( dayOfWeek . match ( / ^ \\d + $ / ) ) if ( ! isNumberValid ( dayOfWeek , 0 , 6 ) ) throw new Error ( \"Invalid day of week value\" ) ; if ( ! regexPatterns . minute . test ( minute ) || ! regexPatterns . hour . test ( hour ) || ! regexPatterns . dayOfMonth . test ( dayOfMonth ) || ! regexPatterns . month . test ( month ) || ! regexPatterns . dayOfWeek . test ( dayOfWeek ) ) throw new Error ( \"Invalid cron expression\" ) ; } /** * Validates schedule-specific arguments * Ensures schedule type and value are compatible and valid * @throws { Error } If schedule configuration is invalid */ function validateProvidedArgumentsForSchedule ( ) { const VALID_MINUTES = new Set ( [ \"5\" , \"15\" , \"30\" , \"60\" , \"720\" , \"1440\" ] ) ; if ( SCHEDULE_TYPE === \"MINUTES\" && ! VALID_MINUTES . has ( SCHEDULE_VALUE ) ) throw new Error ( ` Invalid SCHEDULE_VALUE for MINUTES. Valid options are ${ Array . from ( VALID_MINUTES ) . join ( \", \" ) } ` ) ; if ( SCHEDULE_TYPE === \"CRON\" ) validateCronExpression ( SCHEDULE_VALUE ) ; // SCHEDULE_VALUE is valid for the provided SCHEDULE_TYPE } /** * Validates DMF arguments with dataType checks * @param { string } rawDMFArguments - Raw JSON string of DMF arguments * @throws { Error } If DMF arguments structure is invalid or dataType is missing */ function validateDMFArgumentsWithDataType ( rawDMFArguments ) { const parsedStructure = safelyParseJSON ( rawDMFArguments ) ; if ( ! parsedStructure ) throw new Error ( \"Invalid DMF_ARGUMENTS_JSON. Expected a valid JSON string\" ) ; if ( ! Array . isArray ( parsedStructure ) ) throw new Error ( \"DMF_ARGUMENTS_JSON must be an array\" ) ; const validationFunctions = { arrayItem : ( param ) => { if ( ! [ \"COLUMN\" , \"TABLE\" ] . includes ( param . type ) || ! param . name ) { return false ; } if ( param . type === \"COLUMN\" && ! param . dataType ) { throw new Error ( ` Missing dataType for COLUMN parameter: ${ param . name } ` ) ; } return true ; } , nestedItem : ( param ) => { if ( ! [ \"COLUMN\" ] . includes ( param . type ) || ! param . name ) { return false ; } if ( ! param . dataType ) { throw new Error ( ` Missing dataType for nested COLUMN parameter: ${ param . name } ` ) ; } return true ; } } ; if ( ! parsedStructure . every ( validationFunctions . arrayItem ) ) throw new Error ( \"Each parameter must have a valid type(TABLE/COLUMN) and name field\" ) ; const referencedEntities = parsedStructure . filter ( ( param ) => param . type === \"TABLE\" ) ; for ( const referencedEntity of referencedEntities ) { if ( ! Array . isArray ( referencedEntity . nested ) || ! referencedEntity . nested . every ( validationFunctions . nestedItem ) ) throw new Error ( \"Invalid nested parameters\" ) ; } } /** * Validates DMF name format * @param { string } dmfName - Fully qualified name of the DMF * @throws { Error } If DMF name format is invalid */ function validateDmfName ( dmfName ) { const parts = dmfName . split ( \".\" ) . map ( part => part . trim ( ) ) . filter ( Boolean ) ; if ( parts . length !== 3 ) { throw new Error ( ` Invalid DMF_NAME: ${ dmfName } . Expected format: database.schema.name ` ) ; } } /** * Validates that the provided SQL is read-only and doesn't contain dangerous operations * @param { string } sqlExpression - SQL to validate * @returns { boolean } Whether the SQL is safe * @throws { Error } If SQL contains potentially dangerous operations */ function validateSqlExpression ( sqlExpression ) { if ( ! sqlExpression ) { throw new Error ( \"Please provide a SQL query. The SQL expression cannot be empty.\" ) ; } // Step 1: Normalize Unicode characters to prevent encoding-based attacks const normalizedSql = sqlExpression . normalize ( 'NFKC' ) ; // Step 2: Check for multiple statements (handled by splitIntoSqlStatements) splitIntoSqlStatements ( normalizedSql ) ; // Step 3: Check whether it is a read-query or not if ( ! isReadQuery ( normalizedSql ) ) { throw new Error ( \"Your query must start with SELECT or WITH. Only read operations are allowed.\" ) ; } // Step 4: Check for suspicious patterns that might bypass filters checkForSuspiciousPatterns ( normalizedSql ) ; // Step 5: Check for dangerous operations const dangerousOperation = containsDangerousOperation ( normalizedSql ) ; if ( dangerousOperation ) { throw new Error ( \"For security reasons, this operation is not permitted. Please use only read operations in your query.\" ) ; } return true ; } /** * Enhanced detection of suspicious SQL patterns * @param { string } sql - SQL query to check * @throws { Error } If suspicious patterns are detected */ function checkForSuspiciousPatterns ( sql ) { // Create a copy where string literals are masked to prevent false positives const sqlWithoutStrings = sql . replace ( / ' [ ^ ' ] * ' / g , \"'STRING_LITERAL'\" ) . replace ( / \" [ ^ \" ] * \" / g , '\"STRING_LITERAL\"' ) ; const suspiciousPatterns = [ // Common SQL injection techniques { pattern : / \\b OR \\s + [ 0 - 9 ] + \\s * = \\s * [ 0 - 9 ] + \\b / i , message : \"Suspicious always-true condition detected\" } , // Alias abuse detection { pattern : / \\b AS \\s + [ '\"` ] ? . *? ( DELETE | INSERT | UPDATE | DROP | ALTER | EXEC ) \\b / i , message : \"Suspicious alias detected\" } , // Hex encoding and other obfuscation techniques { pattern : / 0x [ 0 - 9 a - f ] {10,} / i , message : \"Suspicious hex encoding detected\" } , { pattern : / CHAR \\s * \\( \\s * \\d + ( \\s * , \\s * \\d + ) + \\s * \\) / i , message : \"Character code conversion functions are not allowed\" } , ] ; // Check for suspicious patterns outside of string literals for ( const { pattern , message } of suspiciousPatterns ) { if ( pattern . test ( sqlWithoutStrings ) ) { throw new Error ( message ) ; } } } /** * Splits SQL into separate statements based on semicolons not in quotes * @param { string } sql - SQL query * @returns { string } - SQL query without semicolons */ function splitIntoSqlStatements ( sql ) { let inSingleQuote = false ; let inDoubleQuote = false ; for ( let i = 0 ; i < sql . length ; i ++ ) { const char = sql [ i ] ; // Handle quotes if ( char === \"'\" && sql [ i - 1 ] !== '\\\\' ) { inSingleQuote = ! inSingleQuote ; } else if ( char === '\"' && sql [ i - 1 ] !== '\\\\' ) { inDoubleQuote = ! inDoubleQuote ; } // If semicolon outside of quotes, throw error if ( char === ';' && ! inSingleQuote && ! inDoubleQuote ) { throw new Error ( \"Do not use semicolons to break or end your SQL statement. Submit your query without any semicolons.\" ) ; } } // If we get here, there were no semicolons outside quotes return sql . trim ( ) ; } /** * Checks if the SQL is a read-only query * @param { string } sql - SQL query without comments * @returns { boolean } - True if it's a read-only query */ function isReadQuery ( sql ) { const normalizedSql = sql . replace ( / \\s + / g , ' ' ) . toUpperCase ( ) . trim ( ) ; if ( normalizedSql . startsWith ( 'SELECT ' ) ) { return true ; } if ( normalizedSql . startsWith ( 'WITH ' ) ) { return true ; } return false ; } /** * Checks if SQL contains any dangerous operations - using single keywords with word boundaries * @param { string } sql - SQL query without comments * @returns { string | null } - The dangerous operation found or null if safe */ function containsDangerousOperation ( sql ) { // Normalize whitespace and convert to uppercase for comparison const normalizedSql = sql . replace ( / \\s + / g , ' ' ) . toUpperCase ( ) ; // Snowflake-specific dangerous commands - using single keywords with high precision const dangerousCommands = [ // Data Modification 'INSERT' , 'UPDATE' , 'DELETE' , 'MERGE' , 'TRUNCATE' , 'COPY' , // DDL statements 'CREATE' , 'DROP' , 'ALTER' , 'COMMENT' , 'GRANT' , 'REVOKE' , 'UNDROP' , // Transaction control 'BEGIN' , 'COMMIT' , 'ROLLBACK' , // System & session commands 'SET' , 'UNSET' , 'USE' , 'PUT' , 'GET' , 'REMOVE' , 'LIST' , // Information Schema & Metadata 'SHOW' , 'DESCRIBE' , // Procedures and functions 'CALL' , 'EXECUTE' , 'EXEC' , // Additional Snowflake operations 'EXPLAIN' ] ; // Dangerous functions specific to Snowflake const dangerousFunctions = [ 'SYSTEM' , 'CURRENT_USER' , 'CURRENT_ROLE' , 'CURRENT_ACCOUNT' , 'DATABASE' , 'VERSION' , 'SLEEP' , 'CALL_INTEGRATION' , 'PARSE_JSON' , 'RUN_JAVASCRIPT' , 'CALL_JAVASCRIPT' , 'TO_JAVASCRIPT' ] ; // Create a regex pattern with word boundaries for all dangerous commands const commandPattern = new RegExp ( ` \\\\b( ${ dangerousCommands . join ( '|' ) } )\\\\b ` , 'i' ) ; const functionPattern = new RegExp ( ` \\\\b( ${ dangerousFunctions . join ( '|' ) } )\\\\s*\\\\( ` , 'i' ) ; // Check for dangerous commands const commandMatch = normalizedSql . match ( commandPattern ) ; if ( commandMatch ) { return ` Dangerous operation detected: ${ commandMatch [ 0 ] } ` ; } // Check for dangerous functions const functionMatch = normalizedSql . match ( functionPattern ) ; if ( functionMatch ) { return ` Dangerous function call detected: ${ functionMatch [ 1 ] } ` ; } // Check for access to sensitive metadata if ( / \\b INFORMATION_SCHEMA \\b | \\b ACCOUNT_USAGE \\b / i . test ( normalizedSql ) ) { return 'Access to sensitive system metadata detected' ; } return null ; } /** * Executes SQL and returns a numeric result * @param { string } sql - SQL to execute * @returns { number } Numeric result * @throws { Error } If execution fails or result is not numeric */ function executeSqlAndReturnNumber ( sql ) { try { // Execute without returnFirstRow to get full result set const result = executeQuery ( sql , [ ] , false ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } // Check if the result set exists if ( ! result . result ) { throw new Error ( \"Your query didn't return any results. Please check your SQL and try again.\" ) ; } // Check number of columns const columnCount = result . result . getColumnCount ( ) ; if ( columnCount !== 1 ) { throw new Error ( \"Your query should return exactly one column. Please modify your query to return a single numeric value.\" ) ; } // Check if we have exactly one row if ( ! result . result . next ( ) ) { throw new Error ( \"Your query didn't return any rows. Please check your query and try again.\" ) ; } // Get the value const value = result . result . getColumnValue ( 1 ) ; // Check if it's a number if ( typeof value !== 'number' ) { throw new Error ( \"Your query must return a number. Please modify your query to calculate a numeric result.\" ) ; } // Check if there are more rows if ( result . result . next ( ) ) { throw new Error ( \"Your query returned multiple rows. Please modify your query to return a single result.\" ) ; } return value ; } catch ( err ) { throw new Error ( ` ${ err . message } ` ) ; } } /** * Validates all parameters for DMF creation * @throws { Error } If any validation fails */ function validateCreateDmf ( ) { validateDmfName ( DMF_NAME ) ; validateSqlExpression ( DMF_DEFINITION ) ; validateDMFArgumentsWithDataType ( DMF_ARGUMENTS_JSON ) ; } /** * Validates all provided arguments * Performs comprehensive validation on input parameters * @throws { Error } If any validation fails */ function validateAllArguments ( ) { validateMandatoryArguments ( ) ; // Validates all mandatory arguments are provided in the correct format if ( ACTION === \"CREATE_DMF\" ) { validateCreateDmf ( ) ; return ; } else if ( ACTION === \"EXECUTE_SQL\" ) { validateSqlExpression ( DMF_DEFINITION ) ; return ; } else if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { validateSqlExpression ( DMF_DEFINITION ) ; return ; } else if ( ACTION === \"UPDATE_SCHEDULE\" ) { validateProvidedArgumentsForSchedule ( ) ; // Validates the provided schedule type and value } else { validateDMFArgumentsStructure ( DMF_ARGUMENTS_JSON ) ; } validateProvidedIdentifiers ( ENTITY_NAME , DMF_NAME , safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; // All provided arguments are valid and legal } // -----------------------------------------------------MAIN FUNCTION----------------------------------------------------- /** * Extracts database, schema and table name from fully qualified entity name * @param { string } entityName - Fully qualified entity name * @returns { Object } Object containing database, schema and table name */ function parseEntityName ( entityName ) { const [ db , schema , table ] = entityName . split ( \".\" ) ; return { db , schema , table } ; } /** * Gets the owner of a table from information schema * @param { string } db - Database name * @param { string } schema - Schema name * @param { string } table - Table name * @returns { Object } Object containing success status and table owner */ function getTableOwner ( db , schema , table ) { const query = ` SELECT TABLE_OWNER FROM ${ db } .INFORMATION_SCHEMA.TABLES WHERE TABLE_CATALOG = ? AND TABLE_SCHEMA = ? AND TABLE_NAME = ? ` ; const result = executeQuery ( query , [ db , schema , table ] , true ) ; if ( result . isErrored ) { return { isSuccessful : false , message : ` Failed to get table owner: ${ result . message } ` , owner : null } ; } const owner = result . result ?. getColumnValue ( \"TABLE_OWNER\" ) ; if ( ! owner ) { return { isSuccessful : false , message : ` Could not find owner for table ${ db } . ${ schema } . ${ table } ` , owner : null } ; } return { isSuccessful : true , message : \"Successfully retrieved table owner\" , owner } ; } /** * Grants required permissions to a role * @param { string } role - Role to grant permissions to * @returns { Object } Object containing success status and message */ function grantPermissions ( role ) { const query = ` BEGIN GRANT USAGE ON SCHEMA ATLAN_DQ.DMFS TO ROLE \" ${ role } \"; GRANT USAGE ON DATABASE ATLAN_DQ TO ROLE \" ${ role } \"; GRANT USAGE ON ALL FUNCTIONS IN SCHEMA ATLAN_DQ.DMFS TO ROLE \" ${ role } \"; END; ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { return { isSuccessful : false , message : ` Failed to grant permissions: ${ result . message } ` } ; } return { isSuccessful : true , message : ` Successfully granted permissions to role ${ role } ` } ; } /** * Handles permissions for DMF operations * @param { string } entityName - Fully qualified entity name * @returns { Object } Object containing success status and message */ function handleDMFPermissions ( entityName ) { try { // Parse entity name const { db , schema , table } = parseEntityName ( entityName ) ; // Get table owner const ownerResult = getTableOwner ( db , schema , table ) ; if ( ! ownerResult . isSuccessful ) { return ownerResult ; } // Grant permissions return grantPermissions ( ownerResult . owner ) ; } catch ( err ) { return { isSuccessful : false , message : ` Error handling permissions: ${ err . message } ` } ; } } /** * Parses comma-separated object lists into arrays * @param { string } databasesToCheck - Comma-separated list of databases * @param { string } schemasToCheck - Comma-separated list of schemas * @param { string } tablesToCheck - Comma-separated list of tables * @returns { Object } Object with parsed arrays */ function parseCommaSeparatedLists ( databasesToCheck , schemasToCheck , tablesToCheck ) { return { databases : databasesToCheck ? databasesToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] , schemas : schemasToCheck ? schemasToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] , tables : tablesToCheck ? tablesToCheck . split ( ',' ) . map ( s => s . trim ( ) ) . filter ( Boolean ) : [ ] } ; } /** * Checks database access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } databases - Array of databases to check * @returns { Array } Array of accessible databases */ function checkDatabaseAccess ( roleToCheck , databases ) { const accessibleDatabases = [ ] ; for ( const database of databases ) { try { const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE = 'DATABASE' AND OBJECT_NAME = ' ${ database } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"USAGE\" || privilege === \"OWNERSHIP\" ) { accessibleDatabases . push ( database ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for database ' ${ database } '. Role ' ${ roleToCheck } ' may not have access or the database may not exist. ` ) ; } } return accessibleDatabases ; } /** * Checks schema access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } schemas - Array of schemas to check (format: database.schema) * @returns { Array } Array of accessible schemas */ function checkSchemaAccess ( roleToCheck , schemas ) { const accessibleSchemas = [ ] ; for ( const schema of schemas ) { try { const parts = schema . split ( '.' ) ; if ( parts . length !== 2 ) { throw new Error ( ` Invalid schema format: ' ${ schema } '. Expected format: database.schema ` ) ; } const [ database , schemaName ] = parts ; const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE = 'SCHEMA' AND OBJECT_CATALOG = ' ${ database } ' AND OBJECT_NAME = ' ${ schemaName } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"USAGE\" || privilege === \"OWNERSHIP\" ) { accessibleSchemas . push ( schema ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for schema ' ${ schema } '. Role ' ${ roleToCheck } ' may not have access or the schema may not exist. ` ) ; } } return accessibleSchemas ; } /** * Checks table access for a role using information schema * @param { string } roleToCheck - Role to check permissions for * @param { Array } tables - Array of tables to check (format: database.schema.table) * @returns { Array } Array of accessible tables */ function checkTableAccess ( roleToCheck , tables ) { const accessibleTables = [ ] ; for ( const table of tables ) { try { const parts = table . split ( '.' ) ; if ( parts . length !== 3 ) { throw new Error ( ` Invalid table format: ' ${ table } '. Expected format: database.schema.table ` ) ; } const [ database , schema , tableName ] = parts ; const query = ` SELECT PRIVILEGE_TYPE FROM ${ database } .INFORMATION_SCHEMA.OBJECT_PRIVILEGES WHERE GRANTEE = ' ${ roleToCheck } ' AND OBJECT_TYPE IN ('TABLE', 'VIEW') AND OBJECT_CATALOG = ' ${ database } ' AND OBJECT_SCHEMA = ' ${ schema } ' AND OBJECT_NAME = ' ${ tableName } ' ` ; const result = executeQuery ( query , [ ] ) ; if ( result . isErrored ) { throw new Error ( result . message ) ; } while ( result . result . next ( ) ) { const privilege = result . result . getColumnValue ( \"PRIVILEGE_TYPE\" ) ; if ( privilege === \"SELECT\" || privilege === \"OWNERSHIP\" ) { accessibleTables . push ( table ) ; break ; } } } catch ( err ) { throw new Error ( ` Failed to check permissions for table ' ${ table } '. Role ' ${ roleToCheck } ' may not have access or the table may not exist. ` ) ; } } return accessibleTables ; } /** * Validates SQL permissions for a given role and returns accessible objects * @param { string } sql - SQL to validate * @param { string } roleToCheck - Role to check permissions for * @param { string } databasesToCheck - Comma-separated list of databases to check access for * @param { string } schemasToCheck - Comma-separated list of schemas to check access for * @param { string } tablesToCheck - Comma-separated list of tables to check access for * @returns { Object } Object with validation result and accessible objects * @throws { Error } If SQL validation fails */ function validateSqlPermissions ( sql , roleToCheck , databasesToCheck , schemasToCheck , tablesToCheck ) { try { // Step 1: Run EXPLAIN command to validate SQL syntax and plan const explainSql = ` EXPLAIN ${ sql } ` ; const explainResult = executeQuery ( explainSql , [ ] ) ; if ( explainResult . isErrored ) { throw new Error ( \"We couldn't validate your query. Please check your SQL syntax and permissions and try again.\" ) ; } // Step 2: Parse objects to check const objectsToCheck = parseCommaSeparatedLists ( databasesToCheck , schemasToCheck , tablesToCheck ) ; // Step 3: Check access for each object type const accessibleDatabases = checkDatabaseAccess ( roleToCheck , objectsToCheck . databases ) ; const accessibleSchemas = checkSchemaAccess ( roleToCheck , objectsToCheck . schemas ) ; const accessibleTables = checkTableAccess ( roleToCheck , objectsToCheck . tables ) ; return { isSuccessful : true , message : \"SQL permissions validation successful\" , accessibleObjects : { databases : accessibleDatabases , schemas : accessibleSchemas , tables : accessibleTables } } ; } catch ( err ) { throw new Error ( ` ${ err . message } ` ) ; } } /** * Main function to manage DMF operations * Validates all arguments and executes the main logic * @returns { string } JSON string with operation status and result message * @throws { Error } If any operation step fails */ function main ( ) { validateAllArguments ( ) ; // Handle permissions for DMF attachment/detachment operations if ( [ \"ATTACH_DMF\" , \"DETACH_DMF\" , \"SUSPEND_DMF\" , \"RESUME_DMF\" ] . includes ( ACTION ) ) { const permissionResult = handleDMFPermissions ( ENTITY_NAME ) ; if ( ! permissionResult . isSuccessful ) { return JSON . stringify ( permissionResult ) ; } } // If the provided arguments are valid, proceed with the main logic const dmfArguments = generateDMFColumnArguments ( safelyParseJSON ( DMF_ARGUMENTS_JSON ) ) ; const SQL_TEMPLATES = { ATTACH_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } ADD DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) ` , DETACH_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } DROP DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) ` , SUSPEND_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } MODIFY DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) SUSPEND ` , RESUME_DMF : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } MODIFY DATA METRIC FUNCTION ${ DMF_NAME } ON ( ${ dmfArguments } ) RESUME ` , UPDATE_SCHEDULE : { MINUTES : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = ' ${ SCHEDULE_VALUE } MINUTE' ` , CRON : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = 'USING CRON ${ SCHEDULE_VALUE } ' ` , ON_DATA_CHANGE : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } SET DATA_METRIC_SCHEDULE = 'TRIGGER_ON_CHANGES' ` , NOT_SCHEDULED : ` ALTER ${ ENTITY_TYPE } ${ ENTITY_NAME } UNSET DATA_METRIC_SCHEDULE ` , } , } ; let sqlText = \"\" ; let returnMessage = \"\" ; let binds = [ ] ; if ( ACTION === \"UPDATE_SCHEDULE\" ) { sqlText = SQL_TEMPLATES [ ACTION ] [ SCHEDULE_TYPE ] ; returnMessage = ` Successfully updated schedule for ${ ENTITY_NAME } to ${ SCHEDULE_TYPE } ${ SCHEDULE_VALUE } ` ; } else if ( ACTION === \"CREATE_DMF\" ) { const DOLLAR = String . fromCharCode ( 36 ) ; // ASCII code for $ const dmfArguments = safelyParseJSON ( DMF_ARGUMENTS_JSON ) ; const functionParams = generateFunctionParameters ( dmfArguments ) ; sqlText = \"CREATE OR REPLACE DATA METRIC FUNCTION \" + DMF_NAME + \" (\" + functionParams + \" )\" + \"RETURNS NUMBER AS \" + DOLLAR + DOLLAR + \" \" + DMF_DEFINITION + \" \" + DOLLAR + DOLLAR ; returnMessage = ` DMF ${ DMF_NAME } registered successfully ` ; } else if ( ACTION === \"EXECUTE_SQL\" ) { // Execute SQL and get numeric result const result = executeSqlAndReturnNumber ( DMF_DEFINITION ) ; const response = { isSuccessful : true , message : \"SQL executed successfully\" , result : result } ; return JSON . stringify ( response ) ; } else if ( ACTION === \"VALIDATE_SQL_PERMISSIONS\" ) { const validationResult = validateSqlPermissions ( DMF_DEFINITION , ROLE_TO_CHECK , DATABASES_TO_CHECK , SCHEMAS_TO_CHECK , TABLES_TO_CHECK ) ; return JSON . stringify ( validationResult ) ; } else { sqlText = SQL_TEMPLATES [ ACTION ] ; returnMessage = ` ACTION: ${ ACTION } performed successfully on ${ ENTITY_NAME } with DMF: ${ DMF_NAME } and DMF Arguments: ${ dmfArguments } ` ; } const result = executeQuery ( sqlText , binds ) ; return JSON . stringify ( { isSuccessful : ! result . isErrored , message : result . isErrored ? result . message : returnMessage , } ) ; } // Execute the main function and return the result try { return main ( ) ; } catch ( err ) { return JSON . stringify ( { isSuccessful : false , message : err . message , } ) ; } $$ ; Transfer ownership to dq_admin role: GRANT OWNERSHIP ON DATABASE ATLAN_DQ TO ROLE dq_admin ; GRANT OWNERSHIP ON SCHEMA ATLAN_DQ . SHARED TO ROLE dq_admin ; GRANT OWNERSHIP ON SCHEMA ATLAN_DQ . DMFS TO ROLE dq_admin ; GRANT OWNERSHIP ON PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING ) TO ROLE dq_admin ; Grant privileges ​ Grant the necessary permissions to enable data quality operations and maintain proper access control. System privileges: Grant Snowflake system-level permissions to enable data metric functions and monitoring capabilities. -- For DQ Admin Role GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE dq_admin ; -- For Atlan Service Role GRANT APPLICATION ROLE SNOWFLAKE . DATA_QUALITY_MONITORING_VIEWER TO ROLE atlan_dq_service_role ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE atlan_dq_service_role ; GRANT EXECUTE TASK ON ACCOUNT TO ROLE atlan_dq_service_role ; GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE atlan_dq_service_role ; Table owner privileges: For every role that owns tables in your environment (denoted by <table_owner> ), grant the following privileges: GRANT ROLE < table_owner > TO ROLE dq_admin ; GRANT DATABASE ROLE SNOWFLAKE . DATA_METRIC_USER TO ROLE < table_owner > ; GRANT EXECUTE DATA METRIC FUNCTION ON ACCOUNT TO ROLE < table_owner > ; To identify table owner roles in your environment: -- Find table owners SELECT TABLE_CATALOG , TABLE_OWNER FROM SNOWFLAKE . ACCOUNT_USAGE . TABLES WHERE DELETED IS NULL AND TABLE_OWNER IS NOT NULL GROUP BY TABLE_CATALOG , TABLE_OWNER ; Database access: Grant Atlan's service role access to the created objects: GRANT USAGE ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ . SHARED TO ROLE atlan_dq_service_role ; GRANT USAGE ON SCHEMA ATLAN_DQ . DMFS TO ROLE atlan_dq_service_role ; GRANT USAGE ON PROCEDURE ATLAN_DQ . SHARED . MANAGE_DMF ( STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING , STRING ) TO ROLE atlan_dq_service_role ; GRANT CREATE SCHEMA ON DATABASE ATLAN_DQ TO ROLE atlan_dq_service_role ; Next steps ​ Enable data quality on connection - Configure your Snowflake connection for data quality monitoring Need help ​ If you have questions or need assistance with setting up Snowflake for data quality, reach out to Atlan Support by submitting a support request . See also ​ Data quality permissions - Understand the required permissions and roles for data quality operations Configure alerts for data quality rules - Set up real-time notifications for rule failures Tags: snowflake data-quality setup governance Previous Snowflake Data Quality Studio Next Enable data quality on connection System requirements Prerequisites Create roles Create user Set up database objects Grant privileges Next steps Need help See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/create-a-new-tag",
    "content": "Build governance Tags Get Started Create a new tag Create a new tag Who can do this? You will need to be an admin user in Atlan to create tags. To create a new tag: From the left menu of any screen, click Governance . Under the Governance heading, click Tags and then add a new tag: If there are no existing tags, click Add tag . If you have any existing tags, under the Tags heading, click the + New button. Enter details for the tag: For Untitled tag , enter a meaningful name for your tag. (Optional) For Add description..., enter a more detailed description of your tag. (Optional) To personalize your tags, click the tag icon. From the upper right of the Icons dialog: Click Icons to change the icon for your tag. Click the gray box to change the color of your tag icon to green, yellow, or red. Click Emoji to add an emoji to your tag. Click Upload Image to upload an image for your tag. The recommended size for image uploads is 24x24 pixels. Click Create . (Optional) Under Tags , click the funnel icon to filter your tags by source: Click Atlan to filter for tags created in Atlan and not synced to any external sources. Click Snowflake to filter for tags imported from Snowflake . Click dbt to filter for tags imported from dbt . That's it   -  you now have a tag ready for your team to use for tagging assets ! 🎉 For tags created in Atlan, these are displayed in sentence case by design in the governance center, asset sidebar, and tags filter. For imported tags, Atlan will display the source version only in the tag popover when you hover over the tag in the asset sidebar. Did you know? Once you've created a tag, you can also delete it at any time. Tags: atlan documentation Previous Tags Next Delete a tag"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/delete-a-tag",
    "content": "Build governance Tags Tag Management Delete a tag On this page Delete a tag Who can do this? You will need to be an admin user in Atlan to delete tags. To delete tags , you can either: If a tag is not attached to any assets, you can delete the tag right away. If a tag is attached to assets, you will need to remove the tag from the tagged assets first and then you can delete it. Delete a tag ​ To delete a tag without linked assets: From the left menu of any screen, click Governance . Under the Governance heading, click Tags . (Optional) Under Tags , click the funnel icon to filter your tags by source: Click Atlan to filter for tags created in Atlan and not synced to any external sources. Click Snowflake to filter for tags imported from Snowflake . Click dbt to filter for tags imported from dbt . Under the left Tags menu, select a tag to delete. (The total count of linked assets will be displayed as zero if there are no linked assets.) From the top right, click the trash can icon to delete the tag. In the Delete tag dialog, click Delete to confirm deletion. Delete a tag with linked assets ​ danger If a tag is attached to assets, you will need to remove the tag from the tagged assets before deleting it. To delete a tag with linked assets: From the left menu of any screen, click Governance . Under the Governance heading, click Tags . (Optional) Under Tags , click the funnel icon to filter your tags by source: Click Atlan to filter for tags created in Atlan and not synced to any external sources. Click Snowflake to filter for tags imported from Snowflake . Click dbt to filter for tags imported from dbt . Under the Tags menu, select a tag to delete. Under the tag name, click the Linked assets tab to navigate to the linked assets. To remove the tag from the linked assets, you can either: Click a linked asset to open the asset sidebar, and then under Tags in the sidebar, click the pencil icon to edit the tag. Uncheck the tag box and then click Save . Repeat the steps for each tagged asset. Create a playbook to remove the tag from tagged assets at scale. Once the tag has been removed from all the linked assets, from the top right, click the trash can icon to delete the tag. In the Delete tag dialog, click Delete to confirm. Tags: atlan documentation Previous Create a new tag Next Attach a tag Delete a tag Delete a tag with linked assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/stewardship/how-tos/revoke-data-access",
    "content": "Build governance Stewardship Access Management Revoke data access Revoke data access Who can do this? You must be an admin user to revoke data access on governed assets in Atlan. As an admin user, you can revoke data access in Atlan or from other data sources on governed assets. Governed assets are assets that are included within the scope of governance workflows . To revoke data access, complete the following steps. To revoke data access: From the left menu of any screen in Atlan, click Assets . Click on an asset to open the asset sidebar. In the right menu of the Overview sidebar, click the Access tab. In the Data access in Atlan tab, you can view the list of users that have data access to query data and preview sample data. Hover over the username to revoke data access and then click Revoke . In the Revoke data access dialog, you can: For Review and modify how (username) can access this asset in Atlan , you can revoke access provided through the following access control mechanisms: Personas Purposes Governance workflows Connection admin Click Raise Jira ticket to revoke data access on source to revoke data access through Jira. You will need to integrate with Jira Cloud and install or register a webhook to use this option. Click Raise ServiceNow request to revoke data access on source to revoke data access through ServiceNow. You will need to integrate with ServiceNow and link your ServiceNow account in Atlan to use this option. If a webhook has been configured for revoking data access in a source tool, you can optionally add a comment before proceeding with your revocation request. Click Revoke to confirm. (Optional) Click View to view your request(s) in the inbox . Tags: workflow automation orchestration Previous Manage policies Next Create forms"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/add-users-to-groups",
    "content": "Configure Atlan Access control Manage users and groups Add users to groups On this page Add users to groups Who can do this? You will need to be an admin user in Atlan to manage group membership. Add users to a group ​ To add many users to one group: From the left menu on any screen, click Admin . Under Workspace , or from the tiles, click Groups . To the right of a group row, click the user button. Check all users to add to the group. Click the Save button. Add groups to a user ​ To add many groups to one user: From the left menu on any screen, click Admin . Under Workspace , or from the tiles, click Users . To the right of a user row, click the group button. Check all groups to add to the user. Click the Add button. Map users to SSO groups ​ Atlan supports configuring SSO group mappings. You will first need to create groups in Atlan that correspond to the groups you want to map from your SSO provider to Atlan. To automatically assign users to Atlan groups based on their SSO groups, refer to the documentation for supported SSO providers: Azure AD Google JumpCloud Okta OneLogin SAML 2.0 You can also set default roles for new users joining the Atlan workspace via SSO. Tags: atlan documentation Previous Manage users Next Manage user authentication Add users to a group Add groups to a user Map users to SSO groups"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/tags/how-tos/remove-a-tag",
    "content": "Build governance Tags Tag Management Remove a tag On this page Remove a tag Atlan allows you to remove tags from a tagged asset . To remove a tag, you will first need to identify the origin of the tag: Directly added to an asset Propagated to an asset through tag propagation Remove a direct tag ​ To remove a direct tag: In the left menu from any screen in Atlan, click Assets . On the Assets page, select a tagged asset   -  in this example, we'll select the ORDERS table with the Marketing Analysis tag. Under Tags in the right menu, hover over the attached tag to view details. In the metadata popover, Linked by indicates that the tag was directly added to the asset. Click the pencil icon to edit the tag. In the popup, next to the tag name, uncheck the box to remove the tag   -  this will also remove the tag from all the assets to which it was propagated . Click Save to confirm tag removal. Remove a propagated tag ​ To remove a propagated tag: In the left menu from any screen in Atlan, click Assets . On the Assets page, select a tagged asset   -  in this example, we'll select the AUTHORS column with the Publications Department tag. Under Tags in the right menu, you can either: Hover over the attached tag to view details. In the metadata popover, Propagated from indicates that the tag was propagated to the asset. From the popover, click the originating asset to reconfigure tag propagation   -  in this example, the Book_rating table. Click the pencil icon and then click the Propagated tab to view information about tags propagated via upstream assets. From the Propagated tab, click the originating asset to reconfigure tag propagation   -  in this example, the Book_rating table. From the corresponding screen, under Tags in the right menu for the originating asset, click the pencil icon to edit the tag. In the popup, next to the tag name, click Edit : To remove the tag from downstream assets only, from the Propagation dialog, click Hierarchy only (no lineage) . To remove the tag from propagated assets only and not the originating asset, from the Propagation dialog, click No propagation . Click Update to save your changes. Click Save to confirm tag removal. Note that it may take some time for the tag to be removed from all the assets it was propagated to. Did you know? You can also create playbooks to automate the task of removing tags from tagged assets with propagation enabled or disabled . Once the playbook run is completed, tags will be removed from your selected assets. Tags: atlan documentation Previous Attach a tag Next What are tags? Remove a direct tag Remove a propagated tag"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/manage-user-authentication",
    "content": "Configure Atlan Access control Manage users and groups Manage user authentication On this page Manage user authentication Who can do this? You will need to be an admin user in Atlan to configure user authentication settings. When users log into Atlan, a user session begins. You can change the default timeouts for user sessions for all users in your organization, helping you establish secure authentication protocols in Atlan. Once you have configured the settings, these would be applicable to users logging in via both basic and SSO authentication. Atlan currently only supports configuring session settings up to a maximum value of 30 days. You must also enter a minimum value of 1 minute. You can configure the following parameters: Session idle timeout -  the total length of time that a session is allowed to be idle before it expires. Session max timeout -  the maximum amount of time before a session expires. Remember me session idle timeout -  the total length of time that a Remember Me session is allowed to be idle before it expires. If you choose not to set this parameter, Atlan will use the standard SSO session idle value, 7 days. Remember me session max timeout -  the maximum amount of time before a session expires if a user has enabled the Remember Me session option. If you choose not to set this parameter, Atlan will use the standard SSO session max value, 4,745 days. Note that tokens and browser sessions become invalid when a session expires. Configure session settings ​ To configure user authentication settings in Atlan: From the left menu in Atlan , click Admin . Under Workspace , click Authentication . Under Authentication , to the right of Session settings , click the Edit button. Atlan currently only supports setting a maximum value of up to 30 days. In the Change session settings dialog, you can configure the following: Set a numeric value for the following fields (You must enter a minimum value of 1 minute.): Session idle timeout Session max timeout _  - _ you must enter a value same as or higher than Session idle timeout . Remember me session idle timeout Remember me session max timeout _  - _ you must enter a value same as or higher than Remember me session idle timeout . Click the Days dropdown to specify the validity period. You can define parameters in the form of Minutes , Hours , or Days . Click Save changes to save your configuration. Tags: security access-control permissions Previous Add users to groups Next Delegate administration Configure session settings"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/delegate-administration",
    "content": "Configure Atlan Access control Manage users and groups Delegate administration On this page Delegate administration Atlan allows you to define granular access controls and delegate administrative functions with admin subroles. Atlan currently supports the following built-in admin subroles: Workflow admin -  the workflow admin subrole allows Atlan admins to: Grant administrative access to users to manage connectors and connection workflows only. Restrict access to admin capabilities in the admin center and governance capabilities in the governance center. Governance admin -  the governance admin subrole allows Atlan admins to: Grant administrative access to users to manage governance capabilities only. Restrict access to admin capabilities in the admin center and connectors and connection workflows in the workflow center. Assign a subrole ​ Who can do this? You will need to be an admin user in Atlan to assign an admin subrole. To assign an admin subrole: From the left menu of any screen in Atlan, click Admin . Under Workspace , click Users . To assign an admin subrole, you can either: To assign the subrole to an existing user, navigate to any user and click the Role dropdown. In the Select Role dialog, click Workflow Admin or Governance Admin and then click Update . To assign the subrole to a new user, follow the steps in How to invite new users without SSO. Change the role of the user to Workflow Admin or Governance Admin and then click the Send Invite button. Workflow admin ​ The workflow admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for creating and managing connection workflows. Permissions ​ A workflow admin has the following permissions and capabilities: Connections : Create a new connection for supported sources View all connections Manage all connections from the Connections tab in the Governance center Edit an existing connection   -  the user must also be a connection admin for that specific connection or have a policy granting them access to the connection. Workflows : Create and manage workflows from the Workflow center View all workflows and workflow runs Edit or delete any workflow credentials   - connection admin access not required Run any workflow Add, remove, or edit schedules for any workflow The following capabilities work exactly as that of a member user : Asset search and discovery -  can update metadata for assets in a connection that the workflow admin either created or was added to as a connection admin. Glossary -  can view all glossaries but will require edit access through glossary policies . If glossary restrictions are in place , then the workflow admin will only be able to view the glossaries as per their glossary policies. Insights -  requires data policies to query data and preview sample data. Reporting center -  if enabled by admins , can view the assets, glossary, Insights, and usage and cost dashboards. Data products -  requires domain policies to access domains and products. Restrictions ​ A workflow admin has the following explicit restrictions: Can only access the Connections tab in the Governance center . Cannot delete any existing connections using the Connection Delete workflow . Cannot access or perform any actions in the Admin center . Is excluded from the default All Admins group in any workflow configuration. (Optional) Restrict workflow visibility ​ By default, all workflow admins can see the existence of all workflows. However, you may want to limit specific teams from being able to see all workflows in Atlan. You can optionally turn off the default behavior to restrict workflow visibility. Once you have turned off the default behavior, in the Workflow center : The Monitor tab will no longer be visible to workflow admins. The Manage tab will display only the workflows created by workflow admins themselves. If there are no existing workflows, a workflow admin will only have access to the Marketplace tab to create a new one. To restrict workflow visibility: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access Control heading of the Labs page, turn off Allow workflow admins to access all workflows . Your workflow admins will now only have access to the workflows they created by default. If you'd like to restore the default behavior, follow the steps above and then turn it on. Governance admin ​ The governance admin role is a subcategory of the admin role in Atlan. This admin subrole grants specific permissions for managing the governance center. Permissions ​ A governance admin has the following permissions and capabilities: Personas : Create and manage personas from the Governance center View all personas Edit users and policies for existing personas   -  the user must either also be a connection admin or have a policy granting them access to the persona. Purposes : Create and manage purposes from the Governance center View all purposes Edit users and policies for existing purposes   -  the user must either also be a connection admin or have a policy granting them access to the purpose. Governance workflows -  create and manage governance workflows Playbooks -  create and run playbooks Policy center -  create and manage data governance policies README templates -  create and manage README templates Tags -  create and manage tags Domains -  only manage domains, cannot create them Custom metadata , badges , and options -  create and manage custom metadata and associated properties The following capabilities work exactly as that of a member user : Asset search and discovery -  can update metadata for assets in a connection that the governance admin was added to as a connection admin. Glossary -  can view all glossaries but will require edit access through glossary policies . If glossary restrictions are in place , then the governance admin will only be able to view the glossaries as per their glossary policies. Insights -  requires data policies to query data and preview sample data. Reporting center -  if enabled by admins , can view the assets, glossary, Insights, and usage and cost dashboards. Data products -  requires domain policies to access domains and products. Restrictions ​ A governance admin has the following explicit restrictions: Cannot access or perform any actions in the Admin center or Workflow center . Cannot access metadata and data policies if the user is neither a connection admin nor has a policy granting them access to a persona or purpose. Cannot access the Connections tab in the Governance center . Is excluded from the default All Admins group in any workflow configuration. Tags: workflow automation orchestration Previous Manage user authentication Next Automatically assign roles Assign a subrole Workflow admin Governance admin"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/create-groups",
    "content": "Configure Atlan Access control Get started Create groups Create groups Who can do this? You will need to be an admin user in Atlan to create groups. To create a group in Atlan: From the left menu of any screen, click Admin . Under Workspace click Groups . Click the Create Group button. Enter details for the new group in Create Group : For Name enter a descriptive name for the group. (Optional) For Description enter an explanation of the group. (Optional) For Slack enter the name of an existing Slack channel for the group. (You do not need to include the # in the channel name.) Click Test your slack link to attempt to open the channel in your integrated Slack workspace . (Optional) For Users search for and add any users that should be in the group. (You can also add users later.) (Optional) To add this group for all new users automatically, enable Mark as default . Click Create Group button. You now have a new group in Atlan! 🎉 Did you know? Once you've integrated Slack and added the Slack channel for your group, simply click the Slack icon next to the group name in Atlan and you'll be redirected to the group channel on Slack. You can then post questions and share assets directly on that channel. Tags: atlan documentation Previous Invite new users Next Create persona"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/manage-users",
    "content": "Configure Atlan Access control Manage users and groups Manage users On this page Manage users Who can do this? You will need to be an admin user in Atlan to manage other users. As an admin user in Atlan, you can: Change a user's role Temporarily disable or permanently remove a user Reactivate a disabled user Change a user's role ​ To change a user's role: From the left menu of any screen, click Admin . Under Workspace , click Users . Under the Role column, on a given user's row, click the role name. Under Change Role , select the new role to give the user, and then click Change . That's it, the user's role has now been changed! 🎉 Deactivate a user ​ To deactivate a user, you can either disable or remove them from Atlan. For example, you can disable a user to temporarily suspend their access to Atlan and reactivate it at a future date. If a user leaves the organization, you can remove their access. Atlan recommends that you proceed with caution if you want remove a user. While a disabled user can be reactivated , removing a user is a permanent and irreversible action. If a removed user needs to be restored to Atlan later on, you will need to add them as a new user. You can remove users irrespective of whether you're using basic authentication, SSO, or SCIM provisioning. Note that if you're using SCIM provisioning, disabling or reactivating users in Atlan is not allowed . If a SCIM-provisioned user is unassigned from the Atlan app in the identity provider, that user will be disabled in Atlan as well. Only then will you have the option to remove that user permanently. To deactivate a user: From the left menu of any screen, click Admin . Under Workspace , click Users . Scroll the table all the way to the right, if necessary. On an active user's row, click the 3-dot menu button and then from the dropdown: To temporarily remove a user, click Disable user and then click Disable . To permanently remove a user, click Remove user . In the Remove user dialog, for Transfer ownership , select an existing user to transfer any and all assets and workflows owned by the user you want to remove. Depending on the volume of assets to be transferred, the user removal process may take 15 minutes to a few hours to complete. You will be notified via email upon completion. Check the I understand this action can't be reversed and want to permanently remove this user box. Click Remove user . That's it, the user is now deactivated and will no longer be able to access Atlan. 😢 Deactivated user accounts do not count towards the total number of Atlan licenses procured by your organization. danger If the user you want to disable is a connection admin , you will need to ensure that other users can manage the connection before disabling the account. You can modify a connection to add more connection admins. Reactivate a user ​ You can only reactivate a disabled user's account. To reactivate a user: From the left menu of any screen, click Admin . Under Workspace , click Users . Scroll the table all the way to the right, if necessary. On a deactivated user's row, click the 3-dot menu button. Click Enable user , and then Enable . That's it, the user can once again access Atlan! 🎉 Did you know? Atlan has a retention policy of 60 days for user login events. If a user has not logged into Atlan for more than 60 days, the Last Active column will display - for the user. Tags: atlan documentation Previous Create purpose Next Add users to groups Change a user's role Deactivate a user Reactivate a user"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/provide-credentials-to-query-data",
    "content": "Use data Insights Credentials Provide credentials to query data On this page Provide credentials to query data Who can do this? Any Atlan user with data access to the asset and their own credentials for the data store. When your admins have configured bring your own credentials (BYOC) , you must provide your own credentials to query data. Did you know? Connections that require you to provide your own user credentials have a small icon next to them. If the connection you want to query has no icon, you can query it with its default shared credentials. You only need to provide your own credentials for connections with this icon. Set up your own credentials ​ Atlan supports both basic username and password as well as key pair authentication of your credentials. Atlan also supports SSO authentication . To set up your own credentials for a connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the drop-down to select the connection that requires user credentials. A Setup user credentials for ... dialog will appear. In the bottom right of the dialog, click Get started . The options will vary slightly depending on the connection type. Refer to the set up step for your connection for more details of what to fill in for each: Amazon Athena Amazon Redshift Databricks MySQL PostgreSQL Presto Snowflake Trino In the bottom-left of the dialog, click the Test Authentication button. Once successful, in the bottom right of the User credential setup dialog, click Done . You can then click Done again. You can now run queries using your own credentials! 🎉 Manage your credentials ​ Who can do this? Any non-guest user who has previously set up their own credentials for a data store. To manage your credentials for your connections: From the upper-right of any screen, click your username and then Profile . In the sidebar that appears, click the User credentials (bottom) icon. Under User credentials : To remove a credential, hover over the row for the credential you want to delete and click the delete icon. When prompted for confirmation, click the Delete button. To change a credential, click the row for the credential you want to change and follow the instructions above. Note that your existing credentials are not shown, and can only be overwritten. When complete, click the Update button in the lower right. Tags: integration connectors Previous How to query without shared credentials Next What are the query builder actions? Set up your own credentials Manage your credentials"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/query-data",
    "content": "Use data Insights Get Started How to query data On this page query data There are two ways to query data in Atlan: writing your own SQL using the Visual Query Builder Did you know? Atlan pushes all queries to the source (no data is stored in Atlan). In addition, Atlan applies access policies to the results before displaying them. Write your own SQL ​ Who can do this? Anyone with the knowledge to write SQL. Any Atlan user with data access to the asset can query data. To query an asset with your own SQL: From the left menu of any screen, click Insights . Under the Explorer tab, find the asset you want to query: Use the Select database dropdown to choose another database, if necessary. Search for the asset by name in the search bar, or browse for it in the tree structure. Hover over the table or view, and click the play icon. This writes and runs a basic preview query. (Optional) Click the open asset sidebar icon to view more details in the asset sidebar. (Optional) Click the eye icon to view a preview of the query results. (Optional) Click the 3-dot icon for more options: Click Set editor context to set the same connection, database, and schema name in the query editor as selected in the Explorer tab. Click Place name in editor to view the asset name in the query editor. Click Copy path to copy the full path of the asset, including database and schema names. Under the Untitled tab on the right, change the sample query or write your own   -  separate multiple queries with a semicolon ; . Click the Run button in the upper right to test your query as you write it. (Optional) Click the downward arrow next to the Run button to export query results via email or schedule the query . (Optional) If you have multiple tabs open in the query editor, right-click a tab to open the tabs menu. You can close a specific tab or all tabs, or duplicate the query. (Optional) From the top right of the query editor, click the 3-dot icon for additional query editor actions or to customize it further: Click or hover over Duplicate query to create a duplicate version of your query. Click or hover over Open command palette to view the actions you can run inside the query editor. Click or hover over Themes and then select your preferred theme for the query editor. Click or hover over Tab spacing to change the tab spacing for your queries. Click or hover over Font size to change the font size for your queries. Click or hover over Cursor to change the cursor position in the query editor. Click or hover over Autosuggestions to turn off autosuggestions for assets in the query editor. The editor supports all read-based SQL statements, including JOIN . The editor will not run any write-based statements. The following SQL statements are not supported: UPDATE DELETE CREATE ALTER DROP TRUNCATE INSERT INTO Did you know? You can select the context for your query to the left of the Run button. Then you won't need to fully qualify table names with schema and database names. Use the Visual Query Builder ​ Who can do this? Any Atlan user with data access to the asset . No SQL knowledge required! To query an asset using the Visual Query Builder: From the left menu of any screen, click Insights . At the top of the screen, to the right of the Untitled tab, click the + button and select New visual query . Under Select from choose the table or view you want to query. (Optional) In the column selector to the right, select the column you want to query. Then develop your query: Click the Run button to run the query and preview its results. Click the blue circular + button to add an action to the query. Repeat these steps until your query is complete. (Optional) If there are any errors in your query, click Auto fix for Atlan to recommend a fix. (Optional) In the query results set, click Copy to copy the query results or click Download to export them. Did you know? You can learn more about the query builder actions in this example . Tags: atlan documentation Previous Insights Next Save and share queries Write your own SQL Use the Visual Query Builder"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/make-a-query-interactive",
    "content": "Use data Insights Query Management Make a query interactive On this page Make a query interactive If you want to share a query with others, but limit how they can change the query, you can make it interactive . Others can then only change the value(s) that are interactive in your query   -  not the query itself. Who can do this? The owner of the query, or anyone with edit access to the query can make it interactive. Once interactive, anyone with read-only access to the query can also edit only the interactive part. SQL queries ​ To make a SQL query interactive: Open the query in Insights. Highlight the value you want to be interactive. On the bar separating the query from the results, click the curly braces icon and then click the Add variable button. Click the settings icon to the right of the Enter a string text box to define the custom variable: For Name , enter a meaningful name for the custom variable. For Type , change the data type of the custom variable, if necessary. For Default value , enter the value to use for the custom variable if other users do not change it. At the bottom of the Edit dialog for the custom variable, click Save . In the upper right of the query editor, click Run to confirm the query still operates as expected. That's it, your query is now interactive! 🎉 Did you know? You can select from a wide range of data types for your custom variables   -  string, number, date, date and timestamp ranges, and multi-value options. VQB queries ​ To make a Visual Query Builder (VQB) query interactive: Open the query in Insights. Find the value you want to be interactive. In the right of the box for that value, click the lightning bolt icon. In the upper right of the query editor, click Run to confirm the query still operates as expected. That's it, your query is now interactive! 🎉 Tags: atlan documentation Previous Save and share queries Next Schedule a query SQL queries VQB queries"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/query-without-shared-credentials",
    "content": "Use data Insights Credentials How to query without shared credentials On this page query without shared credentials Who can do this? You will need to be a connection admin in Atlan to enable bring your own credentials (BYOC) on a specific connection. Don't want to use a single shared service account to access data? With bring your own credentials (BYOC), users need to provide their own credentials before they can query data. Each user's permissions in the data store itself are then applied to each query. This is helpful for organizations that have already invested in defining granular controls in their data stores. With BYOC, you can reuse those controls without any extra work! Atlan currently supports the following connectors for BYOC: Amazon Athena Amazon Redshift Databricks MySQL PostgreSQL Presto Snowflake Trino Did you know? To query data using SSO credentials instead, refer to Authenticate SSO credentials to query data . Enable BYOC on a connection ​ To enable BYOC on a connection: From the left menu of any screen, click Assets . From the pills below the search bar at the top of the screen, click Connection . From the list of results, select the connection for which you want to enable BYOC. From the sidebar on the right, next to Connection settings , click Edit . In the Connection settings dialog: Under Allow query , for Authentication type , click Basic credentials to enforce user credentials for querying data . Under Display sample data , for Source preview , click Basic credentials to enforce user credentials for viewing sample data . (Optional) Toggle on Enable data policies created at source to apply for querying in Atlan to apply any data policies and user permissions at source to querying data and viewing sample data in Atlan. If toggled on, any existing data policies on the connection in Atlan will be deactivated and creation of new data policies will be disabled. At the bottom right of the Connection settings dialog, click Update . Tags: atlan documentation Previous Schedule a query Next Provide credentials to query data Enable BYOC on a connection"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/governance/users-and-groups/how-tos/invite-new-users",
    "content": "Configure Atlan Access control Get started Invite new users On this page Invite new users Who can do this? You will need to be an admin user in Atlan to invite users. Did you know? Usernames in Atlan are of a permanent nature. Atlan uses usernames as a unique identifier across the platform and does not support making any changes to them. When logging into Atlan for the first time, ensure that you configure your username as per your preference. Without SSO ​ To invite new users to Atlan, without SSO: From the left menu of any screen, click Admin . Under Workspace click Users . Click the Invite Users button. Under Invite users to Default enter one or more email addresses of the users to invite. danger Atlan does not allow the use of disposable email addresses. You will receive an error indicating this if you attempt to send an invitation to one. (Optional) To the right of each email, click Member to change the role of the user . Click the Send Invite button. Your user(s) will now receive an email with a link to sign up on Atlan! 🎉 Note that the invitation link will remain valid for 7 days. If the link expires, you can resend the invitation to your new users. With SSO ​ Did you know? When SSO is enforced , you will not be able to invite users in Atlan   -  they can only be invited through the SSO provider. Any users mapped to the SAML app can log into Atlan via SSO. A user profile will be generated for them automatically, if one does not already exist. Admins can also assign default user roles for SSO to give appropriate permissions to users as soon as they log into Atlan. Tags: atlan documentation Previous Access Control Next Create groups Without SSO With SSO"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/schedule-a-query",
    "content": "Use data Insights Query Management Schedule a query On this page Schedule a query Who can do this? Before scheduling a query, you will need your Atlan administrator to enable scheduled queries . You might want to schedule a query to run repeatedly. For example: If you want to fetch data for the same query at different times. If you want to share data with business teams on a consistent basis, for example, weekly. If a data analyst is out of office but wants the data to be shared with users periodically. danger You must save your query before you can schedule it. Your SMTP configuration must also be in a working state to send results to recipients. Schedule a query ​ To schedule a query: Open Insights . In the upper left, click the papers-in-a-box icon to open your query collections. Choose the query collection where the query you want to schedule is saved. Hover over the saved query you want to schedule, click the three dots to its right, and then click Schedule . In the New Schedule Query dialog, enter the scheduling details: For Name , enter a meaningful name for the scheduled query. For Runs every , choose how often the query should run: For Hour and Day , you can go down to the minute for hourly and daily runs. For Week , you can select multiple days for a weekly run. For Month , you can select multiple dates for a monthly run. For Custom cron , write your own custom cron. For Timezone , select a relevant option or keep the default one. For Share Results , select the user(s) to whom you want to send the results from each run. (Optional) If there are any existing schedules for your query, from the top right, click the existing schedule link to view more details in a sidebar. At the bottom right of the dialog, click Done . (Optional) Under Query successfully scheduled , click the Run Now link if you want to test the scheduled query. At the bottom right of the dialog, click Finish . Your query will now run on the defined schedule and the results will be sent out automatically! 🎉 Note that the download link for the results is only valid for 6 hours. Did you know? If a scheduled query fails, you'll receive an email notification with the name of the query, so you can troubleshoot and get it running in no time. Change a scheduled query ​ To change the schedule for a query: Open Insights . In the upper left, click the repeating-arrow icon to open your scheduled queries. Hover over the scheduled query you want to change, click the three dots to its right, and then: (Optional) Click Run now to run the scheduled query immediately. (Optional) Click Pause to pause the scheduled query. Click Edit to update the scheduled query. In the Update Schedule Query dialog box, make any desired changes to the schedule or recipients. At the bottom right of the dialog box, click Update to save the changes. (Optional) Under Query successfully updated , click the Run Now link if you want to test the scheduled query. At the bottom right of the dialog, click Finish . Your query will now run on the changed schedule and the results will be sent out automatically! 🎉 Remove a scheduled query ​ To remove a scheduled query: Open Insights . In the upper left, click the repeating-arrow icon to open your scheduled queries. Hover over the scheduled query you want to stop, click the three dots to its right, and then click Delete . In the Delete Schedule dialog box, click Delete . Your query will no longer run automatically. Tags: data integration configuration Previous Make a query interactive Next How to query without shared credentials Schedule a query Change a scheduled query Remove a scheduled query"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/how-tos/download-and-export-lineage",
    "content": "Use data Lineage Manage lineage Download and export lineage On this page Download and export lineage Understanding the flow of data from the source to its destination is a critical need for many organizations. To help you build trust in your data assets, Atlan allows you to view , download , and export your impacted assets and share it with others in your organization. The impact report includes upstream and downstream assets, along with the following attributes: Status -  export status and total count of assets exported, only for export option Name -  asset name and link Type -  asset type Business Name (Alias) -  business-oriented alias of assets, if any Connector -  name of supported source Database -  name of the database, if applicable Schema -  name of the schema, if applicable Table -  name of the parent table or view, only for column assets Lineage depth -  starts at 1 and increases with each lineage level Description - asset description , if any Owner Users and Owner Groups - asset owners , if any Certification Status and Certification Message - certification status of asset , if any Tags and Propagated Tags - tags directly attached or propagated to an asset, if any Terms -  glossary terms for linked assets Announcement Type , Announcement Title , and Announcement Message - announcements on assets , if any Source URL -  only supported for dbt Cloud and Core, Matillion, Microsoft Power BI, Mode, Monte Carlo, Salesforce, Sigma, Sisense, Soda, and Tableau assets Usage - number of queries by number of users , only for supported sources Popularity - popularity indicator , only for supported sources Last refresh on source -  metadata last altered at source Workbook , Data source , and Source owner -  only applicable to Tableau assets Qualified Name -  fully qualified name of the asset Source Asset GUID -  globally unique identifier of the source asset GUID -  globally unique identifier of the exported asset Immediate Upstream and Immediate Downstream -  one level directly upstream and downstream, respectively, of the dependent asset. View impacted assets ​ The impacted assets view includes asset metadata, BI source URLs, and Atlan URLs for both upstream and downstream assets. To view impacted assets: In the right menu from any screen, click Assets . Click an asset to navigate to its asset profile. In the asset profile, click Lineage . In the top right of the lineage graph, click the downward arrow and then click View impact to view impacted assets. Note that if the total number of impacted assets exceeds 200, you will only be able to download lineage. In the impacted assets report: To view downstream assets for impact analysis , click Downstream . To view upstream sources for root cause analysis , click Upstream . (Optional) To view the source URL for BI assets, click the source URL option   -  for example, View in Looker . (Optional) To navigate to the impacted asset in Atlan, click the Atlan URL option. (Optional) Click Impact Report to download the impacted assets report in a CSV file or export to Google Sheets or Microsoft Excel . Click Close to return to the lineage graph. Download lineage ​ You can download lineage for your data assets in a CSV file or as an image. Download lineage in a report ​ You can download a lineage report for your assets in a CSV format for further analysis. When you share the CSV file with team members, they will also be provided with the BI source URLs and asset links in Atlan. As long as they have access to the asset in the BI tool and Atlan, they'll be able to open both directly from the CSV file. To download the lineage report: In the right menu from any screen, click Assets . Click on an asset to navigate to its asset profile. In the asset profile, click Lineage . In the top right of the lineage graph, click the downward arrow and then click CSV File to download the lineage report for the asset. Once the report has been fetched, click Download report to download it as a CSV file. You will now be able to view the upstream and downstream lineage of your assets in a CSV file! 🎉 danger Clicking the CSV File button may not instantly result in a link to the CSV file. When you click the button, it triggers a workflow to scan assets with lineage, retrieve a list of those assets via the API, and then generate a CSV file with the lineage report. For high-volume impact reports, the workflow can take a few minutes to run before the link to the CSV file is ready. The progress bar under the button displays the status of the workflow and will turn green once all the tasks in the workflow have been executed. Download lineage as an image ​ Atlan offers you the option to download lineage as a high-resolution PNG file, providing you with the visual clarity you need for your presentations. To download lineage as an image: In the right menu from any screen, click Assets . Click on an asset to navigate to its asset profile. In the asset profile, click Lineage . In the top right of the lineage graph, click the downward arrow and then click As an image to download an image of the lineage graph. You will now be able to view a high-resolution image of your asset lineage! 🎉 Export lineage to spreadsheets ​ Atlan enables you to export lineage to spreadsheets. This can help you assess the downstream impact of any changes made to an upstream asset for impact analysis . Atlan currently supports exporting impacted assets to: Google Sheets Microsoft Excel online Once your Atlan admin has integrated a supported tool, you will be able to export impacted assets to spreadsheets. Your existing permissions and access policies in Atlan will determine whether you can export the impact report, but at a minimum you'll require read permission on the assets you want to export. Did you know? Atlan currently limits the total number of assets you can export to 20,000 rows each for both upstream and downstream assets. Reach out to your customer success manager if you'd like to increase the limit for your organization. Enable lineage export ​ The export to Google Sheets and Microsoft Excel icons and buttons will only be visible if your Atlan admin has integrated a supported tool. If you cannot see the export icon or button, reach out to your Atlan admin to integrate Google Sheets or Microsoft Excel. To integrate a supported tool, your Atlan admin must follow the steps in Enable asset export . Export impacted assets ​ Who can do this? Once an Atlan admin has integrated a supported tool, any admin, member, or guest user in Atlan with read permission on assets can export the impact report to spreadsheets. danger Atlan recommends that you avoid exporting assets during workflow runs. Exporting assets while you have workflows running in the background may lead to duplicate assets on the spreadsheet. Atlan allows you to export your impacted assets to spreadsheets and view asset metadata in bulk. At an individual user level, only one export is allowed for each supported tool at a time, the rest will be auto-queued for execution. There are currently no limitations at a tenant level. You can export lineage from the following areas: Overview and Lineage tabs in sidebar for assets with lineage Lineage graph for asset-level lineage Impacted assets view in lineage graph Column assets with lineage from lineage graph To export lineage: From the left menu of any screen in Atlan, click Assets . To export lineage, in the Assets page, you can: Select an asset, and then from the asset sidebar: In the Overview tab, click the 3-dot icon and then click Impact report . Click the Lineage tab and then click the export icon. Select an asset, and from the top right of the asset card, click the View lineage icon to open the lineage graph: From the top right of the lineage graph, click the downward arrow. For any asset with column-level lineage on the lineage graph, click the view columns menu and hover over a column name to view export options. To export lineage to a spreadsheet tool: Click Google Sheets to export impacted assets to a Google Sheets spreadsheet. Click Microsoft Excel to export impacted assets to a Microsoft Excel workbook. A sign-in dialog will appear and you will be redirected to sign in with your Google or Microsoft account. From the corresponding screen, click Allow to connect to Google Sheets or Microsoft Excel. If you're already signed in, skip this step. To track the progress of the export, navigate to the spreadsheet. The Queued status will change to Success once impacted assets have been exported. On the spreadsheet, you can view metadata for your impacted assets. Atlan exports the impact report to two individual sheets within the main spreadsheet, one each for Upstream and Downstream . For any metadata attribute not applicable to a particular asset, the column will display an empty value. (Optional) To view your asset export history, navigate to the Assets page. Next to the search bar on the Assets page, click the 3-dot icon and then click Export . From the Export dialog, expand the History dropdown to view your last 10 exports. Note that only you can currently view your own export history. That's it, you've successfully exported your impacted assets from Atlan! 🎉 danger Atlan currently does not support updating asset metadata in the spreadsheet and syncing impacted assets back to Atlan. Tags: lineage data-lineage impact-analysis downstream-impact dependencies upstream-dependencies data-sources Previous How to view lineage Next Generate lineage between assets App View impacted assets Download lineage Export lineage to spreadsheets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/insights/how-tos/save-and-share-queries",
    "content": "Use data Insights Query Management Save and share queries On this page Save and share queries You can save queries to re-run them later, schedule them, or share them. Did you know? You can only save queries through a collection . You can share collections with others, to share your queries. Save a query ​ To save a query: Open the query in Insights. At the top right of the query, click the Save button. In the resulting Save query dialog, enter the following details: For Query name , enter a name for the query. (Optional) For Description , add a description for the query. For Collection , you can either: If you have access to existing collections, click the Choose collection dropdown to select an existing collection. If you do not have any existing collections, click the Create Collection button. In the Create collection dialog, enter the following details: For Name , enter a name for the collection. (Optional) To the left of the name, click the image icon to choose an icon for the collection. (Optional) For Description , describe the collection. (Optional) For Share , select other users or groups that can access the collection. (See below for more details.) At the bottom of the Create Collection dialog, click Create . (Optional) For Certificate , click the No certification dropdown to assign a certificate to the query. (Optional) For Linked terms , click the Select terms dropdown to assign a term to the query. At the bottom of the dialog: To only save your query, click Save . To save and share your query, click Save and share . In the Query saved dialog, enter the following details: For Add users or groups , select other users or groups that can access the saved query. (Optional) To the right of the user or group, click the Editor dropdown to change the sharing permissions: Viewer allows users to view and run all queries in the collection, but not edit them. Editor allows users to view, run, and edit all queries in the collection. Click Invite to invite the users or groups. (Optional) Click Copy Link to copy the link for the saved query to share with others in your team. (Optional) Click the Slack or Teams button to share directly on Slack or Microsoft Teams , respectively. Click Done to confirm your selections. Did you know? Atlan currently supports a query length of 2 million characters for saved queries. Share a query collection ​ A collection helps you organize saved queries in Atlan. A collection could represent a topic, department, or team with similar saved queries under one roof. Within each collection, you can have a folder that contains multiple saves queries of a similar type. To share a collection of queries: Open Insights. In the upper left, click the papers-in-a-box icon. Under the icon, click the name of the selected query collection. From the resulting list of collections, hover over the collection you want to share. Click the 3-dot icon to the right of the collection name and choose Edit collection . In the Edit collection dialog, under Share : Search for users or groups with whom to share the collection. (Optional) To the right of the user or group, click the Can edit dropdown to change the sharing permissions: Can edit allows users to view, run, and edit all queries in the collection. Can view allows users to view and run all queries in the collection, but not edit them. Repeat these steps for each user or group with whom you want to share the collection. At the bottom of the Edit collection dialog, click Update . Did you know? Users with only Can view permissions will still be able to change the interactive part of interactive queries . Move a saved query ​ To move a saved query to another query collection: Open Insights. In the upper left, click the papers-in-a-box icon. Under the icon, click the name of the selected query collection. From the resulting list of collections, hover over the collection from which you want to move a query. Click the 3-dot icon to the right of the saved query name, and then click Move to . In the Move to dialog, select the query collection to which you want to move your saved query. Click Move to complete moving the saved query. To duplicate, rename, edit, or delete your saved query, click the 3-dot icon to the right of the saved query name and select the relevant option. Did you know? If you add a Slack channel to the Query output share channels field in your Slack integration , you will be able to share your saved query and query results directly on that Slack channel. Atlan will deliver the query results as a CSV file on the same Slack thread. View query sidebar ​ Once you've saved a query , you can access the query sidebar to view additional context for your saved queries. To open the query sidebar for a saved query: In the left Explorer panel in Insights , hover over a saved query and click the Open query sidebar icon. From the saved query sidebar in the right, you can: View details about your saved query in Overview , including the actual query. (Optional) You can add more details to your saved query: Click the star button to star the query for quick access. For Description , add a description to your saved query. Click +Add README to add a README to your saved query and provide more context. For Collection , click the collection name to view the query collection. Copy the SQL query or expand the query view to fullscreen. For Terms , add a term to link to your saved query . For Owners , update the owner of the saved query or add more owners. For Tags , attach a tag to your saved query. For Certificate , update the certification status of your saved query. Click the Relations tab to view queried assets. (Optional) Select a related asset to open the asset sidebar. From the asset sidebar, click the Queries tab to view the saved query auto-linked to the asset. Click the Activity tab to view the activity log for the saved query. Click the Schedules tab to view associated schedules for the saved query, if any. Click the Resources tab to view any linked resources . Click the Requests tab to view any requests on the saved query. Click the Properties tab to view query properties. Click the Slack or Teams tab to view Slack or Teams messages pertaining to the query. Link saved queries ​ Once you have saved a query, the saved query will be auto-linked to all the assets queried or referenced in the SQL query. Linked queries are displayed in the asset profile and sidebar . You might want to link it to other assets, too. This can help you provide additional context on the assets and quickly find the saved query. For such assets, you can link the saved query as a resource . To link a saved query to an asset: Open Insights. In the upper left, click the papers-in-a-box icon. Under the icon, click the name of a query collection. From the resulting list of collections, select a collection and then select the saved query you want to link as a resource . To copy the link for a saved query, you can either: Click the 3-dot icon to the right of the saved query name and then click Copy link . In the top right of the query editor, click the 3-dot icon and then click Share . From the Share menu, click Copy link . From the left of menu, click Assets to navigate to your assets. From the Assets page, select an asset to open the asset sidebar. From the asset sidebar on the right, click the Resources tab and then click + Add resource . In the Add Resource dialog, enter the following details: For Link , paste the saved query link you copied in Insights. For Title , add a title for your saved query. Click Add to add the saved query as a resource to the asset. Did you know? Any user in Atlan will be able to preview saved queries for auto-linked or manually linked assets from the asset sidebar   -  unless there are access policies prohibiting them. Tags: atlan documentation Previous How to query data Next Make a query interactive Save a query Share a query collection Move a saved query View query sidebar Link saved queries"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/how-tos/generate-lineage-between-assets",
    "content": "Use data Lineage Manage lineage Generate lineage between assets App On this page Generate lineage between assets App App You can use the Lineage Generator (no transformations) app to automatically create lineage between assets across two systems when their names match or follow a consistent pattern. This is useful for keeping track of how data flows between databases, warehouses, or storage systems without needing to configure every connection manually. Prerequisites ​ Before you begin, make sure you have: Access to the Lineage Generator (No Transformations) app. You can verify this by searching for Workflow Alerting in the Atlan marketplace. If you don't have access, contact Atlan support or your Atlan customer team to request it. At least one source connection and one target connection set up in Atlan with assets already crawled. Setup workflow ​ In your Atlan workspace, go to the homepage and click New workflow in the top navigation bar. Search for Lineage Generator (no transformations) , and then select Set up workflow . In the Workflow name field, enter a descriptive name such as: Staging to Reporting Lineage In the Source asset type property, select the type of asset from which lineage originates. For example, choose Table when working with a Snowflake staging schema. This limits scanning and matching to tables in the source prefix. For details on other supported asset types such as View, Materialized View, Column, or file-based objects, see the Source asset type . In the Source qualified name prefix property, enter the prefix that identifies the source assets. This narrows lineage generation to only those assets whose qualified names begin with the specified prefix. For example, if your Snowflake staging schema is stg , set the prefix to: default/snowflake/.../database_name/stg In the Target asset type property, select the type of asset to connect as the downstream target. For example, choose Table when linking Snowflake staging tables to reporting tables. This directs the workflow to search for target tables that match the source tables. For information on other supported asset types, including BI datasets and dashboards, see the Target asset type . In the Target qualified name prefix property, enter the prefix that identifies the target assets. This restricts lineage generation to assets whose qualified names begin with the specified prefix. For example, if your Snowflake reporting schema is rpt , set the prefix to: default/snowflake/.../database_name/rpt By default, the Case sensitive match property is set to Yes, while Ignore circular lineage and Match on schema are set to No. These defaults provide straightforward matching for a first run. For more information about how each option changes lineage generation, see the Lineage Generator reference . In the Output type property, select Preview lineage to generate a CSV preview of proposed matches without creating lineage in Atlan. Use this option to validate the results before making changes. For detailed explanations of each option, see the Output type . Run the workflow with Output type set to Preview lineage . This generates a CSV file showing the proposed matches between source and target assets. If the preview looks correct, update the Output type property to Generate lineage and run the workflow again. This applies the matches as lineage relationships in Atlan, making them visible in the lineage graph. You can configure advanced options such as regex transformations, schema-based matching, or child-asset lineage. These options help handle complex naming patterns, prevent false matches across schemas, and extend lineage to column-level detail. For more details, see the Lineage Generator reference . Need help? ​ If you have any issues related to configuring the app, contact Atlan support . See also ​ Lineage Generator (no transformations) : Detailed explanation of each configuration property, including valid values, examples, and behavior. Tags: lineage automation app data-lineage Previous Download and export lineage Next What is column-level lineage? Prerequisites Setup workflow Need help? See also"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/lineage/how-tos/view-lineage",
    "content": "Use data Lineage Get Started How to view lineage On this page view lineage The lineage graph in Atlan provides a granular view of the data flows and transformations for your assets. To learn how to use the lineage graph, complete the following steps. View the lineage graph ​ To view the lineage graph: From the left menu of any screen in Atlan, click Assets . (Optional) In the Filters menu on the left, expand the Properties menu and then click Has lineage to filter for assets with data lineage. Select an asset, and from the top right of the asset card, click the View lineage icon to open the lineage graph. On the lineage graph, the home icon indicates the base asset: Click the + button to the left of the base asset to view upstream assets. Click the + button to the right of the base asset to view downstream assets. (Optional) Hover over any asset to view the metadata popover for more context on the selected asset   -  including asset type, database and schema names, owners , usage and popularity metrics , announcements , and Monte Carlo and Soda data quality details for supported assets. From the popover, you can also: Click the View impact button to view impacted assets . Click the Download impact button to download the lineage report for the asset. Click the outward-facing arrow icon to open lineage in a new tab. (Optional) Hover over the + button to the right of any asset and then click the Expand all button to view assets further upstream or downstream horizontally. (Optional) Click any circular process button to view more details about the lineage process in the sidebar. (Optional) Click Show all to view more upstream or downstream assets vertically. If there are more than 100 assets, a popup will appear asking you to scroll to the bottom of the list and click Show all again to view all dependencies. (Optional) For any asset on the lineage graph, click the view columns menu to expand the columns view: The default view shows 10 columns. Click Show more columns to view the full list of columns. Use the search bar to search for specific columns. View a total count of columns that have lineage. Next to the search bar, click the sort icon to sort columns in an alphabetical or reverse alphabetical order, ascending or descending order, or by last updated in Atlan. Hover over any column name and then click the curved downward arrow to view impacted assets or click the downward arrow to download the lineage report directly from the column. Select a column, and then from the column sidebar, click the Lineage tab. From the Lineage tab: Click View impact to view and download impacted assets directly from the sidebar. Click the eye icon to customize your view of column-level lineage in the sidebar: For Depth , select the level of depth up to Max Depth for column-level lineage. Turn on Show process nodes to view lineage processes in the sidebar. For Show hierarchy , keep the default view of child assets or toggle it off. For any asset on the lineage graph, from the sidebar, click the Columns tab. From the Columns tab: You can view a list of columns for the selected asset. Click Show columns with lineage to only view columns with lineage: For any column with lineage in the sidebar, click the play button to locate that asset on the lineage graph. (Optional) To collapse the sidebar, to the left of the sidebar, click the rightward arrow. (Optional) From the top right of the lineage graph: Click the Find in canvas search bar to search for any specific assets on the lineage graph. Click the downward arrow to view more options: Click View impact to view impacted assets . Click Download impact to download the lineage report for the asset. Click Download lineage as image to download an image of the lineage graph. Click the eye icon to set preferences for the lineage graph: For Additional metadata , show or hide the following context for your assets   -  schema name, database name, asset type, announcements , data quality, or popularity metrics . For Order columns and fields , order assets on the lineage graph alphabetically, in an ascending or descending order, or by last updated in Atlan. For Line arrows , show or hide the arrows that indicate data flows on the lineage graph. For Pin metadata sidebar , toggle on the slider to pin the sidebar open on the lineage graph. Click the question mark icon to share feedback or view documentation. (Optional) From the bottom right of the lineage graph: Toggle the Visibility slider to customize the visibility of assets not included in your selected lineage path. Click the refocus icon to refocus your view of the lineage graph to reset to the default view, selected node, or back to the base asset. Click the minimap icon to view an abridged version of the lineage graph. Click the information icon to view the map legend. Click the fullscreen icon to expand the lineage view to fullscreen mode. Click the minus or plus icons to zoom out or zoom in on the lineage graph, respectively. Did you know? If the products module is enabled in your Atlan workspace, you can also create data products directly from the lineage graph . Tags: lineage data-lineage impact-analysis Previous Lineage Next Download and export lineage View the lineage graph"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/how-tos/automate-data-profiling",
    "content": "Configure Atlan Playbooks Management Automate data profiling On this page Automate data profiling ➕ Available via the Data Quality Studio package warning 🤓 Who can do this? You need to be an admin user in Atlan to create profiling playbooks. Monitoring and improving data quality is critical to building trust in your data assets. Atlan solves for this with profiling playbooks! Profiling playbooks help power data observability for your assets in Atlan. You can create profiling playbooks to scan your assets at scale, identify any issues or inconsistencies, and improve the data quality of your assets. Supported sources ​ Atlan currently supports column profiling for the following connectors: Amazon Athena Amazon Redshift Databricks Google BigQuery Microsoft SQL Server MySQL PostgreSQL Snowflake Trino Create a profiling playbook ​ To create a profiling playbook: In the left menu in Atlan, click Governance . Under the Governance heading of the Governance center , click Playbooks . To the right of the Create New button, click the downward arrow and then select Profiling Playbook . In the Create new profiling playbook dialog, enter the following details: For Name , enter a name for the task to be accomplished   -  for example, Tables scan . (Atlan recommends that the length of a playbook name must be no longer than 46 characters.) For Connection , select a supported connection from the dropdown menu   -  in this example, we'll select a Google BigQuery connection development . (Optional) For Description , enter a description for your playbook. (Optional) Select an icon for your playbook. Click Create to save your playbook. Set up rules as filters ​ Did you know? The assets to be scanned are pre-filled based on your selected connection. To set up rules as filters for your profiling playbook: In the Build Rules page of your profiling playbook, click Filters . For the name field, add a name to your filter   -  for example, Profiling action . To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. For Attributes , select the relevant option. For this example, we'll select Name listed under Properties . (Optional) To further refine your asset selection: Click Connection to select a specific connection. Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported Snowflake and dbt tags. (Optional) For Snowflake tags only, to the left of the checkbox, click Select value , and then from the Select tag value dialog, select any value(s) to filter assets by tag value. Click Glossary, terms, & categories to filter by a specific glossary or category to bulk update all the nested terms or by multiple glossaries and categories. Click Linked terms to filter assets by linked terms . Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. For Operator , select Is one of for values to include or Is not for values to exclude. Depending on the selected attribute(s), you can also choose from additional operators : Select Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, in the Filters card, click View all for a preview. Confirm profiling actions ​ danger Column profiling is currently only supported for number and text data types. The profiled column assets will be populated with preconfigured metrics. To select the actions to be performed based on your rules: The default profiling actions to be performed include: Base metrics : Distinct count   -  number of rows that contain distinct values, relative to the column. Missing count   -  number of rows that do not contain specific values. Numeric metrics : Minimum and maximum values   -  smallest and greatest values in a numeric column. Average   -  calculated average of values in a numeric column. Standard deviation   -  calculated standard deviation of values in a numeric column. Variance   -  calculated variance of values in a numeric column. String metrics : Average length   -  average length of string values in a column. Minimum and maximum length   -  minimum and maximum length of string values in a column. Click Next to proceed to the next step. In the Optimize your Profiling query popup, the following message will appear: This Profiling playbook will query x rows across y assets. To avoid significant computing costs, review your applied filters before proceeding . Click Review filters to review your existing filters or click Continue anyway to proceed. Note that Atlan is working to support sampling functionality in the future. Run the playbook ​ If you'd like to continue working on your playbook, you can save it as a draft. If your playbook is ready, you can proceed to running it. To run the playbook: You can either: To run the playbook once immediately, click Run once . To schedule the playbook to run hourly, daily, weekly, or monthly, click Schedule and choose the preferred frequency, timezone, and time. Click Complete to confirm your selections. In the resulting screen, click Go to profile to view your playbook profile. Once your playbook run is completed, you will see the data profile updated for your assets! 🎉 View profiled assets ​ To view the profiled assets for your playbook: In the Overview page of your playbook, to the right of Profiling action , click the total count of profiled assets. In the sidebar to the right, profiled assets will be indicated with a bar graph icon. Click any profiled asset to proceed to viewing profiling data. From the table sidebar, click the Column tab to view column assets and then select any of the profiled columns. In the column sidebar to the right, click Profile to view profiling data for the selected column asset. Did you know? Once you've created a profiling playbook, you can monitor, modify, or delete it at any time. Tags: connectors data crawl Previous Manage playbooks Next Troubleshooting playbooks Supported sources Create a profiling playbook Set up rules as filters Confirm profiling actions Run the playbook View profiled assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/how-tos/manage-playbooks",
    "content": "Configure Atlan Playbooks Management Manage playbooks On this page Manage playbooks Once you've created a playbook , you can monitor, modify, or delete it at any time. You can also enable notifications to monitor your playbook runs directly in Slack or Microsoft Teams. Monitor a playbook ​ To monitor your playbooks runs: When running a playbook immediately, you will be redirected to the monitoring page within 5 seconds. At any other moment: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Playbooks . In the playbooks manager, select the playbook you'd like to view. In the Overview section of your playbook, you'll be able to monitor: A summary of the rules, actions, and updated assets. An activity log for recent runs and updates over time. Did you know? The activity log in the playbooks manager can help you keep track of playbook runs, ranging from 24 hours to 30 days. Select any of the entries to navigate to the corresponding playbook. Modify a playbook ​ To modify an existing playbook: From the left menu of any screen in Atlan, click Governance. Under the Governance heading of the Governance center , click Playbooks . In the playbooks manager, click the playbook you'd like to modify. On your playbook page: To edit the name and description of your playbook, hover over your playbook and click Edit . To modify the rules of your playbook, click Rules to make your changes and then click Update to save them. (Optional) To add a new rule to an existing playbook, in the left menu for playbook rules, click + Add new Rule . To turn off a playbook filter, to the right of any filter, click the three horizontal dots and then click Disable . Click Enable to turn on any disabled filters. To modify the schedule for your playbook, in the upper right of your screen: Click Run Now to run it immediately. Click the pencil icon to modify or remove the schedule. Delete a playbook ​ To delete an existing playbook: From the left menu of any screen in Atlan, click Governance. Under the Governance heading of the Governance center , click Playbooks . In the playbooks manager, hover over the playbook you'd like to delete and click Delete Playbook . Click Delete to confirm. Enable playbook notifications ​ You can set up Slack or Microsoft Teams alerts for your playbook runs in Atlan. This can help you monitor your playbooks directly in Slack or Microsoft Teams. You can also choose to receive alerts for failed playbook runs only. Before you can enable notifications for playbooks, you will need to either: Integrate Slack and Atlan Integrate Microsoft Teams and Atlan To enable notifications for playbook runs: From the left menu of any screen in Atlan, click Governance . Under the Governance heading of the Governance center , click Playbooks . In the upper-right of the playbooks manager, under Activity , click the bell icon. In the Enable notifications popup: Click Setup now to integrate Slack or Microsoft Teams . If you have already integrated Slack or Microsoft Teams, click Enable . In the notifications setup dialog, configure the following: For Notifications channel , you can either: If you have already configured a Slack or Microsoft Teams channel to receive workflow alerts, that channel will be preselected. You can use the same channel to receive both workflow and playbook run alerts and skip to the next step. If you have not configured a workflow alerts channel or want to add a different one, enter the channel name to receive notifications for playbook runs. This channel will be displayed as the Playbooks alert channel in your Slack or Microsoft Teams integration. To select the type of notifications you want to receive, you can either: Click Both success and failure alerts to receive notifications for both successful and failed playbook runs. Click Failure alerts only to limit notifications to failed playbook runs only. Click Save to save your notification preferences. (Optional) To disable notifications, from the notifications setup dialog, remove the playbook alerts channel configured for Slack or Microsoft Teams . You will now receive Slack or Microsoft Teams notifications for all your playbook runs in Atlan! 🎉 The Atlan bot will share playbook run alerts, including details like run status, start time, run time, trigger type, last three runs, and more. Tags: atlan documentation Previous Set up playbooks Next Automate data profiling Monitor a playbook Modify a playbook Delete a playbook Enable playbook notifications"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/playbooks/how-tos/set-up-playbooks",
    "content": "Configure Atlan Playbooks Get Started Set up playbooks On this page Set up playbooks warning 🤓 Who can do this? You will need to be an admin user in Atlan to create playbooks. A common question that data teams often face is how to automate metadata at scale. Having started out as a data team ourselves, we know that automating repetitive tasks can help data teams maximize the value they provide to their organization. One way of doing so is through Atlan's playbooks! Playbooks help power metadata automation for your data assets in Atlan. You can create rule-based automations at scale and update metadata in bulk, helping you streamline your workflows. You can update the following asset metadata using playbooks: Certificates Descriptions Owners Terms Tags Domains Custom metadata For example, imagine your organization needs to transfer ownership of several data assets. Instead of your data team manually updating the ownership of each and every asset, you can create a playbook to automate this process and update the metadata of your assets at scale. Playbook recommendations ​ Before you begin, review some general guidelines on running playbooks in Atlan: Avoid running multiple playbooks simultaneously on the same set of assets. Allow one playbook run to be completed before proceeding with another operation on the same set of assets. Otherwise, you may experience performance issues and inconsistencies. Review and understand the depth of your asset lineage or hierarchy prior to enabling a tag propagation playbook. For assets with complex lineage, tag propagation may take longer to complete than the playbook runtime. You may want to review and judiciously select a list of assets that need to be tagged directly. For their child and/or downstream assets, Atlan recommends that you enable tag propagation . Create a playbook ​ To create a playbook in Atlan: From the left menu in Atlan, you can either: Click Assets to navigate to the assets page. From the Filters menu on the left or the tabs along the top, apply any asset filters . Next to the search bar, click the 3-dot icon and then click Create playbook to create a playbook for the filtered assets   -  this option is only visible to admin users. Click Governance to navigate to the governance center. Under the Governance heading of the Governance center , click Playbooks . Click Create New to get started. In the Create new playbook dialog box, enter the following details: For Name , enter a name for the task to be accomplished   -  for example, Update ownership . (Atlan recommends that the length of a playbook name must be no longer than 46 characters.) (Optional) For Description , enter a description. (Optional) Select an icon for your playbook. Click Create to save your playbook. Set up rules as filters ​ To set up rules as filters for your playbook: In the Build Rules page of your playbook, click Filters . For name, add a name to your filter. To set a matching condition for the filters, select Match all or Match any . Match all will logically AND the criteria, while Match any will logically OR the criteria. For Attributes , select a relevant option: For this example, we'll click Connection and then select a Snowflake connection. (Optional) To further refine your asset selection: Click All databases to filter by databases in a selected connection. Click All schemas to filter by schemas in a selected connection. Click Connector to filter assets by supported connectors . Click Asset type to filter by specific asset types   -  for example, tables, columns, queries, glossaries, and more. Click Certificate to filter assets by certification status . Click Owners to filter assets by asset owners . Click Tags to filter assets by your tags in Atlan, including imported Snowflake and dbt tags. (Optional) For Snowflake tags only, to the left of the checkbox, click Select value , and then from the Select tag value dialog, select any value(s) to filter assets by tag value. Click Glossary, terms, & categories to filter by a specific glossary or category to bulk update all the nested terms or by multiple glossaries and categories. Click Linked terms to filter assets by linked terms . Click Domains to filter by specific domains or subdomains to bulk update all the assets included in those data domains or subdomains. Click Products to filter for data products by specific data domains or subdomains. Click Schema qualified Name to filter assets by the qualified name of a given schema. Click Database qualified Name to filter assets by the qualified name of a given database. Click dbt to filter assets by dbt-specific filters and then select a dbt Cloud or dbt Core filter. Click Properties to filter assets by common asset properties . Click Usage to filter assets by usage metrics . Click Monte Carlo to filter assets by Monte Carlo-specific filters . Click Soda to filter assets by Soda-specific filters . Click Table/View to filter tables or views by row count, column count, or size. Click Column to filter columns by column-specific filters , including parent asset type or name, data type, or column keys . Click Process to filter lineage processes by the SQL query. Click Query to filter assets by associated visual queries . Click Measure to filter Microsoft Power BI measures using the external measures filter. For Operator , select Is one of for values to include or Is not for values to exclude. Depending on the selected attribute(s), you can also choose from additional operators : Select Equals (=) or Not Equals (!=) to include or exclude assets through exact match search. Select Starts With or Ends With to filter assets using the starting or ending sequence of values. Select Contains or Does not contain to find assets with or without specified values contained within the attribute. Select Pattern to filter assets using supported Elastic DSL regular expressions . Select Is empty to filter assets with null values. Select Belongs to or Doesn't belong to to filter data products by specific data domains or subdomains . For Values , select the relevant values. The values will vary depending on the selected attributes. (Optional) To add more filters, click Add filter and select Filter to add individual filters or Filter Group to nest more filters in a group. (Optional) To view all the assets that match your rules, in the Filters card, click View all for a preview. (Optional) To remove a playbook filter, to the right of any filter, click the three horizontal dots and then click Delete . (Optional) To turn off a playbook filter, to the right of any filter, click the three horizontal dots and then click Disable . Click Enable to turn on any disabled filters. Select the actions ​ To select the actions to be performed based on your rules: In the Build Rules page of your playbook, click Actions . For Select Action , select the relevant metadata option to update: Click Certificate to update the certification status of assets to Verified , Draft , Deprecated , or No certificate . Click Description to update the description of your assets. Click Owners to add, remove, or replace asset owners . In this example, we'll update the ownership of the assets. Click Terms to add terms to your assets or remove or replace them from linked assets . Click Tags to add tags to your assets or remove or replace them from tagged or propagated assets. Note that if there are multiple tag actions to be performed, Atlan will execute them in the following order: ADD , REMOVE , and then REPLACE . Click Domain to add your assets to a specific domain or subdomain or remove them from an existing linked domain or subdomain . Click any custom metadata structure and then select a custom metadata property to update or unlink it from your assets. For Select operator , select the relevant option. The operators will vary depending on the selected action. For Values , select the relevant option(s). The values will vary depending on the selected actions. (Optional) To add more actions, click Add Action . Did you know? You can control tag propagation when adding tags as an action in playbooks. Tag propagation is disabled by default. If you enable tag propagation, you will also be able to configure how tags are propagated . Run the playbook ​ If you'd like to continue working on your playbook, you can save it as a draft. If your playbook is ready, you can proceed to running it. To run the playbook: You can either: To run the playbook once immediately, click Run once . To schedule the playbook to run hourly, daily, weekly, or monthly, click Schedule and choose the preferred frequency, timezone, and time. danger If you're scheduling multiple playbooks, Atlan recommends spacing out the schedules as much as possible to minimize any overlap between the playbook workflow runs. For more about workflows in general, see workflow recommendations . Click Complete to run the playbook. In the resulting screen, click Go to profile to view your playbook profile. Once your playbook has completed its run, you will see the metadata updated for your assets! 🎉 Did you know? If you have any questions about setting up playbooks, head over here . Tags: atlan documentation Previous Playbooks Next Manage playbooks Playbook recommendations Create a playbook Set up rules as filters Select the actions Run the playbook"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-automations",
    "content": "Use data Reporting Report Types Report on automations On this page Report on automations Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The automations dashboard in the reporting center provides you with an overview of assets enriched using Atlan's automation features. You can view a summary of updates as well as top users for each feature. Did you know? The default date range for metrics is set to 14 days. You can also view metrics for the last 7, 30, or 45 days, or a custom date range of your choice, where applicable. Suggestions ​ You can track asset enrichment through suggestions from similar assets . You can also view top users who have accepted automated suggestions. Playbooks ​ You can monitor metrics related to playbooks , such as a summary of updates, top users, a playbooks-runs-over-time graph, and run activity. Total updates are reported in # counts by default. Click the % icon to view total updates in percentage form. Google Sheets ​ You can view a summary of asset enriched through the Atlan add-on for Google Sheets . For Updates over time , click any metadata option in the graph to view updates only for that option   -  for example, Announcements . Tags: integration connectors glossary business-terms definitions Previous Report on glossaries Next Report on queries Suggestions Playbooks Google Sheets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-governance",
    "content": "Use data Reporting Report Types Report on governance On this page Report on governance Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The governance dashboard in the reporting center helps you review and report on metrics related to all your personas , purposes , tags , and requests . Track query access ​ You can review query access for your personas and purposes from the governance dashboard. This can be especially helpful in managing your data governance setup. To view query access for a persona: From the left menu in Atlan, click Reporting and then click Governance . From the Governance dashboard, under Personas , navigate to Query Access . Under Query Access , click Personas with query access to view more details in the governance center. View assets tagged by propagation ​ Apart from viewing the total count of assets tagged by propagation , you can also view the propagated assets right from the dashboard for lineage analysis. To view assets tagged by propagation: From the left menu in Atlan, click Reporting and then click Governance . From the Governance dashboard, scroll down to the Tag by Propagation section. Click any tag to view a list of propagated assets in the sidebar. Track requests ​ You can view and take action on all your requests from the governance dashboard. To track metadata update requests: From the left menu in Atlan, click Reporting and then click Governance . From the Governance dashboard, scroll down to the Requests section. (Optional) Under Requests , click the date selector dropdown to filter requests by a predefined or custom date range. (Optional) Click the All asset types dropdown to filter requests by a specific asset type. Under Requests overview , view all requests grouped by request status. Click any request to take action in the Governance center . Tags: glossary business-terms definitions Previous Report on usage and cost Next Summarize metadata Track query access View assets tagged by propagation Track requests"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/summarize-metadata",
    "content": "Use data Reporting Metadata Summarize metadata Summarize metadata Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The reporting center helps you summarize and report on what's happening to your assets in Atlan. You can track metrics for asset enrichment, view metadata updates over time, review your data governance setup, and so much more. Use the following dashboards in the reporting center to: Assets -  monitor your assets Glossary -  track metrics for your glossaries, categories, and terms Governance -  review your governance setup Insights -  track metrics for your queries Automations -  monitor asset enrichment through automation features Usage and cost -  track asset usage and associated costs Tags: glossary business-terms definitions Previous Report on governance Next Report on assets"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-queries",
    "content": "Use data Reporting Report Types Report on queries On this page Report on queries Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The Insights dashboard in the reporting center helps you track metrics for all your queries and query runs. You can also use a variety of filters to customize your view of query metrics   -  including query collections , folders, saved queries , and more. Did you know? The default date range for metrics is set to 14 days. You can also view metrics for the last 7, 30, or 45 days, or a custom date range of your choice, where applicable. Filter by query type ​ Atlan allows you to write your own SQL queries or use the Visual Query Builder . This means you can also filter query metrics by query type. To filter queries by type of query: From the left menu in Atlan, click Reporting and then click Insights . From the Insights dashboard, in the upper right, click Query Type . From the Query Type dropdown, click SQL Query to view metrics for your SQL queries or click Visual Query to view metrics for your visual queries. (Optional) Under Queries by certificate , click any data point to view query assets in a sidebar. In the top right of the sidebar, click the Export button to export filtered assets to a spreadsheet. View scheduled queries by user ​ The Insights dashboard also offers you a breakdown of scheduled queries by individual users. You can keep track of your scheduled queries as well as view top users. To view scheduled queries by user: From the left menu in Atlan, click Reporting and then click Insights . From the Insights dashboard, scroll down to Scheduled Queries Under Scheduled Queries , click Created by all users . From the Created by all users dropdown, select the user by whom you want to filter scheduled queries. Tags: lineage data-lineage impact-analysis glossary business-terms definitions Previous Report on automations Next Report on usage and cost Filter by query type View scheduled queries by user"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-usage-and-cost",
    "content": "Use data Reporting Report Types Report on usage and cost On this page Report on usage and cost Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The usage and cost dashboard in the reporting center can help you better understand asset usage and optimize your operations accordingly. You can track storage consumption for your assets, sort asset metrics by popularity, and filter assets by context   -  all in one dashboard. The reporting center in Atlan currently supports usage and cost metrics for the following connectors: Amazon Redshift -  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source. Databricks -  tables, views, and columns. Expensive queries for Databricks assets are currently unavailable due to limitations of the Databricks APIs . Google BigQuery -  tables, views, and columns Snowflake -  tables, views, and columns Usage and cost metrics for Microsoft Power BI will be added in the future. Filter asset usage by users ​ You can track total assets queried and storage consumption by users in your organization. To filter asset usage by a user: From the left menu in Atlan, click Reporting and then click Usage & Cost . From the Usage & Cost dashboard, click the Queried by filter and select the user by whom you want to filter usage metrics. (Optional) To further refine your search , click More filters . You will now be able to view usage and cost metrics for a specific user! 🎉 View least used assets ​ You can find suggestions for deprecating assets based on the following factors: Tables and views without any lineage Asset size less than 100 GB Assets not queried at source in the last 30 days Less popular assets not updated at source in the last 3 or 6 months To view least used assets: From the left menu in Atlan, click Reporting and then click Usage & Cost . From the Usage & Cost dashboard, scroll down to Review your tables & views . (Optional) To further refine your search , click More filters . Click View assets for any card to view a list of suggested assets for deprecation in the sidebar. (Optional) In the top right of the sidebar, click the Export button to export filtered assets to a spreadsheet. Filter assets by context ​ Discover the top 25 most popular, most expensive, and most queried tables and views from the usage and cost dashboard. This can provide you with additional context regarding asset usage across the organization. Did you know? The compute cost for an asset is split between read and write queries, allowing you to better understand the cost breakdown for individual assets. To filter assets by context: From the Usage & Cost dashboard, scroll down to Top tables & views . In the Top tables & views section, click the filters menu, and depending on the type of asset metrics you'd like to view: Click Most Popular to view the top 25 most popular assets. Click Most Expensive to view the top 25 most expensive assets. Click Most Queried to view the top 25 most queried assets. Did you know? If you have any questions about usage and popularity metrics, head over here . Tags: lineage data-lineage impact-analysis glossary business-terms definitions Previous Report on queries Next Report on governance Filter asset usage by users View least used assets Filter assets by context"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/reporting/how-tos/report-on-glossaries",
    "content": "Use data Reporting Get Started Report on glossaries On this page Report on glossaries Who can do this? You must be an admin user in Atlan to view the reporting center. If enabled by admins , member users can also view the assets , glossary , Insights , and usage and cost dashboards. Permission to view the governance and automations dashboards is reserved for admin users only. The glossary dashboard in the reporting center helps you track your glossaries, categories, and terms. You can view a high-level overview of all your glossaries or use a variety of filters to drill down further. Did you know? You can also view and take action on all your requests for updating terms and categories, right from the glossary dashboard. Filter glossaries ​ You can use the filters in the glossary dashboard to customize your view of glossary metrics. To filter glossaries: From the left menu in Atlan, click Reporting and then click Glossary . Under Glossary , for the All glossaries filter, select a glossary   -  for this example, we'll select the Concepts glossary. For the All Categories filter, select a category   -  for this example, we'll select the Marketing category. (Optional) To further refine your search , click More filters . You will now be able to view metrics for your filtered glossary! 🎉 View terms by linked assets ​ You can view metrics for terms with linked assets before making any changes to your terms. This can help you understand the downstream impact of your modifications in advance. To view linked assets for your terms: From the left menu in Atlan, click Reporting and then click Glossary . From the Glossary dashboard, scroll down to Terms by linked assets . Click a term to view a list of all the linked assets in the sidebar. (Optional) In the sidebar, next to Search all assets , click the export icon to export linked assets for a term to a spreadsheet. Update new terms and categories ​ You can also get to work on recently created terms and categories from the glossary dashboard   -  for example, attach a tag to a recently added term. To update a recently created term: From the left menu in Atlan, click Reporting and then click Glossary . From the Glossary dashboard, scroll down to Recently Created Terms & Categories . Click any term to view the term details in the sidebar   -  for this example, we'll select Economic Census . In the term sidebar, navigate to Tags and click the + sign to attach a tag to your term   -  for example, Public . Next to your selected tag in the popup, click the downward arrow to configure tag propagation . Click Save to save your changes. Tags: lineage data-lineage impact-analysis glossary business-terms definitions Previous Reporting Next Report on automations Filter glossaries View terms by linked assets Update new terms and categories"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity/how-tos/find-assets-by-usage",
    "content": "Use data Usage & Popularity Get Started How to find assets by usage On this page Find assets by usage Data teams often lack clarity on which data assets can be considered trustworthy, whether these are frequently used, the freshness of the data itself, or how critical these are for enrichment and governance. With Atlan's usage and popularity metadata, you'll be able to check off all these boxes! You can view usage metrics for your assets collected over the last 30 days. Atlan currently supports usage and popularity metrics for the following connectors: Amazon Redshift -  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source. Databricks -  tables, views, and columns. Expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the Databricks APIs . Google BigQuery -  tables, views, and columns Microsoft Power BI -  reports and dashboards Snowflake -  tables, views, and columns Filter assets by usage ​ Use the usage filters to filter your assets by usage metadata. For instance, you'll be able to filter assets with zero queries and archive them or find costly assets to better optimize your operations. To filter assets by usage metadata: From the left menu in Atlan, click Assets . In the Filters menu in Assets , click Usage to expand the list of filters. From the Usage menu: For SQL assets, use the following filters: Click Number of queries to filter by the number of queries at source in the last 30 days. Click Number of users to filter by the number of users who queried an asset at source in the last 30 days. Click Last queried to filter by the last queried timestamp at source. Click Last row updated at to filter by the last row updated timestamp at source. To filter assets by compute cost , click Snowflake credits for Snowflake assets or click BigQuery query cost for Google BigQuery assets. For BI assets, use the following filters: Click Views count to filter by the number of views at source in the last 30 days. Click Number of users to filter by the number of users who viewed an asset at source in the last 30 days. Click Last viewed to filter by the last viewed timestamp at source. Your search results will now be filtered by usage metadata! 🎉 Sort assets by popularity ​ Sort your data assets by popularity metadata to view the most or least used tables, views, or columns. For example, sorting your assets by popularity can help you deprecate unused or stale data assets, helping you reduce operational costs for your organization. To sort assets by popularity: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. Your assets in the search results will now be sorted by popularity of usage! 🎉 Did you know? You can also deep dive into usage metrics for Snowflake, Databricks, Google BigQuery, and Microsoft Power BI in Atlan. Tags: connectors data Previous Usage and Popularity Next How to interpret usage metrics Filter assets by usage Sort assets by popularity"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/connect-data-sources-for-azure-hosted-atlan-instances",
    "content": "Connect data Connectivity Framework Connector Framework How-tos Connect data sources for Azure-hosted Atlan instances On this page Connect data sources for Azure-hosted Atlan instances This document provides recommended solutions for integrating Atlan instances hosted on Microsoft Azure with the following: Data sources hosted on Microsoft Azure Data sources hosted in data centers Azure-managed data sources ​ To connect your Atlan instance hosted on Microsoft Azure with a Microsoft Azure-managed data source, Atlan recommends the following method. For this purpose, we'll consider a Microsoft Azure-managed Snowflake instance: For data sources like Snowflake, you can use Azure Private Link . Atlan will create a private endpoint in your Atlan instance to connect to your Snowflake instance using the the resource ID. This will create a request in your Atlan instance. Accept the request to proceed. Once the request has been accepted, Atlan will be able to access the data source using a private endpoint over the Azure backbone network, bypassing the internet. Atlan will also create a private DNS and add an A record for the private endpoint previously created in the Azure-managed Atlan instance and share the details with you. You can use this DNS record to connect to the Azure-hosted Snowflake data source. Each data source will require a separate Azure private endpoint. On-premises data sources ​ To connect your Atlan instance hosted on Microsoft Azure with an on-premises data source, Atlan recommends the following method. For this purpose, we'll consider an on-premises MySQL server hosted in a data center: For an on-premises MySQL database, you can consider a combination of Azure Private Link , Azure Load Balancer , and Azure Virtual Machines . For this method, the data source must be accessible from your Microsoft Azure subscription. You will need to create a virtual machine in your Azure-managed Atlan instance to port forward the request to the corresponding data source. Add a network load balancer to the virtual machine and create a Private Link service to the load balancer. Atlan will create a private endpoint in your Atlan instance to connect to the Private Link service of the load balancer. Atlan will also create a private DNS and add an A record for the private endpoint previously created in the Azure-managed Atlan instance and share the details with you. You can use this DNS record to connect to the on-premises data source using the load balancer. Only one private endpoint will be required to connect to all the on-premises data sources through port forwarding. Tags: connectors data Previous Monitor connectivity Next Mine queries through S3 Azure-managed data sources On-premises data sources"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/usage-and-popularity/how-tos/interpret-usage-metrics",
    "content": "Use data Usage & Popularity Analysis How to interpret usage metrics On this page Interpret usage metrics Atlan currently supports usage and popularity metrics for the following connectors: Amazon Redshift -  tables, views, and columns. Expensive queries and compute costs for Amazon Redshift assets are currently unavailable due to limitations at source. Databricks -  tables, views, and columns. Expensive queries and compute costs for Databricks assets are currently unavailable due to limitations of the Databricks APIs . Google BigQuery -  tables, views, and columns Microsoft Power BI -  reports and dashboards Snowflake -  tables, views, and columns Powered by Atlan's enhanced query-mining capabilities, you can view popularity metrics for supported assets: The popularity score of an asset is computed using both the number of queries and the number of users who have queried that asset in the last 30 days. The popularity score of an asset helps determine its relative popularity. All assets with a popularity score are then slotted into one of four percentile groups   - Least popular , Less popular , Popular , and Most popular . Popularity score is calculated using the following formula: number of distinct users * log (total number of read queries) Time period = 30 days The popularity indicator is displayed for all supported assets that have been queried in the last 30 days. This indicator visualizes the relative popularity of an asset on a scale of 1 to 4 blue bars   -  1 being the lowest score and 4 being the highest. A popularity popover will appear when hovering over the popularity indicator. It displays additional information pertaining to an asset, such as a graph for trends in the data, last queried and by whom, and when the data was last updated. View popularity metrics ​ To view popularity metrics for your assets, complete these steps. Identify popular assets ​ Being able to identify your most relevant and trusted data assets can help you increase their adoption and drive usage within your organization. To view popularity metrics for an asset: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. Your assets will now have a popularity indicator. To view the popularity popover for an asset, click or hover over the popularity indicator . You'll now be able to see all the relevant popularity metrics for your asset! 🎉 View usage metrics in the asset sidebar ​ The new Usage tab in the asset sidebar helps you view usage metadata for your assets. For example, if you'd like to appoint a data steward for your data assets, you'll be able to determine the right candidate based on the top users for that asset. You'll also be able to review popular queries or users for a particular table while checking for data compliance. To view usage details for an asset: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. In the bottom right of any asset card, click or hover over the popularity indicator to open the popularity popover. In the popularity popover, click View usage details to view the following: For Usage , view top and recent users in the last 30 days. For Queries , view top five queries by context   - Popular , Slow , and Expensive . Only read queries or SELECT statements are shown for these queries. For Compute , view the total compute cost for an asset. The compute cost is split between read and write queries, allowing you to better understand the cost breakdown for individual assets: Read queries   - SELECT statements. Write queries   -  all non- SELECT statements, for example, UPDATE , INSERT , CREATE , and more. The usage details for the asset will now appear in the asset sidebar! 🎉 View and sort columns by popularity ​ For any Snowflake, Databricks, or Google BigQuery table or view sorted by popularity, you'll also be able to view and sort the columns by popularity in the asset profile. To view column assets by popularity: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. Click any asset to open to its asset profile. In the Column preview tab of the asset profile, hover over the popularity indicator to view the popularity popover for your columns. (Optional) In the search bar under Column preview , click the sort icon and then click Most popular or Least popular to sort columns by popularity. You'll now be able to view the popularity score, number of queries and users, and timestamp for last queried for your columns! 🎉 View queries by context ​ Get the context you need before querying an asset to help you optimize your queries. Query popular, slow, or expensive queries from the Usage tab directly in Insights. To view and work with queries by context: From the left menu in Atlan, click Assets . For Connector on the Assets page, select a supported connector   -  for this example, we'll select Snowflake . Next to the search bar on the Assets page, click the sort button. From the Popularity sorting menu, click Most popular to view most used assets or Least popular to view least used assets. In the bottom right of any asset card, click or hover over the popularity indicator to open the popularity popover. In the popularity popover, click View usage details . In the Usage tab in the asset sidebar, navigate to Queries and depending on the type of query you'd like to see: Click Popular to see the top five most popular queries. Click Slow to see queries sorted by average duration and last run. Click Expensive to see the top five most expensive queries. Once you've selected the relevant query type, hover over a query card to: Click the expand icon to see the query details. Click the copy icon to copy the query and use it as a template for writing your own queries. Click the code icon to open the query directly in Insights and run it. Did you know? If you have any questions about usage and popularity metrics, head over here . Tags: connectors data api Previous How to find assets by usage Next Troubleshooting usage and popularity metrics View popularity metrics View queries by context"
  },
  {
    "url": "https://docs.atlan.com/product/capabilities/requests/how-tos/manage-requests",
    "content": "Use data Requests Get Started Manage requests On this page Manage requests Did you know? Atlan supports governance workflows ! Once you have enabled governance workflows and inbox , Atlan will channel requests and approvals for your governed assets through governance workflows and land them in your inbox . Request changes to an asset ​ Who can do this? Any user without edit access to an asset's metadata can request changes to an asset. To request changes to an asset: Navigate to the asset you want to change. For example, use search or discovery to get there. Click on the part of the asset you want to change, as if you were making the change directly. You'll see that you do not have permission and are instead suggesting a change. Instead of saving your change, Atlan will prompt you to submit it as a request. Did you know? The lock icon and slightly transparent text show that you do not have access to an asset. Track your request(s) ​ For a specific asset ​ To track your requests for a specific asset: Navigate to the asset you want to track. For example, use search or discovery to get there. From the right navbar of the asset, click on the Request tab. (Optional) To the right of the Requests heading, change the drop down to All to see all requests you've made on the asset. Pending requests are those still awaiting approval (or rejection). Approved requests are those that someone has already accepted (and applied). Rejected requests are those that someone has already declined (and will not be applied). All your requests ​ To track all the requests you have raised, across all assets: Click the icon in the upper left of any page to navigate to the Atlan home screen. Scroll to the bottom of the screen to the My Requests card. (Optional) In the upper right of the card, change the drop down to narrow requests by status. Pending requests are those still awaiting approval (or rejection). Approved requests are those that someone has already accepted (and applied). Rejected requests are those that someone has already declined (and will not be applied). (Optional) Hover over any pending request to see who can approve or reject it. Get notified on Slack ​ If your organization's Slack account is integrated with Atlan , you will receive Slack notifications when your requests are approved or rejected. To receive Slack notifications on your requests: The email address used for Slack and Atlan should be the same, even if you haven't personally integrated the accounts. The Slack app should have been installed before August 12, 2022. If installed later, you'll need to update Slack. If different email addresses were used for Slack and Atlan, you'll first need to link your Slack account with Atlan . Approve or reject request(s) ​ Who can do this? Any non-guest user with edit access to an asset's metadata can approve the request. This only includes admin and member users. For a specific asset ​ To approve or reject a request for a specific asset: Navigate to the asset you want to manage. For example, use search or discovery to get there. From the right sidebar of the asset, click on the Request tab. (Optional) To the right of the Requests heading, change the drop-down to Pending to see open requests on the asset. Hover over any pending request to either approve or reject it: Click the arrow next to Approve to approve the change with a comment, or click Approve to approve it without a comment. Click the arrow next to Reject to reject the change with a comment, or click Reject to reject it without a comment. All requests ​ Who can do this? Currently only admin users have access to see all requests. To approve or reject requests against any asset: From the left menu of any screen, click Governance . Under the Governance heading, click Requests . (Optional) In the Requests table, click the search bar to search for requests to take action on or click the funnel icon to filter your requests: Select Connection to filter by a specific connector , or drill down further by connection, database, or schema. Select Status to filter by request status   -  pending, approved, or rejected. Select Requestor to filter by specific users. Select Tags to filter by tag update requests. Select Request type to filter by type of metadata update requested   - description , tag , certificate , term , or owner . Select Asset type to filter by specific asset types   -  tables, columns, queries, and more. Select Raised in to filter by a predefined or custom data range. Hover over any pending request to either approve or reject it: Click the arrow next to Approve to approve the change with a comment, or click Approve to approve it without a comment. Click the arrow next to Reject to reject the change with a comment, or click Reject to reject it without a comment. Did you know? You can also configure the Slack integration to receive notifications for metadata update requests raised in Atlan and take action directly from Slack . Use the requests widget ​ The requests widget brings together all your requests in one location to help you track, manage, and prioritize them more effectively. You can open the requests widget from anywhere in Atlan, find all the requests that need your attention and take action immediately, and view a summary of your requests and track their statuses. Take action on requests ​ To take action on requests, from the requests widget: From the top right of any screen in Atlan, click the Requests icon. In the Requests dialog, under Needs attention , hover over or click any request to view actions. From the request card, you can either: To reject the update request, click Reject or click the downward arrow to the right and then click Reject with comment to add a comment as well. To approve the update request, click Approve or click the downward arrow to the right and then click Approve with comment to add a comment as well. Track your requests ​ To track your requests, from the requests widget: From the top right of any screen in Atlan, click the Requests icon. In the Requests dialog, click My requests to track all your requests. To view your requests sorted by status, click Pending to view pending requests, Approved for approved requests, or Rejected for requests that have been rejected. Tags: integration connectors workflow automation orchestration Previous Requests Next What are requests? Request changes to an asset Track your request(s) Approve or reject request(s) Use the requests widget"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/mine-queries-through-s3",
    "content": "Connect data Connectivity Framework Connector Framework How-tos Mine queries through S3 On this page Mine queries through S3 Once you have crawled assets from a supported connector, you can mine query history. Supported connectors include the following: Amazon Redshift Google BigQuery Hive Microsoft Azure Synapse Analytics Microsoft SQL Server Snowflake Teradata For each of the supported connectors, Atlan supports mining query history via S3. This is useful when you have files that hold query history beyond what the source itself retains. To mine lineage from these sources from S3, complete the following steps. Structure the query files ​ To make the query history files available for Atlan, ensure the files: Use a .json extension. Are present in a single S3 bucket and prefix (directory). To structure the contents of the files for Atlan, ensure: Each line is a single JSON value. (The JSON object cannot be pretty-formatted or span multiple lines.) Each SQL query is on its own line. Commas are not used to separate the lines. Did you know? You can also provide a default database and schema, and session IDs in the JSON. If a SQL query has only the name of the table or view it queries, Atlan will use the default database and schema to generate lineage for the query. Including the session ID speeds up lineage processing. If provided, ensure that all queries belonging to the same session are next to each other in the file. Here is an example of what your JSON should look like. (Here it is split across multiple lines to assist reading, but remember it must all be on a single line in the file!) { \"QUERY_TEXT\" : \"insert into NETFLIX_DB.PUBLIC.MOVIES_FILTERED as select m.* from MOVIES m where m.RATING > 5;\" , \"DATABASE_NAME\" : \"NETFLIX_DB\" , \"SCHEMA_NAME\" : \"PUBLIC\" , \"SESSION_ID\" : \"5c2f0a41-5d02-46f1-b9bd-ef80ad571013\" } The name of the keys or properties in the JSON can be configured while setting up the miner package. In the example above, the default database ( DATABASE_NAME ) and schema ( SCHEMA_NAME ) will be used to qualify the query against the table MOVIES as NETFLIX_DB.PUBLIC.MOVIES . Set up the S3 bucket ​ The query files must be available in an S3 bucket. You can either upload these files to the Atlan deployment bucket or use your own S3 bucket. Option 1: Use the Atlan S3 bucket ​ To avoid access issues, we recommend uploading the required files to the same S3 bucket as Atlan. Raise a support request to get the details of your Atlan bucket and include the ARN value of the IAM user or IAM role we can provision access to. To configure access, add the following IAM policy to the default EC2 instance role used by the Atlan EKS cluster. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:GetObject\" ] , \"Resource\" : [ \"arn:aws:s3:::<bucket-name>\" , \"arn:aws:s3:::<bucket-name>/<prefix>/*\" ] } ] } Replace <bucket-name> with the bucket where the data is uploaded. Replace <prefix> with the prefix (directory) where all the files have been uploaded. If you instead opt to use your own S3 bucket, you will need to complete the following steps: Option 2: Use your own S3 bucket ​ danger S3 buckets with VPC endpoints currently do not support cross-region requests . This may result in workflows not picking up objects from your bucket. You'll first need to create a cross-account bucket policy giving Atlan's IAM role access to your bucket. A cross-account bucket policy is required since your Atlan tenant and S3 bucket may not always be deployed in the same AWS account. The permissions required for the S3 bucket include   - GetBucketLocation , ListBucket , and GetObject . To create a cross-account bucket policy: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new policy to allow access by this ARN and update your bucket policy with the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<role-arn>\" } , \"Action\" : [ \"s3:GetBucketLocation\" , \"s3:ListBucket\" , \"s3:GetObject\" ] , \"Resource\" : [ \"arn:aws:s3:::<bucket-name>\" , \"arn:aws:s3:::<bucket-name>/<prefix>/*\" ] } ] } Replace <role-arn> with the role ARN of Atlan's node instance role. Replace <bucket-name> with the name of the bucket you are creating. Replace <prefix> with the name of the prefix (directory) within that bucket where you will upload the files. Once the new policy has been set up, please notify the support team. Your request should include the S3 bucket name and prefix. This should be done prior to setting up the workflow so that we can create and attach an IAM policy for your bucket to Atlan's IAM role. (Optional) Update KMS policy ​ If your S3 bucket is encrypted, you will need to update your KMS policy. This will allow Atlan to decrypt the objects in your S3 bucket. Provide the KMS key ARN and KMS key alias ARN to the Atlan support team. The KMS key that you provide must be a customer managed KMS key. (This is because you can only change the key policy for a customer managed KMS key, and not for an AWS managed KMS key. Refer to AWS documentation to learn more.) To whitelist the ARN of Atlan's node instance, update the KMS policy with the following: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"Decrypt Cross Account\" , \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<role-arn>\" } , \"Action\" : [ \"kms:Decrypt\" , \"kms:DescribeKey\" ] , \"Resource\" : \"*\" } ] } Replace <role-arn> with the role ARN of Atlan's node instance role. Select the miner ​ To select the S3 miner: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Miner . From the list of packages, select the miner for your source and click on Setup Workflow . Configure the miner ​ To configure the S3 miner: For Connection , select the connection to mine. (To select a connection, a crawler must have already run against that source.) For Miner extraction method , select S3 . Enter the details for your files: For Bucket Name , enter the name of your S3 bucket or Atlan's bucket, including s3:// . For Bucket Prefix , enter the S3 prefix (directory) within the bucket where the files are located. (Optional) For Bucket Region , enter the name of the S3 region in which the bucket exists. For SQL Json key , enter the JSON key containing the SQL query value. (In the example above, this was QUERY_TEXT .) For Default Database Json Key , enter the JSON key containing the name of the default database. (In the example above, this was DATABASE_NAME .) For Default Schema Json Key , enter the JSON key containing the name of the default schema. (In the example above, this was SCHEMA_NAME .) For Session ID Json Key , enter the JSON key containing the session ID under which the query ran. (In the example above, this was SESSION_ID .) (Optional) For Control Config , if Atlan support has provided you a custom control configuration, select Custom and enter the configuration into the Custom Config box. You can also: Enter {“ignore-all-case”: true} to enable crawling assets with case-sensitive identifiers. Run the miner ​ To run the S3 miner, after completing the steps above: To run the miner once, immediately, at the bottom of the screen click the Run button. To schedule the miner to run hourly, daily, weekly or monthly, at the bottom of the screen click the Schedule & Run button. Once the miner has completed running, you will see lineage for your source's assets created by the queries in S3! 🎉 Frequently asked questions ​ If I remove queries from S3 and run the miner, does it remove the lineage generated from those queries? ​ No, we do not remove lineage from older queries that are no longer in the bucket. Does the miner reprocess files in the S3 prefix? ​ Yes, we process all files present in the S3 prefix and publish any new lineage generated. We recommend removing older files when updating the files in the S3 prefix. I used this approach for initial mining. Can I convert the miner I already set up to do its future mining direct from the source (not S3)? ​ Yes, just edit the workflow configuration . Alternatively, you can also set up another miner for the same connection. Are the database and schema name parameters always required in the JSON file? ​ The DATABASE_NAME and SCHEMA_NAME fields can be set to null if that data is already available in the query. These properties are used as a fallback option for when queries are run in the context of a certain schema or database. What SQL statements should be added to the S3 miner JSON file for lineage? ​ You will need to add DDL and DML statements to the S3 miner JSON file for mining lineage. SELECT is not required since it is a DQL statement. Both UPDATE and DELETE can be based on values from another table, so these statements will be required for generating lineage. Tags: connectors data crawl Previous Connect data sources for Azure-hosted Atlan instances Next How to order workflows Structure the query files Set up the S3 bucket Option 1: Use the Atlan S3 bucket Option 2: Use your own S3 bucket Select the miner Configure the miner Run the miner Frequently asked questions"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/manage-connectivity",
    "content": "Connect data Connectivity Framework Connector Framework How-tos Manage connectivity On this page Manage connectivity Once you've scheduled or run a workflow you can modify its configuration at any time. The configuration that can be modified may vary by workflow but the general steps remain consistent. Modify connectivity ​ To modify the configuration of an existing workflow, complete the following steps. On the left of any screen, navigate to Workflow . Under Monitor select an existing workflow tile. (You may need to expand the run history or filter first.) From the Workflow Run History table, click on the previous run of the workflow you want to modify. In the upper left of the screen, change to the Config tab. Modify the parts of the workflow configuration you require: Under <Connector> Credential , use the Edit Credentials button to change the credentials for the source. danger If you're updating the connection credentials, you may also need to update the metadata filters before running the updated workflow. Atlan currently does not detect changes to your connection settings and update the metadata filters automatically. Under Connection settings , use the Edit button to change the connection details: Modify whether or not querying or data previews are allowed for the source. Modify the query row limit to enable exporting large query results via email . Modify the query timeout limit   -  expandable up to 60 minutes. Under Connection Admins , click the pencil icon to add or remove connection admins. danger If you do not specify any user or group, nobody will be able to manage the connection   -  not even admins. Under Metadata , use the selectors to modify which metadata to include and exclude. To check for any permissions or other configuration issues before running the workflow, click Preflight checks . Once you've made your updates, click the Update button to save the changes. You can optionally run the workflow with the new configuration immediately. You will need to confirm your changes by clicking the Yes button. Note that some workflow changes may take a few minutes to come into effect. That's it   -  next time you run the workflow, or it runs on its schedule, it will use your changes! 🎉 danger If you modify the Metadata portion, any previously crawled metadata that is now excluded will be archived on the next workflow run. Tags: integration connectors workflow automation orchestration Previous Connectors Next Monitor connectivity Modify connectivity"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/monitor-connectivity",
    "content": "Connect data Connectivity Framework Connector Framework How-tos Monitor connectivity On this page Monitor connectivity Atlan runs its crawlers through an orchestrated set of automated tasks. To monitor these orchestrated set of tasks follow these steps. Monitor the crawling process ​ You can visualize the individual tasks a workflow runs as a directed acyclic graph (or \"DAG\"). To visualize the crawling process: When running a workflow immediately, you will be redirected to the monitoring page within 5 seconds. At any other moment: From the left menu, click Workflows to navigate to the Workflow center : By default, workflow runs from the last 24 hours will be shown. (Optional) Use the filters along the top to narrow down to the workflow run you want to monitor. From the Workflow run history table, click the workflow run you want to check. On the left of the screen under the Summary tab, you can also see: The current status of the workflow run. The start and finish time of the workflow run. The elapsed time (duration) of the workflow run. Who triggered the workflow run and how (manually or automatically). Identify errors ​ If a crawler fails due to an error, Atlan will show where the failure occurred in the visualization. To review the failure of any workflow with an error: Open the workflow run visualization (using either option above). Under the Summary tab on the left of the screen, click the View Failed tasks button. Atlan will take you to the Failed Tasks tab on the left of the screen. Here you can review details about the specific activity or activities that failed. Review log files ​ Each task in the DAG may produce a log file containing additional details. To review the log file for a specific activity: Click the task (activity node) in the DAG visualization. Open the Failed Tasks tab of a workflow run visualization (see steps above). To the right of each failed step, click the Logs button. If there are any logs available, Atlan will display them on the screen. Did you know? Not every failed activity will produce a log. Look at the Message field of failed tasks for ideas about what went wrong when there is no log file available. Manage all workflows ​ You can monitor and manage all your workflows in Atlan from the workflow center. To manage all your workflows: From the left menu of any screen in Atlan, click Workflows . From the tabs along the top in the Workflow center , click Manage . Search for a specific workflow from the search bar or click Select package to filter by supported connectors . (Optional) In the Filters menu on the left, select a filter to drill down further: Click Created by to filter workflows created by specific users in Atlan. Click Workflow type to filter workflows by type of workflow   -  connectors, utilities, and miners. Each workflow type also displays the total count of workflow runs for that type. Click Schedule to filter workflows by scheduled or unscheduled runs. The workflow preview includes a summary of workflow details. Navigate to the workflow sidebar on the right, from the sidebar: The Overview tab displays run count, when the workflow was created and by whom, workflow schedules if applicable, and last 5 runs. (Optional) Click Run workflow to run the workflow directly from the sidebar. The Runs tab displays a summary of past workflow runs. (Optional) Select a workflow run to view more details or modify connectivity . Select any workflow to open the workflow. Tags: integration connectors Previous Manage connectivity Next Connect data sources for Azure-hosted Atlan instances Monitor the crawling process Identify errors Review log files Manage all workflows"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/order-workflows",
    "content": "Connect data Connectivity Framework Connector Framework How-tos How to order workflows On this page order workflows The order of operations you run in Atlan is important. Follow the specific workflow sequence outlined below when crawling data tools . The right order particularly ensures that lineage is constructed without needing to rerun crawlers. Order of operations ​ To have lineage across tools, you need to: Crawl data stores first -  for example, SQL data sources , NoSQL data sources , event buses , and schema registries . Run data quality tools -  for example, Monte Carlo and Soda . Mine query logs - mine queries through S3 or run miner packages for supported sources. Run extract-load tools -  for example, Fivetran , Airflow/OpenLineage and other supported distributions , and data processing tools like Apache Spark/OpenLineage , Alteryx . Run transformation tools -  for example, dbt and Matillion . Crawl business intelligence tools last -  for example, supported BI tools like Looker , Microsoft Power BI , Tableau , and more. If you use a different order, the upstream assets (data stores) might not yet exist when you load the BI metadata. In that case, you may see lineage within the BI metadata, but not between the BI metadata and data sources. If this happens, no worries—just rerun your existing workflows following the recommended order and Atlan can resolve it. Did you know? As a general rule of thumb, start by crawling the data source—including BI tools—before mining query logs. For example, when aiming to mine Microsoft Power BI, begin with a crawl of Microsoft Power BI. Workflow recommendations ​ The following are general guidelines and best practices for running workflows in Atlan: Schedule your workflows based on how often you want your metadata in Atlan to be updated   -  weekly, monthly, and so on. To configure custom cron schedules, learn more here . Avoid any overlaps between workflow schedules to ensure consistent workflow run times. Remember that the first workflow run can typically take much longer than subsequent runs. The first run establishes the connection, queries the source, extracts and transforms the metadata, and then publishes your assets for the first time in Atlan. If running a miner for the first time, set a start date around 3 days prior to the current date and then schedule it daily to build up to two weeks of query history. Mining two weeks of query history on the first miner run may cause workflows to time out or hit resource consumption errors. For all subsequent miner runs, Atlan requires a minimum lag of 24 to 48 hours to capture all the relevant transformations that were part of a session. Learn more about the miner logic here . Run preflight checks before running the crawler to check for any permissions or other configuration issues, including testing authentication. Troubleshooting tips ​ Here are a few tips to help you troubleshoot workflow failures in Atlan: If test authentication or preflight checks fail, check the source to ensure that your credentials are correct and you have requisite access to crawl the metadata. If you're connecting to Atlan via private link and experience any network-related errors or timeouts during test authentication, it may mean that there is a network connectivity issue between the source and Atlan. Reach out to Atlan support to help you investigate further. If both test authentication and preflight checks fail and succeed intermittently when tried multiple times, this may mean that your cluster is in an unstable state and needs to be restarted. Notify Atlan support to restart your cluster. Tags: connectors data crawl Previous Mine queries through S3 Next How to provide SSL certificates Order of operations Workflow recommendations Troubleshooting tips"
  },
  {
    "url": "https://docs.atlan.com/product/connections/how-tos/provide-ssl-certificates",
    "content": "Connect data Connectivity Framework Connector Framework How-tos How to provide SSL certificates On this page provide SSL certificates SSL (Secure Sockets Layer) encryption helps establish a secure connection between your data source and Atlan. Atlan currently only supports SSL certificates for crawling Tableau . The following types of SSL certificates are supported: Self-signed ​ Paste the public .crt or .cert part of your TLS certificate in the Privacy Enhanced Mail (PEM) format. For example: ----BEGIN CERTIFICATE---- MIIDazCCAlOgAwIBAgIJAOqRDRz0BxIAMA0GCSqGSIb3DQEBCwUAMIGZMQswCQYD ... ... ... u1Q== ----END CERTIFICATE---- Internal CA ​ An SSL certificate chain is a sequence of certificates consisting of three parties: A root certificate authority, One or more intermediate certificate authorities, And the server certificate. Paste the root, intermediate, and server certificates in the following format: ----BEGIN CERTIFICATE---- ABCDE...... ----END CERTIFICATE---- ----BEGIN CERTIFICATE---- EFGHT...... ----END CERTIFICATE---- ----BEGIN CERTIFICATE---- NAMNOP...... ----END CERTIFICATE---- ----BEGIN CERTIFICATE---- KROPS...... ----END CERTIFICATE---- The top certificate is the root certificate Followed by the hops in the right sequence Ending with server certificate Tags: connectors data crawl Previous How to order workflows Next What are preflight checks? Self-signed Internal CA"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/aws-lambda/how-tos/create-an-aws-lambda-trigger",
    "content": "Configure Atlan Integrations Automation AWS Lambda Create an AWS Lambda trigger On this page Create an AWS Lambda trigger Once you have configured the AWS Lambda permissions , you can run an AWS Lambda function. To run a Lambda function, complete the following s teps. Select the utility ​ To select the AWS Lambda trigger utility: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Utility . From the list of packages, select AWS Lambda Trigger and click on Setup Workflow . Provide credentials ​ To enter your AWS credentials: For Authentication choose the method to authenticate with AWS: For IAM User authentication , enter the AWS Access Key , AWS Secret Key , and Region of AWS to use. For IAM Role authentication , enter the Region of AWS and (optional) AWS Role ARN to use. Click Test Authentication to confirm connectivity to AWS using these details. When successful, at the bottom of the screen click Next . Configure the Lambda function ​ To configure the Lambda function: Under Function ARN enter the ARN for the Lambda function to call. (Optional) Under Qualifier enter a specific version of the Lambda function to call. (Or leave this as $LATEST to always run the latest version of the function.) (Optional) Under Payload enter a minimized (compact) form of any JSON payload to pass to the Lambda function. (Leave this as an empty JSON object {} if you have nothing to pass to the Lambda function.) Under Invocation Type select how you would like call the Lambda function: Use Synchronously to use synchronous invocation . With this approach, the response body and headers include details about the response, including errors. Use Asynchronously to use asynchronous invocation . With this approach, AWS queues events and they could be skipped or processed more than once. Run the Lambda function ​ You can now run the Lambda function. At the bottom of the screen, click Run to run the function once, immediately. Click Schedule & Run to scheduled the function to run hourly, daily, weekly, or monthly. Tags: integration authentication setup Previous Set up AWS Lambda Next Always On Select the utility Provide credentials Configure the Lambda function Run the Lambda function"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/aws-lambda/how-tos/set-up-aws-lambda",
    "content": "Configure Atlan Integrations Automation AWS Lambda Set up AWS Lambda On this page Set up AWS Lambda warning 🤓 Who can do this? You will probably need your AWS Lambda administrator to run these commands   -  you may not have access yourself. Create IAM policy ​ To create an IAM policy with the necessary permissions, follow the steps in the AWS Identity and Access Management User Guide . Create the policy using the following JSON: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"lambda:InvokeFunction\" , \"lambda:InvokeAsync\" \"lambda:ListFunctions\" ] , \"Resource\" : \"*\" } ] } Choose authentication mechanism ​ Using the policy created above, configure one of the following options for authentication. User-based authentication ​ To configure user-based authentication: Create an AWS IAM user by following the steps in the AWS Identity and Access Management User Guide . On the Set permissions page, attach the policy created in the previous step to this user. Once the user is created, view or download the user's access key ID and secret access key . danger This will be your only opportunity to view or download the access keys. You will not have access to them again after leaving the user creation screen. Role-based authentication ​ To configure role-based authentication, attach the policy created in the previous step to the EC2 role that Atlan uses for its EC2 instances in the EKS cluster. Please raise a support ticket to use this option. Role delegation-based authentication ​ To configure role delegation-based authentication: Raise a support ticket to get the ARN of the Node Instance Role for your Atlan EKS cluster. Create a new role in your AWS account by following the steps in the AWS Identity and Access Management User Guide . When prompted for policies, attach the policy created in the previous step to this role. When prompted, create a trust relationship for the role using the following trust policy. (Replace <atlan_nodeinstance_role_arn> with the ARN received from Atlan support.) { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"AWS\" : \"<atlan_nodeinstance_role_arn>\" } , \"Action\" : \"sts:AssumeRole\" , \"Condition\" : { } } ] } Now, reach out to Atlan support with: The name of the role you created above. The ID of the AWS account where the role was created. danger Wait until the support team confirms the account is allowlisted to assume the role before running the Lambda function. Tags: atlan documentation Previous AWS Lambda Next Create an AWS Lambda trigger Create IAM policy Choose authentication mechanism"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/enable-embedded-metadata-in-tableau",
    "content": "Configure Atlan Integrations Automation Browser Extension How-tos Enable embedded metadata in Tableau On this page Enable embedded metadata in Tableau Atlan metadata layers Atlan context directly onto the source application, rather than in a sidebar. This embedded experience provides users with immediate access to data lineage, quality metrics, and governance information without leaving their Tableau workflow. Prerequisites ​ Before enabling embedded metadata in Tableau: You must have the Atlan browser extension installed. If not, see the How to use the Atlan browser extension guide for instructions. Tableau dashboards must be available through the browser Permissions required ​ To enable this feature, you need: Admin role in Atlan Access to the Labs settings Enable embedded metadata in Tableau ​ To enable the embedded Atlan experience in Tableau for your users: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Access Control heading of the Labs page, turn on View embedded metadata in Tableau Once enabled, users with the extension installed can view Atlan metadata in Tableau dashboards. See also ​ Troubleshoot the Atlan browser extension Use the Atlan browser extension Tableau connector documentation Tags: integrations tableau browser-extension metadata embedded automation how-to Previous How to use the Atlan browser extension Next Atlan browser extension security Prerequisites Permissions required Enable embedded metadata in Tableau See also"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/configure-the-extension-for-managed-browsers",
    "content": "Configure Atlan Integrations Automation Browser Extension How-tos Configure the extension for managed browsers On this page Configure the extension for managed browsers If you're using managed browsers, you can install and configure the Atlan browser extension for all users in your organization. To do so, you will need to bulk install the extension and deploy a configuration script. Atlan supports managing the Atlan browser extension for the following: Operating systems: macOS and Microsoft Windows Browsers: Google Chrome and Microsoft Edge The deployment scripts   -  .mobileconfig file for macOS and PowerShell script for Microsoft Windows   -  are designed to make only the most necessary modifications required for the Atlan browser extension to function properly. Both deployment methods adhere to the principle of least privilege: The .mobileconfig file for macOS only includes the configuration settings required to install and operate the Atlan browser extension. The PowerShell script creates essential registry keys required for the Atlan browser extension to operate on Microsoft Windows systems. To configure the Atlan browser extension for a managed browser, you must complete these steps in the following order: Configure the browser extension Bulk install the browser extension Deploy the configuration script (Optional) Verify and monitor the installation Configure the browser extension ​ Who can do this? You will need to be an admin in Atlan to configure the browser extension for users in your organization. You will also need inputs and approval from the IT administrator of your organization. You can configure the browser extension and then download a configuration script to bulk install and deploy it for everyone in your organization. To configure the browser extension, from within Atlan: From the left menu on any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Bulk install the browser extension , click the Set up now button. In the Set up browser extension form, enter the following details: For Choose browser , the browser and operating system values will be prefilled based on what you're currently using   -  you can modify the fields, if required. For Your Atlan domain , enter the URL of your Atlan instance   -  for example, https://(instance_name).atlan.com . info 💪 Did you know? If you enable multiple Atlan domains, your users will be able to select the most relevant Atlan domain from a dropdown menu while using the browser extension. The default value in the dropdown will be the Atlan instance entered as Your Atlan domain . If your organization does not have multiple Atlan domains, only the default selection will be displayed. (Optional) For Advanced settings , you can configure the following: If you have multiple Atlan instances, toggle on Multiple Atlan domains and then enter the URLs of your additional Atlan instances. Click + Add to add more Atlan domains. If your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, toggle on Custom data source domain . Click + Add to add more custom domains for data sources. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of your custom data source domain. info 💪 Did you know? For any supported tools configured while setting up the managed browser extension, your users will not be able to update or remove these selections. They can, however, add additional custom domains for data sources. Click the Download Script button to download the corresponding configuration script. The IT administrator(s) in your organization will need to install this configuration file in your organization's devices using a mobile device management (MDM) software. Administrative permissions to the MDM platform are required to complete the setup. Based on your operating system, the downloaded file can be one of the following two types: .mobileconfig -  use this file to configure profiles with specific settings in macOS devices . .ps1 -  use this PowerShell script to create registry keys in Microsoft Windows devices . Bulk install the browser extension ​ Who can do this? You will need to have administrator access to your organization's mobile device management (MDM) software with the permission to add and deploy new policies to all users. You will also need inputs and approval from your Atlan admin. You will need to configure the ExtensionInstallForcelist browser policy for either Google Chrome or Microsoft Edge to force-install the extension for everyone in your organization. The ExtensionInstallForcelist browser policy: Governs extensions that can be silently installed and automatically enabled for all users. Provides extension IDs that the browser will automatically install and enable when a user logs in. Google Chrome ​ To bulk install the Atlan browser extension in Google Chrome, follow the steps in Google documentation: Force install apps and extensions . Microsoft Edge ​ To bulk install the Atlan browser extension in Microsoft Edge, follow the steps in Microsoft documentation: Force-install an extension . For the Extension/App IDs and update URLs to be silently installed (Device) field, copy and paste the following value: fipjfjlalpnbejlmmpfnmlkadjgaaheg;https://clients2.google.com/service/update2/crx fipjfjlalpnbejlmmpfnmlkadjgaaheg is the extension-id for the Atlan browser extension. https://clients2.google.com/service/update2/crx indicates that it needs to be installed from the Chrome Web Store. Deploy the configuration script ​ Who can do this? You will need to have administrator access to your organization's mobile device management (MDM) software with the permission to add and deploy new policies to all users. You will also need inputs and approval from your Atlan admin. The browser extension relies on managed storage for configuring domains in the Atlan extension. The values for managed storage can be configured through: A configuration profile in macOS Registry keys in Microsoft Windows Although Atlan's solution is platform-agnostic, the following example pertains to Microsoft Intune . macOS ​ You will need to create a custom managed profile to configure the domains for the Atlan browser extension. To deploy the .mobileconfig file for your organization, you can use any MDM platform. For example: Microsoft Intune   -  follow the steps in Custom configuration profile settings . Microsoft Windows ​ You will need to create registry keys to deploy the extension. You can create the required registry keys with a PowerShell script, which can then be deployed to your users’ devices using an MDM software. To deploy the PowerShell configuration script for your organization, you can use any MDM platform. For example: Microsoft Intune   -  follow the steps in Create a script policy and assign it . For Script settings , enter the following details: Script location -  upload the .ps1 configuration script downloaded from Atlan. Run this script using the logged on credentials -  change to No . Enforce script signature check -  change to No . Run script in 64 bit PowerShell Host -  change to Yes . Verify and monitor the installation ​ To ensure that the Atlan browser extension has been successfully deployed across all selected devices in your organization, you can: Verify the installation   -  after you have deployed the policies, check a few target devices to ensure that the extension was installed and configured correctly. Monitor compliance   -  monitor the compliance status of the policy and troubleshoot any issues. Your users will now be able to use the Atlan browser extension in a managed browser! 🎉 Once the managed browser has synced with the latest configuration changes for your organization, the Atlan browser extension will be automatically installed and a new tab will open to indicate that the Atlan browser extension is now active. Tags: atlan documentation Previous Browser Extension Next How to use the Atlan browser extension Configure the browser extension Bulk install the browser extension Deploy the configuration script Verify and monitor the installation"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/browser-extension/how-tos/use-the-atlan-browser-extension",
    "content": "Configure Atlan Integrations Automation Browser Extension How-tos How to use the Atlan browser extension On this page Use the Atlan browser extension The Atlan browser extension provides metadata context directly in your supported data tools . You can use the extension in the following Chromium-based browsers: Google Chrome and Microsoft Edge. Install the extension ​ To install the Atlan browser extension, first log into your Atlan instance. Atlan saves your Atlan domain in a cookie when you log in. To install Atlan's browser extension: You can either: Find the extension in the Chrome Web Store: https://chrome.google.com/webstore/detail/atlan/fipjfjlalpnbejlmmpfnmlkadjgaaheg From the upper right of any screen in Atlan, navigate to your name and then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Apps , for Browser extension , click Install . To install the Atlan browser extension: For Google Chrome, in the upper right of your screen, click Add to Chrome . When prompted for confirmation, click the Add extension button. For Microsoft Edge, follow the steps in Add an extension to Microsoft Edge from the Chrome Web Store . Currently, you can't install the browser extension on mobile devices or tablets. Did you know? You can also install Atlan's browser extension at the workspace level . To set this up, you need to be an administrator or have access to the admin console of your organization's Google account. If your organization uses managed browsers, you can configure the extension for managed browsers . Configure the extension ​ Once installed, configure the Atlan browser extension to get started. Optionally, Atlan admins can preconfigure custom domains for data sources , if any. Configure the extension as a user ​ To configure the browser extension, once installed: If you are logged into your Atlan instance, skip to the next step. If you haven't logged into Atlan, log in to your Atlan instance when prompted. In the Options page, to enter the URL of your Atlan instance: If your organization uses an Atlan domain (for example, _mycompany_.atlan.com ), the Atlan instance URL appears preselected. Click Get started . (Optional) Switch to a different Atlan domain, if required. If your organization uses a custom domain (for example, _atlan_.mycompany.com ), enter the URL of your Atlan instance and then click Get started . After a successful login, the message Updated successfully appears. (Optional) If your data tools are hosted on custom domains, rather than the standard SaaS domain of each tool: Click the Configure custom domain link at the bottom. In the dropdown on the left, select your data tool. In the text box on the right, enter the custom domain you use for that tool. Repeat these steps for each tool hosted on a custom domain. Click the Save button when finished. If your Atlan admin has preconfigured custom domains for data sources , you won't be able to update or remove these selections. Click + Add to configure custom domains for additional data sources as required. You can now close the Options tab. The extension is now ready to use! 🎉 (Optional) Configure custom domains as an admin ​ Who can do this? You need to be an admin user in Atlan to configure custom domains for data sources from the admin center. To configure custom domains, from within Atlan: From the left menu of any screen, click Admin . Under Workspace , click Integrations . Under Apps , expand the Browser extension tile. In the Browser extension tile, for Set up your custom data source... , if your data tools are hosted on custom domains rather than the standard SaaS domain of each tool, click the Configure link to configure them for users in your organization. For Connector , select a supported tool for the browser extension. In the adjacent field, enter the URL of the custom domain for your data source. (Optional) Click + Add to add more. Click Save to save your configuration. info 💪 Did you know? For any supported tools that you have configured, your users won't be able to update or remove these selections. They can, however, add additional custom domains for data sources. (Optional) For Download Atlan extension or share with your team , you can either install the Atlan browser extension for your own use or share the link with your users. Usage ​ Who can do this? Anyone with access to Atlan—any admin, member, or guest user—and a supported tool can use the browser extension. First, log into Atlan. Did you know? When using Atlan's browser extension in a supported tool , the extension only reads the URL of your browser tab—no other data is accessed. If using Atlan's browser extension on any website , it only reads the favicon, page title, and URL of your browser tab. Learn more about Atlan browser extension security . Access and enrich context in-flow ​ To access context for an asset, from within a supported tool: Log into the supported tool. Open any supported asset. In the lower-right corner of the page, click the small Atlan icon. danger The icon to activate Atlan is not the extension icon that appears at the top of your Chrome browser. This small Atlan icon in the lower right corner of the page is the only way to access the metadata for the asset you are viewing in another tool. In the sidebar that appears: Click the tabs and links to view all context about the asset. Make changes to any of the metadata you'd like. Now you can understand and enrich assets without leaving your data tools themselves! 🎉 The Atlan sidebar automatically reloads as you browse your assets in a supported tool to show details about the asset you're currently viewing. Did you know? Your permissions in Atlan control what metadata you can see and change in the extension. Search for metadata ​ To search for context for any information on any website: Select the text you'd like to search on the web page you're viewing. Right-click, and then select Search in Atlan 🏡 . The extension opens a new browser tab on Atlan's discovery page, with the results for that text! 🎉 Add a resource ​ You can link any web page as a resource to your assets in Atlan using the browser extension. To add a web page as a resource to an asset: In the top right of the web page you're viewing, click the Atlan Chrome extension . In the resource clipper menu, under Link this page to an asset , select the asset to which you'd like to add the web page as a resource. Click Save to confirm your selection. (Optional) Once the resource has been linked successfully, click the Open in Atlan button to view the linked asset directly in Atlan. You can now add resources to your assets in Atlan from any website! 🎉 Did you know? The Tableau extension offers native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. Supported tools ​ Currently, the Atlan browser extension supports assets in the following tools: Amazon QuickSight : analyses, dashboards, and datasets Databricks : databases, schemas, views, and tables dbt Cloud : models and sources in the model editor and dbt docs Google BigQuery : datasets, schemas, views, and tables IBM Cognos Analytics : folders, dashboards, packages, explorations, reports, files, data sources, and modules Looker : dashboards, explores, and folders Microsoft Power BI : dashboards, reports, dataflows, and datasets Mode : collections, reports, queries, and charts Qlik Sense Cloud : apps, datasets, sheets, and spaces Redash : queries, dashboards, and visualizations Salesforce : objects Sigma : datasets, pages, and data elements Snowflake (via Snowsight schema explorer): databases, schemas, tables, views, dynamic tables, streams, and pipes Tableau : dashboards, data sources, workbooks, and metrics. Additionally, you can choose to switch the Tableau extension to offer native embeddings directly in your dashboards. See Enable embedded metadata in Tableau for more information. ThoughtSpot : liveboards, answers, visualizations, and tables MicroStrategy : dossiers, reports, documents Tags: atlan documentation Previous Configure the extension for managed browsers Next Enable embedded metadata in Tableau Install the extension Configure the extension Usage Supported tools"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/webhooks/how-tos/create-webhooks",
    "content": "Configure Atlan Integrations Automation Webhooks Create webhooks On this page Create webhooks Who can do this? You will need to be an admin user in Atlan to create webhooks. danger If your webhook endpoint is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or submit a request . Webhooks allow you to monitor events happening in Atlan, receive notifications for these events to a URL of your choice, and take action right away. For example, you can create a webhook to send notifications to your email address or messaging app when a term is updated or an asset is tagged . Webhooks send the payload in a specific format that cannot be customized. This is meant for consumption by a programmatic entity down the line   -  for example, AWS Lambda or a microservice. For a webhook to be consumed directly, Atlan will need to customize the payload, which is currently not supported. Alternatively, you can explore out-of-the-box integrations such as Slack and Microsoft Teams . Atlan currently supports creating webhooks for the following event types: Asset creation, deletion, and metadata update Custom metadata update for assets Tag attachment or removal from assets Create a webhook ​ To create a webhook: From the left menu in Atlan, click Admin . Under Workspace , click Webhooks . On the Webhooks page, click + New Webhook to create a new webhook. In the New Webhook dialog, enter the following details: For Name , enter a meaningful name for your webhook. For Webhook URL , enter the URL for where you want to receive event notifications. For Asset type , select the asset types for which you'd like to receive notifications. (This will default to all asset types, if none are specified.) For Event type , under Assets , select all the event types for which you'd like to receive notifications: Create -  to process notifications for asset creation . Update -  to process notifications for when assets are updated . Delete -  to process notifications for asset deletion . Update Custom Metadata -  to process notifications for when custom metadata is updated for assets. Add Tags -  to process notifications for when tags are attached to an asset. Delete Tags -  to process notifications when tags are removed from an asset. To validate the URL you've entered, in the upper right, click the Validate button. danger Atlan will send a sample payload to test if the webhook URL is correct. You will need to respond with a 2xx status for the validation to succeed. Atlan will also run this validation before you save your webhook as a precautionary measure. Click Save to finish creating your webhook. From the Webhook successfully created dialog, under Secret Key , click the clipboard icon to copy the secret key and store it in a secure location to verify requests from Atlan . Click Done to complete setup. Verify requests from Atlan ​ Atlan signs its webhooks using a secret that is unique to your app. With the help of signing secrets, you can verify the authenticity of such requests with confidence. Each HTTP request sent from Atlan will include an x-atlan-signing-secret HTTP header. You can use the secret key for your webhook to validate requests from Atlan. Tags: webhooks automation notifications Previous Webhooks Integration Next Collaboration Integrations Create a webhook Verify requests from Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/automation/connections/how-tos/delete-a-connection",
    "content": "Configure Atlan Integrations Automation Connections Delete a connection On this page Delete a connection To delete a connection and its assets, complete th e following steps. Select the utility ​ To select the connection delete utility: In the top right of any screen, navigate to New and then click New Workflow . From the filters along the top, click Utility . From the list of packages, select Connection Delete and click on Setup Workflow . Select the connection ​ To select the connection to delete: Under Connection select the existing connection you want to delete. For Delete Strategy choose how you want to delete that connection and its assets: To soft-delete the connection and its assets, choose Archive . To fully purge the connection and its assets, choose Delete . Run the utility ​ To delete the connection and its assets, after completing the steps above: At the bottom of the page, click Run . Once the utility has completed running, you will no longer see the assets in Atlan's asset page! 🎉 Tags: integration connectors Previous Connections Integration Next Webhooks Integration Select the utility Select the connection Run the utility"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/how-tos/link-your-microsoft-teams-account",
    "content": "Configure Atlan Integrations Collaboration Microsoft Teams How-tos Link your Microsoft Teams account On this page Link your Microsoft Teams account To get alerts for starred assets directly delivered to your Microsoft Teams account, you may need to first link your Microsoft Teams account. This is done automatically for the user that set up the Microsoft Teams integration , but not for other users. Link your Microsoft Teams account ​ To link your Microsoft Teams account: From any screen, in the upper right, navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Teams , click the Connect link. In the resulting popup, scroll to the bottom and click Allow . Unlink your Microsoft Teams account ​ To unlink your Microsoft Teams account: From any screen, in the upper right, navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Teams , click the Disconnect link. In the confirmation dialog, click Confirm . Tags: integration connectors alerts monitoring notifications Previous How to integrate Microsoft Teams Next Troubleshooting Microsoft Teams Link your Microsoft Teams account Unlink your Microsoft Teams account"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/microsoft-teams/how-tos/integrate-microsoft-teams",
    "content": "Configure Atlan Integrations Collaboration Microsoft Teams How-tos How to integrate Microsoft Teams On this page Integrate Microsoft Teams Who can do this? You will need to be an admin in Atlan to configure the Microsoft Teams integration. You will also need inputs and approval from the Application Administrator and Teams Administrator of your Microsoft Teams workspace. To integrate Microsoft Teams and Atlan, follow the se steps. Retrieve the team link ​ To retrieve the team link, from within Microsoft Teams: In the left menu in Microsoft Teams , click Teams . Under Teams , navigate to your team, and to the right of the team name, click the three dots . From the dropdown menu, click Get link to team . From the Get a link to the team dialog, click Copy to copy the team link and save it in a temporary location. Connect Atlan to Microsoft Teams ​ Once you have retrieved the team link, you can proceed to connecting Atlan to Microsoft Teams. Atlan requires the following delegated permissions for the Microsoft Teams integration: offline_access -  allows the Atlan app to access resources on behalf of the users, even when users are not currently using the app. User.Read -  allows users to sign in to the app, and allows the app to read the profile of signed-in users. AppCatalog.Submit -  allows the app to submit application packages to the catalog and cancel submissions that are pending review on behalf of the signed-in user. ChannelMessage.Read.All -  allows the app to read a channel's messages in Microsoft Teams, on behalf of the signed-in user (only required if you have admin access to publish the app in Microsoft Teams). AppCatalog.ReadWrite.All -  allows the app to create, read, update, and delete apps in the app catalogs (only required if you have admin access to publish the app in Microsoft Teams). TeamsAppInstallation.ReadWriteSelfForUser -  allows a Teams app to read, install, upgrade, and uninstall itself for the signed-in user. From within Atlan: From the left menu on any screen, click Admin . Under Workspace , click Integrations . In the Microsoft Teams tile, click Connect . In the Add to Microsoft Teams dialog, for Team link , paste the team link you copied from Microsoft Teams above. Click Next to continue. For Publish the Atlan app : If you have Application Administrator access to Microsoft Teams: Click Publish now to publish the Atlan app in your Microsoft Teams workspace. In the Permissions requested popover, check the box for Consent on behalf of your organization to automatically grant access to the application for all users and then click Accept to publish the Atlan app. If you do not have admin access to Microsoft Teams: Click Request to publish . In the Permissions requested popover, click Accept to publish the Atlan app. Contact your Microsoft Teams administrator to approve and publish the Atlan app in your Microsoft Teams workspace. Did you know? If your global administrator has enabled the admin consent workflow , you will be prompted to request admin approval while attempting to install the Atlan app in your Microsoft Teams workspace. Reach out to your admin to approve the admin consent request from the Microsoft Entra admin center. Additionally, if there is any expiry date set for such requests, ensure that the request is approved within that period of time, otherwise you will need to request approval again. Add Atlan to the Microsoft Teams directory ​ danger You must allow 24 hours to elapse after setting up the Microsoft Teams integration in Atlan and before adding the Atlan app to your Microsoft Teams directory. This is required for the Atlan app to be activated in Microsoft Teams. To learn more, refer to Microsoft Teams documentation . Once the Atlan app has been published in your Microsoft Teams workspace, you will need your Teams Administrator to add Atlan to your Microsoft Teams directory. From within Microsoft Teams: Log in to the Microsoft Teams admin center with Teams Administrator access. In the left menu of Microsoft Teams , click Teams apps and then click Manage apps . On the Manage apps page, use the search bar to search for the Atlan app. To the left of the app name, click the circle to select the Atlan app. From the tabs along the top of the All apps section, click Add to a team . In the Add to a team form, configure the following: Click the search bar and enter the name of the team to which you want to add the Atlan app. Hover over the team name and then click Add . Click Apply to confirm your selection(s). Atlan is now connected to Microsoft Teams! 🎉 Configure integration from Atlan to Microsoft Teams ​ Now that Atlan is connected to Microsoft Teams, you can configure the Microsoft Teams integration from Atlan. From the Integrations sub-menu: Expand the Microsoft Teams tile. Under the Configurations tab, configure the following: For any existing Microsoft Teams integrations prior to February 22, 2024 only, you will need to update the Atlan app in your Microsoft Teams workspace to use new features like enabling notifications for starred assets : If you have Application Administrator access to Microsoft Teams: Click Install now to update the Atlan app in your Microsoft Teams workspace. In the Permissions requested popover, click Accept to update the Atlan app. If you do not have admin access to Microsoft Teams: Click Request update . Contact your Microsoft Teams administrator to update the Atlan app in your Microsoft Teams workspace. Once updated, click Check status in Atlan to view the status of your requested update. info 💪 Did you know? For all new Microsoft Teams integrations from February 22, 2024 onward, no additional configuration required. For Channels , enter channels to use in your Microsoft Teams workspace to allow users to post to from within Microsoft Teams or get notified about glossary updates . Enter the channel name without a # , and press tab after each channel if you want to enter multiple channels. (Optional) For Announcements channel , enter the name of a single channel that can be used to view announcements on assets in Atlan. (Optional) For Workflows alert channel , enter the name of a single channel that can be used to see alerts for workflow activities in Atlan. You can also choose to receive failure alerts only by toggling on the Receive failure alerts only slider. (Optional) For Playbooks alert channel , enter the name of a single channel that can be used to see alerts for playbook runs in Atlan. You can also choose to receive failure alerts only by toggling on the Receive failure alerts only slider. At the bottom of the tile, click the Update button. Users can now post to Microsoft Teams without leaving Atlan! 🎉 Did you know? Channels need to be standard for Atlan to be able to post to them. If you try to integrate a private or shared channel, you will see an error for that channel when you try to update. Tags: integration connectors Previous Microsoft Teams Next Link your Microsoft Teams account Retrieve the team link Connect Atlan to Microsoft Teams Add Atlan to the Microsoft Teams directory Configure integration from Atlan to Microsoft Teams"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/how-tos/integrate-slack",
    "content": "Configure Atlan Integrations Collaboration Slack How-tos How to integrate Slack On this page Integrate Slack Who can do this? You will need to be an admin in Atlan to configure the Slack integration. You will also need inputs and approval from an administrator of your Slack workspace. To integrate Slack and Atlan, follow these steps. Retrieve Slack tokens ​ To retrieve Slack's integration tokens: Access your Slack apps console at: https://api.slack.com/apps At the bottom of the page, in the Your App Configuration Tokens box click the Generate Token button. In the Generate Your App Configuration Token dialog, from the Workspace drop-down choose your Slack workspace and then click the Generate button. From the Your App Configuration Tokens box: Under Access Token click the Copy button and save this temporarily. Under Refresh Token click the Copy button and save this temporarily. danger These tokens will usually expire after 12 hours, so will need to be used the same day they are generated. Connect Atlan to Slack ​ To connect Atlan to Slack, from within Atlan: From the left menu, click Admin . Under Workspace , click Integrations . In the Slack tile, click the Connect button. Enter the tokens copied above: For Access token enter the access token value. For Refresh token enter the refresh token value. Click Next to continue. Under Install the Atlan app in your Slack workspace click the Install now button. At the bottom of the resulting Slack popup, click the Allow button. (If you want more details on what each permission does, see What does Atlan do with each Slack permission? ) (Optional) Request permission from your Slack admin ​ If you are not a workspace administrator in Slack, you will be prompted to request permission to install. To request permission to install the integration: Under Add a message for your App Managers enter an explanation for installing the app. At the bottom of the form, click the Submit button. Contact your Slack workspace administrator and ask them to approve the Atlan app. Once approved, you'll get an alert in Slack from Slackbot. Once you receive the Slackbot alert, return to the Integrations menu in Atlan (in the Admin Center) and click the Add to Slack button. Atlan is now connected to Slack! 🎉 Configure integration from Atlan to Slack ​ To configure the Slack integration from Atlan, from the Integrations sub-menu: Expand the Slack tile. Under the Configurations tab, enter channels to use in your Slack workspace. Enter the channel name or provide a link to the channel without a # , and press tab or enter after each channel to add multiple channels. (Optional) For Channels , add any channels that users should be able to post to from within Slack or get notified on glossary updates . (Optional) For Announcements channel , enter the name of a single channel that can be used to view announcements on assets in Atlan. (Optional) For Workflows alert channel , enter the name of a single channel that can be used to view alerts for workflow activities in Atlan. You can also choose to receive failure alerts only by toggling on the Receive failure alerts only slider. (Optional) For Playbooks alert channel , enter the name of a single channel that can be used to view alerts for playbook runs in Atlan. You can also choose to receive failure alerts only by toggling on the Receive failure alerts only slider. (Optional) For Query output share channels , add any channels where users should be able to share query output. (Optional) For Request notifications , toggle on the slider to receive Slack notifications when requests are raised in Atlan and approve or reject them directly from Slack. At the bottom of the tile, click the Update button. Users can now post to Slack without leaving Atlan! 🎉 Did you know? Channels need to be public for Atlan to be able to post to them. If you try to integrate a private channel you will see an error for that channel when you try to update. Configure integration from Slack to Atlan ​ To configure the Atlan integration from Slack, from within Slack: Open each channel you want Slack users to be able to query Atlan from within. At the top of the channel, click the name of the channel. Change to the Integrations tab. In the Apps tile, click the Add an app button. Find the Atlan app under the In your workspace heading and click the Add button next to it. (Optional) To add an icon to the Atlan app in Slack, from the Apps page, click the Atlan app. On the Display Information page, under App icon & Preview , click + Add App Icon and upload the Atlan icon for the app. Users can now search for assets in Atlan without leaving Slack! 🎉 Did you know? You can even add the Atlan app to private channels. Tags: integration api configuration Previous Slack Next Link your Slack account Retrieve Slack tokens Connect Atlan to Slack Configure integration from Atlan to Slack Configure integration from Slack to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/slack/how-tos/link-your-slack-account",
    "content": "Configure Atlan Integrations Collaboration Slack How-tos Link your Slack account On this page Link your Slack account To see previews of Slack messages inside Atlan, you may need to first link your Slack account. This is done automatically for the user that set up the Slack integration , but not for other users. Link your Slack account ​ To link your Slack account: From any screen, in the upper right navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Slack , click the Connect link. In the resulting popup, scroll to the bottom and click Allow . Unlink your Slack account ​ To unlink your Slack account: From any screen, in the upper right navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under Slack , click the Disconnect link. In the confirmation dialog, click Confirm . Tags: integration connectors Previous How to integrate Slack Next Troubleshooting Slack Link your Slack account Unlink your Slack account"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/bulk-enrich-metadata",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Bulk enrich metadata On this page Bulk enrich metadata Atlan enables you to use spreadsheet tools to collaborate on assets with your team, make bulk metadata updates, and sync changes back to Atlan. Atlan currently supports the following options for bulk metadata enrichment: Export assets to spreadsheets ​ For a fairly large number of assets to be documented, you can export your assets from Atlan to a supported spreadsheet tool and bulk enrich metadata directly from spreadsheets. Atlan currently supports exporting assets to: Google Sheets Microsoft Excel online Here is a quick summary of this option: An Atlan admin must integrate a supported spreadsheet tool in Atlan to export assets. Once assets have been exported, you must install the Atlan extension to enrich assets and sync changes to Atlan. Only supported for online versions of Google Sheets and Microsoft Excel. Must use an organizational email address to export assets from Atlan. Exports basic as well as custom metadata. Supports exporting impacted assets . Creation of new terms or tags is not supported. To create new terms, admins and members with edit access can bulk upload glossaries . Deletion of terms is not supported. Does not provide cron support, use custom packages instead. Refer to How to export assets to get started. Use Atlan extension in spreadsheets ​ For a small number of assets to be documented, you can install the Atlan extension in a supported spreadsheet tool and bulk enrich metadata directly from spreadsheets. Atlan currently supports installing the Atlan extension in: Google Sheets Microsoft Excel online and desktop versions Here is a quick summary of this option: Must install the Atlan extension for Google Sheets or Microsoft Excel. Supports both online and desktop versions of supported spreadsheet tools. Only supports basic metadata   -  custom metadata is not supported, use export assets option instead. Supports importing and updating impacted assets. Creation of new terms or tags is not supported. To create new terms, admins and members with edit access can bulk upload glossaries . Glossary assets are not supported, use export assets option instead. Refer to How to integrate Atlan with Google Sheets or How to integrate Atlan with Microsoft Excel to get started. Use custom packages ​ Atlan's basic and advanced asset export custom packages can power miscellaneous use cases: Export to your file storage for reporting and analytics outside Atlan. Export to a CSV file, enrich metadata, and then use the asset import package to update in Atlan. Here is a quick summary of this option: Exports to a CSV file. Provides an alternative to out-of-the-box solutions if your administrative policies prohibit such integrations. The basic package exports almost all backend attributes, recommended for one-off migrations. The advanced package offers filtering capabilities, recommended if the scope of the export is limited. Both packages are compatible with the asset import package. Only admin users can run custom package workflows. All attributes except asset relationships are supported. Refer to Asset export (basic) or Asset export (advanced) to get started. Tags: integration connectors Previous What is included in the Slack integration? Next Configure custom domains for Microsoft Excel Export assets to spreadsheets Use Atlan extension in spreadsheets Use custom packages"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/configure-custom-domains-for-microsoft-excel",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Configure custom domains for Microsoft Excel On this page Configure custom domains for Microsoft Excel Who can do this? You will need your Microsoft 365 administrator to complete these steps   -  you may not have access yourself. Before you begin, you may need to Determine if Centralized Deployment of add-ins works for your organization . If your Atlan tenant is hosted on a custom domain   -  for example, https://<your-tenant-name>.mycompany.com -  you will need to configure your Atlan tenant to deploy the Atlan add-in for Microsoft Excel. Prerequisites ​ The Atlan add-in must be centrally deployed from the Microsoft 365 admin center . You must have access to a Windows machine to install Microsoft PowerShell. Install PowerShell ​ Install PowerShell to deploy the Atlan add-in for custom domains: Windows: PowerShell comes pre-installed on most modern Windows systems. If you need to upgrade the package, you can download it from the Microsoft website . macOS: Open Terminal and run: brew install --cask powershell Linux: Open Ubuntu and run: sudo apt-get update sudo apt-get install -y powershell For other distributions, refer to Microsoft's PowerShell installation guide . Configure the add-in for Microsoft Excel ​ To install the Atlan add-in directly in Microsoft Excel: Open PowerShell and run the following command to install the required module: Install-Module -Name O365CentralizedAddInDeployment -Scope CurrentUser Run the following command to import the module: Import-Module -Name O365CentralizedAddInDeployment Run the following command to connect to the organization add-in service: Connect-OrganizationAddInService In the Microsoft authorization dialog, select an account to authenticate the connection. (Optional) Run the following command to list existing add-ins in the organization: Get-OrganizationAddIn Run the following command to set a custom domain for the Atlan add-in: Set-OrganizationAddInOverrides -ProductId <your-product-ID>  -AppDomains \"<your-custom-domain>\" Replace <your-product-ID> with the product ID of the Atlan add-in. Replace <your-custom-domain> with your organization's custom domain. (Optional) Troubleshooting add-in connectivity ​ Who can do this? Any individual in your organization with access to Microsoft Excel and Atlan tenant on a custom domain. This section is optional if you deployed the Atlan add-in for the first time in your organization after completing the steps above. Your users will be able to connect Atlan to Microsoft Excel from your custom domain. However, if any user tried to set up the add-in prior to the configuration above, they may not able to connect Atlan to Microsoft Excel. In that case, Atlan recommends clearing the add-in cache using the following steps. Clear Microsoft Excel Online cache ​ To clear the local storage of your Microsoft Excel online app, from Google Chrome: Open a blank Microsoft Excel workbook. From the top right of your browser, click the vertical three dots icon for more menu options. From the more options menu, click More Tools and then click Developer Tools . In the top menu of developer tools, click Application . In the left menu of the Application tab, under Storage , click Local storage . Find an entry for https://***-excel.officeapps.live.com . Right-click on the entry and then click Clear . Refresh the Microsoft Excel Online app in your browser. Clear Microsoft Excel cache on Mac ​ To clear the Microsoft Excel cache on Mac: Close Microsoft Excel. Use the Finder to navigate to the /Users/<user>/Library/Containers/com.microsoft.Excel/Data/Library/Application   Support/Microsoft/Office/16.0/Wef. folder. Search for a313dc5c-6ca5-4346-abfc-c84c57d4b9dc and delete all the files and folders returned in the search results. Restart your Microsoft Excel app. Clear Microsoft Excel cache on Windows ​ To clear the Microsoft Excel cache on Windows: Open Microsoft Excel. From the ribbon, navigate to File > Options > Trust Center > Trust Center Settings > Trusted Add-in Catalogs . In Trusted Web Add-in Catalogs , select the checkbox Next time Office starts, clear all previously-started web add-ins cache . Close Microsoft Excel and then restart it to clear the add-in cache. Once it has restarted, the checkbox will be deselected automatically. Tags: atlan documentation Previous Bulk enrich metadata Next Download impacted assets in Google Sheets Prerequisites Install PowerShell Configure the add-in for Microsoft Excel (Optional) Troubleshooting add-in connectivity"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/download-impacted-assets-in-google-sheets",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Download impacted assets in Google Sheets On this page Download impacted assets in Google Sheets Once you've connected Atlan with Google Sheets , you can download impacted assets in Google Sheets. This can help you assess the downstream impact of any changes made to an upstream asset for impact analysis . danger You need to be logged into your Atlan instance before you can download impacted assets from Atlan in Google Sheets. Download impacted assets in Google Sheets ​ To import lineage in Google Sheets: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan . From the list of options in the Atlan add-on, click Import Lineage to open a list of your assets in a sidebar. (Optional) To filter your assets by a specific asset type, in the Atlan sidebar, select the asset type. In the Atlan sidebar on your spreadsheet, select the asset(s) you want to import. To select the type of impacted assets you'd like to download, in the Atlan sidebar, from the Direction dropdown: To download downstream assets for impact analysis , click Downstream . To download upstream assets for root cause analysis , click Upstream . To download all impacted assets, click Both . To download the impacted assets in Google Sheets: Click Import Lineage to download all the impacted assets in one sheet. Click the vertical three dots and then click Import to new sheet to download the assets in separate sheet tabs. (Optional) Once download is successful, click the asset links in your spreadsheet to view the assets in Atlan. The impacted assets are now available in Google Sheets! 🎉 Update column metadata for impacted assets ​ Once you've imported your impacted assets from Atlan, you can edit the column metadata for impacted assets in Google Sheets. You can make changes to the column metadata once all the columns have been imported successfully. You can only make changes to the metadata in the following columns: Description Owner Users Owner Groups Certification Status Certification Message Announcement Type Announcement Title Announcement Message Tags You cannot make the following changes: Edit headers for any of the columns Edit the metadata in the following columns: Source Asset Source Asset Connector Source Asset Type Impacted Asset Impacted Asset Connector Impacted Asset Type Direction Terms Propagated Tags Source URL Qualified Name Source Asset GUID Impacted Asset GUID Immediate Upstream and Immediate Downstream Delete any columns or rows Any of these changes will not be pushed to Atlan and you'll receive an error message. Did you know? When adding tags or owners for impacted assets in Google Sheets, the tag or user name for the owner user or group must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple tags or owners as comma-separated values. Push changes to Atlan ​ Once you've made changes to the column metadata, to push your changes: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan and then click Push to Atlan . A dialog box will appear once the changes have synced. danger If you do not have the permission to update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Google Sheets. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. Tags: lineage data-lineage impact-analysis integration connectors downstream-impact dependencies upstream-dependencies data-sources Previous Configure custom domains for Microsoft Excel Next Download impacted assets in Microsoft Excel Download impacted assets in Google Sheets Update column metadata for impacted assets Push changes to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-google-sheets",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to integrate Atlan with Google Sheets On this page Integrate Atlan with Google Sheets The Atlan add-on for Google Sheets makes it easy to edit column metadata in bulk for your data assets in Atlan. Integrating Atlan with Google Sheets allows you to: Import column metadata for your data assets into Google Sheets Update column metadata for your imported assets directly in Google Sheets View data asset profiles on Google Sheets Download downstream impacted assets in Google Sheets Who can do this? Any individual in your organization with access to Atlan can install the Atlan add-on for Google Sheets. You can even install the Atlan add-on if you have a Google account with a non-gmail address. However, the Atlan add-on can also be installed at the workspace level. To install the app for users in your organization, follow this guide . You'll also need the following permissions for Google Sheets within your organization: Display and run third-party web content in prompts and sidebars within Google apps View and manage the spreadsheets with the Atlan add-on Install Atlan in Google Sheets ​ To install the Atlan add-on from the Google Workspace Marketplace, use this link . To install the Atlan add-on directly in Google Sheets, follow these steps: In the menu bar of your Google spreadsheet, click Extensions . In the dropdown menu, click Add-ons . Click Get add-ons to view available add-ons in the Google Workspace Marketplace. In the search bar of your Google Workspace Marketplace, type Atlan and press enter. Click Install to install the Atlan add-on. If you see a dialog box pop up asking for permissions, click Allow to continue. Connect Atlan with Google Sheets ​ To connect Atlan with your Google spreadsheets: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan and then click Setup Atlan . In the dialog box, enter your Atlan instance URL and click Continue . Congrats on connecting Atlan with Google Sheets! 🎉 danger For every new spreadsheet that you create using Google Sheets, you will need to follow the steps outlined above to connect Atlan with that spreadsheet. The Atlan add-on will remain connected for all tabs within an already connected spreadsheet. Tags: integration connectors downstream-impact dependencies Previous How to export assets Next How to integrate Atlan with Microsoft Excel Install Atlan in Google Sheets Connect Atlan with Google Sheets"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/export-assets",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to export assets On this page Export Assets Who can do this? Before you can export assets to spreadsheets, you will first need your Atlan admin to enable asset export . Atlan enables you to export all your assets or a filtered subset of assets to spreadsheets. Atlan currently supports exporting assets to: Google Sheets Microsoft Excel online Once your Atlan admin has integrated a supported tool, you will be able to export your assets and asset metadata to spreadsheets. Your existing permissions and access policies in Atlan will determine whether you can export assets, but at a minimum you'll require read permission on the assets you want to export. For example, you can: Export a list of assets that need enrichment and share the spreadsheet widely to crowdsource documentation. Export your business glossary and bring everyone up to speed on commonly used terminology across the organization. To export impacted assets, see How to download and export lineage . Did you know? Atlan currently limits the total number of assets you can export to 150,000 rows. Reach out to your customer success manager if you'd like to increase the limit for your organization. Supported assets for export ​ You can export assets from the following tabs: Assets tab ​ All assets Filtered subset of assets Child assets such as columns or fields from the parent asset profile Glossary tab ​ Glossary profile -  all categories and terms within a glossary Category profile -  all direct subcategories and terms within a category Term profile - linked assets for terms from the Linked Assets tab Reporting tab ​ Assets dashboard - total assets , archived assets, assets by certificates, SQL assets, assets without enrichment , assets with lineage , and assets with announcements Glossary dashboard - linked assets for terms Insights dashboard - query assets with certificate Usage & Cost dashboard - suggested assets for deprecation Enable asset export ​ Who can do this? You will need to be an admin user in Atlan to integrate a supported tool   -  Google Sheets and Microsoft Excel   -  and enable your users to export assets. For Atlan admins, you do not have to set the following permissions manually. You can simply sign in to your Google or Microsoft account while integrating the supported tool, which in turn will automatically set the permissions and grant appropriate access to your users. The export icon or button will only be visible if your Atlan admin has integrated a supported tool. If you cannot see the export icon or button, reach out to your Atlan admin to integrate Google Sheets or Microsoft Excel. Atlan uses the following permissions to integrate supported tools: Google Sheets: https://www.googleapis.com/auth/drive.file -  allows the app to see, edit, create, and delete only specific Google Drive files used with the Google Sheets app. This enables Atlan to create and update spreadsheets with exported assets. Refer to Google documentation . Microsoft Excel: Files.ReadWrite -  allows the app to read, create, update, and delete the signed-in user's OneDrive files. This enables Atlan to create and update spreadsheets with exported assets. Note that access is restricted to the signed-in user's files   -  Atlan does not access any shared files. Refer to Microsoft documentation . offline_access -  allows the app to access resources on behalf of the users, even when users are not currently using the app. Refer to Microsoft documentation . User.Read -  allows users to sign in to the app, and allows the app to read the profile of signed-in users. This enables Atlan to authenticate the user. Atlan only supports authenticating with an organizational account. Refer to Microsoft documentation . Did you know? For Microsoft users, if your global administrator has enabled the admin consent workflow , you will be prompted to request admin approval while attempting to integrate Microsoft Excel. Reach out to your admin to approve the admin consent request from the Microsoft Entra admin center. Additionally, if there is any expiry date set for such requests, ensure that the request is approved within that period of time, otherwise you will need to request approval again. To integrate a supported tool for exporting assets: From the left menu of any screen, click Admin . Under the Workspace heading, click Integrations . To connect Atlan to a supported tool for exporting assets: In the Google Sheets tile, click the Connect button. A sign-in dialog will appear and you will be redirected to sign in with your Google account. From the corresponding screen, click Allow to integrate Atlan with Google Sheets. In the Microsoft Excel title, click the Connect button. A sign-in dialog will appear and you will be redirected to sign in with your Microsoft account to integrate Atlan with Microsoft Excel. Your users can now export assets from Atlan! 🎉 Export assets ​ Who can do this? Once an Atlan admin has integrated a supported tool, any admin, member, or guest user in Atlan with read permission on assets can export assets to spreadsheets. danger Atlan recommends that you avoid exporting assets during workflow runs. Exporting assets while you have workflows running in the background may lead to duplicate assets on the spreadsheet. Atlan allows you to export your assets to spreadsheets and view asset metadata in bulk. At a tenant level, Atlan supports running five exports concurrently for each supported tool. If both Google Sheets and Microsoft Excel have been integrated, a total of 10 concurrent exports is supported. At an individual user level, only one export is allowed for each supported tool at a time, the rest will be auto-queued for execution. To export your assets: From the left menu of any screen in Atlan, click Assets . To export assets, in the Assets page, you can either: Next to the search bar, click the 3-dot icon and then click Export to export all assets. Apply any filters , click the 3-dot icon, and then click Export to export a list of filtered assets. The Export dialog displays a total count of assets available for export: Click Google Sheets to export your assets to a Google Sheets spreadsheet. Click Microsoft Excel to export your assets to a Microsoft Excel spreadsheet. A sign-in dialog will appear and you will be redirected to sign in with your Google or Microsoft account. From the corresponding screen, click Allow to connect to Google Sheets or Microsoft Excel. To track the progress of the export, you can either: In the Export in progress popup, click Open Sheets or Open Excel to navigate to the spreadsheet. The Queued status will change to Success once assets have been exported. Click the 3-dot icon and then click Export to view asset export in progress, along with an estimated time of completion. (Optional) To cancel the export, hover over In queue and click the Stop button. On the spreadsheet, you will be able to view the following details and asset metadata: Status -  export status and total count of assets exported Title -  asset name and link Type -  asset type Connector -  name of supported source Business Name (Alias) -  business-oriented alias of assets, if any Created By and Created At -  username for user who created the asset and when it was created, only applicable to glossary exports Description - asset description , if any Owner Users and Owner Groups - asset owners , if any Certification Status and Certification Message - certification status of asset , if any Announcement Type , Announcement Title , and Announcement Message - announcements on assets , if any Tags and Propagated Tags - tags directly attached or propagated to an asset, if any Terms - linked assets Qualified Name -  fully qualified name of the asset GUID -  globally unique identifier of the asset Custom Metadata - customized metadata with organizational context, if any (Optional) To view your asset export history, click the 3-dot icon and then click Export . From the Export dialog, expand the History dropdown to view your last 10 exports. Note that only you can currently view your own export history. That's it, you've successfully exported your assets from Atlan! 🎉 Update metadata in spreadsheets and sync to Atlan ​ Once you've exported your assets, you can use the Atlan extension for spreadsheets to update metadata for exported assets and sync your updates to Atlan. Atlan currently supports updating metadata and syncing updates for Google Sheets and Microsoft Excel. Google Sheets ​ Microsoft Excel ​ Update metadata ​ Who can do this? Any non-guest user with edit access to an asset's metadata can update metadata for exported assets and sync changes to Atlan. This only includes admin and member users. Atlan also recommends noting the following while updating metadata: If asset import from Atlan to a spreadsheet is in progress, avoid making any updates until the import is completed. You cannot make any changes to columns marked read-only or grayed out. Any changes to those columns will not be synced to Atlan and you will receive an error message. You can remove or change the order of assets in the spreadsheet, but you cannot add or replace it with an entirely new set of assets. You cannot add extra columns to the spreadsheet   -  sync to Atlan will fail. Avoid having any empty rows between assets. Although you can make changes in the spreadsheet while a sync is in progress, avoid starting a new sync until the current one has ended. You can easily collaborate with other users on the spreadsheet. However, sync to Atlan will only be recorded for the username that initiated the sync. Atlan uses hidden sheets to create a snapshot of the changes you've made. Refrain from making any changes in the hidden sheets. To update metadata for exported assets in a spreadsheet: Install the Atlan extension for: Google Sheets Microsoft Excel Connect your Atlan instance to: Google Sheets Microsoft Excel To update metadata for your exported assets: You can only make changes to the metadata in the following columns: Business Name (Alias) Description Owner Users and Owner Groups -  when adding an owner user or group, they must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple owners or groups as comma-separated values. Certification Status -  select valid values from the dropdown. Certification Message Announcement Type -  select valid values from the dropdown. Announcement Title Announcement Message Tags -  when adding a tag, it must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple tags as comma-separated values. Terms -  when adding a term, it must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple terms as comma-separated values. Custom Metadata -  enter custom metadata values for any properties you want to enrich based on the description and expected data type and format. Atlan uses two placeholder values, N/A indicates that the custom metadata property is not applicable on a specific asset and Not Permitted indicates that the user lacks permission to make metadata updates. Atlan recommends that you do not make any changes to these values. Neither of these values will be synced to Atlan. Additionally, Atlan currently only allows you to select a single value for Options data type when updating from spreadsheets. You cannot make the following changes: Edit headers for any of the columns. Edit the metadata in the following columns: Title Type Connector , Glossary and Categories Propagated Tags Qualified Name _GUID _ Created By and Created At Delete any columns or rows. danger Any of these changes will not be pushed to Atlan and you'll receive an error message. Sync to Atlan ​ danger If you do not have the permissions to update asset metadata in Atlan, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in the spreadsheet. Ensure that you have the requisite permissions to update an asset before syncing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. To sync your metadata updates to Atlan: To sync your metadata updates: For Google Sheets, in the menu bar of your Google Sheets spreadsheet, click Extensions . From the dropdown menu, click Atlan and then click Sync to Atlan . For Microsoft Excel, in the menu bar of your Microsoft Excel workbook, click Atlan . From the Atlan tab, click Sync to Atlan . danger Atlan recommends that you do not change the name of your Microsoft Excel workbook if you want to sync metadata updates to Atlan, the sync will fail otherwise. Additionally, Microsoft Excel may take longer to autosave changes on the workbook. If you notice that zero changes were detected, retry syncing after a few seconds. Once the changes have synced, the Atlan sidebar will indicate completion of the sync. (Optional) At the bottom of the spreadsheet, switch to the Sync-Logs tab to view a record of your changes: View all changes synced to Atlan   -  the latest sync details will be displayed at the top of the spreadsheet. Changes are displayed on an asset level. For example, even if you have updated three attributes for an asset, it will be recorded as one change per asset. View the status of each update   - Synced or Failed . For Failed , Atlan will display a failure message: Invalid value -  for unsupported values. Unauthorized -  you do not have the permissions to update asset metadata. Not synced due to conflict -  if any attributes were updated in Atlan since the assets were last exported, Atlan will signal a conflict. In that case, you will need to export your assets once again with the latest changes to proceed. If the error message does not match any of the above options, reach out to Atlan support . Click any asset to verify the changes directly in Atlan. In Atlan, when viewing the activity log: An Updated via Google Sheets stamp will appear in the activity log for updated assets. Click the Google Sheets link to view the source spreadsheet from Atlan. An Updated via Microsoft Excel stamp will appear in the activity log for updated assets. Click the Excel link to view the source spreadsheet from Atlan. Tags: integrations spreadsheets assests Previous Download impacted assets in Microsoft Excel Next How to integrate Atlan with Google Sheets Supported assets for export Enable asset export Export assets Update metadata in spreadsheets and sync to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/download-impacted-assets-in-microsoft-excel",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Download impacted assets in Microsoft Excel On this page Download impacted assets in Microsoft Excel Once you've connected Atlan with Microsoft Excel , you can download impacted assets in Microsoft Excel. This can help you assess the downstream impact of any changes made to an upstream asset for impact analysis . danger You need to be logged into your Atlan instance before you can download impacted assets from Atlan in Microsoft Excel. If you do not have the permission to update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Microsoft Excel. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. Download impacted assets in Microsoft Excel ​ To import impacted assets in Microsoft Excel: In the menu bar of your Microsoft Excel workbook, click Atlan . From the Atlan tab, click Import Lineage to open a list of your assets in a sidebar. (Optional) To filter your assets by a specific asset type, in the Atlan sidebar, select the asset type. In the Atlan sidebar on your worksheet, you can either: Individually select the data asset(s) you want to import. To the left of the Import button, click the Select All checkbox to select all the assets that have loaded in the sidebar. (Optional) Scroll down and click Load more to load more assets in the sidebar. To select the type of impacted assets you'd like to download, in the Atlan sidebar, from the Direction dropdown: To download downstream assets for impact analysis , click Downstream . To download upstream assets for root cause analysis , click Upstream . To download all impacted assets, click Both . To download the impacted assets in Microsoft Excel: Click Import Lineage to download all the impacted assets in one sheet. Click the vertical three dots and then click Import to new sheet to download the assets in separate sheet tabs. (Optional) Once download is successful, click the asset links in your spreadsheet to view the assets in Atlan. The impacted assets are now available in Microsoft Excel! 🎉 Update column metadata for impacted assets ​ Once you've imported your impacted assets from Atlan, you can edit the column metadata for impacted assets in Microsoft Excel. You can make changes to the column metadata once all the columns have been imported successfully. You can only make changes to the metadata in the following columns: Description Owner Users Owner Groups Certification Status Certification Message Announcement Type Announcement Title Announcement Message Tags You cannot make the following changes: Edit headers for any of the columns Edit the metadata in the following columns: Source Asset Source Asset Connector Source Asset Type Impacted Asset Impacted Asset Connector Impacted Asset Type Direction Terms Propagated Tags Source URL Qualified Name Source Asset GUID Impacted Asset GUID Immediate Upstream and Immediate Downstream Delete any columns or rows Any of these changes will not be pushed to Atlan and you'll receive an error message. Did you know? When adding tags or owners for impacted assets in Microsoft Excel, the tag or user name for the owner user or group must already exist in Atlan; name match is case-sensitive, format the value exactly as it exists in Atlan; and you can add multiple tags or owners as comma-separated values. Push changes to Atlan ​ Once you've made changes to the column metadata, to push your changes: In the menu bar of your Microsoft Excel workbook, click the Atlan tab. From the top left of the Atlan tab, click Atlan and then click Sync to Atlan . A dialog box will appear once the changes have synced Tags: data integration Previous Download impacted assets in Google Sheets Next How to export assets Download impacted assets in Microsoft Excel Update column metadata for impacted assets Push changes to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/link-your-account",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos Link your account On this page Link your account To export assets to and bulk enrich metadata from a supported spreadsheet tool, you may first need to link your Google or Microsoft online account. This is done automatically for the admin user that integrated the spreadsheet tool to enable asset export, but not for other users. Although you will be prompted to sign in with your Google or Microsoft account while exporting assets, Atlan also provides an additional option to connect your account from the user profile. Atlan uses the same set of permissions to connect to your organizational Google or Microsoft online account as specified here . Link your account ​ To link your spreadsheet tool account: From any screen, in the upper right, navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. From the Integrations tab, you can either: For Google Sheets , click the Connect link. For Microsoft Excel , click the Connect link. In the resulting popup, scroll to the bottom and click Allow . Unlink your account ​ To unlink your spreadsheet tool account: From any screen, in the upper right, navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. From the Integrations tab, you can either: For Google Sheets , click the Disconnect link. For Microsoft Excel , click the Disconnect link. In the confirmation dialog, click Confirm . Tags: data integration Previous How to integrate Atlan with Microsoft Excel Next How to update column metadata in Google Sheets Link your account Unlink your account"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/update-column-metadata-in-google-sheets",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to update column metadata in Google Sheets On this page update column metadata in Google Sheets Once you've connected Atlan with Google Sheets , you can import the column metadata for all your data assets in Atlan and make changes to them directly in Google Sheets. Atlan currently supports importing and updating column metadata for the following asset types: Tables Views Materialized views Looker explores Microsoft Power BI tables Salesforce objects Tableau data sources danger You need to be logged into your Atlan instance before you can start importing column metadata for your Atlan assets in Google Sheets. Import column metadata ​ You can import column metadata for your data assets directly into Google Sheets. To import column metadata for your data assets into Google Sheets: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan . From the list of options in the Atlan add-on, click Enrich metadata to view a list of your data assets in a sidebar. (Optional) To filter your assets by a specific asset type, in the Atlan sidebar, click the asset type   -  for example, Tableau Datasources . In the Atlan sidebar on your spreadsheet, select the data asset(s) you want to import. Click Import to import column metadata for your selected assets. The column metadata for your selected assets are now available in Google Sheets! 🎉 Did you know? If any changes are made to your imported columns in Atlan, you'll need to import those columns once again to access the updated version in Google Sheets. Update column metadata ​ Once you've imported your data assets from Atlan, you can edit the metadata for your selected data asset in Google Sheets. You can make changes to the column metadata once all the columns have been successfully imported. You can only make changes to the metadata in the following columns: Description Certification Status Certification Message Announcement Type Announcement Title Announcement Message Tags You cannot make the following changes: Edit headers for any of the columns Edit the metadata in the Column Name , Data Type , Propagated Tags , and Qualified Name columns Delete any columns or rows Any of these changes will not be pushed to Atlan and you'll receive an error message. Bulk update tags for columns ​ danger You cannot make any changes to the metadata in the Propagated Tags column. Navigate to the Tags column to add tags to your column assets in Google Sheets: When adding tags to columns: The tag must already exist in Atlan. If the tag does not exist in Atlan, updates will not sync and you will receive an error message. Tag match is case-sensitive, ensure that the tag is formatted exactly as it exists in Atlan. For example, if the tag in Atlan is formatted as Marketing Analysis , then columns tagged with marketing analysis will not sync. You can add multiple tags in the Tags column   -  separate multiple tags with a comma , . If you are in a region that uses a separator other than a comma, you will need to modify your spreadsheet’s settings to use commas as separators. If you have added tags that exist in Atlan as well as ones that do not, only the existing tags will be synced. The unsupported tags will not sync and you will receive an error message. Tag propagation is disabled by default in Atlan, hence tags will not be propagated. Push your changes to Atlan ​ Once you've made changes to the column metadata, complete these steps to push your changes: In the menu bar of your Google spreadsheet, click Extensions . From the dropdown menu, click Atlan and then click Push to Atlan . A dialog box will appear once the changes have synced. (Optional) Click Open in Atlan to verify the changes. In Atlan, an Updated using Google Sheets stamp will appear in the activity log for updated assets. (Optional) Click the Google Sheets link to view the source spreadsheet from Atlan. danger If you do not have the permission to update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Google Sheets. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. View asset profiles in Google Sheets ​ Once you've imported your data assets into Google Sheets, you can also view their asset profiles . Complete these steps: In the menu bar of your Google spreadsheet, click Extensions . In the dropdown menu, click Atlan . Next, click Open asset in sidebar to view the asset profile on your Google spreadsheet. You can also update components of your asset profile directly in Google Sheets. Your changes will sync automatically to Atlan. Did you know? You can download impacted assets for impact analysis in Google Sheets. Tags: connectors data integration crawl Previous Link your account Next How to update column metadata in Microsoft Excel Import column metadata Update column metadata Push your changes to Atlan View asset profiles in Google Sheets"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/integrate-atlan-with-microsoft-excel",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to integrate Atlan with Microsoft Excel On this page Integrate Atlan with Microsoft Excel The Atlan add-in for Microsoft Excel makes it easy to enrich metadata in bulk for your data assets in Atlan. You can use the Atlan add-in for both the web and desktop versions of Microsoft Excel. Integrating Atlan with Microsoft Excel will allow you to: Import column metadata for your data assets to Microsoft Excel Update column metadata for your imported assets directly in Microsoft Excel Download impacted assets in Microsoft Excel To integrate Atlan with Microsoft Excel: If your Atlan tenant is hosted on the standard domain https://<your-tenant-name>.atlan.com , you can either: Install and connect the Atlan add-in as an individual user Deploy and publish the Atlan add-in for your organization as a Microsoft 365 admin If your Atlan tenant is hosted on a custom domain https://<your-tenant-name>.mycompany.com , your Microsoft 365 admin will need to configure the Atlan add-in using PowerShell Set up the add-in as a user ​ Who can do this? Any individual in your organization with access to Atlan can install the Atlan add-in for Microsoft Excel. Your Atlan tenant must be hosted on the standard domain atlan.com to set up the add-in as a user. Install Atlan in Microsoft Excel ​ To install the Atlan add-in directly in Microsoft Excel: Open a new Microsoft Excel workbook. From the upper right of the Home tab, click the Add-ins button, and then from the dropdown, click More Add-ins . In the Office add-ins dialog, click Store . In the search bar of your Office Store , type Atlan and press enter. Select the Atlan add-in and click Add . If you see a dialog asking for permissions, click Continue to proceed. Connect Atlan to Microsoft Excel ​ To connect Atlan with your Microsoft Excel workbook: In the menu bar of your Microsoft Excel workbook, click the Atlan tab. From the top left of the Atlan tab, click Setup to set up Atlan in your Microsoft Excel workbook. In the Atlan sidebar on the right, enter your Atlan tenant name   -  for example, https://<your-tenant-name>.atlan.com . If you have a custom domain, additional configuration will be required. Click Login to connect Atlan to Microsoft Excel. If you haven't logged into Atlan, you will be prompted to enter your credentials   -  including SSO, if enforced in your organization. Once connected, you can either enrich column metadata or download impacted assets for lineage analysis . Congrats on connecting Atlan with Microsoft Excel! 🎉 danger For every new Microsoft Excel workbook that you create, you will need to follow the steps outlined above to connect Atlan to that workbook. The Atlan add-in will remain connected for all worksheets within an already connected workbook. Deploy and publish the add-in as an admin ​ Who can do this? You will need your Microsoft 365 administrator to complete these steps   -  you may not have access yourself. Before you begin, you may need to Determine if Centralized Deployment of add-ins works for your organization . The Atlan add-in can be installed at the workspace level for: Standard domains -  your Atlan tenant must be hosted as a subdomain of atlan.com to deploy the add-in using the steps below. Custom domains -  if your Atlan tenant is hosted under a custom domain belonging to your organization, you will need to configure the Atlan add-in using PowerShell . To install the Atlan add-in directly in Microsoft Excel: Sign in at admin.microsoft.com . From the left menu of the admin center, click the Settings dropdown and then click Integrated apps . On the Integrated apps page, under Deployed apps , click Get apps . In the top right of the Microsoft 365 Apps published apps page, navigate to the search bar, type Atlan and press enter. Select the Atlan add-in for Microsoft Excel and click Get it now . If you see a dialog asking for permissions, click Get it now to proceed. In the Deploy New App dialog, enter the following details: In the Add users page, for Assign users , you can either: Click Entire organization to deploy the add-in to all users in your organization. Click Specific users/groups to deploy the add-in to a subset of users in your organization. Use the search box to find specific users or groups. Click Next to continue. In the Accept permissions requests page, the app capabilities and permissions of the apps are listed. If the app needs consent, select Accept permissions . Otherwise, click Next to continue. In the Review and finish deployment page, review the deployment and click Finish deployment . Once deployment is completed, click Done to finish setup. Note that it can take up to 24 hours for an add-in to show up for all your users. All your users will need to do next is connect Atlan to Microsoft Excel ! 🎉 Tags: data integration Previous How to integrate Atlan with Google Sheets Next Link your account Set up the add-in as a user Deploy and publish the add-in as an admin"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/collaboration/spreadsheets/how-tos/update-column-metadata-in-microsoft-excel",
    "content": "Configure Atlan Integrations Collaboration Spreadsheets How-tos How to update column metadata in Microsoft Excel On this page Update column metadata in Microsoft Excel Once you've connected Atlan with Microsoft Excel , you can import the column metadata for all your data assets in Atlan and make changes to them directly in Microsoft Excel. Atlan currently supports importing and updating column metadata for the following asset types: Tables Views Materialized views Looker explores Microsoft Power BI tables Salesforce objects Tableau data sources danger You will need to be logged into your Atlan instance before you can start importing column metadata for your Atlan assets in Microsoft Excel. If you do not have the permission to update asset metadata, your changes will neither be pushed to Atlan nor will a request be created for approval   -  even if you receive a success message in Microsoft Excel. Ensure that you have the requisite permissions to update an asset before pushing your changes. Your user permissions are used to determine the bulk updates you can make to ensure that there is no adverse impact on assets beyond your scope of access. Import column metadata ​ You can import column metadata for your data assets directly into Microsoft Excel. To import column metadata for your data assets to Microsoft Excel: In the menu bar of your Microsoft Excel workbook, click Atlan . From the Atlan tab, click Enrich metadata to view a list of your data assets in a sidebar. (Optional) To filter your assets by a specific asset type, in the Atlan sidebar, click the asset type   -  for example, Tableau Datasources . In the Atlan sidebar on your worksheet, you can either: Individually select the data asset(s) you want to import. To the left of the Import button, click the Select All checkbox to select all the assets that have loaded in the sidebar. (Optional) Scroll down and click Load more to load more assets in the sidebar. Click Import to import column metadata for your selected assets. The column metadata for your selected assets are now available in Microsoft Excel! 🎉 Did you know? If any changes are made to your imported columns in Atlan, you'll need to import those columns once again to access the updated version in Microsoft Excel. Update column metadata ​ Once you've imported your data assets from Atlan, you can edit the metadata for your selected data asset in Microsoft Excel. You can make changes to the column metadata once all the columns have been successfully imported. You can only make changes to the metadata in the following columns: Description Certification Status Certification Message Announcement Type Announcement Title Announcement Message Tags You cannot make the following changes: Edit headers for any of the columns Edit the metadata in the Column Name , Data Type , Propagated Tags , and Qualified Name columns Delete any columns or rows Any of these changes will not be pushed to Atlan and you'll receive an error message. Bulk update tags for columns ​ danger You cannot make any changes to the metadata in the Propagated Tags column. Navigate to the Tags column to add tags to your column assets in Microsoft Excel: When adding tags to columns: The tag must already exist in Atlan. If the tag does not exist in Atlan, updates will not sync and you will receive an error message. Tag match is case-sensitive, ensure that the tag is formatted exactly as it exists in Atlan. For example, if the tag in Atlan is formatted as Marketing Analysis , then columns tagged with marketing analysis will not sync. You can add multiple tags in the Tags column   -  separate multiple tags with a comma , . If you have added tags that exist in Atlan as well as ones that do not, only the existing tags will be synced. The unsupported tags will not sync and you will receive an error message. Tag propagation is disabled by default in Atlan, hence tags will not be propagated. Push your changes to Atlan ​ Once you've made changes to the column metadata, to push your changes: In the menu bar of your Microsoft Excel workbook, click Atlan . From the Atlan tab, click Sync to Atlan . The Atlan sidebar will appear once the changes have synced. (Optional) Click Open in Atlan to verify the changes. In Atlan, an Updated using Microsoft Excel stamp will appear in the activity log for updated assets. (Optional) Click the Microsoft Excel link to view the source spreadsheet from Atlan. Did you know? You can download impacted assets for impact analysis in Microsoft Excel. Tags: connectors data integration crawl Previous How to update column metadata in Google Sheets Next Troubleshooting spreadsheets Import column metadata Update column metadata Push your changes to Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements/how-tos/configure-smtp",
    "content": "Configure Atlan Integrations Communication SMTP and Announcements Configure SMTP Configure SMTP Who can do this? You will need to be an admin user in Atlan to configure SMTP. The default SMTP setup is preconfigured by Atlan. You should only update this configuration if you want to set up custom SMTP. Atlan uses SMTP to send emails, primarily for things like inviting users, login failure alerts, and scheduled queries . We provide an embedded SMTP server to do this, out-of-the-box. If desired, you can override this embedded SMTP se rver with your own. To override the embedded SMTP server: From the left menu of any screen, click Admin . Under Workspace , click SMTP . Fill in the configuration of your SMTP server, at least: For Host the fully-qualified hostname of the SMTP server. For From Email the email address that should be used to send emails from the server. For Username the username required by your SMTP server. At the bottom of the page, click the Test SMTP Config button to test your configuration. This will attempt to send an email to your profile's email address. Once you successfully receive the test email, at the bottom of the page click Save . Tags: setup configuration Previous SMTP and Announcements Integration Next Create announcements"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements/how-tos/manage-system-announcements",
    "content": "Configure Atlan Integrations Communication SMTP and Announcements Manage system announcements On this page Manage system announcements Who can do this? You will need to be an admin user in Atlan to manage system announcements. Have you ever wanted to notify everyone on the system of something? System announcements allow you to do just that. For example, you could welcome your users when you're first launching Atlan. Or you could use it to warn them about planned outages to large parts of your IT landscape. System announcements appear on the homepage of all users, in an announcement box under Recent updates . Did you know? You can only create one system announcement per instance. To add more information to your system announcement, you can either edit the existing one to update it or delete the old one and create a new system announcement. Add a system announcement ​ To add a system announcement: From the left menu of any screen, click Admin . In the upper right of the page, click the New announcement button. (Optional) At the top of the dialog, click on Information to change the style of announcement: To give general information, in blue, choose Information . To display a problem, in red, choose Issue . To give a warning, in yellow, choose Warning . For Add announcement header... enter a brief title for your announcement. For Add description... enter the detailed explanation for the announcement. (Optional) You can use Markdown syntax to write a description. Any headings will be rendered in heading 6 ( <h6> ). danger Atlan currently does not support adding images to your announcements. At the bottom of the dialog, click Save . Your announcement will now appear on the home page of every user that visits Atlan! 🎉 Remove a system announcement ​ To remove a system announcement: From the left menu of any screen, click Admin (or from the homepage where the announcement is displayed). In the upper-right corner of the announcement box, click the 3-dot button. From the options, click Delete . Your announcement has now been removed from the home page of every user that visits Atlan! 🎉 Tags: atlan documentation Previous Create announcements Next Identity Management Integrations Add a system announcement Remove a system announcement"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/communication/smtp-and-announcements/how-tos/create-announcements",
    "content": "Configure Atlan Integrations Communication SMTP and Announcements Create announcements On this page Create announcements Adding an announcement to your data asset helps you call attention to an important feature or notify others about a change coming down the pipeline. Since announcements in Atlan display the time stamp and author information, you can easily identify whether an announcement is still relevant and who to ask for questions. What type of announcements would you want to share with your team? Here are a few examples: Add an announcements ​ To add an announcement to an asset: From the left menu of any screen in Atlan, click Assets . On the Assets page, select an asset to add an announcement. You can either: Open the asset profile. From the top right of the Overview tab in the asset profile, click the vertical 3-dot icon, and then from the dropdown, click Add announcement . In the Overview tab of the asset sidebar, click the horizontal 3-dot icon, and then from the dropdown, click Add announcement . In the New Announcement dialog, enter the following details: From the top right, click the downward arrow and choose from three announcement types: Information , Issue , or Warning . For Add title , enter a title for your announcement. For Description , enter a description for your announcement. (Optional) You can include HTML hyperlinks to direct users to additional information   -  for example, wrap the text with <a href=\"https://my.url.com\">description text</a> . (Optional) You can use Markdown syntax to write a description. Any headings will be rendered in heading 6 ( <h6> ). danger Atlan currently does not support adding images to your announcements. (Optional) To share your announcement on Slack or Teams, you can either: Click the Share button to integrate Slack or Microsoft Teams . If you have already integrated Slack or Microsoft Teams , click the checkbox for Share on Slack or Share on Teams , respectively. (Optional) To configure the announcements channel for Slack or Teams: If you have already configured a Slack or Microsoft Teams channel to receive announcement alerts, that channel will be preselected. You can change to a different channel, if available. If you have not configured a channel for announcements, enter the channel name to receive notifications for announcements. This channel will be displayed as the Announcements channel in your Slack or Microsoft Teams integration. Click Add to create your announcement. (Optional) To edit an announcement, from the top right of the announcement box, click the 3-dot icon and then click Edit . (Optional) To delete an announcement, from the top right of the announcement box, click the 3-dot icon and then click Delete . You just created an announcement! 🎉 This announcement will be visible to anyone who views the asset. You can also create similar announcements for other types of data assets, including glossaries, categories, and terms . To create, remove, and manage announcements using API, refer to our developer documentation . Did you know? You can only create one announcement per asset. To add more information to your announcement, you can either edit the existing one to update it or delete the old one and create a new announcement. Tags: data integration Previous Configure SMTP Next Manage system announcements Add an announcements"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/how-tos/configure-scim-provisioning",
    "content": "Configure Atlan Integrations Identity Management SCIM Configure SCIM provisioning Configure SCIM provisioning You can automate the process of provisioning and deprovisioning your users and groups in Atlan with System for Cross-domain Identity Management (SCIM). Atlan supports SCIM 2.0 for SCIM provisioning. SCIM provisioning works in combination with your single sign-on (SSO) setup. Setting up SCIM enables you to manage all your users from one central location. Atlan currently supports SCIM provisioning for the following SSO providers: Azure AD Okta For more questions about SCIM provisioning, head over to Troubleshooting SCIM provisioning . Tags: integration setup Previous SCIM Integration Next How to enable Azure AD for SCIM provisioning"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/authenticate-sso-credentials-to-query-data",
    "content": "Configure Atlan Integrations Identity Management SSO Guides Authenticate SSO credentials to query data On this page Authenticate SSO credentials to query data Who can do this? Any Atlan user with data access to the asset and SSO credentials for the connection. Once your connection admins have configured SSO authentication, you can query data using your SSO credentials. Atlan currently supports the following connectors for SSO authentication: Amazon Redshift -  currently only Okta is supported as the identity provider. Google BigQuery -  Atlan uses Google OAuth 2.0, which handles integration with all identity providers. However, Atlan has only validated the integration with Okta. Snowflake -  Atlan uses Snowflake External OAuth for SSO authentication, thus supporting all Snowflake-supported identity providers . To query data using shared credentials instead, refer to Provide credentials to query data . Did you know? Connections that require you to provide SSO credentials have a small icon next to them. If the connection you want to query has no icon, you can query it with its default shared credentials. You only need to authenticate for connections with this icon. Set up your SSO credentials ​ Atlan supports SSO authentication for querying data from the following connections: Amazon Redshift ​ Atlan supports Okta SSO authentication for Amazon Redshift connections. Before you can query data with SSO credentials, you will first need to enable Okta SSO authentication for Amazon Redshift in Atlan. To set up your Okta SSO credentials for an Amazon Redshift connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the dropdown to select the Amazon Redshift connection that requires SSO credentials. A Set up SSO authentication for Redshift dialog will appear. Click Get started to set up your SSO credentials for the connection: For Authentication , Okta is the default selection. For Username , enter your Okta username. For Password , enter the password for your Okta username. Click the Test Authentication button to confirm connectivity. Once authentication is successful, click Done . Close the SSO authentication completed dialog to return to the query editor. You can now run queries using your Okta SSO credentials! 🎉 Google BigQuery ​ Atlan supports Google OAuth 2.0 SSO authentication for Google BigQuery connections. Before you can query data with SSO credentials, you will first need to enable SSO authentication for Google BigQuery in Atlan. To set up your SSO credentials for a Google BigQuery connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the dropdown to select the Google BigQuery connection that requires SSO credentials. An Authorizing dialog will appear and you will be redirected to sign in with your Google account. From the corresponding screen, click Allow to enable authentication. Once authorization is successful, close the Authorizing dialog to return to the query editor. You can now run queries using your SSO credentials! 🎉 Snowflake ​ Atlan supports SSO authentication via Snowflake External OAuth for Snowflake connections. Before you can query data with SSO credentials, you will first need to enable SSO authentication for Snowflake in Atlan. To set up your Snowflake OAuth credentials for a Snowflake connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the dropdown to select the Snowflake connection that requires Snowflake OAuth credentials. An Authorizing dialog will appear. Once authorization is successful, close the Authorizing dialog to return to the query editor. (Optional) To change roles and warehouses, click the connection name in the left menu or the Editor context tab with the connection name: For Role , click the Select role dropdown to select a granted role assigned to you in Snowflake. If no role is selected, Atlan will use the default PUBLIC role in Snowflake for authentication. For Warehouse , click the Select warehouse dropdown to change warehouses. You can now run queries using your Snowflake OAuth credentials! 🎉 Remove your SSO credentials ​ To remove your SSO credentials for a connection: From the left menu of any screen, click Insights . Under the Explorer tab on the left, use the dropdown to select the connection with SSO authentication enabled. From the upper right of the query editor, click the Editor context tab with the connection name. In the Editor context dialog, hover over the timestamp and then click Log out . In the Log out from Insights dialog, click Log out to confirm. Did you know? You can refer to troubleshooting connector-specific SSO authentication to troubleshoot any errors. Tags: atlan documentation Previous How to enable SAML 2.0 for SSO Next Authenticate SSO credentials to view sample data Set up your SSO credentials Remove your SSO credentials"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/how-tos/enable-okta-for-scim-provisioning",
    "content": "Configure Atlan Integrations Identity Management SCIM How to enable Okta for SCIM provisioning On this page Enable Okta for SCIM provisioning You can automate the process of provisioning and deprovisioning your Okta users and groups in Atlan with System for Cross-domain Identity Management (SCIM). To enable Okta for SCIM provisioning, complete the following steps. Did you know? For any questions about SCIM provisioning, head over here . Prerequisites ​ Okta SSO must be enabled for Atlan . Okta users must be assigned to Atlan . Group mapping must be configured , only required if syncing mapped groups from Okta to Atlan. For any new groups created in Okta, you will first need to create corresponding groups in Atlan and then map the groups to sync them through SCIM provisioning. Retrieve SCIM token in Atlan ​ Who can do this? You will need your Atlan admin to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Okta administrator. You will need to generate a SCIM token in Atlan for authentication in Okta . To retrieve the SCIM token, from within Atlan: From the left menu on any screen, click Admin . Under the Workspace heading, click SSO . On the Single Sign on page for Okta, under Overview , navigate to Automate Provisioning with SCIM and toggle it on. Under SCIM token , click the + Generate token button to create a SCIM token. In the SCIM token generated dialog, click the Copy button to copy the SCIM token and store it in a secure location. danger The SCIM token will only be displayed once after it has been generated, you cannot retrieve it later. Enable SCIM provisioning in Okta ​ Who can do this? You will need your Okta administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Atlan admin . You can enable SCIM provisioning in Okta to automatically sync your users and groups to Atlan. Configure SCIM provisioning in Okta ​ To configure SCIM provisioning, from within Okta: Log in to your Okta admin console. From the menu on the left, expand the Applications menu and then click Applications . Under Applications , select the SAML application you created to configure SSO in Atlan. From the tabs along the top of your application page, click the General tab and then click Edit . Under App Settings , for Provisioning , click SCIM and then click Save to confirm. From the tabs along the top of your application page, click the Provisioning tab and then click Edit . For SCIM connection , enter the following details: For SCIM connector base URL , enter your Atlan tenant URL in the following format   - https://<your-tenant-dns>/api/service/scim/ . For Unique identifier field for users , enter userName as the field name of the unique identifier for your users on your SCIM server. For Supported provisioning actions , click to enable the following provisioning actions: Import New Users and Profile Updates -  this allows Okta to import new users and user profile updates to Atlan. Push New Users -  this allows user information to flow from Okta to Atlan. Push Profile Updates -  this allows profile information to flow from Okta to Atlan. Push Groups -  this allows group information to flow from Okta to Atlan. Import Groups -  this allows Okta to import new groups and group profile updates to Atlan. For Authentication Mode , click the dropdown and then select HTTP Header . To authenticate using HTTP Header , you will need to provide a bearer token that will provide authorization against Atlan. For Authorization , in the Token field, enter the SCIM token you copied in Atlan. Click the Test Connector Configuration button to confirm connectivity to Atlan. Once successful, at the bottom of the form, click Save to save the configuration. Under the left Settings menu of the Provisioning tab, two new tabs will appear   - To App and To Okta . Click To App to configure settings for SCIM provisioning to Atlan. For Provisioning to App page, click Edit and then click to enable the following: Create Users -  assigns a new Atlan account to each user managed by Okta. Okta does not create a new account if it detects that the username specified in Okta already exists in Atlan. The user's Okta username is assigned by default. Update User Attributes -  updates the user profiles of users assigned to Atlan. Profile changes made in Atlan will be overwritten with their respective Okta profile values. Deactivate Users -  automatically deactivates user accounts when they are unassigned in Okta or their Okta accounts are deactivated. Okta will also reactivate the Atlan account if the app integration is reassigned to a user in Okta. Click Save to save the configuration. Map Okta user attributes to Atlan ​ danger You will need to assign users to Atlan from Okta before you can provision them. After you have enabled SCIM provisioning and assigned users to Atlan in Okta, you can provision them to Atlan. Note the following: The username and email address of new and existing users cannot be changed once users have been provisioned to Atlan. If provisioning any users that already exist in Atlan, ensure that their Okta credentials match the existing credentials in Atlan for provisioning to be successful. To provision users to Atlan, from within Okta: Log in to your Okta admin console. From the menu on the left, expand the Directory menu and then click Profile Editor . On the Profile Editor page, in the left menu under Users , click Apps and select the SAML application you created to configure SSO in Atlan. On your application page, under Attributes , click Mappings . In the User Profile Mappings dialog box, click Okta User to App . In the Okta User to App page, userName is already set by Atlan. Define the following mappings from Okta on the left to Atlan on the right: user.firstName - > givenName user.lastName - > familyName user.email - > email Click Save to save your selections. Once saved, at the bottom of the dialog, click Apply updates now . (Optional) Navigate to the Provisioning tab of the SAML application you created to configure SSO in Atlan to confirm the attribute mappings. Enable group push in Okta to Atlan ​ danger You will need to configure group mapping in Atlan before you can enable group push from Okta to Atlan. To enable group push to Atlan, from within Okta: Log in to your Okta admin console. From the menu on the left, expand the Applications menu and then click Applications . Under Applications , select the SAML application you created to configure SSO in Atlan. From the tabs along the top of your application page, click the Push Groups tab and then click Edit . Under Push Groups to App , click the settings icon. From the Group Push Settings dialog, click Rename app groups to match group name in Okta and then click Save to rename groups in Atlan when linking groups. Under Push Groups to App , click the Push Groups button and then select Find groups by name to push your Okta groups to Atlan: For Push groups by name , in the Enter a group to push... field, enter the name of an Okta group you want to push to Atlan. To the right of your selected Okta group, under Match result & push action , click the Create Group dropdown and then select Link Group . Click Save to save your selections. (Optional) Repeat steps 1 to 3 to push additional Okta groups to Atlan. Tags: integration connectors Previous How to enable Azure AD for SCIM provisioning Next Troubleshooting SCIM provisioning Prerequisites Retrieve SCIM token in Atlan Enable SCIM provisioning in Okta"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-azure-ad-for-sso",
    "content": "Configure Atlan Integrations Identity Management SSO Get Started How to enable Azure AD for SSO On this page Enable  Azure AD for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your Azure AD administrator to carry out the tasks below in Azure AD. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Azure AD, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Azure AD, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate Azure AD SSO for Atlan, complete the following steps. Choose SSO provider (in Atlan) ​ To choose Azure AD as your SSO provider, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select Azure AD and then click Configure . Under Service provider metadata , copy the Identifier (Entity ID) , Reply URL (Assert Consumer Service URL) , and Logout Url . Set up SAML app (in Azure AD) ​ To set up a SAML app, within Azure's portal : From the menu on the left, open Azure Active Directory . Under Azure Active Directory | Overview click the Add button and then Enterprise application . Under Browser Azure AD Gallery click the Create your own application button: For What's the name of your app? enter a name, such as Atlan . For What are you looking to do with your application? select Integrate any other application you don't find in the gallery (Non-gallery) . At the bottom of the Create your own application dialog, click the Create button. Wait for the application details to be shown   -  this can take around 1 minute. Under Getting Started and within the Set up single sign on tile, click the Get started link. Under Select a single sign-on method click the SAML tile. In the upper-right of the Basic SAML Configuration card, click the Edit button and enter: For Identifier (Entity ID) click Add identifier and enter the value you copied from Atlan above. For Reply URL (Assertion Consumer Service URL) click Add reply URL (twice) and enter the two values you copied from Atlan above. The longer URL should be enabled under the Default column. For Logout Url (Optional) enter the value you copied from Atlan above. At the top of the page, under Basic SAML Configuration , click the Save button. Close the Basic SAML Configuration dialog by clicking X in the upper-right. In the upper-right of the Attributes & Claims card, click the Edit button: Navigate to the Additional claims section, click each of the following claims to modify their Name exactly as suggested below and remove the Namespace value: email   - > user.mail firstName   - > user.givenname lastName   - > user.surname (Optional) username   - > ExtractMailPrefix(user.mail) info 💪 Did you know? For users assigned to Atlan through SSO, the username will be populated from the username mapping. Otherwise, the username will be the email prefix by default, which users can update while registering on Atlan for the first time. To configure group claims: From the options along the top, click + Add a group claim . In the popover, under Which groups associated with the user should be returned in the claim? , select Groups assigned to the application . From the Source attribute dropdown, select Cloud-only group display names (Preview) . If you have a hybrid setup , select sAMAccountName instead and then check the Emit group name for cloud-only groups checkbox. danger Please ensure that the Cloud-only group display names attribute contains the actual group display names. If not, you will need to update the source attribute with the relevant one that contains group display names. Click Advanced options to expand the dropdown menu: Check the Customize the name of the group claim box. For Name , enter memberOf . This is required if you want to retain group membership in Atlan. Click Save and close the popover by clicking X in the upper-right. Download Azure AD's metadata file (in Azure AD) ​ To download Azure AD's metadata file, within the same Azure AD app's SAML-based Sign-on page: Within the SAML Signing Certificate card, to the right of Federation Metadata XML , click the Download link. Within the Set up <application> card, copy the Logout URL . Assign users or groups to the app (in Azure AD) ​ To assign users or groups to the app, within the Azure AD application's page: Under Manage, click Users and groups . At the top of the table, click the Add user/group button. In the resulting Add Assignment dialog, under the Users or Groups heading, click the None Selected link. In the resulting Users or Groups dialog, search for users or groups to add and click to select them. When finished, at the bottom of the Users or Groups dialog, click the Select button. At the bottom of the Add Assignment dialog, click the Assign button. Upload Azure AD's metadata file (in Atlan) ​ To complete the configuration of Azure AD SSO, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select Azure AD and then click Configure . To the right of Identity provider metadata click the Import from XML button. Select the XML file downloaded from Azure AD above. Under Single Logout Service URL , enter the logout URL copied from Azure AD above. At the bottom of the screen, click Save . Congratulations   -  you have successfully set up Azure AD SSO in Atlan! 🎉 Did you know? By default, users can now log into Atlan with either Azure AD SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your Azure AD administrator to provision access to users through the Azure portal and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings ​ danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from Azure AD to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan   -  irrespective of whether or not you enable SCIM . To automatically assign Azure AD users to Atlan groups based on their Azure AD groups, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in Azure AD to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each Azure AD group with access to Atlan. Click the Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! 🎉 Did you know? You can configure SCIM provisioning in Azure AD to manage your users and groups in Atlan. Plus, once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous SSO Integration Next How to enable Google for SSO Choose SSO provider (in Atlan) Set up SAML app (in Azure AD) Download Azure AD's metadata file (in Azure AD) Assign users or groups to the app (in Azure AD) Upload Azure AD's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/scim/how-tos/enable-azure-ad-for-scim-provisioning",
    "content": "Configure Atlan Integrations Identity Management SCIM How to enable Azure AD for SCIM provisioning On this page Enable  Azure AD for SCIM provisioning You can automate the process of provisioning and deprovisioning your Azure Active Directory (AD) users and groups in Atlan with System for Cross-domain Identity Management (SCIM). To enable Azure AD for SCIM provisioning, complete the following steps. Did you know? For any questions about SCIM provisioning, head over here . Prerequisites ​ Azure AD SSO must be enabled for Atlan . Azure AD users or groups must be assigned to Atlan . Group mapping must be configured , only required if syncing mapped groups from Azure AD to Atlan. For any new groups created in Azure AD, you will first need to create corresponding groups in Atlan and then map the groups to sync them through SCIM provisioning. Retrieve SCIM token in Atlan ​ Who can do this? You will need your Atlan admin to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Azure AD administrator. You will need to generate a SCIM token in Atlan for authentication in Azure AD . To retrieve the SCIM token, within Atlan: From the left menu on any screen, click Admin . Under the Workspace heading, click SSO . On the Single Sign on page for Azure AD, under Overview , navigate to Automate Provisioning with SCIM and toggle it on. Under SCIM token , click the + Generate token button to create a SCIM token. In the SCIM token generated dialog, click the Copy button to copy the SCIM token and store it in a secure location. danger The SCIM token will only be displayed once after it has been generated, you cannot retrieve it later. Enable SCIM provisioning in Azure AD ​ Who can do this? You will need your Azure AD administrator to complete these steps   -  you may not have access yourself. You will also need inputs and approval from your Atlan admin . You can enable SCIM provisioning in Azure AD to automatically sync your users and groups to Atlan. Configure SCIM provisioning in Azure AD ​ To configure SCIM provisioning, within Azure AD: Log in to your Azure portal and search for and select Azure Active Directory . From the left menu under Manage , select Enterprise applications . From the All applications page, select the SAML application you created to configure SSO in Atlan. In the left menu of your application page, under Manage , click Provisioning . From the Provisioning mode dropdown, click Automatic . Under Admin credentials , enter the following details: For Tenant URL , enter your Atlan tenant URL in the following format   - https://<your-tenant-dns>/api/service/scim . For Secret Token , enter the SCIM token you copied in Atlan. Click the Test connection button to confirm connectivity to Atlan. When successful, in the top right, click Save to save the configuration. In the Mappings section, verify that Provision Azure Active Directory Groups and Provision Azure Active Directory Users are enabled. Under Mappings : Click Provision Azure Active Directory Groups , and under Attribute Mappings , define the following mappings from Azure AD on the left to Atlan on the right: displayName - > displayName -  Note that this field is currently unsupported in Atlan. objectId - > externalId members - > members Click Provision Azure Active Directory Users , and under Attribute Mappings , define the following mappings from Azure AD on the left to Atlan on the right: mailNickname - > userName -  If the username is not mapped, the default username will be the UserPrincipalName (UPN). Switch([IsSoftDeleted], , \"False\", \"True\", \"True\", \"False\") - > active displayName - > displayName mail - > emails[type eq \"work\"].value givenName - > name.givenName surname - > name.familyName objectId - > externalId To save any changes, click Save . Provision users and groups ​ danger You will need to assign users or groups to Atlan from Azure AD before you can provision them. You will also need to configure group mapping to sync mapped groups from Azure AD to Atlan. For any new groups created in Azure AD, you will first need to map the groups in Atlan to sync them through SCIM provisioning. After you have enabled SCIM provisioning and assigned users and groups to Atlan in Azure AD, you can provision them to Atlan. In Azure AD, users and groups can be provisioned in two ways   - provisioning cycle and on-demand provisioning . Note the following: The username and email address of new and existing users cannot be changed once users have been provisioned to Atlan. If provisioning any users that already exist in Atlan, ensure that their Azure AD credentials match the existing credentials in Atlan for provisioning to be successful. To provision users and groups, within Azure AD: Log in to your Azure portal and search for and select Azure Active Directory . From the left menu under Manage , select Enterprise applications . From the All applications page, select the SAML application you created to configure SSO in Atlan. In the left menu of your application page, under Manage , click Provisioning and select a provisioning method: To enable provisioning cycle , in the upper left of the Overview page, click Start provisioning and toggle the Provisioning Status to On . To enable on-demand provisioning , from the left menu, click Provision on demand . To provision users or groups on demand: For Select a user or group , search for and select a user or group. At the bottom of the screen, click Provision . Repeat the steps for every user or group you want to provision. Once you have enabled SCIM provisioning, Azure AD will automatically provision and update user accounts in Atlan. However, the sync typically happens every 40 minutes . So, it may take up to 40 minutes for user provisioning to be completed in Atlan. Did you know? There are known limitations to on-demand provisioning in Azure AD. Tags: atlan documentation Previous Configure SCIM provisioning Next How to enable Okta for SCIM provisioning Prerequisites Retrieve SCIM token in Atlan Enable SCIM provisioning in Azure AD"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-jumpcloud-for-sso",
    "content": "Configure Atlan Integrations Identity Management SSO Get Started How to enable JumpCloud for SSO On this page Enable  JumpCloud for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your JumpCloud administrator to carry out the tasks below in JumpCloud. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in JumpCloud, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in JumpCloud, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate JumpCloud SSO for Atlan, complete the following steps. Choose SSO provider (in Atlan) ​ To choose JumpCloud as your SSO provider, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select Jumpcloud and then click Configure . Under Service provider metadata , copy the IdP Entity ID , SP Entity ID , and ACS URL . Set up SAML app (in JumpCloud) ​ To set up a SAML app, within JumpCloud Console : From the menu on the left, under User Authentication click SSO . To the left of the search box, click the large circular + icon. At the bottom of the page, click the Custom SAML App button. Under the General Info tab, for Display Label enter a name such as Atlan . Change to the SSO tab and enter your Atlan SAML settings: For IdP Entity ID enter the value you copied from Atlan above. For SP Entity ID enter the value you copied from Atlan above. For ACS URL enter the value you copied from Atlan above. Below Signature Algorithm ensure Sign Assertion is enabled. Scroll to the bottom of the SSO tab and under User Attribute Mapping click the add attribute button. Define the following mappings from Service Provider Attribute Name on the left to JumpCloud Attribute Name on the right: email   - > email firstName   - > firstname lastName   - > lastname group   - > group (you may need to select Custom User or Group Attribute from the JumpCloud Attribute Name drop-down, and then type in group ) username   - > username Under the Group Attributes heading, enable the include group attribute box and enter the value memberOf . This is required if you want to retain group membership in Atlan. Change to the User Groups tab and check the box for each user group you want to be enabled for SSO. Below the form, click the activate button and when prompted click the continue button. Download JumpCloud metadata file (in JumpCloud) ​ To download the JumpCloud metadata file, within JumpCloud Console : From the SSO app page, click your Atlan SSO application to open it. Change to the SSO tab and under JumpCloud Metadata click the Export Metadata button. Upload JumpCloud's metadata file (in Atlan) ​ To complete the configuration of JumpCloud SSO, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select Jumpcloud and then click Configure . To the right of Identity provider metadata click the Import from XML button. Select the JumpCloud-saml2-metadata.xml file downloaded from JumpCloud above. At the bottom of the screen, click Save . Congratulations   -  you have successfully set up JumpCloud SSO in Atlan! 🎉 Did you know? By default, users can now log into Atlan with either JumpCloud SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your JumpCloud administrator to provision access to users through JumpCloud and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings ​ danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from JumpCloud to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan. To automatically assign JumpCloud users to Atlan groups based on their JumpCloud groups, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in JumpCloud to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each JumpCloud group with access to Atlan. Click the Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! 🎉 Did you know? Once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable Google for SSO Next How to enable Okta for SSO Choose SSO provider (in Atlan) Set up SAML app (in JumpCloud) Download JumpCloud metadata file (in JumpCloud) Upload JumpCloud's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-google-for-sso",
    "content": "Configure Atlan Integrations Identity Management SSO Get Started How to enable Google for SSO On this page Enable  Google for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your Google domain administrator to carry out the tasks below in the Google Admin Center. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Google, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Google, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate Google SSO for Atlan, complete the fo llowing steps. Choose SSO provider (in Atlan) ​ To choose Google as your SSO provider, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select Google and then click Configure . Under Service provider metadata , copy the ACS URL and Entity ID . Set up SAML app (in Google Admin Center) ​ To set up a SAML app, within Google Admin Center : From the menu on the left, expand Apps and then click on Web and mobile apps . At the top of the table, click the Add app link and then click Add custom SAML app . Enter a name for your app, such as Atlan and then click the Continue button. Under Option 1: Download IdP metadata click the Download metadata button, save the file, and then click the Continue button. Under Service provider details , enter your Atlan SAML settings: For ACS URL , enter the value you copied from Atlan above. For Entity ID , enter the value you copied from Atlan above. Click the Continue button. Under Attributes , define the following mappings from Google Directory attributes on the left to App attributes on the right: Primary email   - > email First name   - > firstName Last name   - > lastName (Optional) To configure group mapping in Atlan , under Group membership (optional) , enter the following details: For Google Groups , select all the Google groups you want to map to Atlan. You can select up to 75 groups in total. For App attribute , enter memberOf . This is required if you want to retain group membership in Atlan. At the end of the form, click the Finish button. Assign users to the app (in Google Admin Center) ​ To assign users to the app, within Google Admin Center : From the app page, expand User access . Under Service status change to ON for everyone and then click Save . Upload Google's metadata file (in Atlan) ​ To complete the configuration of Google SSO, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select Google and then click Configure . To the right of Identity provider metadata click the Import from XML button. Select the GoogleIDPMetadata.xml file downloaded from Google above. At the bottom of the screen, click Save . Congratulations   -  you have successfully set up Google SSO in Atlan! 🎉 Did you know? By default, users can now log into Atlan with either Google SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your Google domain administrator to provision access to users through the Google Admin Center and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings ​ danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from Google to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan. To automatically assign Google users to Atlan groups based on their Google groups, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in Google to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each Google group with access to Atlan. Click the Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! 🎉 Did you know? Once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable Azure AD for SSO Next How to enable JumpCloud for SSO Choose SSO provider (in Atlan) Set up SAML app (in Google Admin Center) Assign users to the app (in Google Admin Center) Upload Google's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/authenticate-sso-credentials-to-view-sample-data",
    "content": "Configure Atlan Integrations Identity Management SSO Guides Authenticate SSO credentials to view sample data On this page Authenticate SSO credentials to view sample data Who can do this? Any Atlan user with data access to the asset and SSO credentials for the connection. Atlan will display a 100-row sample of the data . Once your connection admins have configured SSO authentication, you can view sample data using your SSO credentials. Atlan currently supports the following connectors for SSO authentication: Amazon Redshift -  currently only Okta is supported as the identity provider. Google BigQuery -  Atlan uses Google OAuth 2.0, which handles integration with all identity providers. However, Atlan has only validated the integration with Okta. Snowflake -  Atlan uses Snowflake External OAuth for SSO authentication, thus supporting all Snowflake-supported identity providers . To view sample data using shared credentials instead, refer to Provide credentials to view sample data . Set up your SSO credentials ​ Atlan supports SSO authentication for viewing sample data from the following connections: Amazon Redshift ​ Atlan supports Okta SSO authentication for Amazon Redshift connections. Before you can view sample data with SSO credentials, you will first need to enable Okta SSO authentication for Amazon Redshift in Atlan. To set up your Okta SSO credentials for viewing sample data, for Amazon Redshift: On the Assets page, click an Amazon Redshift asset to view its asset profile. From the asset profile, click Sample data . A Set up SSO authentication for Redshift dialog will appear. Click Get started to set up your SSO credentials for the connection: For Authentication , Okta is the default selection. For Username , enter your Okta username. For Password , enter the password for your Okta username. Click the Test Authentication button to confirm connectivity. Once authentication is successful, click Done . You can now view sample data using your Okta SSO credentials! 🎉 Google BigQuery ​ Atlan supports Google OAuth 2.0 SSO authentication for Google BigQuery connections. Before you can view sample data with SSO credentials, you will first need to enable SSO authentication for Google BigQuery in Atlan. To set up SSO credentials for viewing sample data, for Google BigQuery: On the Assets page, click a Google BigQuery asset to view its asset profile. From the asset profile, click Sample data . To set up your SSO credentials for viewing sample data, click Get started . An Authorizing dialog will appear and you will be redirected to sign in with your Google account. From the corresponding screen, click Allow to enable authentication. Once authorization is successful, close the Authorizing dialog. You can now view sample data using SSO credentials! 🎉 Snowflake ​ Atlan supports SSO authentication via Snowflake External OAuth for Snowflake connections. Before you can view sample data with SSO credentials, you will first need to enable SSO authentication for Snowflake in Atlan. To set up Snowflake OAuth credentials for viewing sample data, for Snowflake: On the Assets page, click a Snowflake asset to view its asset profile. From the asset profile, click Sample data . To set up your Snowflake OAuth credentials for viewing sample data, click Get started . An Authorizing dialog will appear. Once authorization is successful, close the Authorizing dialog. You can now view sample data using Snowflake OAuth credentials! 🎉 Did you know? If your Atlan admin has enabled sample data download , you will be able to export sample data in a CSV file. Remove SSO credentials ​ To remove your SSO credentials for a connection: On the Assets page, click an asset to view its asset profile. From the asset profile, click Sample data . At the bottom of the Sample data screen, hover over the timestamp and then click Log out . In the Log out from sample data preview dialog, click Log out to confirm. Did you know? You can refer to troubleshooting connector-specific SSO authentication to troubleshoot any errors. Tags: atlan documentation Previous Authenticate SSO credentials to query data Next Limit SSO automatically creating users when they log in Set up your SSO credentials Remove SSO credentials"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-associated-terms",
    "content": "Configure Atlan Administration Feature Management How to enable associated terms On this page Enable  associated terms Who can do this? You will need to be an admin user in Atlan to enable associated terms . To enable associated terms, follow these steps. Enable associated terms ​ To enable associated terms for your users: From the left menu in Atlan, click Admin . Under Workspace , click Labs . Under the Glossary heading of the Labs page, for Term attributes , click the No attributes applied dropdown. From the terms menu, select the associated terms you'd like to enable for your users. Your users will now be able to add associated terms to their glossaries! 🎉 If you'd like to turn off any of the associated terms, follow the steps above and then deselect the corresponding checkboxes. Tags: glossary business-terms definitions Previous Disable user activity Next How to enable discovery of process assets Enable associated terms"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-okta-for-sso",
    "content": "Configure Atlan Integrations Identity Management SSO Get Started How to enable Okta for SSO On this page Enable  Okta for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your Okta administrator to carry out the tasks below in Okta. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in Okta, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in Okta, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate Okta SSO for Atlan, complete the foll owing steps. Choose SSO provider (in Atlan) ​ To choose Okta as your SSO provider, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select Okta and then click Configure . Under Service provider metadata , copy the Single sign on URL and Audience URI (SP Entity ID) . Set up SAML app (in Okta) ​ To set up a SAML app, within Okta's administration console: From the menu on the left, expand Applications and then click on Applications . At the top of the table, click the Create App Integration button. In the Create a new app integration dialog, select SAML 2.0 and then click Next . Under General Settings enter: For App name , enter a name for the application, such as Atlan . Click the Next button. Under SAML Settings - General enter: For Single sign on URL enter the value you copied from the field of the same name in Atlan above. Ensure Use this for Recipient URL and Destination URL is enabled. For Audience URI (SP Entity ID) enter the value you copied from the field of the same name in Atlan above. Under Attribute Statements (optional) define the following mappings from Name (Name format) on the left to Value on the right: firstName (Basic)   - > user.firstName lastName (Basic)   - > user.lastName email (Basic)   - > user.email group (Basic)   - > user.group info 💪 Did you know? For users assigned to Atlan through SSO, the username will be populated from the username mapping. Otherwise, the username will be the email prefix by default, which users can update while registering on Atlan for the first time. Under Group Attribute Statements (optional for SSO login, required for group sync) define the following mappings from Name (Name format) on the left to Filter on the right: memberOf (Unspecified)   - > Matches regex [\\s\\S]+ -  for examples of how to filter groups with regex in Okta, refer to Okta documentation . This is required if you want to retain group membership in Atlan. While this step is optional for basic SSO authentication, you must configure the memberOf attribute if you want to sync Okta groups to Atlan and use group-based access control At the bottom of the form, click the Next button. Under Help Okta Support understand how you configured this application select I'm an Okta customer adding an internal app and for App type enable This is an internal app that we have created . Click the Finish button. Download Okta's metadata file (in Okta) ​ To download Okta's metadata file, within the Okta app's page: Open the Sign On tab. Under the SAML Signing Certificates heading, in the table, click the Actions link under the Actions column. From the drop-down, click View IdP metadata . Save the XML file, if it appears in plain text in your browser. Assign users to the app (in Okta) ​ To assign users to the app, within the Okta app's page: Open the Assignments tab. At the top of the table, click the Assign button and select Assign to People to add individual users or Assign to Groups to add groups. To the right of each user to whom you want to assign the application, click Assign . To assign the application to a group, you may have to locate it first. For individual users, confirm that the data is correct in the Assign Atlan to People dialog. For groups, complete the fields in the Assign Atlan to Groups dialog if it appears. Click Save and Go Back . Repeat steps 3 to 5 for each user or group to which you want to assign the application. When finished, in the respective dialog box, click Done . Upload Okta's metadata file (in Atlan) ​ To complete the configuration of Okta SSO, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select Okta and then click Configure . To the right of Identity provider metadata click the Import from XML button. Select the XML file saved from Okta above. At the bottom of the screen, click Save . Congratulations   -  you have successfully set up Okta SSO in Atlan! 🎉 Did you know? By default, users can now log into Atlan with either Okta SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your Okta administrator to provision access to users through Okta and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings ​ danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from Okta to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan   -  irrespective of whether or not you enable SCIM . To automatically assign Okta users to Atlan groups based on their Okta groups, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in Okta to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each Okta group with access to Atlan. Click the Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! 🎉 Did you know? You can configure SCIM provisioning in Okta to manage your users and groups in Atlan. Plus, once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable JumpCloud for SSO Next How to enable OneLogin for SSO Choose SSO provider (in Atlan) Set up SAML app (in Okta) Download Okta's metadata file (in Okta) Assign users to the app (in Okta) Upload Okta's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-saml-2-0-for-sso",
    "content": "Configure Atlan Integrations Identity Management SSO Get Started How to enable SAML 2.0 for SSO On this page Enable  SAML 2.0 for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your SAML 2.0 administrator to carry out the tasks below in your custom IdP. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in SAML 2.0, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in SAML 2.0, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate SAML 2.0 SSO for Atlan, complete the following steps. Choose SSO provider (in Atlan) ​ To choose SAML 2.0 as your SSO provider, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select SAML 2.0 and then click Configure . For Alias , type in an alias for the SAML 2.0 connection and then click Next . Under Service provider metadata , copy the Atlan SAML Assertion URL and Atlan Audience URI (SP Entity ID) . Set up SAML app (in custom IdP) ​ If you have PingFederate as your IdP, refer to SSO integration with PingFederate using SAML for the SAML assertion URL to use. To set up a SAML app within your custom IdP: Create a new SAML application in your IdP with the name Atlan . For Entity/Issuer ID , enter the Atlan Audience URI (SP Entity ID) value you copied from above. For Assertion Consumer Service (ACS) URL , enter the Atlan SAML Assertion URL value you copied from above. Add the required users and groups to the application. Configure the IdP to return the following attributes in the SAML response: firstName lastName email memberOf (listing the user's group memberships, which will be required for group mapping in Atlan) Save the SAML metadata XML file for the SSO URL and X.509 public certificate file of the IdP. danger The SSO URL must be accessible from Atlan via an internet connection. Configure IdP details (in Atlan) ​ To complete the configuration of SAML 2.0 SSO, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select SAML 2.0 and then click Configure . For Alias , type in an alias for the SAML 2.0 connection and then click Next . To the right of Identity provider metadata, click the Import from XML button. Select the XML file saved from the IdP above. For Attribute Mapper , modify the IdP attribute names for email, first name, and last name if these will be different in the IdP SAML response. (Optional) For Customize , under Sign in button text , type any custom message you'd like your users to see on the Atlan login screen. At the bottom of the screen, click Save . Congratulations   -  you have successfully set up SSO for your custom IdP in Atlan! 🎉 Did you know? By default, users can now log into Atlan with either SAML 2.0 SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your SAML 2.0 administrator to provision access to users through your custom IdP and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings ​ danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from your custom IdP to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan. To automatically assign SSO users to Atlan groups based on their custom IdP groups, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in your custom IdP to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each custom IdP group with access to Atlan. Click the Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! 🎉 Did you know? Once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable OneLogin for SSO Next Authenticate SSO credentials to query data Choose SSO provider (in Atlan) Set up SAML app (in custom IdP) Configure IdP details (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/limit-sso-automatically-creating-users-when-they-log-in",
    "content": "Configure Atlan Integrations Identity Management SSO Guides Limit SSO automatically creating users when they log in Limit SSO automatically creating users when they log in Only users in the SSO provider's application configuration for Atlan can log in via SSO and have their user profiles created automatically in Atlan. To restrict access to certain users, edit the list of users configured for Atlan in your SSO provider to a limited set of users. Each time a new user needs to be onboarded, they will need to be added to this list in your SSO provider before they can access Atlan. Without being added to this list, no user in Atlan will be automatically created even if they attempt to log in. Tags: atlan documentation Previous Authenticate SSO credentials to view sample data Next Set default user roles for SSO"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/enable-onelogin-for-sso",
    "content": "Configure Atlan Integrations Identity Management SSO Get Started How to enable OneLogin for SSO On this page Enable  OneLogin for SSO Who can do this? You will need to be an admin user within Atlan to configure SSO. You will also need to work with your OneLogin administrator to carry out the tasks below in OneLogin. danger SSO group mappings are triggered every time a user authenticates in Atlan. A user may need to log out and then log into Atlan again to view the changes. If a user is added to a new group or removed from an existing one in OneLogin, the updates will also be reflected in Atlan. To ensure that the sync is successful, the groups that the user belongs to should be mapped in Atlan, and if a group name has changed in OneLogin, you will need to update the group name in Atlan as well. For any questions about group mapping sync, head over here . To integrate OneLogin SSO for Atlan, complete the following steps. Choose SSO provider (in Atlan) ​ To choose OneLogin as your SSO provider, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select OneLogin and then click Configure . Under Service provider metadata , copy the Audience (EntityID) , Recipient , ACS (Consumer) URL Validator , and ACS (Consumer) URL . Set up SAML application (in OneLogin) ​ To set up a SAML application, within OneLogin admin console: From the menu along the top, navigate to Applications and then click on Applications . In the upper right, click the Add App button. In the search box, enter saml custom and then click SAML Custom Connector (Advanced) . Under Display Name enter a name for your app, such as Atlan and then click the Save button. Change to the Configuration tab and under Application details enter your Atlan SAML settings: For Audience (EntityID) enter the value you copied from Atlan above. For Recipient enter the value you copied from Atlan above. For ACS (Consumer) URL Validator enter the value you copied from Atlan above. For ACS (Consumer) URL enter the value you copied from Atlan above. For Login URL enter the same value used for the fields above. Change to the SSO tab and change the following: For SAML Signature Algorithm set SHA-512 . Under Login Hint ensure Enable login hint is checked. Change to the Parameters tab and use the circular + icon to add mappings for the following: email   - > Email firstName   - > First Name lastName   - > Last Name In the upper right, click the Save button. Download OneLogin's metadata file (in OneLogin) ​ To download the metadata file for the application, within OneLogin: From the application page, in the upper right navigate to More Actions and click SAML Metadata . (Optional) Map groups to the app (in OneLogin) ​ To map OneLogin groups to the app, within the OneLogin application: In the top left, click the Users tab, and from the dropdown, select Mappings . Under Mappings , click New Mapping to create a new group mapping for Atlan. In the New Mapping dialog, enter the following details: For Name , enter a meaningful name for your group mapping   -  for example, SSOGroupA . Under Conditions , click the + button and enter the following details: From the attributes dropdown, select Group to map all your OneLogin groups to Atlan. From the operators dropdown, select is . From the values dropdown, select the group name. Under Actions , enter the following details: From the Set role dropdown, select Set memberOf . This is required if you want to retain group membership in Atlan. From the Set memberOf to dropdown, enter the group name. Click Save to confirm your selections. Under Mappings , click New Mapping to remove any group mappings if none are selected. In the New Mapping dialog, enter the following details: For Name , enter a meaningful name for your group mapping   -  for example, clearMemberOf . Under Conditions , click the + button and enter the following details: From the attributes dropdown, select Group . From the operators dropdown, select is . From the values dropdown, keep the default selection None . Under Actions , enter the following details: From the Set role dropdown, select Set memberOf . From the Set memberOf to dropdown, leave as blank. Click Save to confirm your selections. Under Mappings , click the Reapply All Mappings tab, and in the corresponding screen, click Continue to confirm. In the top left, click the Applications tab, and from the dropdown, click Applications . Under Applications , select your SAML application. From the left menu of SAML Custom Connector (Advanced) , click Parameters . In the upper right of the parameters page, click the + button to add a new parameter. In the New Field dialog, enter the following details: For Field name , enter memberOf . For Flags , check the Include in SAML assertion box. Click Save to proceed to the next step. In the corresponding Edit Field memberOf dialog, from the Value dropdown, select MemberOf . Click Save to confirm your selections. If any of your OneLogin users do not belong to any groups, you can either add them to an existing group or create a new one. Once you have configured group mapping in Atlan , they will be able to log in to Atlan and assigned the same permissions as their OneLogin group. Upload OneLogin's metadata file (in Atlan) ​ To complete the configuration of OneLogin SSO, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Under Choose SAML provider , select OneLogin and then click Configure . To the right of Identity provider metadata click the Import from XML button. Select the onelogin_metadata_1234567.xml file downloaded from OneLogin above. At the bottom of the screen, click Save . Congratulations   -  you have successfully set up OneLogin SSO in Atlan! 🎉 Did you know? By default, users can now log into Atlan with either OneLogin SSO or a local Atlan account (via email). To only allow logins via SSO, enable the Enforce SSO option in Atlan. Once SSO is enforced, we recommend asking your OneLogin administrator to provision access to users through OneLogin and not directly from Atlan . When access has been provided, a user will be able to log into Atlan directly and their profile will be generated automatically. (Optional) Configure group mappings ​ danger Before you can configure group mapping, you will first need to create groups in Atlan that correspond to the groups you want to map from OneLogin to Atlan. In addition, you must configure the memberOf attribute and group mapping to retain group membership in Atlan. To automatically assign OneLogin users to Atlan groups based on their OneLogin groups, within Atlan: From the left menu on any screen, navigate to Admin . Under the Workspace heading, click SSO . Change to the Groups Mapping tab. To the right of each Atlan group listed: Under the SSO Groups column, type the name of the corresponding group in OneLogin to map to the Atlan group on that row   -  for example, Data Engineering , Business Analysts , and so on. You will need to provide each OneLogin group with access to Atlan. Click the Save button on that row. As each user signs up to Atlan, they will be automatically assigned groups in Atlan based on these mappings! 🎉 Did you know? Once you've configured group mapping, you can add the mapped groups to a persona or purpose to auto-assign relevant permissions to users as they sign up in Atlan. Tags: integration connectors Previous How to enable Okta for SSO Next How to enable SAML 2.0 for SSO Choose SSO provider (in Atlan) Set up SAML application (in OneLogin) Download OneLogin's metadata file (in OneLogin) (Optional) Map groups to the app (in OneLogin) Upload OneLogin's metadata file (in Atlan) (Optional) Configure group mappings"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/identity-management/sso/how-tos/set-default-user-roles-for-sso",
    "content": "Configure Atlan Integrations Identity Management SSO Guides Set default user roles for SSO Set default user roles for SSO Who can do this? You will need to be an admin user and configure SSO with a provider first. Admins can set default roles for new users joining the Atlan workspace via SSO. Setting the default role to admin, member, or guest will provide the appropriate permissions to users as soon as they log into Atlan. To set a default user role for SSO: In the left menu from any screen in Atlan, click Admin . Under Admin center , click SSO . On the SSO page for your provider, under Default Role , click the dropdown menu. From the dropdown menu, select Guest , Member , or Admin as the default role for your users. Tags: integration connectors Previous Limit SSO automatically creating users when they log in Next SSO integration with PingFederate using SAML"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/how-tos/integrate-jira-data-center",
    "content": "Configure Atlan Integrations Project Management Jira Get Started How to integrate Jira Data Center On this page Integrate Jira Data Center Who can do this? You will need to be an admin in Atlan to configure the Jira Data Center integration. You will also need inputs and approval from an administrator of your Jira workspace. danger If your Jira Data Center instance is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan, or submit a request . (If you are not using the IP allowlist, you can skip this step.) To integrate Jira Data Center and Atlan, follow th ese steps. Configure an incoming app link in Jira Data Center ​ You will need to configure an incoming link with an external application   -  in this case, Atlan. This will allow Atlan to access Jira data, which means that Jira will act as the OAuth provider. Atlan requires the minimum scope of WRITE to create issues in Jira Data Center. However, actual permissions are capped at what the authorizing user can do. For example, if the authorizing user lacks the permission to delete issues or projects, then Atlan will not have the permission to do so even with the WRITE scope. To configure an incoming link for Atlan, from within Jira Data Center: Log in to your Jira instance. Copy the Jira site URL from your browser's address bar and store it in a secure location. If you're viewing the dashboard, the site URL is everything that comes before /secure/Dashboard.jspa . From the top right of your Jira instance, click the settings icon, and from the dropdown, click Applications . In the left menu of the Applications page, under Integrations , click Application links . From the top right of the Application links page, click Create link to create a new application link for Atlan. In the Create link dialog, enter the following details: For Application type , click External application to link to an external application using OAuth 2.0. For Direction , click Incoming to allow Atlan to access data from Jira. In the corresponding Configure an incoming link page, enter the following details: For Name , enter a meaningful name for your application   -  for example, Atlan_integration . Under Application details , for Redirect URL , enter the redirect URL in the following format   - https://${client-domain}.atlan.com/oauth-callback . Under Application permissions , for Permissions , click the dropdown and then click Write . The WRITE scope will allow Atlan to: View projects and issues Create, update, and delete projects and issues Click Save to save your selections. From the corresponding Credentials page, click Copy to copy the Client ID and Client secret and store them in a secure location. Connect Atlan to Jira Data Center ​ Once you have retrieved the Jira instance URL and client ID and client secret from Jira, you can proceed to connecting Atlan to Jira Data Center. danger You must have at least one issue already created in Jira before integrating it with Atlan. To connect Atlan to Jira Data Center, from within Atlan: From the left menu, click Admin . Under Workspace , click Integrations . In the Jira tile, to the right of the Connect button, click the downward arrow and then click Connect with Jira Data Center . A new Jira Data Center window will open and you'll be asked to install the Atlan app and create an application link in Jira. Click Next to proceed. In the corresponding Jira Data Center dialog, for Add credentials , enter the following details: For Instance URL , enter the URL of your Jira instance . For Client ID , enter the client ID you copied in Jira. For Client secret , enter the client secret you copied in Jira. In the OAuth 2.0 Authorization Consent popup, click Allow to complete the connection. Click the Add to Jira button to install the Atlan app in Jira Data Center. Install Atlan app in Jira Data Center ​ Before you can install the Atlan app in Jira Data Center, navigate to the Atlan for Jira Data Center app URL and click Get it now to download the app. To install the Atlan app, from within Jira Data Center: Log in to your Jira instance. Under Administration , from the tabs along the top, click Manage apps . From the left menu under Atlassian Marketplace , click Manage apps . From the upper right of the Manage apps page, click Upload app . In the Upload app dialog, for Upload the .jar or .obr file for a custom or third-party app here. , select the app file you downloaded . Click Upload to complete the installation. Changes to the apps in your instance will affect Jira search index. After you make changes to the app, you'll get the following message in the Administration view: We recommend that you perform a re-index, as configuration changes were made to SECTION by USER at TIME . If you have other changes to make, complete them first so that you don't perform multiple re-indexes . You will need to perform a full re-index for the integration to succeed, follow the steps in the official Jira documentation to do so. Configure integration from Atlan to Jira Data Center ​ To configure the Jira Data Center integration from Atlan, from the Integrations sub-menu: Expand the Jira tile. (You may need to refresh the page before the following options appear.) Under the Configurations tab, for Projects , select the Jira project to use as your default project from the dropdown and click Update . (Optional) At any future time, you can review the Overview tab to see the number of linked issues between Jira Data Center and Atlan. Atlan is now connected to Jira Data Center! 🎉 Did you know? The default project is preselected when creating or linking issues to an asset in Atlan. You can change the project while creating or linking issues as needed. Tags: data integration Previous How to integrate Jira Cloud Next Link your Jira account Configure an incoming app link in Jira Data Center Connect Atlan to Jira Data Center Install Atlan app in Jira Data Center Configure integration from Atlan to Jira Data Center"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/how-tos/integrate-jira-cloud",
    "content": "Configure Atlan Integrations Project Management Jira Get Started How to integrate Jira Cloud On this page Integrate Jira Cloud Who can do this? You will need to be an admin in Atlan to configure the Jira Cloud integration. You will also need inputs and approval from an administrator of your Jira Cloud workspace. To integrate Jira Cloud and Atlan, follow these st eps. Connect Atlan to Jira Cloud ​ danger You must have at least one issue already created in Jira before integrating it with Atlan. This will enable Atlan to detect whether the Atlan app is installed in your Jira workspace for the integration to work. Atlan uses the following scopes for the Jira Cloud integration: read:jira-user -  view user profiles read:jira-work -  view Jira issue data write:jira-work -  create and manage Jira issues manage:jira-configuration -  manage Jira global settings offline_access -  allows the Atlan app to refresh the access token To connect Atlan to Jira Cloud from within Atlan: From the left menu, click Admin . Under Workspace , click Integrations . In the Jira tile, click the Connect button. A new window will open, where you'll be asked to log into your Atlassian account: To login with an email address, enter your email address, click Continue , and then enter your password and click Log in . To login with Google, Microsoft, or Apple, click the appropriate button and follow the instructions. Once you're logged in, you will be asked to allow Atlan to access your Atlassian account. Scroll to the bottom of the window and click the Accept button. If you have access to multiple Jira Cloud sites, for the Connecting to Jira dialog, click the Select environment dropdown to select the Jira environment you want to connect to Atlan and then click Connect . Install Atlan Jira app ​ To install the Atlan Jira app: Open the Atlan Jira app's page in the Atlassian marketplace, through either of these ways: From the Integrations page of Atlan, in the Jira tile, click the Add to Jira button. Directly navigate to the URL: https://marketplace.atlassian.com/apps/1225577/atlan In the upper right of the page, click the Get it now button. At the bottom of the Add to Jira dialog, click the Get it now button. (Optional) Request permission from your Jira Cloud admin ​ If you are not a workspace administrator in Jira, you will be prompted to request permission to install. To request permission to install the integration: Under Want this app? Let your admin know why enter an explanation for installing the app. At the bottom of the form, click the Submit request button. Contact your Jira workspace administrator and ask them to approve the Atlan app. Once approved, you'll get an email from Jira telling you that the Atlan app is installed. Configure integration from Atlan to Jira Cloud ​ To configure the Jira Cloud integration from Atlan, from the Integrations sub-menu: Expand the Jira tile. (You may need to refresh the page before the following options appear.) Under the Configurations tab, you can configure the following: For Projects and issue type , select the Jira projects for which your users are allowed to create tickets in Atlan. For each selected project, next to Issue types , click Edit to select the allowed issue types. (This will default to all projects and issue types, if none are specified.) For Default Project , select the Jira project to use as your default project from the allowed list of projects. Click Update to save your configuration. (Optional) At any future time, you can review the Overview tab to see the number of linked issues between Jira and Atlan. Atlan is now connected to Jira Cloud! 🎉 Did you know? The default project is preselected when creating or linking issues to an asset in Atlan. You can change the project while creating or linking issues as needed. (Optional) Create a webhook for access management workflows ​ If your Atlan admin has enabled the governance workflows and inbox module in your Atlan workspace, you can either register a webhook in the Jira administration console or install it directly in Atlan to allow your requesters to view the latest status of their data access approval or revocation requests for governed assets. This is only required if you: Enable governance workflows . Want to use the access management workflow to grant or revoke data access in a source tool using Jira . In addition to the scopes mentioned here , Atlan uses the following scope to manage Jira webhooks   - manage:jira-webhook . From Atlan ​ You can directly install the webhook in Atlan if you're an admin in both your Atlan and Jira Cloud workspaces. To install a webhook: From the left menu, click Admin . Under Workspace , click Integrations . Expand the Jira tile. For Get updates on data access requests (optional) , click Install . Atlan will automatically install the webhook. From Jira ​ Your Jira admin will need to register the webhook in the Jira administration console if your Atlan admin is not a Jira admin. To register a webhook: Contact Atlan support to provide you with the webhook secret. Log in to the Jira administration console with the Administer Jira global permission . From the top right of the Jira administration console, open the settings menu, and then under _Jira Setting_s, click System . From the left menu, under Advanced , click WebHooks . From the top right of your screen, click the Create a WebHook button. In the webhook creation form, enter the following details: For Name , enter a meaningful name   -  for example, Atlan_webhook . For Status , click Enabled . For URL , copy and paste https://{atlan-domain}.com/api/service/jira/events -  replace atlan-domain with the name of your Atlan tenant. For Secret , enter the webhook secret provided by Atlan support. Note that if an invalid secret is used or this field is kept blank, the webhook configuration will be automatically removed from Atlan once an event is received in Atlan. Under Events , in the text box for Issue related events , paste issue.property[atlan].guid is NOT EMPTY . For Issue , click the checkboxes for updated (jira :issue _updated) and deleted (jira :issue _deleted) . Skip the rest of the fields. Click Create to register your webhook. Tags: integration connectors Previous Jira Next How to integrate Jira Data Center Connect Atlan to Jira Cloud Configure integration from Atlan to Jira Cloud (Optional) Create a webhook for access management workflows"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow/how-tos/integrate-servicenow",
    "content": "Configure Atlan Integrations Project Management ServiceNow Get Started How to integrate ServiceNow On this page Integrate ServiceNow Who can do this? You will need to be an admin in Atlan to configure the ServiceNow integration. You will also need inputs and approval from a System Administrator of your ServiceNow instance with a security_admin role . danger If your ServiceNow instance is behind a VPN or firewall, you must add the Atlan IP to your allowlist. Please raise a support ticket from within Atlan or submit a request . (If you are not using the IP allowlist, you can skip this step.) If your Atlan admin has enabled the governance workflows and inbox module in your Atlan workspace, you can create a ServiceNow integration to allow your users to grant or revoke data access for governed assets in Atlan or any other data source. This is only applicable if you: Enable governance workflows Want to use the data access approval workflow To integrate ServiceNow and Atlan, follow these st eps. Create an OAuth application in ServiceNow ​ You will need to create an OAuth application endpoint for Atlan to access your ServiceNow instance. To create an OAuth application , from within ServiceNow: Log in to your ServiceNow instance as a System Administrator with a security_admin role. From the address bar at the top of your browser window, copy the ServiceNow instance URL   -  for example, https://<instance_name>.service-now.com . This will be required to connect Atlan to your ServiceNow instance . From the top header of your ServiceNow instance, click All . From the dropdown, search for and select System OAuth and then click Application Registry . In the top-right corner of the Application Registries page, click New to create a new OAuth application. In the corresponding screen, for What kind of OAuth application? , click Create an OAuth API endpoint for external clients . In the Application Registries New record form, enter the following details: For Name , enter Atlan OAuth App . For Redirect URL , enter the redirect URL in the following format   - https://<atlan_instance_name>/oauth-callback . Replace <atlan_instance_name> with the name of your Atlan instance. For Logo URL , copy and paste https://assets.atlan.com/assets/atlan-primary-logo-png.png . Click Submit to create the OAuth application in ServiceNow. From the Application Registries page, select the OAuth application you created above. Copy the values for Client ID and Client Secret and store them in a secure location. Connect Atlan to ServiceNow ​ To connect Atlan to ServiceNow, you will need the following: ServiceNow instance URL   -  for example, https://<instance_name>.service-now.com Client ID and client secret of the OAuth application you created in ServiceNow To connect Atlan to ServiceNow, from within Atlan: Log in to your Atlan instance as an admin user. From the left menu, click Admin . Under Workspace , click Integrations . In the ServiceNow tile, click the Connect button. In the corresponding Add to ServiceNow dialog, for ServiceNow URL , enter the URL of your ServiceNow instance -  for example, https://<instance_name>.service-now.com . Click Next to proceed. This step requires the creation of an OAuth application in ServiceNow, follow the steps to do so . If you have already created it, in the Create OAuth app section, for Copy the Client ID and Secret from the new OAuth app and paste below , enter the following: For Client ID , enter the client ID you copied from ServiceNow . For Client Secret , enter the client secret you copied from ServiceNow . Click Next to proceed. In the Commit update set section, click Atlan Update Set.xml to download the update set XML file from Atlan to import and commit in ServiceNow . Configure the Atlan integration in ServiceNow ​ To configure the Atlan integration in ServiceNow, your ServiceNow System Administrator with a security_admin role will need to complete the following two steps: Import and commit the update set XML file downloaded from Atlan to create an Atlan data access catalog and business rule in ServiceNow. Create a new user in ServiceNow for the Atlan integration. Import and commit the update set XML file ​ To import and commit the update set XML file , from within ServiceNow: Log in to your ServiceNow instance as a System Administrator with a security_admin role. From the top header of your ServiceNow instance, click All . From the dropdown, search for and select System Update Sets and then click Retrieved Update Sets . On the Retrieved Update Sets page, under Related Links , click the Import Update Set from XML link. On the Import XML page, to upload the update set XML file downloaded from Atlan : For Step 1: Choose file to upload , click Choose file to upload the Atlan Update Set.xml file. For Step 2: Upload the file , click the Upload button. In the top-left corner of your screen, click the back button to return to the Retrieved Update Sets page. The Atlan update set will appear on the Retrieved Update Set list in a Loaded state. Once the XML file has successfully loaded, select the Atlan Update Set . Click Preview Update Set to preview the update set and address any issues. The update set includes the following: Atlan Data Access catalog   -  Atlan will create data access requests in this catalog. Atlan Business Rule -  this is required for Atlan to receive events from your ServiceNow instance to detect any changes in the status of data access requests created in Atlan and automatically update governance workflow requests. Atlan service role and access control list (ACL) updates   -  the Atlan service account requires a role with write access on the sc_request table to update specific fields such as description , short_description , and more. This operation especially requires the security_admin role to commit the update set from Atlan in ServiceNow. Scripted REST API -  this is initially required to retrieve the username and sys_id of the Atlan user completing the ServiceNow integration. Atlan creates a Scripted REST API /api/snc/oauth_userinfo that returns the username and sys_id for an authenticated user. Once the integration has been completed, Atlan will have the access token required for the integration to continue working. Click Commit Update Set . If the commit action fails, contact Atlan support . Create a new user ​ To create a new user , from within ServiceNow: Log in to your ServiceNow instance as a System Administrator with a security_admin role. From the top header of your ServiceNow instance, click All . From the dropdown, search for and select User Administration and then click Users . In the top-right corner of the Users page, click New to create a new user. In the User New record form, enter the following details: For User ID , enter atlan.service . For First name and Last name , enter Atlan and Service , respectively. Click Submit to create the new user. From the Users page, search for and select the atlan.service user you created. On the User atlan.service page, scroll down to the table at the bottom of the screen. In the table, change to the Roles tab and then click the Edit button. On the Edit Members page, configure the following: In the Collection list, search for and select atlan_service_account_role . Click the greater than icon to add the role to the atlan.service user you created. Click Save to save your role assignment for the new user. In the User atlan.service page, click the Set Password button to create a password for the new user. In the Set Password dialog, click the Generate button to generate a password. Once a password has been generated, click the clipboard icon to copy the value and store it in a secure location. Click Save Password . This password will be required to configure the ServiceNow integration in Atlan . Configure the ServiceNow integration in Atlan ​ To configure the ServiceNow integration in Atlan, from within Atlan: Log in to your Atlan instance as an admin user. From the left menu, click Admin . Under Workspace , click Integrations . Expand the ServiceNow tile. In the Commit update set section, for Password , enter the password you copied from ServiceNow for the new user . Click the Connect and test button to test the ServiceNow integration. This ensures that the update set was committed successfully in ServiceNow and Atlan can receive webhook events. To test the latter, Atlan will create a sample request in ServiceNow, wait for a few seconds to receive a webhook event, and then display the status of the connection as CONNECTED . (Optional) Under the Configurations tab, for Test Connection , click the Run test button to verify that the ServiceNow integration is working properly in Atlan at any time. (Optional) At any future time, you can review the Overview tab to see the number of linked issues between ServiceNow and Atlan. Atlan is now connected to ServiceNow! 🎉 Tags: data integration Previous ServiceNow Next Link your ServiceNow account Create an OAuth application in ServiceNow Connect Atlan to ServiceNow Configure the Atlan integration in ServiceNow Configure the ServiceNow integration in Atlan"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/servicenow/how-tos/link-your-servicenow-account",
    "content": "Configure Atlan Integrations Project Management ServiceNow Get Started Link your ServiceNow account On this page Link your ServiceNow account To request or revoke data access through ServiceNow inside Atlan, you may first need to link your ServiceNow account. This is done automatically for the user that set up the ServiceNow integration , but not for other users. Link your ServiceNow account ​ To link your ServiceNow account: From any screen, in the upper right navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under ServiceNow , click the Connect link. In the resulting popup, scroll to the bottom and click Allow . Unlink your ServiceNow account ​ To unlink your ServiceNow account: From any screen, in the upper right navigate to your name, then click Profile . Click the four dots icon in the resulting dialog to get to integrations. Under ServiceNow , click the Disconnect link. In the confirmation dialog, click Confirm . Tags: data integration Previous How to integrate ServiceNow Next Troubleshooting ServiceNow Link your ServiceNow account Unlink your ServiceNow account"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/how-tos/aws-eks/install-secure-agent-on-aws-eks",
    "content": "Connect data Secure Agent Manage Agent AWS EKS Install on AWS EKS On this page Install on AWS EKS This guide provides step-by-step instructions to install the Secure Agent on an Amazon Elastic Kubernetes Service (AWS EKS) cluster. System requirements ​ To deploy the Secure Agent on AWS EKS, ensure the following system requirements are met: Configure network access between your Secure Agent and Atlan tenant. For more information, see Whitelisting Secure Agent . You need Kubernetes version 1.19 or higher. You need to install Helm and kubectl on the machine you're using to connect to the AWS EKS cluster. You need at least 1 node for base services with a disk space of 20 GB and instance configuration as below: Environment Minimum instance type Recommended instance type Production t3.large Custom based on workload Non-production t3.large t3.xlarge info 💪 Did you know? For optimal autoscaling, scale nodes based on the number of concurrent workflows. Permissions required ​ Before installing the Secure Agent, make sure the following permissions are in place: Permissions for the Installer ​ The user, service or system account performing the installation needs access to the EKS cluster and permissions to manage Custom Resource Definitions (CRDs). Ensure the kubeconfig is correctly configured for your target EKS cluster. If needed, use the following command to configure or update your kubeconfig file. aws eks update-kubeconfig --region ``<region>`` --name ``<cluster-name>`` Replace <region> with your AWS region (for example, us-east-1) and <cluster-name> with the name of your EKS cluster. The installer needs permission to create, update, and delete Custom Resource Definitions (CRDs). If not using the cluster-admin role, grant the following: Create a file named agent-crd-permissions.yaml on your machine. Copy the following content into the file: apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: # Use a descriptive name name: helm-crd-installer-role rules: - apiGroups: [\"apiextensions.k8s.io\"] resources: [\"customresourcedefinitions\"] # Recommended verbs for Helm CRD management verbs: [\"create\", \"get\", \"list\", \"watch\", \"update\", \"patch\", \"delete\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: # Use a descriptive name name: helm-crd-installer-binding subjects: # *** IMPORTANT: Modify this section based on who is running Helm *** # Choose ONE of the following options and replace placeholders. # Option 1: Bind to a specific User - kind: User name: \"your-kubernetes-username\" # Replace with the installing user's K8s username recognized by the cluster apiGroup: rbac.authorization.k8s.io # Option 2: Bind to a specific Group # - kind: Group #   name: \"your-kubernetes-groupname\" # Replace with the installing user's K8s group name #   apiGroup: rbac.authorization.k8s.io # Option 3: Bind to a Service Account (e.g., for CI/CD pipelines) # - kind: ServiceAccount #   name: \"installer-sa-name\" # Replace with the installer SA's name #   namespace: \"installer-sa-namespace\" # Replace with the installer SA's namespace roleRef: # This refers to the ClusterRole created above kind: ClusterRole name: helm-crd-installer-role apiGroup: rbac.authorization.k8s.io Follow the comments in the file to replace the placeholders. In the above file: Resource: customresourcedefinitions - needed for managing CRDs in the cluster. API Group: apiextensions.k8s.io - required to work with CRDs. Verbs: create, get, list, update, delete - necessary for installing, inspecting, updating, and cleaning up CRDs using Helm. ClusterRoleBinding: needed to assign the role to the user or group performing the installation. Once you’ve updated the placeholders, use the below kubectl command to apply the configuration: Once you’ve updated the placeholders, use the below kubectl command to apply the configuration: kubectl apply -f agent-crd-permissions.yaml Permissions for the Secure Agent Pod (Runtime) ​ The Secure Agent runs as pods in your EKS cluster and requires permissions to interact with AWS services like S3. These permissions are granted through IAM Roles for Service Accounts (IRSA). Create a new IAM role for the Secure Agent pod. Configure the trust policy to enable the Secure Agent’s Kubernetes service account to assume the role. Make sure the argo-workflow service account exists in the same namespace where you plan to install the agent. For more information, see the AWS documentation on IAM roles for service accounts (IRSA) . Example: Trust policy for the argo-workflow service account: \"Condition\": { \"StringEquals\": { \":sub\": \"system:serviceaccount::argo-workflow\" } } Replace <namespace> with the namespace where you plan to install agent. Create an S3 bucket (or use an existing one), and attach the following permissions to the IAM role used by the Secure Agent: s3:PutObject : Needed to write logs and artifacts s3:GetObject : Needed to read logs and artifacts. s3:ListBucket : Needed by Argo artifact repository for listing objects. Did you know? The Helm chart automatically configures the necessary Kubernetes RBAC for Argo Workflows, which the Secure Agent uses. No additional configuration is required for the Secure Agent pod.. Prerequisites ​ Before proceeding, complete the following setup steps to prepare your Atlan tenant and AWS EKS cluster. Configure Atlan tenant ​ In your Atlan tenant: Sign in as an Atlan admin. Go to Admin from the left menu. Under Workspace , click Labs . Navigate to Workflow Center . Enable the Crawl assets using Secure Agent toggle. Configure Secure Agent settings ​ The agent_config_values.yaml file is used to configure the Secure Agent, Argo Workflows, and storage for the AWS EKS cluster. Follow these instructions on the machine where you're performing the installation. Create a file named agent_config_values.yaml file. Copy the configuration below into the file: # ----------------------------------------------------------------------------------------- # Agent core settings   -  Follow the comments to update: # 1. Image registry settings - To be updated only if you are using a private image registry # 2. Atlan connection settings - To be updated only if you want agent to use the S3 bucket # 3. Argo Private repository settings - To be updated only if you are using private repository for Argo workflows # 4. Kubernetes Pod Annotation settings - To be updated only if you want to customize the Kubernetes pod’s metadata # 5. Argo Private repository settings - To be updated only if you are using private repository for Argo workflows # 6. S3 storage settings - To be updated with S3 bucket details. # ----------------------------------------------------------------------------------------- agent: enabled: true enableStorageProxy: false ca: crt: \"\" #Provide a base64-encoded string of a JSON object, e.g., {\"client_id\": 123, \"client_secret\": 1243}. #Set this only if you need to include custom headers in API calls made by the agent. restAPIHeaders: \"\" versions: k3s: \"\" k8s: \"\" helm: \"\" # 1. Image Registry Settings image: # Only update if you're using a private image registry registry: \"public.ecr.aws\" repository: \"atlanhq\" # Only update if you're using custom images restImageName: \"rest-2\" restImageTag: \"1.0\" # Only update if you're using custom images jdbcImageName: \"jdbc-metadata-extractor-with-jars\" jdbcImageTag: \"1.0\" # Only update if you're using custom images credentialImageName: \"connector-auth\" credentialImageTag: \"1.0\" # Only update if you're using custom images csaScriptsImageName: \"marketplace-csa-scripts\" csaScriptsImageTag: \"1.0\" # Marketplace scripts image details - keep these values as is unless using custom images marketplaceScriptsImageName: \"marketplace-scripts-agent\" marketplaceScriptsImageTag: \"1.0\" pullPolicy: IfNotPresent pullSecrets: []  # Add pull secrets if using private registry annotations: {} labels: {} serviceAccountName: \"\" automountServiceAccountToken: true resources: {} # 2. Atlan connection settings - Only update if you want to agent to use the S3 bucket atlan: argoToken: \"\" vaultEnvEnabled: false # Set to true only if the agent should store metadata # in your bucket instead of sending it to Atlan via presigned URL. useAgentBucket: false metadataBucket: \"\" persistentVolume: scripts: enabled: false data: enabled: false minio: enabled: false argo-workflows: images: pullPolicy: IfNotPresent pullSecrets: [] crds: install: true keep: true annotations: {} singleNamespace: true workflow: serviceAccount: create: true rbac: create: true controller: # 3. Argo Private repository settings - Only update if you are using a private image repository for Argo image: # update the private image repository details registry: quay.io repository: argoproj/workflow-controller tag: \"\" parallelism: 10 resourceRateLimit: limit: 10 burst: 5 rbac: create: true secretWhitelist: [] accessAllSecrets: false writeConfigMaps: false configMap: create: true name: \"\" namespaceParallelism: 10 workflowDefaults: # 4. Kubernetes Pod Annotation settings - Only update if you want to customize the Pod metadata. ## For example, the annotation might be used by external systems such as proxies, or monitoring tools, and more. spec: podMetadata: annotations: argo.workflow/agent-type: \"atlan-agent-service\" labels: app.kubernetes.io/name: \"atlan-agent\" podGC: strategy: OnPodSuccess serviceAccountName: argo-workflow automountServiceAccountToken: true ttlStrategy: secondsAfterCompletion: 84600 templateDefaults: container: securityContext: allowPrivilegeEscalation: false resources: {} env: - name: CA_CERT valueFrom: configMapKeyRef: name: cert-config key: ca.crt optional: true - name: REST_API_HEADERS valueFrom: configMapKeyRef: name: agent-registry-settings key: restAPIHeaders optional: true serviceAccount: create: true name: workflow-controller workflowNamespaces: - default replicas: 1 revisionHistoryLimit: 10 nodeEvents: enabled: false server: enabled: true # 5. Argo Private repository settings - Only update if you are using a private image repository for Argo image: registry: quay.io repository: argoproj/argocli tag: \"\" rbac: create: true serviceAccount: create: true replicas: 1 autoscaling: enabled: false ingress: enabled: false annotations: ingress.kubernetes.io/ssl-redirect: \"false\" resources: {} executor: securityContext: {} resources: {} artifactRepository: archiveLogs: true useStaticCredentials: false # 6. S3 bucket settings - needed by the secure agent to store logs and artifacts s3: # S3 bucket name - Update with the bucket name you created in the Permissions required section. bucket: \"atlan--bucket\" # S3 endpoint endpoint: \"s3.us-east-2.amazonaws.com\" # AWS region - Update with the region where you created bucket in the Permissions required section. region: \"us-east-2\" # Artifact path format keyFormat: \"argo-artifacts/{{workflow.namespace}}/{{workflow.name}}/{{pod.name}}\" # Whether to use insecure connections insecure: false # Use AWS SDK credentials (IAM role) useSDKCreds: true In the configuration file, follow the comments to replace the necessary attributes. You may want to update the below configurations if: You are using a private image registry (Image registry settings) You want the agent to use an S3 bucket (Atlan connection settings) You are using a private repository for Argo workflows (Argo Private repository settings) You want to customize the Kubernetes pod's metadata (Kubernetes Pod Annotation settings) You need specific S3 storage configuration (S3 storage settings) Install using Helm chart ​ Follow these steps to install the Secure Agent and its dependencies into your AWS EKS cluster using Helm charts. Install the Argo Custom Resource Definitions (CRDs) required by the Secure Agent. This step installs only the CRDs. The Secure Agent is installed in the subsequent step using a Helm upgrade. helm install <helm-app-name> oci://registry-1.docker.io/atlanhq/workflow-offline-agent \\ --version 0.1.0 \\ -n <namespace> \\ --create-namespace -f <path/to/agent_config_values.yaml> \\ --set agent.name=\"<secure-agent-name>\" \\ --set agent.atlan.domain=\"<atlan-tenant-domain>\" \\ --set agent.atlan.token=\"<atlan-api-token>\" \\ --set argo-workflows.controller.workflowNamespaces={<namespace>} \\ --set IsUpgrade=false Replace the placeholders: <namespace> : The Kubernetes namespace where you want to deploy the Secure Agent. <path/to/agent_config_values.yaml> : The path to the YAML config file. <secure-agent-name> : Unique name, like agent-us-east-cdw. <helm-app-name> : Unique Helm release name, like atlan-agent-v1. <atlan-tenant-domain> : Your Atlan domain, e.g., mycompany.atlan.com. <atlan-api-token> : Token used for authentication. See Create a bearer token . Use the following kubectl command to associate the IAM role with the service account. This enables the Secure Agent to access the S3 bucket securely using IAM Roles for Service Accounts (IRSA). Make sure the IAM role’s trust policy enables the argo-workflow service account to assume the role. kubectl annotate serviceaccount argo-workflow \\ -n  \\ eks.amazonaws.com/role-arn=arn:aws:iam:::role/ Replace the placeholders: <namespace : The Kubernetes namespace where you want to deploy the Secure Agent. <AWS_ACCOUNT_ID> : Your AWS Account ID. <YourAgentIAMRoleName> : The IAM role name you created for the Secure Agent using IRSA. Install the Secure Agent by upgrading the Helm release. This step performs the actual Secure Agent installation after CRDs are in place. helm upgrade <helm-app-name> oci://registry-1.docker.io/atlanhq/workflow-offline-agent \\ --version 0.1.0 \\ -n <namespace> \\ --create-namespace -f <path/to/agent_config_values.yaml> \\ --set agent.name=\"<secure-agent-name>\" \\ --set agent.atlan.domain=\"<atlan-tenant-domain>\" \\ --set agent.atlan.token=\"<atlan-api-token>\" \\ --set argo-workflows.controller.workflowNamespaces={<namespace>} \\ --set IsUpgrade=true Replace the placeholders: <namespace> : The Kubernetes namespace where you want to deploy the Secure Agent. <path/to/agent_config_values.yaml> : The path to the YAML config file. <secure-agent-name> : Unique name, like agent-us-east-cdw. <helm-app-name> : Unique Helm release name, like atlan-agent-v1. <atlan-tenant-domain> : Your Atlan domain, e.g., mycompany.atlan.com. <atlan-api-token> : Token used for authentication. See Create a bearer token . While the installation is in progress, you can run the following command to verify the progress: kubectl get pods -n <namespace> Replace <namespace> with the Kubernetes namespace used for deployment. Verify installation ​ To confirm successful installation: Sign in to your Atlan tenant as an admin. For example, https://<tenant>.atlan.com . Navigate to the Agent tab. Search for your Secure Agent name. If the agent appears in the list and is marked Active , installation is complete. Tags: security access-control permissions Previous Install on Virtual Machine (K3s) Next Configure workflow execution System requirements Permissions required Prerequisites Install using Helm chart Verify installation"
  },
  {
    "url": "https://docs.atlan.com/product/integrations/project-management/jira/how-tos/link-your-jira-account",
    "content": "Configure Atlan Integrations Project Management Jira Get Started Link your Jira account On this page Link your Jira account To create and link Jira issues inside Atlan, you may first need to link your Jira account. This is done automatically for the admin user that set up the Jira integration , but not for other users. An Atlan admin must set up the tenant-level Jira integration in Atlan before any other user can perform a user-level integration. This is because the Atlan app installation requires inputs and approval from an administrator of your Jira workspace. Once the Jira integration has been completed, you can link your Jira account to Atlan without requiring any additional permissions from your Atlan or Jira admin. Link your Jira account ​ To link your Jira account: From any screen, in the upper right navigate to your name, then click Profile . Click the icon at the bottom of the resulting dialog to get to integrations. Under Jira click the Connect link. In the resulting popup, log in if necessary, then scroll to the bottom and click Allow . Unlink your Jira account ​ To unlink your Jira account: From any screen, in the upper right navigate to your name, then click Profile . Click the icon at the bottom of the resulting dialog to get to integrations. Under Jira click the Disconnect link. In the confirmation dialog, click Confirm . Tags: integration connectors Previous How to integrate Jira Data Center Next Troubleshooting Jira Link your Jira account Unlink your Jira account"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/how-tos/configure-secure-agent-for-workflow-execution",
    "content": "Connect data Secure Agent Manage Agent Configure workflow execution On this page Configure workflow execution When using Secure Agent for extraction, source system credentials (secrets) required for workflow execution are stored in a Secret Manager. This guide provides steps to set up workflows with Secure Agent and specify the secret details it uses during workflow execution. Before you begin ​ Before configuring Secure Agent for workflow execution, ensure you have: A registered and active Secure Agent. Access to one of the supported secret stores: AWS Secrets Manager, Azure Key Vault, GCP Secret Manager, environment variable-based secret injection technique, or a custom secret store. Configure secrets retrieval for workflow execution ​ Follow these steps to configure Secure Agent to retrieve secrets from a secret store required for the workflow execution. This is necessary for secure data access while running your workflows. 💪 Did you know? For each field, you can enter either the name of a secret stored in your secret manager or the actual value. Use secret names when using a secret store with Secure Agent, or enter values directly if no secret is required. AWS Azure GCP Environment variables Custom store Secure Agent retrieves the required secrets from AWS Secrets Manager during workflow execution. Follow these steps to configure retrieval under the Secure Agent configuration section: Secret path in Secret Manager: Provide the Amazon Resource Name (ARN) or the path of the secret that contains the sensitive configuration details required for the connector. These details may include credentials such as username, password, or other sensitive information needed by the Secure Agent to securely access data during workflow execution. AWS region: Select the region where your AWS Secrets Manager is located. AWS authentication method: Select how you want the Secure Agent to authenticate when executing the workflow. Choose one: IAM (Recommended) : Use this method if the secure agent was configured to use the AWS IAM permissions to access secrets. IAM Assume Role : Use this method if the agent was configured to access secrets via cross-account roles. AWS Assume Role ARN : Provide the IAM Role ARN that grants the Secure Agent permission to retrieve secrets. Access Key & Secret Key : Use this method if the agent was configured to use the AWS Access Key ID and Secret Access Key via environment variables or Kubernetes secrets. Secure Agent retrieves secrets from Azure Key Vault during workflow execution. Follow these steps to configure retrieval under the Secure Agent configuration section: Secret path in Secret Manager: Provide the URL of the Azure Key Vault secret that contains the sensitive configuration details required for the connector. These details may include credentials such as username, password, or other sensitive information needed by the Secure Agent to securely access data during workflow execution. Azure authentication method: Select how you want the Secure Agent to authenticate when accessing the Azure Key Vault secret. Choose one: Managed Identity (Recommended) : Use this method if the agent was configured to use an Azure-managed identity assigned to the agent environment for authentication. Service Principal Authentication : Use this method if the agent was configured to authenticate via a Service Principal using Tenant ID, Client ID, and Client Secret. Azure Key Vault Name: Provide the name of your Azure Key Vault that stores your secrets. Secure Agent retrieves secrets from GCP Secret Manager during workflow execution. The secret is uniquely identified by its name in GCP Secret Manager, without requiring additional attributes. Secure Agent retrieves secrets from environment variables during workflow execution. Secure Agent retrieves secrets from Custom Secret Store during workflow execution. Follow these steps to configure retrieval under the Secure Agent configuration section: Agent Custom configuration: Secure agent needs information for connecting to the custom secret store. Add the configuration details in JSON format to specify the connection settings and the secrets to retrieve during workflow execution. For example, the JSON configuration to initiate a sample custom store may look like below: { \"store_url\" : \"https://custom-secret-store.example.com\" , \"secret_name\" : \"my-custom-secret\" } Next steps ​ After configuring the Secure Agent, return to your connector’s setup guide and continue the workflow setup. Tags: integration connectors workflow automation orchestration Previous Install on AWS EKS Next Deployment architecture Before you begin Configure secrets retrieval for workflow execution Next steps"
  },
  {
    "url": "https://docs.atlan.com/secure-agent/how-tos/k3s/install-secure-agent-on-virtual-machine-k3s",
    "content": "Connect data Secure Agent Manage Agent K3s Install on Virtual Machine (K3s) On this page Install on Virtual Machine (K3s) Did you know? Secure Agent installation can be done by a non-root user. Root access is only needed for setting up system prerequisites before installation. This page provides instructions for installing the Secure Agent on a virtual machine (VM) by deploying K3s in a rootless execution mode . System requirements ​ Before installing the Secure Agent, ensure that the virtual machine (VM) meets the following requirements: At least 80GB of available disk space. A Linux-based OS running on an amd64 (x86_64) architecture with systemd enabled. The Secure Agent requires the following ports for internal services. Ensure these ports are open and accessible: Kubernetes API: 6443 Internal K3s proxy: 10443 , 10080 MinIO storage: 9000 , 32075 MinIO console: 9001 , 30614 Traefik ingress: 31037 , 32547 Prerequisites ​ Before installing the Secure Agent, complete the following setup steps to prepare your Atlan tenant and virtual machine. Configure Atlan tenant ​ In Atlan, complete the following steps to configure the tenant: Sign in to your tenant as an Atlan admin. From the left menu of any screen, click Admin . Under Workspace click Labs . Navigate to Workflow Center . Enable the Crawl assets using Secure Agent toggle. Configure virtual machine ​ On the virtual machine, complete the following steps to configure it: Log in as a root user. Create the required directory to configure cgroup delegation with: sudo mkdir -p /etc/systemd/system/ [email protected] Use the below cat command to create the delegation file with required configuration: cat <<EOF | sudo tee /etc/systemd/system/ [email protected] /delegate.conf [Service] Delegate=cpu cpuset io memory pids EOF Use the below command to reload systemd: sudo systemctl daemon-reload && sudo reboot To keep the Secure Agent running after logout, the root user must enable service persistence for the user installing it by running the following command: sudo loginctl enable-linger ``<user_installing_secure_agent>`` Replace <user_installing_secure_agent> with the actual username of the user installing the Secure Agent. Run the following commands to enable IP forwarding so Secure Agent can communicate with other Secure Agent instances and make network requests to the Atlan tenant. IPv4 forwarding: sudo sysctl -w net.ipv4.ip_forward=1 IPv6 forwarding: sudo sysctl -w net.ipv6.conf.all.forwarding=1 To manage containerized workloads, install fuse-overlayfs with: sudo yum install fuse-overlayfs The VM must have access to the source system’s secret manager to retrieve secrets. For more information, see how to provide access for some popular secret managers listed below: AWS: Configure access for AWS Secrets Manager. Azure: Configure access for Azure Key Vault. GCP: Configure access for GCP Secret Manager. Permissions required ​ Before installing the Secure Agent, the user must have the following permissions: Create and modify directories in the user’s home directory: ~/.config/systemd/user , ~/bin , ~/.local/bin , and ~/.rancher . Create and write log files. Execute standard Linux commands: mkdir , chmod , tar , and sed . Download Agent packages ​ Follow these steps to download the necessary packages for setting up the Secure Agent. Did you know? The steps require Internet access to download files. In case the VM has no Internet connectivity, one can download them separately and copy the files to the VM. Create a folder for deployment and navigate to it: mkdir -p atlan-secure-agent && cd atlan-secure-agent Run the following commands to download the required packages: Download the Kubernetes install package , which contains files to run K3s on an air-gapped VM: curl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/k3s_offline_package_main.tar Download the Container images package if an image registry isn't available: curl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/atlan_images_main.tar Download the Secure Agent install package , which contains files for running the Secure Agent: curl -O https://atlan-public.s3.amazonaws.com/workflow-offline-agent/container/atlan_install_config_main.tar.gz Verify that all the files are downloaded. Install Secure Agent ​ Follow these steps to install and configure the Secure Agent on the virtual machine. Did you know? The installation can be performed by both root (administrative) and non-root (standard) users. Navigate to the deployment folder (if not already): cd atlan-secure-agent Run the following command to extract the Secure Agent install package: tar -xvf atlan_install_config_main.tar.gz The rootless-install folder is extracted from the Secure Agent install package. Run the following command to create an environment file using the env.sample file located in the rootless-install folder: cp ./rootless-install/.env.sample .env Open the .env file and update these variables: VAR_ATLAN_SECURE_AGENT_NAME=prod-atlan-agent-vm VAR_ATLAN_DOMAIN=tenant.atlan.com VAR_ATLAN_TOKEN=<atlan-api-token> VAR_ATLAN_DATA_PATH=</absolute/path/to/atlan-secure-agent> Replace the environment variable values: VAR_ATLAN_SECURE_AGENT_NAME: Specify a meaningful and unique name for the Secure Agent. For example, prod-atlan-agent-vm . VAR_ATLAN_DOMAIN: Enter your Atlan tenant domain. For example, tenant.atlan.com . VAR_ATLAN_TOKEN: Provide the API key (Bearer token). For more information on generating an API key, see Create a bearer token . VAR_ATLAN_DATA_PATH: Specify the path where the atlan-secure-agent directory is located. Run the following command to grant execution permission for the setup script: chmod +x rootless-install/setup.sh The extracted setup.sh file installs the Secure Agent and K3s. Run the following command to execute the installer: ./rootless-install/setup.sh .env While the installation is in progress, you can run the following command to verify the progress: kubectl get pods -A Verify installation ​ After installing the Secure Agent, verify that it's running correctly. You can check its status through the Atlan UI or by accessing the Agent UI on K3s. Log in as an Atlan admin or a similar role to access your tenant. For example: https://<tenant>.atlan.com . Navigate to the Agent tab. In the Secure Agents list, use the Search for agents box to enter your Secure Agent name. If the agent appears in the list and is marked Active , installation is complete. Troubleshooting ​ If you encounter issues during installation, follow these steps: Check the logs using the following command for detailed error messages that may indicate the root cause: tail -f logs/k3s.log For K3s rootless mode issues, follow the K3s official documentation for troubleshooting rootless issues. If you continue to face issues, contact Atlan support by creating a ticket . Tags: atlan documentation Previous Secure Agent Next Install on AWS EKS System requirements Prerequisites Permissions required Download Agent packages Install Secure Agent Verify installation Troubleshooting"
  }
]